2024-08-12 17:03:02,613 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-12 17:03:02,655 DEBUG OpenAI client created
2024-08-12 17:03:07,723 DEBUG Querying model with image: gpt-4o
2024-08-12 17:03:58,208 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-12 17:03:58,235 DEBUG OpenAI client created
2024-08-12 17:04:03,398 DEBUG Querying model with image: gpt-4o
2024-08-12 17:04:12,617 DEBUG Query with image successful
2024-08-12 17:04:12,696 DEBUG Querying model with image: gpt-4o
2024-08-12 17:04:22,896 DEBUG Query with image successful
2024-08-12 17:04:23,183 DEBUG Querying model with image: gpt-4o
2024-08-12 17:04:32,650 DEBUG Query with image successful
2024-08-12 17:04:32,742 DEBUG Querying model with image: gpt-4o
2024-08-12 17:04:42,076 DEBUG Query with image successful
2024-08-12 17:04:52,348 DEBUG Querying model with image: gpt-4o
2024-08-12 17:05:03,375 DEBUG Query with image successful
2024-08-12 17:05:03,457 DEBUG Querying model with image: gpt-4o
2024-08-12 17:05:12,798 DEBUG Query with image successful
2024-08-12 17:05:13,068 DEBUG Querying model with image: gpt-4o
2024-08-12 17:05:23,522 DEBUG Query with image successful
2024-08-12 17:05:23,602 DEBUG Querying model with image: gpt-4o
2024-08-12 17:05:39,327 DEBUG Query with image successful
2024-08-12 17:05:39,638 DEBUG Querying model with image: gpt-4o
2024-08-12 17:05:50,695 DEBUG Query with image successful
2024-08-12 17:05:50,774 DEBUG Querying model with image: gpt-4o
2024-08-12 17:06:01,878 DEBUG Query with image successful
2024-08-12 17:06:02,159 DEBUG Querying model with image: gpt-4o
2024-08-12 17:06:17,805 DEBUG Query with image successful
2024-08-12 17:06:17,891 DEBUG Querying model with image: gpt-4o
2024-08-12 17:06:32,486 DEBUG Query with image successful
2024-08-12 17:06:32,844 DEBUG Querying model with image: gpt-4o
2024-08-12 17:06:43,649 DEBUG Query with image successful
2024-08-12 17:06:43,728 DEBUG Querying model with image: gpt-4o
2024-08-12 17:06:57,066 DEBUG Query with image successful
2024-08-12 17:06:57,384 DEBUG Querying model with image: gpt-4o
2024-08-12 17:07:08,041 DEBUG Query with image successful
2024-08-12 17:07:08,120 DEBUG Querying model with image: gpt-4o
2024-08-12 17:07:18,040 DEBUG Query with image successful
2024-08-12 17:07:18,349 DEBUG Querying model with image: gpt-4o
2024-08-12 17:07:29,238 DEBUG Query with image successful
2024-08-12 17:07:29,330 DEBUG Querying model with image: gpt-4o
2024-08-12 17:07:41,600 DEBUG Query with image successful
2024-08-12 17:07:41,880 DEBUG Querying model with image: gpt-4o
2024-08-12 17:07:54,776 DEBUG Query with image successful
2024-08-12 17:07:54,855 DEBUG Querying model with image: gpt-4o
2024-08-12 17:08:05,402 DEBUG Query with image successful
2024-08-12 17:08:05,723 DEBUG Querying model with image: gpt-4o
2024-08-12 17:08:17,358 DEBUG Query with image successful
2024-08-12 17:08:17,440 DEBUG Querying model with image: gpt-4o
2024-08-12 17:08:27,904 DEBUG Query with image successful
2024-08-12 17:08:28,171 DEBUG Querying model with image: gpt-4o
2024-08-12 17:08:44,267 DEBUG Query with image successful
2024-08-12 17:08:44,345 DEBUG Querying model with image: gpt-4o
2024-08-12 17:08:57,622 DEBUG Query with image successful
2024-08-12 17:08:57,900 DEBUG Querying model with image: gpt-4o
2024-08-12 17:09:12,162 DEBUG Query with image successful
2024-08-12 17:09:12,240 DEBUG Querying model with image: gpt-4o
2024-08-12 17:09:23,877 DEBUG Query with image successful
2024-08-12 17:09:24,157 DEBUG Querying model with image: gpt-4o
2024-08-12 17:09:36,582 DEBUG Query with image successful
2024-08-12 17:09:36,663 DEBUG Querying model with image: gpt-4o
2024-08-12 17:09:48,862 DEBUG Query with image successful
2024-08-12 17:09:49,173 DEBUG Querying model with image: gpt-4o
2024-08-12 17:10:00,192 DEBUG Query with image successful
2024-08-12 17:10:00,280 DEBUG Querying model with image: gpt-4o
2024-08-12 17:10:10,292 DEBUG Query with image successful
2024-08-12 17:10:10,575 DEBUG Querying model with image: gpt-4o
2024-08-12 17:10:21,413 DEBUG Query with image successful
2024-08-12 17:10:21,490 DEBUG Querying model with image: gpt-4o
2024-08-12 17:10:31,740 DEBUG Query with image successful
2024-08-12 17:10:32,025 DEBUG Querying model with image: gpt-4o
2024-08-12 17:10:43,527 DEBUG Query with image successful
2024-08-12 17:10:43,609 DEBUG Querying model with image: gpt-4o
2024-08-12 17:10:56,009 DEBUG Query with image successful
2024-08-12 17:10:56,271 DEBUG Querying model with image: gpt-4o
2024-08-12 17:11:11,114 DEBUG Query with image successful
2024-08-12 17:11:11,193 DEBUG Querying model with image: gpt-4o
2024-08-12 17:11:22,372 DEBUG Query with image successful
2024-08-12 17:11:22,656 DEBUG Querying model with image: gpt-4o
2024-08-12 17:11:34,328 DEBUG Query with image successful
2024-08-12 17:11:34,407 DEBUG Querying model with image: gpt-4o
2024-08-12 17:11:42,895 DEBUG Query with image successful
2024-08-12 18:31:28,360 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-12 18:31:28,386 DEBUG OpenAI client created
2024-08-12 18:31:33,534 DEBUG Querying model with image: gpt-4o
2024-08-12 18:31:43,567 DEBUG Query with image successful
2024-08-12 18:39:24,066 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-12 18:39:24,104 DEBUG OpenAI client created
2024-08-12 18:39:29,279 DEBUG Querying model with image: gpt-4o
2024-08-12 18:39:38,469 DEBUG Query with image successful
2024-08-12 18:57:04,048 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-12 18:57:04,074 DEBUG OpenAI client created
2024-08-12 18:57:09,259 DEBUG Querying model with image: gpt-4o
2024-08-12 18:57:19,797 DEBUG Query with image successful
2024-08-12 18:57:19,873 DEBUG Querying model with image: gpt-4o
2024-08-12 18:57:28,590 DEBUG Query with image successful
2024-08-12 18:57:28,986 DEBUG Querying model with image: gpt-4o
2024-08-12 18:57:40,085 DEBUG Query with image successful
2024-08-12 18:57:40,162 DEBUG Querying model with image: gpt-4o
2024-08-12 18:57:50,525 DEBUG Query with image successful
2024-08-12 18:58:00,834 DEBUG Querying model with image: gpt-4o
2024-08-12 18:58:08,752 DEBUG Query with image successful
2024-08-12 18:58:08,829 DEBUG Querying model with image: gpt-4o
2024-08-12 18:58:17,068 DEBUG Query with image successful
2024-08-12 18:58:17,367 DEBUG Querying model with image: gpt-4o
2024-08-12 18:58:24,180 DEBUG Query with image successful
2024-08-12 18:58:24,271 DEBUG Querying model with image: gpt-4o
2024-08-12 18:58:31,680 DEBUG Query with image successful
2024-08-12 18:58:31,972 DEBUG Querying model with image: gpt-4o
2024-08-12 18:58:39,887 DEBUG Query with image successful
2024-08-12 18:58:39,972 DEBUG Querying model with image: gpt-4o
2024-08-12 18:58:48,584 DEBUG Query with image successful
2024-08-12 18:58:48,909 DEBUG Querying model with image: gpt-4o
2024-08-12 18:58:55,793 DEBUG Query with image successful
2024-08-12 18:58:55,873 DEBUG Querying model with image: gpt-4o
2024-08-12 18:59:07,080 DEBUG Query with image successful
2024-08-12 18:59:07,398 DEBUG Querying model with image: gpt-4o
2024-08-12 18:59:17,673 DEBUG Query with image successful
2024-08-12 18:59:17,754 DEBUG Querying model with image: gpt-4o
2024-08-12 18:59:26,822 DEBUG Query with image successful
2024-08-12 18:59:27,123 DEBUG Querying model with image: gpt-4o
2024-08-12 18:59:36,426 DEBUG Query with image successful
2024-08-12 18:59:36,508 DEBUG Querying model with image: gpt-4o
2024-08-12 18:59:46,665 DEBUG Query with image successful
2024-08-12 18:59:46,949 DEBUG Querying model with image: gpt-4o
2024-08-12 18:59:58,083 DEBUG Query with image successful
2024-08-12 18:59:58,175 DEBUG Querying model with image: gpt-4o
2024-08-12 19:00:10,539 DEBUG Query with image successful
2024-08-12 19:00:10,852 DEBUG Querying model with image: gpt-4o
2024-08-12 19:00:21,216 DEBUG Query with image successful
2024-08-12 19:00:21,298 DEBUG Querying model with image: gpt-4o
2024-08-12 19:00:31,726 DEBUG Query with image successful
2024-08-12 19:00:32,016 DEBUG Querying model with image: gpt-4o
2024-08-12 19:00:40,088 DEBUG Query with image successful
2024-08-12 19:00:40,169 DEBUG Querying model with image: gpt-4o
2024-08-12 19:00:49,068 DEBUG Query with image successful
2024-08-12 19:00:49,347 DEBUG Querying model with image: gpt-4o
2024-08-12 19:00:56,132 DEBUG Query with image successful
2024-08-12 19:00:56,214 DEBUG Querying model with image: gpt-4o
2024-08-12 19:01:07,697 DEBUG Query with image successful
2024-08-12 19:01:07,981 DEBUG Querying model with image: gpt-4o
2024-08-12 19:01:16,302 DEBUG Query with image successful
2024-08-12 19:01:16,383 DEBUG Querying model with image: gpt-4o
2024-08-12 19:01:24,442 DEBUG Query with image successful
2024-08-12 19:01:24,718 DEBUG Querying model with image: gpt-4o
2024-08-12 19:01:32,575 DEBUG Query with image successful
2024-08-12 19:01:32,656 DEBUG Querying model with image: gpt-4o
2024-08-12 19:01:42,708 DEBUG Query with image successful
2024-08-12 19:01:42,987 DEBUG Querying model with image: gpt-4o
2024-08-12 19:01:52,003 DEBUG Query with image successful
2024-08-12 19:01:52,083 DEBUG Querying model with image: gpt-4o
2024-08-12 19:02:01,363 DEBUG Query with image successful
2024-08-12 19:02:01,655 DEBUG Querying model with image: gpt-4o
2024-08-12 19:02:09,191 DEBUG Query with image successful
2024-08-12 19:02:09,271 DEBUG Querying model with image: gpt-4o
2024-08-12 19:02:18,717 DEBUG Query with image successful
2024-08-12 19:02:18,998 DEBUG Querying model with image: gpt-4o
2024-08-12 19:02:34,965 DEBUG Query with image successful
2024-08-12 19:02:35,062 DEBUG Querying model with image: gpt-4o
2024-08-12 19:02:50,706 DEBUG Query with image successful
2024-08-12 19:02:51,008 DEBUG Querying model with image: gpt-4o
2024-08-12 19:02:58,603 DEBUG Query with image successful
2024-08-12 19:02:58,683 DEBUG Querying model with image: gpt-4o
2024-08-12 19:03:10,055 DEBUG Query with image successful
2024-08-12 19:03:10,324 DEBUG Querying model with image: gpt-4o
2024-08-12 19:03:19,145 DEBUG Query with image successful
2024-08-12 19:03:19,236 DEBUG Querying model with image: gpt-4o
2024-08-12 19:03:28,126 DEBUG Query with image successful
2024-08-12 19:03:28,413 DEBUG Querying model with image: gpt-4o
2024-08-12 19:03:36,830 DEBUG Query with image successful
2024-08-12 19:03:36,912 DEBUG Querying model with image: gpt-4o
2024-08-12 19:03:46,191 DEBUG Query with image successful
2024-08-12 19:03:46,453 DEBUG Querying model with image: gpt-4o
2024-08-12 19:03:54,712 DEBUG Query with image successful
2024-08-12 19:03:54,792 DEBUG Querying model with image: gpt-4o
2024-08-12 19:04:04,751 DEBUG Query with image successful
2024-08-12 19:04:05,032 DEBUG Querying model with image: gpt-4o
2024-08-12 19:04:13,839 DEBUG Query with image successful
2024-08-12 19:04:13,919 DEBUG Querying model with image: gpt-4o
2024-08-12 19:04:21,981 DEBUG Query with image successful
2024-08-12 19:04:22,255 DEBUG Querying model with image: gpt-4o
2024-08-12 19:04:29,613 DEBUG Query with image successful
2024-08-12 19:04:29,694 DEBUG Querying model with image: gpt-4o
2024-08-12 19:04:39,115 DEBUG Query with image successful
2024-08-12 19:04:39,383 DEBUG Querying model with image: gpt-4o
2024-08-12 19:04:49,149 DEBUG Query with image successful
2024-08-12 19:04:49,234 DEBUG Querying model with image: gpt-4o
2024-08-12 19:04:58,093 DEBUG Query with image successful
2024-08-12 19:04:58,356 DEBUG Querying model with image: gpt-4o
2024-08-12 19:05:06,446 DEBUG Query with image successful
2024-08-12 19:05:06,528 DEBUG Querying model with image: gpt-4o
2024-08-12 19:05:14,108 DEBUG Query with image successful
2024-08-13 08:27:56,959 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 08:27:56,996 DEBUG OpenAI client created
2024-08-13 08:28:02,154 DEBUG Querying model with image: gpt-4o
2024-08-13 08:28:10,809 DEBUG Query with image successful
2024-08-13 08:28:10,892 DEBUG Querying model with image: gpt-4o
2024-08-13 08:28:19,118 DEBUG Query with image successful
2024-08-13 08:28:19,424 DEBUG Querying model with image: gpt-4o
2024-08-13 08:28:26,422 DEBUG Query with image successful
2024-08-13 08:28:26,502 DEBUG Querying model with image: gpt-4o
2024-08-13 08:28:34,703 DEBUG Query with image successful
2024-08-13 08:28:45,016 DEBUG Querying model with image: gpt-4o
2024-08-13 08:28:54,330 DEBUG Query with image successful
2024-08-13 08:28:54,408 DEBUG Querying model with image: gpt-4o
2024-08-13 08:29:03,085 DEBUG Query with image successful
2024-08-13 08:29:03,504 DEBUG Querying model with image: gpt-4o
2024-08-13 08:42:35,314 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 08:42:35,340 DEBUG OpenAI client created
2024-08-13 08:42:40,507 DEBUG Querying model with image: gpt-4o
2024-08-13 08:42:54,255 DEBUG Query with image successful
2024-08-13 08:42:54,333 DEBUG Querying model with image: gpt-4o
2024-08-13 08:43:02,802 DEBUG Query with image successful
2024-08-13 08:43:03,134 DEBUG Querying model with image: gpt-4o
2024-08-13 08:43:12,466 DEBUG Query with image successful
2024-08-13 08:43:12,549 DEBUG Querying model with image: gpt-4o
2024-08-13 08:43:20,717 DEBUG Query with image successful
2024-08-13 08:43:31,015 DEBUG Querying model with image: gpt-4o
2024-08-13 08:43:45,560 DEBUG Query with image successful
2024-08-13 08:43:45,640 DEBUG Querying model with image: gpt-4o
2024-08-13 08:43:55,787 DEBUG Query with image successful
2024-08-13 08:43:56,078 DEBUG Querying model with image: gpt-4o
2024-08-13 08:44:03,366 DEBUG Query with image successful
2024-08-13 08:44:03,458 DEBUG Querying model with image: gpt-4o
2024-08-13 08:44:12,349 DEBUG Query with image successful
2024-08-13 08:44:12,648 DEBUG Querying model with image: gpt-4o
2024-08-13 08:44:21,966 DEBUG Query with image successful
2024-08-13 08:44:22,048 DEBUG Querying model with image: gpt-4o
2024-08-13 08:44:30,590 DEBUG Query with image successful
2024-08-13 08:44:30,896 DEBUG Querying model with image: gpt-4o
2024-08-13 08:44:38,500 DEBUG Query with image successful
2024-08-13 08:44:38,582 DEBUG Querying model with image: gpt-4o
2024-08-13 08:44:45,633 DEBUG Query with image successful
2024-08-13 08:44:45,928 DEBUG Querying model with image: gpt-4o
2024-08-13 08:44:54,295 DEBUG Query with image successful
2024-08-13 08:44:54,376 DEBUG Querying model with image: gpt-4o
2024-08-13 08:45:03,415 DEBUG Query with image successful
2024-08-13 08:45:03,726 DEBUG Querying model with image: gpt-4o
2024-08-13 08:45:12,250 DEBUG Query with image successful
2024-08-13 08:45:12,333 DEBUG Querying model with image: gpt-4o
2024-08-13 08:45:21,927 DEBUG Query with image successful
2024-08-13 08:45:22,230 DEBUG Querying model with image: gpt-4o
2024-08-13 08:45:30,141 DEBUG Query with image successful
2024-08-13 08:45:30,225 DEBUG Querying model with image: gpt-4o
2024-08-13 08:45:41,055 DEBUG Query with image successful
2024-08-13 08:45:41,348 DEBUG Querying model with image: gpt-4o
2024-08-13 08:45:51,008 DEBUG Query with image successful
2024-08-13 08:45:51,089 DEBUG Querying model with image: gpt-4o
2024-08-13 08:46:52,467 DEBUG Query with image successful
2024-08-13 08:46:52,773 DEBUG Querying model with image: gpt-4o
2024-08-13 08:47:00,031 DEBUG Query with image successful
2024-08-13 08:47:00,113 DEBUG Querying model with image: gpt-4o
2024-08-13 08:47:08,667 DEBUG Query with image successful
2024-08-13 08:47:08,978 DEBUG Querying model with image: gpt-4o
2024-08-13 08:47:16,895 DEBUG Query with image successful
2024-08-13 08:47:16,987 DEBUG Querying model with image: gpt-4o
2024-08-13 08:47:25,709 DEBUG Query with image successful
2024-08-13 08:47:25,997 DEBUG Querying model with image: gpt-4o
2024-08-13 08:47:36,532 DEBUG Query with image successful
2024-08-13 08:47:36,621 DEBUG Querying model with image: gpt-4o
2024-08-13 08:47:44,246 DEBUG Query with image successful
2024-08-13 08:47:44,526 DEBUG Querying model with image: gpt-4o
2024-08-13 08:47:52,293 DEBUG Query with image successful
2024-08-13 08:47:52,392 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:00,884 DEBUG Query with image successful
2024-08-13 08:48:01,169 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:10,128 DEBUG Query with image successful
2024-08-13 08:48:10,213 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:17,035 DEBUG Query with image successful
2024-08-13 08:48:17,318 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:24,950 DEBUG Query with image successful
2024-08-13 08:48:25,031 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:32,234 DEBUG Query with image successful
2024-08-13 08:48:32,524 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:40,866 DEBUG Query with image successful
2024-08-13 08:48:40,947 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:48,326 DEBUG Query with image successful
2024-08-13 08:48:48,618 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:56,262 DEBUG Query with image successful
2024-08-13 08:48:56,350 DEBUG Querying model with image: gpt-4o
2024-08-13 08:49:03,091 DEBUG Query with image successful
2024-08-13 08:49:03,376 DEBUG Querying model with image: gpt-4o
2024-08-13 08:49:15,607 DEBUG Query with image successful
2024-08-13 08:49:15,690 DEBUG Querying model with image: gpt-4o
2024-08-13 08:49:24,048 DEBUG Query with image successful
2024-08-13 08:49:24,324 DEBUG Querying model with image: gpt-4o
2024-08-13 08:49:32,695 DEBUG Query with image successful
2024-08-13 08:49:32,775 DEBUG Querying model with image: gpt-4o
2024-08-13 08:49:49,454 DEBUG Query with image successful
2024-08-13 08:49:49,723 DEBUG Querying model with image: gpt-4o
2024-08-13 08:49:57,688 DEBUG Query with image successful
2024-08-13 08:49:57,768 DEBUG Querying model with image: gpt-4o
2024-08-13 08:50:05,125 DEBUG Query with image successful
2024-08-13 08:50:05,393 DEBUG Querying model with image: gpt-4o
2024-08-13 08:50:13,685 DEBUG Query with image successful
2024-08-13 08:50:13,766 DEBUG Querying model with image: gpt-4o
2024-08-13 08:50:21,569 DEBUG Query with image successful
2024-08-13 08:50:21,836 DEBUG Querying model with image: gpt-4o
2024-08-13 08:50:31,874 DEBUG Query with image successful
2024-08-13 08:50:31,954 DEBUG Querying model with image: gpt-4o
2024-08-13 08:50:40,080 DEBUG Query with image successful
2024-08-13 08:50:40,353 DEBUG Querying model with image: gpt-4o
2024-08-13 08:50:51,914 DEBUG Query with image successful
2024-08-13 08:50:51,998 DEBUG Querying model with image: gpt-4o
2024-08-13 08:51:00,289 DEBUG Query with image successful
2024-08-13 08:51:00,595 DEBUG Querying model with image: gpt-4o
2024-08-13 08:51:14,520 DEBUG Query with image successful
2024-08-13 08:51:14,599 DEBUG Querying model with image: gpt-4o
2024-08-13 08:51:21,457 DEBUG Query with image successful
2024-08-13 09:23:37,253 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 09:23:37,286 DEBUG OpenAI client created
2024-08-13 09:23:42,452 DEBUG Querying model with image: gpt-4o
2024-08-13 09:23:51,442 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 94, in connect_tcp
    sock = socket.create_connection(
  File "D:\ide\conda\envs\vlm_sc2\lib\socket.py", line 845, in create_connection
    raise err
  File "D:\ide\conda\envs\vlm_sc2\lib\socket.py", line 833, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 261, in handle_request
    connect_response = self._connection.handle_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 86, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 63, in handle_request
    stream = self._connect(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 111, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 93, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 972, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 94, in connect_tcp
    sock = socket.create_connection(
  File "D:\ide\conda\envs\vlm_sc2\lib\socket.py", line 845, in create_connection
    raise err
  File "D:\ide\conda\envs\vlm_sc2\lib\socket.py", line 833, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 261, in handle_request
    connect_response = self._connection.handle_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 86, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 63, in handle_request
    stream = self._connect(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 111, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 93, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 972, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 94, in connect_tcp
    sock = socket.create_connection(
  File "D:\ide\conda\envs\vlm_sc2\lib\socket.py", line 845, in create_connection
    raise err
  File "D:\ide\conda\envs\vlm_sc2\lib\socket.py", line 833, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 261, in handle_request
    connect_response = self._connection.handle_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 86, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 63, in handle_request
    stream = self._connect(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 111, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 93, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 972, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 61, in query_with_image
    response = self.client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 936, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 996, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1074, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 996, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1074, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1006, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-08-13 09:23:51,659 DEBUG Querying model with image: gpt-4o
2024-08-13 09:24:22,206 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 09:24:22,255 DEBUG OpenAI client created
2024-08-13 09:24:27,422 DEBUG Querying model with image: gpt-4o
2024-08-13 09:24:36,502 DEBUG Query with image successful
2024-08-13 09:24:36,577 DEBUG Querying model with image: gpt-4o
2024-08-13 09:24:45,561 DEBUG Query with image successful
2024-08-13 09:24:45,872 DEBUG Querying model with image: gpt-4o
2024-08-13 09:24:53,250 DEBUG Query with image successful
2024-08-13 09:24:53,327 DEBUG Querying model with image: gpt-4o
2024-08-13 09:25:01,779 DEBUG Query with image successful
2024-08-13 09:25:12,046 DEBUG Querying model with image: gpt-4o
2024-08-13 09:25:19,968 DEBUG Query with image successful
2024-08-13 09:25:20,045 DEBUG Querying model with image: gpt-4o
2024-08-13 09:25:26,750 DEBUG Query with image successful
2024-08-13 09:25:27,035 DEBUG Querying model with image: gpt-4o
2024-08-13 09:25:33,717 DEBUG Query with image successful
2024-08-13 09:25:33,796 DEBUG Querying model with image: gpt-4o
2024-08-13 09:30:02,650 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 09:30:02,682 DEBUG OpenAI client created
2024-08-13 09:30:07,846 DEBUG Querying model with image: gpt-4o
2024-08-13 09:30:17,508 DEBUG Query with image successful
2024-08-13 09:30:17,585 DEBUG Querying model with image: gpt-4o
2024-08-13 09:30:27,358 DEBUG Query with image successful
2024-08-13 09:30:27,656 DEBUG Querying model with image: gpt-4o
2024-08-13 09:30:36,581 DEBUG Query with image successful
2024-08-13 09:30:36,665 DEBUG Querying model with image: gpt-4o
2024-08-13 09:30:45,913 DEBUG Query with image successful
2024-08-13 09:30:56,185 DEBUG Querying model with image: gpt-4o
2024-08-13 09:31:06,306 DEBUG Query with image successful
2024-08-13 09:31:06,384 DEBUG Querying model with image: gpt-4o
2024-08-13 09:31:13,372 DEBUG Query with image successful
2024-08-13 09:31:13,642 DEBUG Querying model with image: gpt-4o
2024-08-13 09:31:21,093 DEBUG Query with image successful
2024-08-13 09:31:21,174 DEBUG Querying model with image: gpt-4o
2024-08-13 09:31:28,785 DEBUG Query with image successful
2024-08-13 09:31:29,053 DEBUG Querying model with image: gpt-4o
2024-08-13 09:31:38,090 DEBUG Query with image successful
2024-08-13 09:31:38,168 DEBUG Querying model with image: gpt-4o
2024-08-13 09:31:47,894 DEBUG Query with image successful
2024-08-13 09:31:48,236 DEBUG Querying model with image: gpt-4o
2024-08-13 09:31:56,416 DEBUG Query with image successful
2024-08-13 09:31:56,495 DEBUG Querying model with image: gpt-4o
2024-08-13 09:32:04,070 DEBUG Query with image successful
2024-08-13 09:32:04,339 DEBUG Querying model with image: gpt-4o
2024-08-13 09:32:11,564 DEBUG Query with image successful
2024-08-13 09:32:11,644 DEBUG Querying model with image: gpt-4o
2024-08-13 09:32:20,111 DEBUG Query with image successful
2024-08-13 09:32:20,382 DEBUG Querying model with image: gpt-4o
2024-08-13 09:32:28,409 DEBUG Query with image successful
2024-08-13 09:32:28,495 DEBUG Querying model with image: gpt-4o
2024-08-13 09:32:37,221 DEBUG Query with image successful
2024-08-13 09:32:37,503 DEBUG Querying model with image: gpt-4o
2024-08-13 09:32:45,392 DEBUG Query with image successful
2024-08-13 09:32:45,484 DEBUG Querying model with image: gpt-4o
2024-08-13 09:32:53,793 DEBUG Query with image successful
2024-08-13 09:32:54,062 DEBUG Querying model with image: gpt-4o
2024-08-13 09:33:01,455 DEBUG Query with image successful
2024-08-13 09:33:01,533 DEBUG Querying model with image: gpt-4o
2024-08-13 09:33:10,734 DEBUG Query with image successful
2024-08-13 09:33:11,002 DEBUG Querying model with image: gpt-4o
2024-08-13 09:33:18,033 DEBUG Query with image successful
2024-08-13 09:33:18,113 DEBUG Querying model with image: gpt-4o
2024-08-13 09:33:27,422 DEBUG Query with image successful
2024-08-13 09:33:27,687 DEBUG Querying model with image: gpt-4o
2024-08-13 09:33:38,562 DEBUG Query with image successful
2024-08-13 09:33:38,643 DEBUG Querying model with image: gpt-4o
2024-08-13 09:33:45,388 DEBUG Query with image successful
2024-08-13 09:33:45,656 DEBUG Querying model with image: gpt-4o
2024-08-13 09:33:55,773 DEBUG Query with image successful
2024-08-13 09:33:55,853 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:03,034 DEBUG Query with image successful
2024-08-13 09:34:03,298 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:10,463 DEBUG Query with image successful
2024-08-13 09:34:10,541 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:17,898 DEBUG Query with image successful
2024-08-13 09:34:18,165 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:25,744 DEBUG Query with image successful
2024-08-13 09:34:25,822 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:34,537 DEBUG Query with image successful
2024-08-13 09:34:34,875 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:43,312 DEBUG Query with image successful
2024-08-13 09:34:43,391 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:50,635 DEBUG Query with image successful
2024-08-13 09:34:50,909 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:58,691 DEBUG Query with image successful
2024-08-13 09:34:58,782 DEBUG Querying model with image: gpt-4o
2024-08-13 09:35:05,551 DEBUG Query with image successful
2024-08-13 09:35:05,813 DEBUG Querying model with image: gpt-4o
2024-08-13 09:35:15,808 DEBUG Query with image successful
2024-08-13 09:35:15,887 DEBUG Querying model with image: gpt-4o
2024-08-13 09:35:22,854 DEBUG Query with image successful
2024-08-13 09:35:23,116 DEBUG Querying model with image: gpt-4o
2024-08-13 09:35:31,827 DEBUG Query with image successful
2024-08-13 09:35:31,915 DEBUG Querying model with image: gpt-4o
2024-08-13 09:35:41,535 DEBUG Query with image successful
2024-08-13 09:35:41,819 DEBUG Querying model with image: gpt-4o
2024-08-13 09:35:49,231 DEBUG Query with image successful
2024-08-13 09:35:49,309 DEBUG Querying model with image: gpt-4o
2024-08-13 09:35:57,558 DEBUG Query with image successful
2024-08-13 09:35:57,831 DEBUG Querying model with image: gpt-4o
2024-08-13 09:36:04,891 DEBUG Query with image successful
2024-08-13 09:36:04,968 DEBUG Querying model with image: gpt-4o
2024-08-13 09:36:12,636 DEBUG Query with image successful
2024-08-13 09:36:12,922 DEBUG Querying model with image: gpt-4o
2024-08-13 09:36:24,019 DEBUG Query with image successful
2024-08-13 09:36:24,098 DEBUG Querying model with image: gpt-4o
2024-08-13 09:36:34,883 DEBUG Query with image successful
2024-08-13 09:36:35,159 DEBUG Querying model with image: gpt-4o
2024-08-13 09:36:42,788 DEBUG Query with image successful
2024-08-13 09:36:42,865 DEBUG Querying model with image: gpt-4o
2024-08-13 09:36:49,202 DEBUG Query with image successful
2024-08-13 09:36:49,468 DEBUG Querying model with image: gpt-4o
2024-08-13 09:36:56,475 DEBUG Query with image successful
2024-08-13 09:36:56,553 DEBUG Querying model with image: gpt-4o
2024-08-13 09:37:02,872 DEBUG Query with image successful
2024-08-13 09:37:03,132 DEBUG Querying model with image: gpt-4o
2024-08-13 09:37:12,863 DEBUG Query with image successful
2024-08-13 09:37:12,941 DEBUG Querying model with image: gpt-4o
2024-08-13 09:37:20,841 DEBUG Query with image successful
2024-08-13 11:17:50,987 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 11:17:51,016 DEBUG OpenAI client created
2024-08-13 11:17:56,192 DEBUG Querying model with image: gpt-4o
2024-08-13 11:18:05,435 DEBUG Query with image successful
2024-08-13 11:18:05,512 DEBUG Querying model with image: gpt-4o
2024-08-13 11:18:18,377 DEBUG Query with image successful
2024-08-13 11:18:18,634 DEBUG Querying model with image: gpt-4o
2024-08-13 11:18:27,813 DEBUG Query with image successful
2024-08-13 11:18:27,910 DEBUG Querying model with image: gpt-4o
2024-08-13 11:18:35,682 DEBUG Query with image successful
2024-08-13 11:18:46,000 DEBUG Querying model with image: gpt-4o
2024-08-13 11:18:55,052 DEBUG Query with image successful
2024-08-13 11:18:55,141 DEBUG Querying model with image: gpt-4o
2024-08-13 11:19:03,432 DEBUG Query with image successful
2024-08-13 11:19:03,703 DEBUG Querying model with image: gpt-4o
2024-08-13 11:19:11,162 DEBUG Query with image successful
2024-08-13 11:19:11,240 DEBUG Querying model with image: gpt-4o
2024-08-13 11:19:18,191 DEBUG Query with image successful
2024-08-13 11:19:18,461 DEBUG Querying model with image: gpt-4o
2024-08-13 11:19:27,003 DEBUG Query with image successful
2024-08-13 11:19:27,081 DEBUG Querying model with image: gpt-4o
2024-08-13 11:19:38,453 DEBUG Query with image successful
2024-08-13 11:19:38,720 DEBUG Querying model with image: gpt-4o
2024-08-13 11:19:46,498 DEBUG Query with image successful
2024-08-13 11:19:46,578 DEBUG Querying model with image: gpt-4o
2024-08-13 11:19:56,123 DEBUG Query with image successful
2024-08-13 11:19:56,468 DEBUG Querying model with image: gpt-4o
2024-08-13 11:20:06,564 DEBUG Query with image successful
2024-08-13 11:20:06,642 DEBUG Querying model with image: gpt-4o
2024-08-13 11:20:16,050 DEBUG Query with image successful
2024-08-13 11:20:16,336 DEBUG Querying model with image: gpt-4o
2024-08-13 11:20:25,493 DEBUG Query with image successful
2024-08-13 11:20:25,573 DEBUG Querying model with image: gpt-4o
2024-08-13 11:20:33,206 DEBUG Query with image successful
2024-08-13 11:20:33,471 DEBUG Querying model with image: gpt-4o
2024-08-13 11:20:40,801 DEBUG Query with image successful
2024-08-13 11:20:40,910 DEBUG Querying model with image: gpt-4o
2024-08-13 11:20:55,150 DEBUG Query with image successful
2024-08-13 11:20:55,461 DEBUG Querying model with image: gpt-4o
2024-08-13 11:21:05,581 DEBUG Query with image successful
2024-08-13 11:21:05,665 DEBUG Querying model with image: gpt-4o
2024-08-13 11:23:28,443 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 11:23:28,471 DEBUG OpenAI client created
2024-08-13 11:23:33,640 DEBUG Querying model with image: gpt-4o
2024-08-13 11:23:41,661 DEBUG Query with image successful
2024-08-13 11:23:41,745 DEBUG Querying model with image: gpt-4o
2024-08-13 11:23:50,673 DEBUG Query with image successful
2024-08-13 11:23:50,958 DEBUG Querying model with image: gpt-4o
2024-08-13 11:23:59,743 DEBUG Query with image successful
2024-08-13 11:23:59,841 DEBUG Querying model with image: gpt-4o
2024-08-13 11:24:16,496 DEBUG Query with image successful
2024-08-13 11:24:26,765 DEBUG Querying model with image: gpt-4o
2024-08-13 11:24:35,164 DEBUG Query with image successful
2024-08-13 11:24:35,249 DEBUG Querying model with image: gpt-4o
2024-08-13 11:24:41,930 DEBUG Query with image successful
2024-08-13 11:24:42,202 DEBUG Querying model with image: gpt-4o
2024-08-13 11:24:50,777 DEBUG Query with image successful
2024-08-13 11:24:50,862 DEBUG Querying model with image: gpt-4o
2024-08-13 11:24:58,863 DEBUG Query with image successful
2024-08-13 11:24:59,136 DEBUG Querying model with image: gpt-4o
2024-08-13 11:25:06,810 DEBUG Query with image successful
2024-08-13 11:25:06,901 DEBUG Querying model with image: gpt-4o
2024-08-13 11:25:14,303 DEBUG Query with image successful
2024-08-13 11:25:14,666 DEBUG Querying model with image: gpt-4o
2024-08-13 11:25:24,279 DEBUG Query with image successful
2024-08-13 11:25:24,362 DEBUG Querying model with image: gpt-4o
2024-08-13 11:25:32,327 DEBUG Query with image successful
2024-08-13 11:25:32,606 DEBUG Querying model with image: gpt-4o
2024-08-13 11:25:41,305 DEBUG Query with image successful
2024-08-13 11:25:41,391 DEBUG Querying model with image: gpt-4o
2024-08-13 11:25:50,270 DEBUG Query with image successful
2024-08-13 11:25:50,580 DEBUG Querying model with image: gpt-4o
2024-08-13 11:26:00,294 DEBUG Query with image successful
2024-08-13 11:26:00,382 DEBUG Querying model with image: gpt-4o
2024-08-13 11:26:09,668 DEBUG Query with image successful
2024-08-13 11:26:09,938 DEBUG Querying model with image: gpt-4o
2024-08-13 11:26:17,300 DEBUG Query with image successful
2024-08-13 11:26:17,395 DEBUG Querying model with image: gpt-4o
2024-08-13 11:26:27,577 DEBUG Query with image successful
2024-08-13 11:26:27,850 DEBUG Querying model with image: gpt-4o
2024-08-13 11:26:34,971 DEBUG Query with image successful
2024-08-13 11:26:35,055 DEBUG Querying model with image: gpt-4o
2024-08-13 11:26:44,639 DEBUG Query with image successful
2024-08-13 11:26:44,931 DEBUG Querying model with image: gpt-4o
2024-08-13 11:26:52,369 DEBUG Query with image successful
2024-08-13 11:26:52,454 DEBUG Querying model with image: gpt-4o
2024-08-13 11:27:00,419 DEBUG Query with image successful
2024-08-13 11:27:00,692 DEBUG Querying model with image: gpt-4o
2024-08-13 11:27:09,089 DEBUG Query with image successful
2024-08-13 11:27:09,174 DEBUG Querying model with image: gpt-4o
2024-08-13 11:27:18,032 DEBUG Query with image successful
2024-08-13 11:27:18,308 DEBUG Querying model with image: gpt-4o
2024-08-13 11:27:27,152 DEBUG Query with image successful
2024-08-13 11:27:27,236 DEBUG Querying model with image: gpt-4o
2024-08-13 11:27:34,665 DEBUG Query with image successful
2024-08-13 11:27:34,936 DEBUG Querying model with image: gpt-4o
2024-08-13 11:27:42,457 DEBUG Query with image successful
2024-08-13 11:27:42,551 DEBUG Querying model with image: gpt-4o
2024-08-13 11:27:50,231 DEBUG Query with image successful
2024-08-13 11:27:50,531 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:00,610 DEBUG Query with image successful
2024-08-13 11:28:00,697 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:07,072 DEBUG Query with image successful
2024-08-13 11:28:07,354 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:13,933 DEBUG Query with image successful
2024-08-13 11:28:14,023 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:21,257 DEBUG Query with image successful
2024-08-13 11:28:21,544 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:29,099 DEBUG Query with image successful
2024-08-13 11:28:29,182 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:36,371 DEBUG Query with image successful
2024-08-13 11:28:36,653 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:46,539 DEBUG Query with image successful
2024-08-13 11:28:46,623 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:54,776 DEBUG Query with image successful
2024-08-13 11:28:55,047 DEBUG Querying model with image: gpt-4o
2024-08-13 11:29:05,142 DEBUG Query with image successful
2024-08-13 11:29:05,238 DEBUG Querying model with image: gpt-4o
2024-08-13 11:29:11,486 DEBUG Query with image successful
2024-08-13 11:29:11,767 DEBUG Querying model with image: gpt-4o
2024-08-13 11:29:20,231 DEBUG Query with image successful
2024-08-13 11:29:20,315 DEBUG Querying model with image: gpt-4o
2024-08-13 11:29:29,040 DEBUG Query with image successful
2024-08-13 11:29:29,327 DEBUG Querying model with image: gpt-4o
2024-08-13 11:29:36,498 DEBUG Query with image successful
2024-08-13 11:29:36,581 DEBUG Querying model with image: gpt-4o
2024-08-13 11:29:44,919 DEBUG Query with image successful
2024-08-13 11:29:45,187 DEBUG Querying model with image: gpt-4o
2024-08-13 11:29:54,712 DEBUG Query with image successful
2024-08-13 11:29:54,796 DEBUG Querying model with image: gpt-4o
2024-08-13 11:30:01,891 DEBUG Query with image successful
2024-08-13 11:30:02,158 DEBUG Querying model with image: gpt-4o
2024-08-13 11:30:09,608 DEBUG Query with image successful
2024-08-13 11:30:09,692 DEBUG Querying model with image: gpt-4o
2024-08-13 11:30:16,679 DEBUG Query with image successful
2024-08-13 11:30:16,944 DEBUG Querying model with image: gpt-4o
2024-08-13 11:30:27,663 DEBUG Query with image successful
2024-08-13 11:30:27,749 DEBUG Querying model with image: gpt-4o
2024-08-13 11:30:34,123 DEBUG Query with image successful
2024-08-13 11:30:34,417 DEBUG Querying model with image: gpt-4o
2024-08-13 11:30:44,066 DEBUG Query with image successful
2024-08-13 11:30:44,148 DEBUG Querying model with image: gpt-4o
2024-08-13 11:30:50,009 DEBUG Query with image successful
2024-08-13 12:52:24,894 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 12:52:24,920 DEBUG OpenAI client created
2024-08-13 12:52:30,078 DEBUG Querying model with image: gpt-4o
2024-08-13 12:52:39,686 DEBUG Query with image successful
2024-08-13 12:52:39,775 DEBUG Querying model with image: gpt-4o
2024-08-13 12:52:48,916 DEBUG Query with image successful
2024-08-13 12:52:49,187 DEBUG Querying model with image: gpt-4o
2024-08-13 12:52:58,630 DEBUG Query with image successful
2024-08-13 12:52:58,726 DEBUG Querying model with image: gpt-4o
2024-08-13 12:53:08,374 DEBUG Query with image successful
2024-08-13 12:53:18,697 DEBUG Querying model with image: gpt-4o
2024-08-13 12:53:26,825 DEBUG Query with image successful
2024-08-13 12:53:26,919 DEBUG Querying model with image: gpt-4o
2024-08-13 12:53:35,473 DEBUG Query with image successful
2024-08-13 12:53:35,737 DEBUG Querying model with image: gpt-4o
2024-08-13 12:53:48,204 DEBUG Query with image successful
2024-08-13 12:53:48,288 DEBUG Querying model with image: gpt-4o
2024-08-13 12:53:56,594 DEBUG Query with image successful
2024-08-13 12:53:56,867 DEBUG Querying model with image: gpt-4o
2024-08-13 12:54:29,245 DEBUG Query with image successful
2024-08-13 12:54:29,330 DEBUG Querying model with image: gpt-4o
2024-08-13 12:54:38,128 DEBUG Query with image successful
2024-08-13 12:54:38,400 DEBUG Querying model with image: gpt-4o
2024-08-13 12:54:47,430 DEBUG Query with image successful
2024-08-13 12:54:47,519 DEBUG Querying model with image: gpt-4o
2024-08-13 12:55:13,166 DEBUG Query with image successful
2024-08-13 12:55:13,437 DEBUG Querying model with image: gpt-4o
2024-08-13 12:55:21,325 DEBUG Query with image successful
2024-08-13 12:55:21,419 DEBUG Querying model with image: gpt-4o
2024-08-13 12:55:32,723 DEBUG Query with image successful
2024-08-13 12:55:32,997 DEBUG Querying model with image: gpt-4o
2024-08-13 12:55:41,696 DEBUG Query with image successful
2024-08-13 12:55:41,780 DEBUG Querying model with image: gpt-4o
2024-08-13 12:55:52,550 DEBUG Query with image successful
2024-08-13 12:55:52,854 DEBUG Querying model with image: gpt-4o
2024-08-13 12:56:01,421 DEBUG Query with image successful
2024-08-13 12:56:01,509 DEBUG Querying model with image: gpt-4o
2024-08-13 12:56:10,389 DEBUG Query with image successful
2024-08-13 12:56:10,658 DEBUG Querying model with image: gpt-4o
2024-08-13 12:56:56,273 DEBUG Query with image successful
2024-08-13 12:56:56,356 DEBUG Querying model with image: gpt-4o
2024-08-13 12:57:05,453 DEBUG Query with image successful
2024-08-13 12:57:05,774 DEBUG Querying model with image: gpt-4o
2024-08-13 12:57:13,984 DEBUG Query with image successful
2024-08-13 12:57:14,079 DEBUG Querying model with image: gpt-4o
2024-08-13 12:57:23,873 DEBUG Query with image successful
2024-08-13 12:57:24,155 DEBUG Querying model with image: gpt-4o
2024-08-13 12:57:33,153 DEBUG Query with image successful
2024-08-13 12:57:33,237 DEBUG Querying model with image: gpt-4o
2024-08-13 12:57:44,671 DEBUG Query with image successful
2024-08-13 12:57:44,950 DEBUG Querying model with image: gpt-4o
2024-08-13 12:57:52,962 DEBUG Query with image successful
2024-08-13 12:57:53,045 DEBUG Querying model with image: gpt-4o
2024-08-13 12:58:00,358 DEBUG Query with image successful
2024-08-13 12:58:00,628 DEBUG Querying model with image: gpt-4o
2024-08-13 12:58:08,714 DEBUG Query with image successful
2024-08-13 12:58:08,798 DEBUG Querying model with image: gpt-4o
2024-08-13 12:58:19,791 DEBUG Query with image successful
2024-08-13 12:58:20,070 DEBUG Querying model with image: gpt-4o
2024-08-13 12:58:27,894 DEBUG Query with image successful
2024-08-13 12:58:27,994 DEBUG Querying model with image: gpt-4o
2024-08-13 12:58:34,844 DEBUG Query with image successful
2024-08-13 12:58:35,177 DEBUG Querying model with image: gpt-4o
2024-08-13 12:58:42,617 DEBUG Query with image successful
2024-08-13 12:58:42,711 DEBUG Querying model with image: gpt-4o
2024-08-13 12:58:51,766 DEBUG Query with image successful
2024-08-13 12:58:52,044 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:00,045 DEBUG Query with image successful
2024-08-13 12:59:00,140 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:07,184 DEBUG Query with image successful
2024-08-13 12:59:07,470 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:16,099 DEBUG Query with image successful
2024-08-13 12:59:16,188 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:24,297 DEBUG Query with image successful
2024-08-13 12:59:24,569 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:33,942 DEBUG Query with image successful
2024-08-13 12:59:34,040 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:39,886 DEBUG Query with image successful
2024-08-13 12:59:40,153 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:49,400 DEBUG Query with image successful
2024-08-13 12:59:49,483 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:59,259 DEBUG Query with image successful
2024-08-13 12:59:59,575 DEBUG Querying model with image: gpt-4o
2024-08-13 13:00:06,306 DEBUG Query with image successful
2024-08-13 13:00:06,387 DEBUG Querying model with image: gpt-4o
2024-08-13 13:00:44,058 DEBUG Query with image successful
2024-08-13 13:00:44,323 DEBUG Querying model with image: gpt-4o
2024-08-13 13:00:52,920 DEBUG Query with image successful
2024-08-13 13:00:53,006 DEBUG Querying model with image: gpt-4o
2024-08-13 13:01:00,377 DEBUG Query with image successful
2024-08-13 13:01:00,640 DEBUG Querying model with image: gpt-4o
2024-08-13 13:01:06,751 DEBUG Query with image successful
2024-08-13 13:01:06,834 DEBUG Querying model with image: gpt-4o
2024-08-13 13:01:14,200 DEBUG Query with image successful
2024-08-13 13:01:14,511 DEBUG Querying model with image: gpt-4o
2024-08-13 13:01:22,329 DEBUG Query with image successful
2024-08-13 13:01:22,412 DEBUG Querying model with image: gpt-4o
2024-08-13 13:01:29,155 DEBUG Query with image successful
2024-08-13 13:01:29,421 DEBUG Querying model with image: gpt-4o
2024-08-13 13:01:36,822 DEBUG Query with image successful
2024-08-13 13:01:36,909 DEBUG Querying model with image: gpt-4o
2024-08-13 13:01:45,135 DEBUG Query with image successful
2024-08-13 15:13:13,454 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 15:13:13,489 DEBUG OpenAI client created
2024-08-13 15:13:18,660 DEBUG Querying model with image: gpt-4o
2024-08-13 15:13:35,428 DEBUG Query with image successful
2024-08-13 15:13:35,510 DEBUG Querying model with image: gpt-4o
2024-08-13 15:13:49,766 DEBUG Query with image successful
2024-08-13 15:13:50,072 DEBUG Querying model with image: gpt-4o
2024-08-13 15:14:00,392 DEBUG Query with image successful
2024-08-13 15:14:00,487 DEBUG Querying model with image: gpt-4o
2024-08-13 15:14:15,489 DEBUG Query with image successful
2024-08-13 15:14:25,763 DEBUG Querying model with image: gpt-4o
2024-08-13 15:14:36,734 DEBUG Query with image successful
2024-08-13 15:14:36,829 DEBUG Querying model with image: gpt-4o
2024-08-13 15:14:45,127 DEBUG Query with image successful
2024-08-13 15:14:45,413 DEBUG Querying model with image: gpt-4o
2024-08-13 15:14:54,443 DEBUG Query with image successful
2024-08-13 15:14:54,530 DEBUG Querying model with image: gpt-4o
2024-08-13 15:15:05,201 DEBUG Query with image successful
2024-08-13 15:15:05,469 DEBUG Querying model with image: gpt-4o
2024-08-13 15:15:13,707 DEBUG Query with image successful
2024-08-13 15:15:13,791 DEBUG Querying model with image: gpt-4o
2024-08-13 15:15:24,968 DEBUG Query with image successful
2024-08-13 15:15:25,240 DEBUG Querying model with image: gpt-4o
2024-08-13 15:15:35,457 DEBUG Query with image successful
2024-08-13 15:15:35,542 DEBUG Querying model with image: gpt-4o
2024-08-13 15:15:46,804 DEBUG Query with image successful
2024-08-13 15:15:47,151 DEBUG Querying model with image: gpt-4o
2024-08-13 15:16:00,019 DEBUG Query with image successful
2024-08-13 15:16:00,104 DEBUG Querying model with image: gpt-4o
2024-08-13 15:16:13,357 DEBUG Query with image successful
2024-08-13 15:16:13,653 DEBUG Querying model with image: gpt-4o
2024-08-13 15:16:24,714 DEBUG Query with image successful
2024-08-13 15:16:24,799 DEBUG Querying model with image: gpt-4o
2024-08-13 15:16:42,797 DEBUG Query with image successful
2024-08-13 15:16:43,069 DEBUG Querying model with image: gpt-4o
2024-08-13 15:16:51,571 DEBUG Query with image successful
2024-08-13 15:16:51,666 DEBUG Querying model with image: gpt-4o
2024-08-13 15:17:03,962 DEBUG Query with image successful
2024-08-13 15:17:04,253 DEBUG Querying model with image: gpt-4o
2024-08-13 15:17:16,217 DEBUG Query with image successful
2024-08-13 15:17:16,301 DEBUG Querying model with image: gpt-4o
2024-08-13 15:17:28,369 DEBUG Query with image successful
2024-08-13 15:17:28,657 DEBUG Querying model with image: gpt-4o
2024-08-13 15:17:37,931 DEBUG Query with image successful
2024-08-13 15:17:38,018 DEBUG Querying model with image: gpt-4o
2024-08-13 15:17:48,356 DEBUG Query with image successful
2024-08-13 15:17:48,626 DEBUG Querying model with image: gpt-4o
2024-08-13 15:17:59,561 DEBUG Query with image successful
2024-08-13 15:17:59,644 DEBUG Querying model with image: gpt-4o
2024-08-13 15:18:12,161 DEBUG Query with image successful
2024-08-13 15:18:12,443 DEBUG Querying model with image: gpt-4o
2024-08-13 15:18:24,739 DEBUG Query with image successful
2024-08-13 15:18:24,833 DEBUG Querying model with image: gpt-4o
2024-08-13 15:18:33,676 DEBUG Query with image successful
2024-08-13 15:18:33,974 DEBUG Querying model with image: gpt-4o
2024-08-13 15:18:45,353 DEBUG Query with image successful
2024-08-13 15:18:45,436 DEBUG Querying model with image: gpt-4o
2024-08-13 15:18:56,041 DEBUG Query with image successful
2024-08-13 15:18:56,325 DEBUG Querying model with image: gpt-4o
2024-08-13 15:19:10,102 DEBUG Query with image successful
2024-08-13 15:19:10,209 DEBUG Querying model with image: gpt-4o
2024-08-13 15:19:19,462 DEBUG Query with image successful
2024-08-13 15:19:19,730 DEBUG Querying model with image: gpt-4o
2024-08-13 15:19:31,747 DEBUG Query with image successful
2024-08-13 15:19:31,831 DEBUG Querying model with image: gpt-4o
2024-08-13 15:19:41,760 DEBUG Query with image successful
2024-08-13 15:19:42,044 DEBUG Querying model with image: gpt-4o
2024-08-13 15:19:52,942 DEBUG Query with image successful
2024-08-13 15:19:53,039 DEBUG Querying model with image: gpt-4o
2024-08-13 15:20:02,348 DEBUG Query with image successful
2024-08-13 15:20:02,629 DEBUG Querying model with image: gpt-4o
2024-08-13 15:20:12,263 DEBUG Query with image successful
2024-08-13 15:20:12,348 DEBUG Querying model with image: gpt-4o
2024-08-13 15:20:21,557 DEBUG Query with image successful
2024-08-13 15:20:21,844 DEBUG Querying model with image: gpt-4o
2024-08-13 15:20:32,860 DEBUG Query with image successful
2024-08-13 15:20:32,945 DEBUG Querying model with image: gpt-4o
2024-08-13 15:20:43,801 DEBUG Query with image successful
2024-08-13 15:20:44,066 DEBUG Querying model with image: gpt-4o
2024-08-13 15:20:54,043 DEBUG Query with image successful
2024-08-13 15:20:54,127 DEBUG Querying model with image: gpt-4o
2024-08-13 15:21:04,169 DEBUG Query with image successful
2024-08-13 15:21:04,449 DEBUG Querying model with image: gpt-4o
2024-08-13 15:21:15,239 DEBUG Query with image successful
2024-08-13 15:21:15,323 DEBUG Querying model with image: gpt-4o
2024-08-13 15:21:26,965 DEBUG Query with image successful
2024-08-13 15:21:27,235 DEBUG Querying model with image: gpt-4o
2024-08-13 15:21:36,455 DEBUG Query with image successful
2024-08-13 15:21:36,540 DEBUG Querying model with image: gpt-4o
2024-08-13 15:21:44,507 DEBUG Query with image successful
2024-08-13 15:21:44,819 DEBUG Querying model with image: gpt-4o
2024-08-13 15:21:57,938 DEBUG Query with image successful
2024-08-13 15:21:58,026 DEBUG Querying model with image: gpt-4o
2024-08-13 15:22:07,107 DEBUG Query with image successful
2024-08-13 15:22:07,384 DEBUG Querying model with image: gpt-4o
2024-08-13 15:22:21,665 DEBUG Query with image successful
2024-08-13 15:22:21,752 DEBUG Querying model with image: gpt-4o
2024-08-13 15:22:34,766 DEBUG Query with image successful
2024-08-13 15:22:35,161 DEBUG Querying model with image: gpt-4o
2024-08-13 15:22:47,436 DEBUG Query with image successful
2024-08-13 15:22:47,531 DEBUG Querying model with image: gpt-4o
2024-08-13 15:22:56,921 DEBUG Query with image successful
2024-08-13 15:29:38,412 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 15:29:38,443 DEBUG OpenAI client created
2024-08-13 15:29:43,480 DEBUG Querying model with image: gpt-4o
2024-08-13 15:29:47,825 DEBUG Query with image successful
2024-08-13 15:29:47,843 DEBUG Querying model with image: gpt-4o
2024-08-13 15:29:54,384 DEBUG Query with image successful
2024-08-13 15:29:54,708 DEBUG Querying model with image: gpt-4o
2024-08-13 15:30:02,581 DEBUG Query with image successful
2024-08-13 15:30:02,677 DEBUG Querying model with image: gpt-4o
2024-08-13 15:30:14,635 DEBUG Query with image successful
2024-08-13 15:30:24,920 DEBUG Querying model with image: gpt-4o
2024-08-13 15:30:35,396 DEBUG Query with image successful
2024-08-13 15:30:35,489 DEBUG Querying model with image: gpt-4o
2024-08-13 15:30:46,489 DEBUG Query with image successful
2024-08-13 15:30:46,774 DEBUG Querying model with image: gpt-4o
2024-08-13 15:30:57,564 DEBUG Query with image successful
2024-08-13 15:30:57,648 DEBUG Querying model with image: gpt-4o
2024-08-13 15:31:07,611 DEBUG Query with image successful
2024-08-13 15:31:07,879 DEBUG Querying model with image: gpt-4o
2024-08-13 15:31:19,260 DEBUG Query with image successful
2024-08-13 15:31:19,346 DEBUG Querying model with image: gpt-4o
2024-08-13 15:31:29,576 DEBUG Query with image successful
2024-08-13 15:31:29,857 DEBUG Querying model with image: gpt-4o
2024-08-13 15:31:42,494 DEBUG Query with image successful
2024-08-13 15:31:42,579 DEBUG Querying model with image: gpt-4o
2024-08-13 15:31:50,785 DEBUG Query with image successful
2024-08-13 15:31:51,150 DEBUG Querying model with image: gpt-4o
2024-08-13 15:32:01,253 DEBUG Query with image successful
2024-08-13 15:32:01,354 DEBUG Querying model with image: gpt-4o
2024-08-13 15:32:13,264 DEBUG Query with image successful
2024-08-13 15:32:13,581 DEBUG Querying model with image: gpt-4o
2024-08-13 15:32:27,021 DEBUG Query with image successful
2024-08-13 15:32:27,105 DEBUG Querying model with image: gpt-4o
2024-08-13 15:32:39,876 DEBUG Query with image successful
2024-08-13 15:32:40,153 DEBUG Querying model with image: gpt-4o
2024-08-13 15:32:51,184 DEBUG Query with image successful
2024-08-13 15:32:51,268 DEBUG Querying model with image: gpt-4o
2024-08-13 15:33:03,358 DEBUG Query with image successful
2024-08-13 15:33:03,628 DEBUG Querying model with image: gpt-4o
2024-08-13 15:33:20,528 DEBUG Query with image successful
2024-08-13 15:33:20,613 DEBUG Querying model with image: gpt-4o
2024-08-13 15:33:35,578 DEBUG Query with image successful
2024-08-13 15:33:35,884 DEBUG Querying model with image: gpt-4o
2024-08-13 15:33:45,379 DEBUG Query with image successful
2024-08-13 15:33:45,461 DEBUG Querying model with image: gpt-4o
2024-08-13 15:33:55,889 DEBUG Query with image successful
2024-08-13 15:33:56,188 DEBUG Querying model with image: gpt-4o
2024-08-13 15:34:08,033 DEBUG Query with image successful
2024-08-13 15:34:08,117 DEBUG Querying model with image: gpt-4o
2024-08-13 15:34:23,353 DEBUG Query with image successful
2024-08-13 15:34:23,625 DEBUG Querying model with image: gpt-4o
2024-08-13 15:34:36,037 DEBUG Query with image successful
2024-08-13 15:34:36,121 DEBUG Querying model with image: gpt-4o
2024-08-13 15:34:46,758 DEBUG Query with image successful
2024-08-13 15:34:47,028 DEBUG Querying model with image: gpt-4o
2024-08-13 15:35:02,017 DEBUG Query with image successful
2024-08-13 15:35:02,103 DEBUG Querying model with image: gpt-4o
2024-08-13 15:35:17,153 DEBUG Query with image successful
2024-08-13 15:35:17,436 DEBUG Querying model with image: gpt-4o
2024-08-13 15:35:35,682 DEBUG Query with image successful
2024-08-13 15:35:35,766 DEBUG Querying model with image: gpt-4o
2024-08-13 15:35:45,807 DEBUG Query with image successful
2024-08-13 15:35:46,123 DEBUG Querying model with image: gpt-4o
2024-08-13 15:35:58,335 DEBUG Query with image successful
2024-08-13 15:35:58,424 DEBUG Querying model with image: gpt-4o
2024-08-13 15:36:10,601 DEBUG Query with image successful
2024-08-13 15:36:10,881 DEBUG Querying model with image: gpt-4o
2024-08-13 15:36:22,570 DEBUG Query with image successful
2024-08-13 15:36:22,671 DEBUG Querying model with image: gpt-4o
2024-08-13 15:36:37,020 DEBUG Query with image successful
2024-08-13 15:36:37,301 DEBUG Querying model with image: gpt-4o
2024-08-13 15:36:47,257 DEBUG Query with image successful
2024-08-13 15:36:47,341 DEBUG Querying model with image: gpt-4o
2024-08-13 15:36:57,874 DEBUG Query with image successful
2024-08-13 15:36:58,170 DEBUG Querying model with image: gpt-4o
2024-08-13 15:37:09,160 DEBUG Query with image successful
2024-08-13 15:37:09,258 DEBUG Querying model with image: gpt-4o
2024-08-13 15:37:21,453 DEBUG Query with image successful
2024-08-13 15:37:21,735 DEBUG Querying model with image: gpt-4o
2024-08-13 15:37:34,926 DEBUG Query with image successful
2024-08-13 15:37:35,009 DEBUG Querying model with image: gpt-4o
2024-08-13 15:37:44,004 DEBUG Query with image successful
2024-08-13 15:37:44,271 DEBUG Querying model with image: gpt-4o
2024-08-13 15:37:56,498 DEBUG Query with image successful
2024-08-13 15:37:56,582 DEBUG Querying model with image: gpt-4o
2024-08-13 15:38:07,861 DEBUG Query with image successful
2024-08-13 15:38:08,149 DEBUG Querying model with image: gpt-4o
2024-08-13 15:38:21,425 DEBUG Query with image successful
2024-08-13 15:38:21,509 DEBUG Querying model with image: gpt-4o
2024-08-13 15:38:30,173 DEBUG Query with image successful
2024-08-13 15:38:30,446 DEBUG Querying model with image: gpt-4o
2024-08-13 15:38:38,907 DEBUG Query with image successful
2024-08-13 15:38:38,990 DEBUG Querying model with image: gpt-4o
2024-08-13 15:38:46,758 DEBUG Query with image successful
2024-08-13 15:38:47,027 DEBUG Querying model with image: gpt-4o
2024-08-13 15:39:00,431 DEBUG Query with image successful
2024-08-13 15:39:00,514 DEBUG Querying model with image: gpt-4o
2024-08-13 15:39:12,676 DEBUG Query with image successful
2024-08-13 15:39:12,945 DEBUG Querying model with image: gpt-4o
2024-08-13 15:39:24,000 DEBUG Query with image successful
2024-08-13 15:39:24,084 DEBUG Querying model with image: gpt-4o
2024-08-13 15:39:37,120 DEBUG Query with image successful
2024-08-13 16:22:38,223 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 16:22:38,251 DEBUG OpenAI client created
2024-08-13 16:22:43,427 DEBUG Querying model with image: gpt-4o
2024-08-13 16:22:55,896 DEBUG Query with image successful
2024-08-13 16:22:55,986 DEBUG Querying model with image: gpt-4o
2024-08-13 16:23:05,890 DEBUG Query with image successful
2024-08-13 16:23:06,174 DEBUG Querying model with image: gpt-4o
2024-08-13 16:23:18,441 DEBUG Query with image successful
2024-08-13 16:23:18,535 DEBUG Querying model with image: gpt-4o
2024-08-13 16:23:46,953 DEBUG Query with image successful
2024-08-13 16:23:57,236 DEBUG Querying model with image: gpt-4o
2024-08-13 16:24:08,588 DEBUG Query with image successful
2024-08-13 16:24:08,674 DEBUG Querying model with image: gpt-4o
2024-08-13 16:24:19,112 DEBUG Query with image successful
2024-08-13 16:24:19,391 DEBUG Querying model with image: gpt-4o
2024-08-13 16:24:29,181 DEBUG Query with image successful
2024-08-13 16:24:29,281 DEBUG Querying model with image: gpt-4o
2024-08-13 16:24:40,717 DEBUG Query with image successful
2024-08-13 16:24:41,013 DEBUG Querying model with image: gpt-4o
2024-08-13 16:24:50,856 DEBUG Query with image successful
2024-08-13 16:24:50,943 DEBUG Querying model with image: gpt-4o
2024-08-13 16:25:05,214 DEBUG Query with image successful
2024-08-13 16:25:05,505 DEBUG Querying model with image: gpt-4o
2024-08-13 16:25:16,668 DEBUG Query with image successful
2024-08-13 16:25:16,756 DEBUG Querying model with image: gpt-4o
2024-08-13 16:25:25,031 DEBUG Query with image successful
2024-08-13 16:25:25,310 DEBUG Querying model with image: gpt-4o
2024-08-13 16:25:35,040 DEBUG Query with image successful
2024-08-13 16:25:35,128 DEBUG Querying model with image: gpt-4o
2024-08-13 16:25:44,512 DEBUG Query with image successful
2024-08-13 16:25:44,814 DEBUG Querying model with image: gpt-4o
2024-08-13 16:25:53,801 DEBUG Query with image successful
2024-08-13 16:25:53,890 DEBUG Querying model with image: gpt-4o
2024-08-13 16:26:04,704 DEBUG Query with image successful
2024-08-13 16:26:04,985 DEBUG Querying model with image: gpt-4o
2024-08-13 16:26:16,688 DEBUG Query with image successful
2024-08-13 16:26:16,786 DEBUG Querying model with image: gpt-4o
2024-08-13 16:26:29,719 DEBUG Query with image successful
2024-08-13 16:26:30,017 DEBUG Querying model with image: gpt-4o
2024-08-13 16:26:38,574 DEBUG Query with image successful
2024-08-13 16:26:38,667 DEBUG Querying model with image: gpt-4o
2024-08-13 16:26:48,329 DEBUG Query with image successful
2024-08-13 16:26:48,609 DEBUG Querying model with image: gpt-4o
2024-08-13 16:26:59,570 DEBUG Query with image successful
2024-08-13 16:26:59,657 DEBUG Querying model with image: gpt-4o
2024-08-13 16:27:09,642 DEBUG Query with image successful
2024-08-13 16:27:09,924 DEBUG Querying model with image: gpt-4o
2024-08-13 16:27:21,751 DEBUG Query with image successful
2024-08-13 16:27:21,838 DEBUG Querying model with image: gpt-4o
2024-08-13 16:27:35,420 DEBUG Query with image successful
2024-08-13 16:27:35,692 DEBUG Querying model with image: gpt-4o
2024-08-13 16:27:47,482 DEBUG Query with image successful
2024-08-13 16:27:47,575 DEBUG Querying model with image: gpt-4o
2024-08-13 16:27:57,228 DEBUG Query with image successful
2024-08-13 16:27:57,510 DEBUG Querying model with image: gpt-4o
2024-08-13 16:28:07,498 DEBUG Query with image successful
2024-08-13 16:28:07,583 DEBUG Querying model with image: gpt-4o
2024-08-13 16:28:16,581 DEBUG Query with image successful
2024-08-13 16:28:16,988 DEBUG Querying model with image: gpt-4o
2024-08-13 16:28:28,549 DEBUG Query with image successful
2024-08-13 16:28:28,640 DEBUG Querying model with image: gpt-4o
2024-08-13 16:28:38,730 DEBUG Query with image successful
2024-08-13 16:28:39,018 DEBUG Querying model with image: gpt-4o
2024-08-13 16:28:49,567 DEBUG Query with image successful
2024-08-13 16:28:49,656 DEBUG Querying model with image: gpt-4o
2024-08-13 16:28:57,817 DEBUG Query with image successful
2024-08-13 16:28:58,213 DEBUG Querying model with image: gpt-4o
2024-08-13 16:29:08,137 DEBUG Query with image successful
2024-08-13 16:29:08,226 DEBUG Querying model with image: gpt-4o
2024-08-13 16:29:19,048 DEBUG Query with image successful
2024-08-13 16:29:19,347 DEBUG Querying model with image: gpt-4o
2024-08-13 16:29:31,169 DEBUG Query with image successful
2024-08-13 16:29:31,264 DEBUG Querying model with image: gpt-4o
2024-08-13 16:29:42,930 DEBUG Query with image successful
2024-08-13 16:29:43,222 DEBUG Querying model with image: gpt-4o
2024-08-13 16:29:51,511 DEBUG Query with image successful
2024-08-13 16:29:51,611 DEBUG Querying model with image: gpt-4o
2024-08-13 16:29:59,998 DEBUG Query with image successful
2024-08-13 16:30:00,298 DEBUG Querying model with image: gpt-4o
2024-08-13 16:30:10,611 DEBUG Query with image successful
2024-08-13 16:30:10,699 DEBUG Querying model with image: gpt-4o
2024-08-13 16:30:20,377 DEBUG Query with image successful
2024-08-13 16:30:20,656 DEBUG Querying model with image: gpt-4o
2024-08-13 16:30:30,208 DEBUG Query with image successful
2024-08-13 16:30:30,298 DEBUG Querying model with image: gpt-4o
2024-08-13 16:30:38,118 DEBUG Query with image successful
2024-08-13 16:30:38,390 DEBUG Querying model with image: gpt-4o
2024-08-13 16:30:49,191 DEBUG Query with image successful
2024-08-13 16:30:49,285 DEBUG Querying model with image: gpt-4o
2024-08-13 16:30:59,527 DEBUG Query with image successful
2024-08-13 16:30:59,815 DEBUG Querying model with image: gpt-4o
2024-08-13 16:31:09,838 DEBUG Query with image successful
2024-08-13 16:31:09,935 DEBUG Querying model with image: gpt-4o
2024-08-13 16:31:17,816 DEBUG Query with image successful
2024-08-13 16:31:18,147 DEBUG Querying model with image: gpt-4o
2024-08-13 16:31:26,927 DEBUG Query with image successful
2024-08-13 16:31:27,019 DEBUG Querying model with image: gpt-4o
2024-08-13 16:31:36,164 DEBUG Query with image successful
2024-08-13 16:31:36,448 DEBUG Querying model with image: gpt-4o
2024-08-13 16:31:46,464 DEBUG Query with image successful
2024-08-13 16:31:46,564 DEBUG Querying model with image: gpt-4o
2024-08-13 16:31:54,201 DEBUG Query with image successful
2024-08-13 18:50:53,540 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 18:50:53,578 DEBUG OpenAI client created
2024-08-13 18:50:58,778 DEBUG Querying model with image: gpt-4o
2024-08-13 18:51:07,055 DEBUG Query with image successful
2024-08-13 18:51:07,158 DEBUG Querying model with image: gpt-4o
2024-08-13 18:51:17,610 DEBUG Query with image successful
2024-08-13 18:51:17,896 DEBUG Querying model with image: gpt-4o
2024-08-13 18:51:30,094 DEBUG Query with image successful
2024-08-13 18:51:30,189 DEBUG Querying model with image: gpt-4o
2024-08-13 18:51:47,706 DEBUG Query with image successful
2024-08-13 18:51:58,001 DEBUG Querying model with image: gpt-4o
2024-08-13 18:52:07,105 DEBUG Query with image successful
2024-08-13 18:52:07,206 DEBUG Querying model with image: gpt-4o
2024-08-13 18:52:15,856 DEBUG Query with image successful
2024-08-13 18:52:16,170 DEBUG Querying model with image: gpt-4o
2024-08-13 18:52:24,559 DEBUG Query with image successful
2024-08-13 18:52:24,646 DEBUG Querying model with image: gpt-4o
2024-08-13 18:52:33,714 DEBUG Query with image successful
2024-08-13 18:52:33,996 DEBUG Querying model with image: gpt-4o
2024-08-13 18:52:43,174 DEBUG Query with image successful
2024-08-13 18:52:43,260 DEBUG Querying model with image: gpt-4o
2024-08-13 18:52:53,816 DEBUG Query with image successful
2024-08-13 18:52:54,157 DEBUG Querying model with image: gpt-4o
2024-08-13 18:53:04,028 DEBUG Query with image successful
2024-08-13 18:53:04,112 DEBUG Querying model with image: gpt-4o
2024-08-13 18:53:11,793 DEBUG Query with image successful
2024-08-13 18:53:12,131 DEBUG Querying model with image: gpt-4o
2024-08-13 18:53:20,380 DEBUG Query with image successful
2024-08-13 18:53:20,467 DEBUG Querying model with image: gpt-4o
2024-08-13 18:53:31,384 DEBUG Query with image successful
2024-08-13 18:53:31,717 DEBUG Querying model with image: gpt-4o
2024-08-13 18:53:41,312 DEBUG Query with image successful
2024-08-13 18:53:41,398 DEBUG Querying model with image: gpt-4o
2024-08-13 18:53:49,759 DEBUG Query with image successful
2024-08-13 18:53:50,041 DEBUG Querying model with image: gpt-4o
2024-08-13 18:53:58,627 DEBUG Query with image successful
2024-08-13 18:53:58,725 DEBUG Querying model with image: gpt-4o
2024-08-13 18:54:11,847 DEBUG Query with image successful
2024-08-13 18:54:12,120 DEBUG Querying model with image: gpt-4o
2024-08-13 18:54:23,185 DEBUG Query with image successful
2024-08-13 18:54:23,269 DEBUG Querying model with image: gpt-4o
2024-08-13 18:54:34,726 DEBUG Query with image successful
2024-08-13 18:54:34,996 DEBUG Querying model with image: gpt-4o
2024-08-13 18:54:47,666 DEBUG Query with image successful
2024-08-13 18:54:47,750 DEBUG Querying model with image: gpt-4o
2024-08-13 18:54:58,249 DEBUG Query with image successful
2024-08-13 18:54:58,533 DEBUG Querying model with image: gpt-4o
2024-08-13 18:55:11,183 DEBUG Query with image successful
2024-08-13 18:55:11,273 DEBUG Querying model with image: gpt-4o
2024-08-13 18:55:22,855 DEBUG Query with image successful
2024-08-13 18:55:23,131 DEBUG Querying model with image: gpt-4o
2024-08-13 18:55:34,185 DEBUG Query with image successful
2024-08-13 18:55:34,270 DEBUG Querying model with image: gpt-4o
2024-08-13 18:55:45,150 DEBUG Query with image successful
2024-08-13 18:55:45,421 DEBUG Querying model with image: gpt-4o
2024-08-13 18:55:55,814 DEBUG Query with image successful
2024-08-13 18:55:55,920 DEBUG Querying model with image: gpt-4o
2024-08-13 18:56:04,003 DEBUG Query with image successful
2024-08-13 18:56:04,275 DEBUG Querying model with image: gpt-4o
2024-08-13 18:56:13,896 DEBUG Query with image successful
2024-08-13 18:56:13,979 DEBUG Querying model with image: gpt-4o
2024-08-13 18:56:22,303 DEBUG Query with image successful
2024-08-13 18:56:22,639 DEBUG Querying model with image: gpt-4o
2024-08-13 18:56:31,574 DEBUG Query with image successful
2024-08-13 18:56:31,658 DEBUG Querying model with image: gpt-4o
2024-08-13 18:56:39,984 DEBUG Query with image successful
2024-08-13 18:56:40,272 DEBUG Querying model with image: gpt-4o
2024-08-13 18:56:50,513 DEBUG Query with image successful
2024-08-13 18:56:50,610 DEBUG Querying model with image: gpt-4o
2024-08-13 18:56:58,458 DEBUG Query with image successful
2024-08-13 18:56:58,740 DEBUG Querying model with image: gpt-4o
2024-08-13 18:57:12,151 DEBUG Query with image successful
2024-08-13 18:57:12,234 DEBUG Querying model with image: gpt-4o
2024-08-13 18:57:23,182 DEBUG Query with image successful
2024-08-13 18:57:23,451 DEBUG Querying model with image: gpt-4o
2024-08-13 18:57:33,856 DEBUG Query with image successful
2024-08-13 18:57:33,961 DEBUG Querying model with image: gpt-4o
2024-08-13 18:57:44,776 DEBUG Query with image successful
2024-08-13 18:57:45,062 DEBUG Querying model with image: gpt-4o
2024-08-13 18:57:57,351 DEBUG Query with image successful
2024-08-13 18:57:57,434 DEBUG Querying model with image: gpt-4o
2024-08-13 18:58:09,341 DEBUG Query with image successful
2024-08-13 18:58:09,614 DEBUG Querying model with image: gpt-4o
2024-08-13 18:58:20,744 DEBUG Query with image successful
2024-08-13 18:58:20,826 DEBUG Querying model with image: gpt-4o
2024-08-13 18:58:32,782 DEBUG Query with image successful
2024-08-13 18:58:33,069 DEBUG Querying model with image: gpt-4o
2024-08-13 18:58:45,039 DEBUG Query with image successful
2024-08-13 18:58:45,130 DEBUG Querying model with image: gpt-4o
2024-08-13 18:58:55,624 DEBUG Query with image successful
2024-08-13 18:58:55,894 DEBUG Querying model with image: gpt-4o
2024-08-13 18:59:04,369 DEBUG Query with image successful
2024-08-13 18:59:04,462 DEBUG Querying model with image: gpt-4o
2024-08-13 18:59:11,819 DEBUG Query with image successful
2024-08-13 18:59:12,093 DEBUG Querying model with image: gpt-4o
2024-08-13 18:59:21,334 DEBUG Query with image successful
2024-08-13 18:59:21,418 DEBUG Querying model with image: gpt-4o
2024-08-13 18:59:30,537 DEBUG Query with image successful
2024-08-13 18:59:30,823 DEBUG Querying model with image: gpt-4o
2024-08-13 18:59:43,526 DEBUG Query with image successful
2024-08-13 18:59:43,618 DEBUG Querying model with image: gpt-4o
2024-08-13 18:59:50,960 DEBUG Query with image successful
2024-08-14 15:16:58,271 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-14 15:16:58,313 DEBUG OpenAI client created
2024-08-14 15:17:03,477 DEBUG Querying model with image: gpt-4o
2024-08-14 15:17:15,126 DEBUG Query with image successful
2024-08-14 15:17:15,210 DEBUG Querying model with image: gpt-4o
2024-08-14 15:17:25,572 DEBUG Query with image successful
2024-08-14 15:17:25,840 DEBUG Querying model with image: gpt-4o
2024-08-14 15:17:34,638 DEBUG Query with image successful
2024-08-14 15:17:34,721 DEBUG Querying model with image: gpt-4o
2024-08-14 15:17:42,579 DEBUG Query with image successful
2024-08-14 15:17:52,901 DEBUG Querying model with image: gpt-4o
2024-08-14 15:18:01,901 DEBUG Query with image successful
2024-08-14 15:18:01,996 DEBUG Querying model with image: gpt-4o
2024-08-14 15:18:14,854 DEBUG Query with image successful
2024-08-14 15:18:15,138 DEBUG Querying model with image: gpt-4o
2024-08-14 15:18:27,634 DEBUG Query with image successful
2024-08-14 15:18:27,718 DEBUG Querying model with image: gpt-4o
2024-08-14 15:18:39,752 DEBUG Query with image successful
2024-08-14 15:18:40,056 DEBUG Querying model with image: gpt-4o
2024-08-14 15:18:51,190 DEBUG Query with image successful
2024-08-14 15:18:51,275 DEBUG Querying model with image: gpt-4o
2024-08-14 15:19:04,832 DEBUG Query with image successful
2024-08-14 15:19:05,159 DEBUG Querying model with image: gpt-4o
2024-08-14 15:19:14,110 DEBUG Query with image successful
2024-08-14 15:19:14,197 DEBUG Querying model with image: gpt-4o
2024-08-14 15:19:24,437 DEBUG Query with image successful
2024-08-14 15:19:24,722 DEBUG Querying model with image: gpt-4o
2024-08-14 15:19:37,208 DEBUG Query with image successful
2024-08-14 15:19:37,301 DEBUG Querying model with image: gpt-4o
2024-08-14 15:19:51,281 DEBUG Query with image successful
2024-08-14 15:19:51,562 DEBUG Querying model with image: gpt-4o
2024-08-14 15:20:00,289 DEBUG Query with image successful
2024-08-14 15:20:00,382 DEBUG Querying model with image: gpt-4o
2024-08-14 15:20:17,068 DEBUG Query with image successful
2024-08-14 15:20:17,350 DEBUG Querying model with image: gpt-4o
2024-08-14 15:20:26,735 DEBUG Query with image successful
2024-08-14 15:20:26,822 DEBUG Querying model with image: gpt-4o
2024-08-14 15:20:38,040 DEBUG Query with image successful
2024-08-14 15:20:38,347 DEBUG Querying model with image: gpt-4o
2024-08-14 15:20:48,566 DEBUG Query with image successful
2024-08-14 15:20:48,657 DEBUG Querying model with image: gpt-4o
2024-08-14 15:21:06,106 DEBUG Query with image successful
2024-08-14 15:21:06,382 DEBUG Querying model with image: gpt-4o
2024-08-14 15:21:15,739 DEBUG Query with image successful
2024-08-14 15:21:15,830 DEBUG Querying model with image: gpt-4o
2024-08-14 15:21:26,836 DEBUG Query with image successful
2024-08-14 15:21:27,116 DEBUG Querying model with image: gpt-4o
2024-08-14 15:21:36,156 DEBUG Query with image successful
2024-08-14 15:21:36,255 DEBUG Querying model with image: gpt-4o
2024-08-14 15:21:44,332 DEBUG Query with image successful
2024-08-14 15:21:44,613 DEBUG Querying model with image: gpt-4o
2024-08-14 15:21:53,836 DEBUG Query with image successful
2024-08-14 15:21:53,920 DEBUG Querying model with image: gpt-4o
2024-08-14 15:22:03,327 DEBUG Query with image successful
2024-08-14 15:22:03,619 DEBUG Querying model with image: gpt-4o
2024-08-14 15:22:13,648 DEBUG Query with image successful
2024-08-14 15:22:13,739 DEBUG Querying model with image: gpt-4o
2024-08-14 15:22:25,022 DEBUG Query with image successful
2024-08-14 15:22:25,308 DEBUG Querying model with image: gpt-4o
2024-08-14 15:22:37,236 DEBUG Query with image successful
2024-08-14 15:22:37,323 DEBUG Querying model with image: gpt-4o
2024-08-14 15:22:46,330 DEBUG Query with image successful
2024-08-14 15:22:46,617 DEBUG Querying model with image: gpt-4o
2024-08-14 15:22:53,987 DEBUG Query with image successful
2024-08-14 15:22:54,073 DEBUG Querying model with image: gpt-4o
2024-08-14 15:23:02,045 DEBUG Query with image successful
2024-08-14 15:23:02,325 DEBUG Querying model with image: gpt-4o
2024-08-14 15:23:12,956 DEBUG Query with image successful
2024-08-14 15:23:13,061 DEBUG Querying model with image: gpt-4o
2024-08-14 15:23:26,094 DEBUG Query with image successful
2024-08-14 15:23:26,407 DEBUG Querying model with image: gpt-4o
2024-08-14 15:23:34,973 DEBUG Query with image successful
2024-08-14 15:23:35,063 DEBUG Querying model with image: gpt-4o
2024-08-14 15:23:45,697 DEBUG Query with image successful
2024-08-14 15:23:46,020 DEBUG Querying model with image: gpt-4o
2024-08-14 15:23:54,571 DEBUG Query with image successful
2024-08-14 15:23:54,669 DEBUG Querying model with image: gpt-4o
2024-08-14 15:24:02,075 DEBUG Query with image successful
2024-08-14 15:24:02,375 DEBUG Querying model with image: gpt-4o
2024-08-14 15:24:23,089 DEBUG Query with image successful
2024-08-14 15:24:23,176 DEBUG Querying model with image: gpt-4o
2024-08-14 15:24:33,867 DEBUG Query with image successful
2024-08-14 15:24:34,149 DEBUG Querying model with image: gpt-4o
2024-08-14 15:24:44,180 DEBUG Query with image successful
2024-08-14 15:24:44,267 DEBUG Querying model with image: gpt-4o
2024-08-14 15:24:52,558 DEBUG Query with image successful
2024-08-14 15:24:52,842 DEBUG Querying model with image: gpt-4o
2024-08-14 15:25:02,843 DEBUG Query with image successful
2024-08-14 15:25:02,932 DEBUG Querying model with image: gpt-4o
2024-08-14 15:25:13,565 DEBUG Query with image successful
2024-08-14 15:25:13,844 DEBUG Querying model with image: gpt-4o
2024-08-14 15:25:25,539 DEBUG Query with image successful
2024-08-14 15:25:25,627 DEBUG Querying model with image: gpt-4o
2024-08-14 15:25:32,948 DEBUG Query with image successful
2024-08-14 15:25:33,226 DEBUG Querying model with image: gpt-4o
2024-08-14 15:25:46,833 DEBUG Query with image successful
2024-08-14 15:25:46,919 DEBUG Querying model with image: gpt-4o
2024-08-14 15:25:55,618 DEBUG Query with image successful
2024-08-14 15:25:55,904 DEBUG Querying model with image: gpt-4o
2024-08-14 15:26:07,530 DEBUG Query with image successful
2024-08-14 15:26:07,629 DEBUG Querying model with image: gpt-4o
2024-08-14 15:26:18,127 DEBUG Query with image successful
2024-08-14 15:36:48,167 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-14 15:36:48,194 DEBUG OpenAI client created
2024-08-14 15:36:53,365 DEBUG Querying model with image: gpt-4o
2024-08-14 15:37:05,815 DEBUG Query with image successful
2024-08-14 15:37:05,898 DEBUG Querying model with image: gpt-4o
2024-08-14 15:37:17,034 DEBUG Query with image successful
2024-08-14 15:37:17,311 DEBUG Querying model with image: gpt-4o
2024-08-14 15:37:25,264 DEBUG Query with image successful
2024-08-14 15:37:25,349 DEBUG Querying model with image: gpt-4o
2024-08-14 15:37:34,702 DEBUG Query with image successful
2024-08-14 15:37:44,978 DEBUG Querying model with image: gpt-4o
2024-08-14 15:37:53,802 DEBUG Query with image successful
2024-08-14 15:37:53,887 DEBUG Querying model with image: gpt-4o
2024-08-14 15:38:03,753 DEBUG Query with image successful
2024-08-14 15:38:04,063 DEBUG Querying model with image: gpt-4o
2024-08-14 15:38:12,802 DEBUG Query with image successful
2024-08-14 15:38:12,889 DEBUG Querying model with image: gpt-4o
2024-08-14 15:38:21,344 DEBUG Query with image successful
2024-08-14 15:38:21,618 DEBUG Querying model with image: gpt-4o
2024-08-14 15:38:31,098 DEBUG Query with image successful
2024-08-14 15:38:31,185 DEBUG Querying model with image: gpt-4o
2024-08-14 15:38:42,882 DEBUG Query with image successful
2024-08-14 15:38:43,162 DEBUG Querying model with image: gpt-4o
2024-08-14 15:38:52,254 DEBUG Query with image successful
2024-08-14 15:38:52,344 DEBUG Querying model with image: gpt-4o
2024-08-14 15:39:02,355 DEBUG Query with image successful
2024-08-14 15:39:02,640 DEBUG Querying model with image: gpt-4o
2024-08-14 15:39:12,322 DEBUG Query with image successful
2024-08-14 15:39:12,408 DEBUG Querying model with image: gpt-4o
2024-08-14 15:39:23,836 DEBUG Query with image successful
2024-08-14 15:39:24,147 DEBUG Querying model with image: gpt-4o
2024-08-14 15:39:35,161 DEBUG Query with image successful
2024-08-14 15:39:35,247 DEBUG Querying model with image: gpt-4o
2024-08-14 15:39:48,528 DEBUG Query with image successful
2024-08-14 15:39:48,847 DEBUG Querying model with image: gpt-4o
2024-08-14 15:39:57,238 DEBUG Query with image successful
2024-08-14 15:39:57,334 DEBUG Querying model with image: gpt-4o
2024-08-14 15:40:09,464 DEBUG Query with image successful
2024-08-14 15:40:09,750 DEBUG Querying model with image: gpt-4o
2024-08-14 15:40:19,053 DEBUG Query with image successful
2024-08-14 15:40:19,141 DEBUG Querying model with image: gpt-4o
2024-08-14 15:40:29,590 DEBUG Query with image successful
2024-08-14 15:40:29,877 DEBUG Querying model with image: gpt-4o
2024-08-14 15:40:39,855 DEBUG Query with image successful
2024-08-14 15:40:39,942 DEBUG Querying model with image: gpt-4o
2024-08-14 15:40:48,627 DEBUG Query with image successful
2024-08-14 15:40:48,906 DEBUG Querying model with image: gpt-4o
2024-08-14 15:41:00,102 DEBUG Query with image successful
2024-08-14 15:41:00,186 DEBUG Querying model with image: gpt-4o
2024-08-14 15:41:10,000 DEBUG Query with image successful
2024-08-14 15:41:10,291 DEBUG Querying model with image: gpt-4o
2024-08-14 15:41:20,569 DEBUG Query with image successful
2024-08-14 15:41:20,666 DEBUG Querying model with image: gpt-4o
2024-08-14 15:41:30,424 DEBUG Query with image successful
2024-08-14 15:41:30,699 DEBUG Querying model with image: gpt-4o
2024-08-14 15:41:39,207 DEBUG Query with image successful
2024-08-14 15:41:39,293 DEBUG Querying model with image: gpt-4o
2024-08-14 15:41:47,062 DEBUG Query with image successful
2024-08-14 15:41:47,353 DEBUG Querying model with image: gpt-4o
2024-08-14 15:41:57,606 DEBUG Query with image successful
2024-08-14 15:41:57,692 DEBUG Querying model with image: gpt-4o
2024-08-14 15:42:06,194 DEBUG Query with image successful
2024-08-14 15:42:06,496 DEBUG Querying model with image: gpt-4o
2024-08-14 15:42:18,121 DEBUG Query with image successful
2024-08-14 15:42:18,214 DEBUG Querying model with image: gpt-4o
2024-08-14 15:42:25,300 DEBUG Query with image successful
2024-08-14 15:42:25,575 DEBUG Querying model with image: gpt-4o
2024-08-14 15:42:34,683 DEBUG Query with image successful
2024-08-14 15:42:34,774 DEBUG Querying model with image: gpt-4o
2024-08-14 15:42:42,987 DEBUG Query with image successful
2024-08-14 15:42:43,270 DEBUG Querying model with image: gpt-4o
2024-08-14 15:42:53,030 DEBUG Query with image successful
2024-08-14 15:42:53,117 DEBUG Querying model with image: gpt-4o
2024-08-14 15:43:08,650 DEBUG Query with image successful
2024-08-14 15:43:08,957 DEBUG Querying model with image: gpt-4o
2024-08-14 15:43:18,651 DEBUG Query with image successful
2024-08-14 15:43:18,748 DEBUG Querying model with image: gpt-4o
2024-08-14 15:43:25,450 DEBUG Query with image successful
2024-08-14 15:43:25,719 DEBUG Querying model with image: gpt-4o
2024-08-14 15:43:34,030 DEBUG Query with image successful
2024-08-14 15:43:34,116 DEBUG Querying model with image: gpt-4o
2024-08-14 15:43:43,378 DEBUG Query with image successful
2024-08-14 15:43:43,680 DEBUG Querying model with image: gpt-4o
2024-08-14 15:43:51,801 DEBUG Query with image successful
2024-08-14 15:43:51,886 DEBUG Querying model with image: gpt-4o
2024-08-14 15:44:01,972 DEBUG Query with image successful
2024-08-14 15:44:02,244 DEBUG Querying model with image: gpt-4o
2024-08-14 15:44:10,482 DEBUG Query with image successful
2024-08-14 15:44:10,572 DEBUG Querying model with image: gpt-4o
2024-08-14 15:44:20,775 DEBUG Query with image successful
2024-08-14 15:44:21,096 DEBUG Querying model with image: gpt-4o
2024-08-14 15:44:35,799 DEBUG Query with image successful
2024-08-14 15:44:35,905 DEBUG Querying model with image: gpt-4o
2024-08-14 15:44:43,716 DEBUG Query with image successful
2024-08-14 15:44:44,001 DEBUG Querying model with image: gpt-4o
2024-08-14 15:44:52,732 DEBUG Query with image successful
2024-08-14 15:44:52,835 DEBUG Querying model with image: gpt-4o
2024-08-14 15:45:03,034 DEBUG Query with image successful
2024-08-14 15:45:03,357 DEBUG Querying model with image: gpt-4o
2024-08-14 15:45:12,303 DEBUG Query with image successful
2024-08-14 15:45:12,393 DEBUG Querying model with image: gpt-4o
2024-08-14 15:45:21,691 DEBUG Query with image successful
2024-08-14 15:51:57,042 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-14 15:51:57,069 DEBUG OpenAI client created
2024-08-14 15:52:02,243 DEBUG Querying model with image: gpt-4o
2024-08-14 15:52:13,377 DEBUG Query with image successful
2024-08-14 15:52:13,464 DEBUG Querying model with image: gpt-4o
2024-08-14 15:52:22,976 DEBUG Query with image successful
2024-08-14 15:52:23,270 DEBUG Querying model with image: gpt-4o
2024-08-14 15:52:32,482 DEBUG Query with image successful
2024-08-14 15:52:32,578 DEBUG Querying model with image: gpt-4o
2024-08-14 15:52:43,367 DEBUG Query with image successful
2024-08-14 15:52:53,687 DEBUG Querying model with image: gpt-4o
2024-08-14 15:53:02,537 DEBUG Query with image successful
2024-08-14 15:53:02,636 DEBUG Querying model with image: gpt-4o
2024-08-14 15:53:21,447 DEBUG Query with image successful
2024-08-14 15:53:21,763 DEBUG Querying model with image: gpt-4o
2024-08-14 15:53:30,246 DEBUG Query with image successful
2024-08-14 15:53:30,335 DEBUG Querying model with image: gpt-4o
2024-08-14 15:53:40,966 DEBUG Query with image successful
2024-08-14 15:53:41,324 DEBUG Querying model with image: gpt-4o
2024-08-14 15:53:52,599 DEBUG Query with image successful
2024-08-14 15:53:52,688 DEBUG Querying model with image: gpt-4o
2024-08-14 15:54:03,756 DEBUG Query with image successful
2024-08-14 15:54:04,141 DEBUG Querying model with image: gpt-4o
2024-08-14 15:54:13,389 DEBUG Query with image successful
2024-08-14 15:54:13,479 DEBUG Querying model with image: gpt-4o
2024-08-14 15:54:24,779 DEBUG Query with image successful
2024-08-14 15:54:25,092 DEBUG Querying model with image: gpt-4o
2024-08-14 15:54:33,353 DEBUG Query with image successful
2024-08-14 15:54:33,446 DEBUG Querying model with image: gpt-4o
2024-08-14 15:54:41,674 DEBUG Query with image successful
2024-08-14 15:54:41,992 DEBUG Querying model with image: gpt-4o
2024-08-14 15:54:49,170 DEBUG Query with image successful
2024-08-14 15:54:49,258 DEBUG Querying model with image: gpt-4o
2024-08-14 15:55:01,351 DEBUG Query with image successful
2024-08-14 15:55:01,680 DEBUG Querying model with image: gpt-4o
2024-08-14 15:55:11,627 DEBUG Query with image successful
2024-08-14 15:55:11,728 DEBUG Querying model with image: gpt-4o
2024-08-14 15:55:21,195 DEBUG Query with image successful
2024-08-14 15:55:21,499 DEBUG Querying model with image: gpt-4o
2024-08-14 15:55:32,103 DEBUG Query with image successful
2024-08-14 15:55:32,197 DEBUG Querying model with image: gpt-4o
2024-08-14 15:55:43,921 DEBUG Query with image successful
2024-08-14 15:55:44,305 DEBUG Querying model with image: gpt-4o
2024-08-14 15:55:56,130 DEBUG Query with image successful
2024-08-14 15:55:56,219 DEBUG Querying model with image: gpt-4o
2024-08-14 15:56:07,598 DEBUG Query with image successful
2024-08-14 15:56:07,935 DEBUG Querying model with image: gpt-4o
2024-08-14 15:56:16,399 DEBUG Query with image successful
2024-08-14 15:56:16,491 DEBUG Querying model with image: gpt-4o
2024-08-14 15:56:26,323 DEBUG Query with image successful
2024-08-14 15:56:26,637 DEBUG Querying model with image: gpt-4o
2024-08-14 15:56:36,475 DEBUG Query with image successful
2024-08-14 15:56:36,574 DEBUG Querying model with image: gpt-4o
2024-08-14 15:56:47,100 DEBUG Query with image successful
2024-08-14 15:56:47,421 DEBUG Querying model with image: gpt-4o
2024-08-14 15:56:55,526 DEBUG Query with image successful
2024-08-14 15:56:55,615 DEBUG Querying model with image: gpt-4o
2024-08-14 15:57:02,898 DEBUG Query with image successful
2024-08-14 15:57:03,193 DEBUG Querying model with image: gpt-4o
2024-08-14 15:57:12,566 DEBUG Query with image successful
2024-08-14 15:57:12,653 DEBUG Querying model with image: gpt-4o
2024-08-14 15:57:20,854 DEBUG Query with image successful
2024-08-14 15:57:21,140 DEBUG Querying model with image: gpt-4o
2024-08-14 15:57:30,524 DEBUG Query with image successful
2024-08-14 15:57:30,612 DEBUG Querying model with image: gpt-4o
2024-08-14 15:57:40,234 DEBUG Query with image successful
2024-08-14 15:57:40,555 DEBUG Querying model with image: gpt-4o
2024-08-14 15:57:50,046 DEBUG Query with image successful
2024-08-14 15:57:50,134 DEBUG Querying model with image: gpt-4o
2024-08-14 15:57:59,327 DEBUG Query with image successful
2024-08-14 15:57:59,613 DEBUG Querying model with image: gpt-4o
2024-08-14 15:58:09,084 DEBUG Query with image successful
2024-08-14 15:58:09,172 DEBUG Querying model with image: gpt-4o
2024-08-14 15:58:17,713 DEBUG Query with image successful
2024-08-14 15:58:18,015 DEBUG Querying model with image: gpt-4o
2024-08-14 15:58:30,455 DEBUG Query with image successful
2024-08-14 15:58:30,554 DEBUG Querying model with image: gpt-4o
2024-08-14 15:58:38,310 DEBUG Query with image successful
2024-08-14 15:58:38,584 DEBUG Querying model with image: gpt-4o
2024-08-14 15:58:50,621 DEBUG Query with image successful
2024-08-14 15:58:50,707 DEBUG Querying model with image: gpt-4o
2024-08-14 15:59:05,645 DEBUG Query with image successful
2024-08-14 15:59:05,920 DEBUG Querying model with image: gpt-4o
2024-08-14 15:59:14,660 DEBUG Query with image successful
2024-08-14 15:59:14,746 DEBUG Querying model with image: gpt-4o
2024-08-14 15:59:21,974 DEBUG Query with image successful
2024-08-14 15:59:22,247 DEBUG Querying model with image: gpt-4o
2024-08-14 15:59:31,305 DEBUG Query with image successful
2024-08-14 15:59:31,390 DEBUG Querying model with image: gpt-4o
2024-08-14 15:59:42,357 DEBUG Query with image successful
2024-08-14 15:59:42,640 DEBUG Querying model with image: gpt-4o
2024-08-14 15:59:51,968 DEBUG Query with image successful
2024-08-14 15:59:52,055 DEBUG Querying model with image: gpt-4o
2024-08-14 16:00:00,500 DEBUG Query with image successful
2024-08-14 16:00:00,777 DEBUG Querying model with image: gpt-4o
2024-08-14 16:00:10,190 DEBUG Query with image successful
2024-08-14 16:00:10,277 DEBUG Querying model with image: gpt-4o
2024-08-14 16:00:18,918 DEBUG Query with image successful
2024-08-14 16:00:19,199 DEBUG Querying model with image: gpt-4o
2024-08-14 16:00:29,864 DEBUG Query with image successful
2024-08-14 16:00:29,951 DEBUG Querying model with image: gpt-4o
2024-08-14 16:00:37,971 DEBUG Query with image successful
2024-08-14 16:05:58,632 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-14 16:05:58,657 DEBUG OpenAI client created
2024-08-14 16:06:03,824 DEBUG Querying model with image: gpt-4o
2024-08-14 16:06:13,783 DEBUG Query with image successful
2024-08-14 16:06:13,866 DEBUG Querying model with image: gpt-4o
2024-08-14 16:06:24,371 DEBUG Query with image successful
2024-08-14 16:06:24,648 DEBUG Querying model with image: gpt-4o
2024-08-14 16:06:35,123 DEBUG Query with image successful
2024-08-14 16:06:35,243 DEBUG Querying model with image: gpt-4o
2024-08-14 16:06:45,928 DEBUG Query with image successful
2024-08-14 16:06:56,320 DEBUG Querying model with image: gpt-4o
2024-08-14 16:07:07,203 DEBUG Query with image successful
2024-08-14 16:07:07,310 DEBUG Querying model with image: gpt-4o
2024-08-14 16:07:18,482 DEBUG Query with image successful
2024-08-14 16:07:18,820 DEBUG Querying model with image: gpt-4o
2024-08-14 16:07:28,836 DEBUG Query with image successful
2024-08-14 16:07:28,922 DEBUG Querying model with image: gpt-4o
2024-08-14 16:07:43,053 DEBUG Query with image successful
2024-08-14 16:07:43,454 DEBUG Querying model with image: gpt-4o
2024-08-14 16:07:54,662 DEBUG Query with image successful
2024-08-14 16:07:54,751 DEBUG Querying model with image: gpt-4o
2024-08-14 16:08:12,273 DEBUG Query with image successful
2024-08-14 16:08:12,670 DEBUG Querying model with image: gpt-4o
2024-08-14 16:08:23,455 DEBUG Query with image successful
2024-08-14 16:08:23,542 DEBUG Querying model with image: gpt-4o
2024-08-14 16:08:34,354 DEBUG Query with image successful
2024-08-14 16:08:34,671 DEBUG Querying model with image: gpt-4o
2024-08-14 16:08:45,977 DEBUG Query with image successful
2024-08-14 16:08:46,065 DEBUG Querying model with image: gpt-4o
2024-08-14 16:08:56,974 DEBUG Query with image successful
2024-08-14 16:08:57,309 DEBUG Querying model with image: gpt-4o
2024-08-14 16:09:07,175 DEBUG Query with image successful
2024-08-14 16:09:07,279 DEBUG Querying model with image: gpt-4o
2024-08-14 16:09:15,412 DEBUG Query with image successful
2024-08-14 16:09:15,751 DEBUG Querying model with image: gpt-4o
2024-08-14 16:09:25,280 DEBUG Query with image successful
2024-08-14 16:09:25,381 DEBUG Querying model with image: gpt-4o
2024-08-14 16:09:37,398 DEBUG Query with image successful
2024-08-14 16:09:37,719 DEBUG Querying model with image: gpt-4o
2024-08-14 16:09:50,044 DEBUG Query with image successful
2024-08-14 16:09:50,147 DEBUG Querying model with image: gpt-4o
2024-08-14 16:10:01,961 DEBUG Query with image successful
2024-08-14 16:10:02,281 DEBUG Querying model with image: gpt-4o
2024-08-14 16:10:12,461 DEBUG Query with image successful
2024-08-14 16:10:12,555 DEBUG Querying model with image: gpt-4o
2024-08-14 16:10:25,284 DEBUG Query with image successful
2024-08-14 16:10:25,589 DEBUG Querying model with image: gpt-4o
2024-08-14 16:10:35,695 DEBUG Query with image successful
2024-08-14 16:10:35,790 DEBUG Querying model with image: gpt-4o
2024-08-14 16:10:46,985 DEBUG Query with image successful
2024-08-14 16:10:47,351 DEBUG Querying model with image: gpt-4o
2024-08-14 16:10:57,652 DEBUG Query with image successful
2024-08-14 16:10:57,750 DEBUG Querying model with image: gpt-4o
2024-08-14 16:11:09,872 DEBUG Query with image successful
2024-08-14 16:11:10,219 DEBUG Querying model with image: gpt-4o
2024-08-14 16:11:19,953 DEBUG Query with image successful
2024-08-14 16:11:20,044 DEBUG Querying model with image: gpt-4o
2024-08-14 16:11:27,730 DEBUG Query with image successful
2024-08-14 16:11:28,076 DEBUG Querying model with image: gpt-4o
2024-08-14 16:11:38,229 DEBUG Query with image successful
2024-08-14 16:11:38,320 DEBUG Querying model with image: gpt-4o
2024-08-14 16:11:58,075 DEBUG Query with image successful
2024-08-14 16:11:58,406 DEBUG Querying model with image: gpt-4o
2024-08-14 16:12:08,535 DEBUG Query with image successful
2024-08-14 16:12:08,630 DEBUG Querying model with image: gpt-4o
2024-08-14 16:12:20,615 DEBUG Query with image successful
2024-08-14 16:12:20,915 DEBUG Querying model with image: gpt-4o
2024-08-14 16:12:35,988 DEBUG Query with image successful
2024-08-14 16:12:36,091 DEBUG Querying model with image: gpt-4o
2024-08-14 16:12:44,876 DEBUG Query with image successful
2024-08-14 16:12:45,170 DEBUG Querying model with image: gpt-4o
2024-08-14 16:12:54,721 DEBUG Query with image successful
2024-08-14 16:12:54,819 DEBUG Querying model with image: gpt-4o
2024-08-14 16:13:04,210 DEBUG Query with image successful
2024-08-14 16:13:04,487 DEBUG Querying model with image: gpt-4o
2024-08-14 16:13:25,874 DEBUG Query with image successful
2024-08-14 16:13:25,969 DEBUG Querying model with image: gpt-4o
2024-08-14 16:13:34,935 DEBUG Query with image successful
2024-08-14 16:13:35,238 DEBUG Querying model with image: gpt-4o
2024-08-14 16:13:44,605 DEBUG Query with image successful
2024-08-14 16:13:44,700 DEBUG Querying model with image: gpt-4o
2024-08-14 16:13:53,644 DEBUG Query with image successful
2024-08-14 16:13:53,914 DEBUG Querying model with image: gpt-4o
2024-08-14 16:14:01,848 DEBUG Query with image successful
2024-08-14 16:14:01,934 DEBUG Querying model with image: gpt-4o
2024-08-14 16:14:14,900 DEBUG Query with image successful
2024-08-14 16:14:15,173 DEBUG Querying model with image: gpt-4o
2024-08-14 16:14:25,280 DEBUG Query with image successful
2024-08-14 16:14:25,366 DEBUG Querying model with image: gpt-4o
2024-08-14 16:14:36,029 DEBUG Query with image successful
2024-08-14 16:14:36,484 DEBUG Querying model with image: gpt-4o
2024-08-14 16:14:44,279 DEBUG Query with image successful
2024-08-14 16:14:44,378 DEBUG Querying model with image: gpt-4o
2024-08-14 16:14:56,453 DEBUG Query with image successful
2024-08-14 16:14:56,763 DEBUG Querying model with image: gpt-4o
2024-08-14 16:15:08,925 DEBUG Query with image successful
2024-08-14 16:15:09,014 DEBUG Querying model with image: gpt-4o
2024-08-14 16:15:20,943 DEBUG Query with image successful
2024-08-14 16:15:21,247 DEBUG Querying model with image: gpt-4o
2024-08-14 16:15:30,532 DEBUG Query with image successful
2024-08-14 16:15:30,623 DEBUG Querying model with image: gpt-4o
2024-08-14 16:15:40,530 DEBUG Query with image successful
2024-08-14 16:18:18,658 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-14 16:18:18,685 DEBUG OpenAI client created
2024-08-14 16:18:23,870 DEBUG Querying model with image: gpt-4o
2024-08-14 16:18:32,703 DEBUG Query with image successful
2024-08-14 16:18:32,796 DEBUG Querying model with image: gpt-4o
2024-08-14 16:18:44,158 DEBUG Query with image successful
2024-08-14 16:18:44,461 DEBUG Querying model with image: gpt-4o
2024-08-14 16:18:53,511 DEBUG Query with image successful
2024-08-14 16:18:53,600 DEBUG Querying model with image: gpt-4o
2024-08-14 16:19:03,980 DEBUG Query with image successful
2024-08-14 16:19:14,313 DEBUG Querying model with image: gpt-4o
2024-08-14 16:19:25,450 DEBUG Query with image successful
2024-08-14 16:19:25,536 DEBUG Querying model with image: gpt-4o
2024-08-14 16:19:36,745 DEBUG Query with image successful
2024-08-14 16:19:37,143 DEBUG Querying model with image: gpt-4o
2024-08-14 16:19:46,849 DEBUG Query with image successful
2024-08-14 16:19:46,943 DEBUG Querying model with image: gpt-4o
2024-08-14 16:20:00,515 DEBUG Query with image successful
2024-08-14 16:20:00,883 DEBUG Querying model with image: gpt-4o
2024-08-14 16:20:12,653 DEBUG Query with image successful
2024-08-14 16:20:12,741 DEBUG Querying model with image: gpt-4o
2024-08-14 16:20:25,012 DEBUG Query with image successful
2024-08-14 16:20:25,335 DEBUG Querying model with image: gpt-4o
2024-08-14 16:20:36,385 DEBUG Query with image successful
2024-08-14 16:20:36,494 DEBUG Querying model with image: gpt-4o
2024-08-14 16:20:47,071 DEBUG Query with image successful
2024-08-14 16:20:47,419 DEBUG Querying model with image: gpt-4o
2024-08-14 16:20:58,790 DEBUG Query with image successful
2024-08-14 16:20:58,879 DEBUG Querying model with image: gpt-4o
2024-08-14 16:21:10,400 DEBUG Query with image successful
2024-08-14 16:21:10,702 DEBUG Querying model with image: gpt-4o
2024-08-14 16:21:24,872 DEBUG Query with image successful
2024-08-14 16:21:24,967 DEBUG Querying model with image: gpt-4o
2024-08-14 16:21:36,116 DEBUG Query with image successful
2024-08-14 16:21:36,430 DEBUG Querying model with image: gpt-4o
2024-08-14 16:21:46,509 DEBUG Query with image successful
2024-08-14 16:21:46,609 DEBUG Querying model with image: gpt-4o
2024-08-14 16:22:00,478 DEBUG Query with image successful
2024-08-14 16:22:00,769 DEBUG Querying model with image: gpt-4o
2024-08-14 16:22:11,651 DEBUG Query with image successful
2024-08-14 16:22:11,739 DEBUG Querying model with image: gpt-4o
2024-08-14 16:22:23,963 DEBUG Query with image successful
2024-08-14 16:22:24,263 DEBUG Querying model with image: gpt-4o
2024-08-14 16:22:34,172 DEBUG Query with image successful
2024-08-14 16:22:34,259 DEBUG Querying model with image: gpt-4o
2024-08-14 16:22:42,519 DEBUG Query with image successful
2024-08-14 16:22:42,819 DEBUG Querying model with image: gpt-4o
2024-08-14 16:22:54,969 DEBUG Query with image successful
2024-08-14 16:22:55,056 DEBUG Querying model with image: gpt-4o
2024-08-14 16:23:03,777 DEBUG Query with image successful
2024-08-14 16:23:04,085 DEBUG Querying model with image: gpt-4o
2024-08-14 16:23:16,664 DEBUG Query with image successful
2024-08-14 16:23:16,768 DEBUG Querying model with image: gpt-4o
2024-08-14 16:23:26,238 DEBUG Query with image successful
2024-08-14 16:23:26,547 DEBUG Querying model with image: gpt-4o
2024-08-14 16:23:36,369 DEBUG Query with image successful
2024-08-14 16:23:36,468 DEBUG Querying model with image: gpt-4o
2024-08-14 16:23:46,857 DEBUG Query with image successful
2024-08-14 16:23:47,227 DEBUG Querying model with image: gpt-4o
2024-08-14 16:23:56,223 DEBUG Query with image successful
2024-08-14 16:23:56,311 DEBUG Querying model with image: gpt-4o
2024-08-14 16:24:06,663 DEBUG Query with image successful
2024-08-14 16:24:06,971 DEBUG Querying model with image: gpt-4o
2024-08-14 16:24:14,470 DEBUG Query with image successful
2024-08-14 16:24:14,567 DEBUG Querying model with image: gpt-4o
2024-08-14 16:24:22,510 DEBUG Query with image successful
2024-08-14 16:24:22,814 DEBUG Querying model with image: gpt-4o
2024-08-14 16:24:32,704 DEBUG Query with image successful
2024-08-14 16:24:32,801 DEBUG Querying model with image: gpt-4o
2024-08-14 16:24:42,496 DEBUG Query with image successful
2024-08-14 16:24:42,792 DEBUG Querying model with image: gpt-4o
2024-08-14 16:24:54,782 DEBUG Query with image successful
2024-08-14 16:24:54,876 DEBUG Querying model with image: gpt-4o
2024-08-14 16:25:02,674 DEBUG Query with image successful
2024-08-14 16:25:02,950 DEBUG Querying model with image: gpt-4o
2024-08-14 16:25:13,201 DEBUG Query with image successful
2024-08-14 16:25:13,293 DEBUG Querying model with image: gpt-4o
2024-08-14 16:25:22,931 DEBUG Query with image successful
2024-08-14 16:25:23,194 DEBUG Querying model with image: gpt-4o
2024-08-14 16:25:31,697 DEBUG Query with image successful
2024-08-14 16:25:31,780 DEBUG Querying model with image: gpt-4o
2024-08-14 16:25:40,828 DEBUG Query with image successful
2024-08-14 16:25:41,095 DEBUG Querying model with image: gpt-4o
2024-08-14 16:25:49,776 DEBUG Query with image successful
2024-08-14 16:25:49,859 DEBUG Querying model with image: gpt-4o
2024-08-14 16:25:59,011 DEBUG Query with image successful
2024-08-14 16:25:59,285 DEBUG Querying model with image: gpt-4o
2024-08-14 16:26:08,498 DEBUG Query with image successful
2024-08-14 16:26:08,581 DEBUG Querying model with image: gpt-4o
2024-08-14 16:26:16,400 DEBUG Query with image successful
2024-08-14 16:26:16,678 DEBUG Querying model with image: gpt-4o
2024-08-14 16:26:26,655 DEBUG Query with image successful
2024-08-14 16:26:26,742 DEBUG Querying model with image: gpt-4o
2024-08-14 16:26:35,246 DEBUG Query with image successful
2024-08-14 16:26:35,515 DEBUG Querying model with image: gpt-4o
2024-08-14 16:26:45,457 DEBUG Query with image successful
2024-08-14 16:26:45,539 DEBUG Querying model with image: gpt-4o
2024-08-14 16:26:54,476 DEBUG Query with image successful
2024-08-14 16:26:54,742 DEBUG Querying model with image: gpt-4o
2024-08-14 16:27:07,315 DEBUG Query with image successful
2024-08-14 16:27:07,398 DEBUG Querying model with image: gpt-4o
2024-08-14 16:27:15,750 DEBUG Query with image successful
2024-08-19 13:00:38,321 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 13:00:38,345 DEBUG OpenAI client created
2024-08-19 13:00:43,540 DEBUG Querying model with image: gpt-4o
2024-08-19 13:00:51,704 DEBUG Query with image successful
2024-08-19 13:00:51,798 DEBUG Querying model with image: gpt-4o
2024-08-19 13:00:59,463 DEBUG Query with image successful
2024-08-19 13:00:59,747 DEBUG Querying model with image: gpt-4o
2024-08-19 13:01:07,007 DEBUG Query with image successful
2024-08-19 13:01:07,099 DEBUG Querying model with image: gpt-4o
2024-08-19 13:01:20,745 DEBUG Query with image successful
2024-08-19 13:01:31,051 DEBUG Querying model with image: gpt-4o
2024-08-19 13:01:42,414 DEBUG Query with image successful
2024-08-19 13:01:42,511 DEBUG Querying model with image: gpt-4o
2024-08-19 13:01:51,072 DEBUG Query with image successful
2024-08-19 13:01:51,861 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:00,374 DEBUG Query with image successful
2024-08-19 13:02:00,473 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:08,766 DEBUG Query with image successful
2024-08-19 13:02:09,099 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:15,610 DEBUG Query with image successful
2024-08-19 13:02:15,717 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:24,935 DEBUG Query with image successful
2024-08-19 13:02:25,298 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:32,778 DEBUG Query with image successful
2024-08-19 13:02:32,878 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:40,545 DEBUG Query with image successful
2024-08-19 13:02:40,919 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:48,373 DEBUG Query with image successful
2024-08-19 13:02:48,465 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:57,178 DEBUG Query with image successful
2024-08-19 13:02:57,489 DEBUG Querying model with image: gpt-4o
2024-08-19 13:03:05,252 DEBUG Query with image successful
2024-08-19 13:03:05,344 DEBUG Querying model with image: gpt-4o
2024-08-19 13:03:13,960 DEBUG Query with image successful
2024-08-19 13:03:14,273 DEBUG Querying model with image: gpt-4o
2024-08-19 13:03:22,129 DEBUG Query with image successful
2024-08-19 13:03:22,221 DEBUG Querying model with image: gpt-4o
2024-08-19 13:03:29,589 DEBUG Query with image successful
2024-08-19 13:03:29,955 DEBUG Querying model with image: gpt-4o
2024-08-19 13:03:36,311 DEBUG Query with image successful
2024-08-19 13:03:36,410 DEBUG Querying model with image: gpt-4o
2024-08-19 13:03:44,106 DEBUG Query with image successful
2024-08-19 13:03:44,427 DEBUG Querying model with image: gpt-4o
2024-08-19 13:03:52,306 DEBUG Query with image successful
2024-08-19 13:03:52,405 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:01,800 DEBUG Query with image successful
2024-08-19 13:04:02,132 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:09,111 DEBUG Query with image successful
2024-08-19 13:04:09,207 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:16,770 DEBUG Query with image successful
2024-08-19 13:04:17,148 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:23,655 DEBUG Query with image successful
2024-08-19 13:04:23,754 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:32,874 DEBUG Query with image successful
2024-08-19 13:04:33,180 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:41,916 DEBUG Query with image successful
2024-08-19 13:04:42,012 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:49,649 DEBUG Query with image successful
2024-08-19 13:04:49,955 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:57,029 DEBUG Query with image successful
2024-08-19 13:04:57,126 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:05,701 DEBUG Query with image successful
2024-08-19 13:05:05,997 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:14,033 DEBUG Query with image successful
2024-08-19 13:05:14,128 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:21,882 DEBUG Query with image successful
2024-08-19 13:05:22,176 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:28,709 DEBUG Query with image successful
2024-08-19 13:05:28,803 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:36,603 DEBUG Query with image successful
2024-08-19 13:05:36,878 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:44,443 DEBUG Query with image successful
2024-08-19 13:05:44,533 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:51,593 DEBUG Query with image successful
2024-08-19 13:05:51,881 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:59,431 DEBUG Query with image successful
2024-08-19 13:05:59,523 DEBUG Querying model with image: gpt-4o
2024-08-19 13:06:06,343 DEBUG Query with image successful
2024-08-19 13:06:06,626 DEBUG Querying model with image: gpt-4o
2024-08-19 13:06:13,005 DEBUG Query with image successful
2024-08-19 13:06:13,098 DEBUG Querying model with image: gpt-4o
2024-08-19 13:06:21,466 DEBUG Query with image successful
2024-08-19 13:06:21,750 DEBUG Querying model with image: gpt-4o
2024-08-19 13:06:28,332 DEBUG Query with image successful
2024-08-19 13:06:28,422 DEBUG Querying model with image: gpt-4o
2024-08-19 13:06:35,175 DEBUG Query with image successful
2024-08-19 13:06:35,441 DEBUG Querying model with image: gpt-4o
2024-08-19 13:06:43,213 DEBUG Query with image successful
2024-08-19 13:06:43,306 DEBUG Querying model with image: gpt-4o
2024-08-19 13:06:51,069 DEBUG Query with image successful
2024-08-19 13:06:51,366 DEBUG Querying model with image: gpt-4o
2024-08-19 13:07:00,866 DEBUG Query with image successful
2024-08-19 13:07:00,968 DEBUG Querying model with image: gpt-4o
2024-08-19 13:07:07,924 DEBUG Query with image successful
2024-08-19 13:07:08,205 DEBUG Querying model with image: gpt-4o
2024-08-19 13:07:15,629 DEBUG Query with image successful
2024-08-19 13:07:15,730 DEBUG Querying model with image: gpt-4o
2024-08-19 13:07:23,067 DEBUG Query with image successful
2024-08-19 13:07:23,349 DEBUG Querying model with image: gpt-4o
2024-08-19 13:07:34,848 DEBUG Query with image successful
2024-08-19 13:07:34,940 DEBUG Querying model with image: gpt-4o
2024-08-19 13:07:42,045 DEBUG Query with image successful
2024-08-19 13:41:26,214 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 13:41:26,241 DEBUG OpenAI client created
2024-08-19 13:41:31,418 DEBUG Querying model with image: gpt-4o
2024-08-19 13:41:40,408 DEBUG Query with image successful
2024-08-19 13:41:40,508 DEBUG Querying model with image: gpt-4o
2024-08-19 13:41:50,453 DEBUG Query with image successful
2024-08-19 13:41:50,792 DEBUG Querying model with image: gpt-4o
2024-08-19 13:41:58,327 DEBUG Query with image successful
2024-08-19 13:41:58,424 DEBUG Querying model with image: gpt-4o
2024-08-19 13:42:05,286 DEBUG Query with image successful
2024-08-19 13:42:15,659 DEBUG Querying model with image: gpt-4o
2024-08-19 13:42:26,287 DEBUG Query with image successful
2024-08-19 13:42:26,393 DEBUG Querying model with image: gpt-4o
2024-08-19 13:42:36,414 DEBUG Query with image successful
2024-08-19 13:42:36,755 DEBUG Querying model with image: gpt-4o
2024-08-19 13:42:45,176 DEBUG Query with image successful
2024-08-19 13:42:45,282 DEBUG Querying model with image: gpt-4o
2024-08-19 13:42:52,885 DEBUG Query with image successful
2024-08-19 13:42:53,302 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:00,502 DEBUG Query with image successful
2024-08-19 13:43:00,606 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:09,265 DEBUG Query with image successful
2024-08-19 13:43:09,611 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:19,271 DEBUG Query with image successful
2024-08-19 13:43:19,388 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:27,417 DEBUG Query with image successful
2024-08-19 13:43:27,810 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:34,575 DEBUG Query with image successful
2024-08-19 13:43:34,683 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:41,173 DEBUG Query with image successful
2024-08-19 13:43:41,497 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:48,235 DEBUG Query with image successful
2024-08-19 13:43:48,334 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:54,942 DEBUG Query with image successful
2024-08-19 13:43:55,276 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:01,935 DEBUG Query with image successful
2024-08-19 13:44:02,047 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:09,405 DEBUG Query with image successful
2024-08-19 13:44:09,727 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:17,227 DEBUG Query with image successful
2024-08-19 13:44:17,327 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:24,967 DEBUG Query with image successful
2024-08-19 13:44:25,396 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:33,072 DEBUG Query with image successful
2024-08-19 13:44:33,175 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:42,955 DEBUG Query with image successful
2024-08-19 13:44:43,283 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:50,171 DEBUG Query with image successful
2024-08-19 13:44:50,280 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:57,670 DEBUG Query with image successful
2024-08-19 13:44:57,992 DEBUG Querying model with image: gpt-4o
2024-08-19 13:45:05,310 DEBUG Query with image successful
2024-08-19 13:45:05,416 DEBUG Querying model with image: gpt-4o
2024-08-19 13:45:15,746 DEBUG Query with image successful
2024-08-19 13:45:16,074 DEBUG Querying model with image: gpt-4o
2024-08-19 13:45:22,355 DEBUG Query with image successful
2024-08-19 13:45:22,461 DEBUG Querying model with image: gpt-4o
2024-08-19 13:45:30,757 DEBUG Query with image successful
2024-08-19 13:45:31,141 DEBUG Querying model with image: gpt-4o
2024-08-19 13:45:37,932 DEBUG Query with image successful
2024-08-19 13:45:38,034 DEBUG Querying model with image: gpt-4o
2024-08-19 13:45:46,913 DEBUG Query with image successful
2024-08-19 13:45:47,262 DEBUG Querying model with image: gpt-4o
2024-08-19 13:45:54,072 DEBUG Query with image successful
2024-08-19 13:45:54,169 DEBUG Querying model with image: gpt-4o
2024-08-19 13:46:02,299 DEBUG Query with image successful
2024-08-19 13:46:02,636 DEBUG Querying model with image: gpt-4o
2024-08-19 13:46:12,234 DEBUG Query with image successful
2024-08-19 13:46:12,335 DEBUG Querying model with image: gpt-4o
2024-08-19 13:46:19,967 DEBUG Query with image successful
2024-08-19 13:46:20,296 DEBUG Querying model with image: gpt-4o
2024-08-19 13:46:29,315 DEBUG Query with image successful
2024-08-19 13:46:29,417 DEBUG Querying model with image: gpt-4o
2024-08-19 13:46:36,094 DEBUG Query with image successful
2024-08-19 13:46:36,405 DEBUG Querying model with image: gpt-4o
2024-08-19 13:46:43,935 DEBUG Query with image successful
2024-08-19 13:46:44,041 DEBUG Querying model with image: gpt-4o
2024-08-19 13:46:52,454 DEBUG Query with image successful
2024-08-19 13:46:52,783 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:00,915 DEBUG Query with image successful
2024-08-19 13:47:01,012 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:08,029 DEBUG Query with image successful
2024-08-19 13:47:08,352 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:15,313 DEBUG Query with image successful
2024-08-19 13:47:15,414 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:22,633 DEBUG Query with image successful
2024-08-19 13:47:22,953 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:31,554 DEBUG Query with image successful
2024-08-19 13:47:31,659 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:39,047 DEBUG Query with image successful
2024-08-19 13:47:39,350 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:46,576 DEBUG Query with image successful
2024-08-19 13:47:46,674 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:53,212 DEBUG Query with image successful
2024-08-19 13:47:53,500 DEBUG Querying model with image: gpt-4o
2024-08-19 13:48:00,359 DEBUG Query with image successful
2024-08-19 13:48:00,454 DEBUG Querying model with image: gpt-4o
2024-08-19 13:48:07,726 DEBUG Query with image successful
2024-08-19 13:48:08,010 DEBUG Querying model with image: gpt-4o
2024-08-19 13:48:16,873 DEBUG Query with image successful
2024-08-19 13:48:16,966 DEBUG Querying model with image: gpt-4o
2024-08-19 13:48:25,654 DEBUG Query with image successful
2024-08-19 13:59:29,281 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 13:59:29,308 DEBUG OpenAI client created
2024-08-19 13:59:34,482 DEBUG Querying model with image: gpt-4o
2024-08-19 13:59:44,475 DEBUG Query with image successful
2024-08-19 14:01:53,225 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 14:01:53,253 DEBUG OpenAI client created
2024-08-19 14:01:58,420 DEBUG Querying model with image: gpt-4o
2024-08-19 14:02:07,631 DEBUG Query with image successful
2024-08-19 14:02:07,731 DEBUG Querying model with image: gpt-4o
2024-08-19 14:02:16,940 DEBUG Query with image successful
2024-08-19 14:06:45,449 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 14:06:45,474 DEBUG OpenAI client created
2024-08-19 14:06:50,519 DEBUG Querying model with image: gpt-4o
2024-08-19 14:06:55,394 DEBUG Query with image successful
2024-08-19 14:06:55,422 DEBUG Querying model with image: gpt-4o
2024-08-19 14:07:00,674 DEBUG Query with image successful
2024-08-19 14:11:40,008 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 14:11:40,032 DEBUG OpenAI client created
2024-08-19 14:11:45,204 DEBUG Querying model with image: gpt-4o
2024-08-19 14:11:55,239 DEBUG Query with image successful
2024-08-19 14:11:55,330 DEBUG Querying model with image: gpt-4o
2024-08-19 14:12:03,384 DEBUG Query with image successful
2024-08-19 14:12:03,692 DEBUG Querying model with image: gpt-4o
2024-08-19 14:12:12,772 DEBUG Query with image successful
2024-08-19 14:12:12,864 DEBUG Querying model with image: gpt-4o
2024-08-19 14:12:22,362 DEBUG Query with image successful
2024-08-19 14:14:23,351 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 14:14:23,379 DEBUG OpenAI client created
2024-08-19 14:14:28,543 DEBUG Querying model with image: gpt-4o
2024-08-19 14:14:36,849 DEBUG Query with image successful
2024-08-19 14:14:36,942 DEBUG Querying model with image: gpt-4o
2024-08-19 14:14:45,403 DEBUG Query with image successful
2024-08-19 14:20:00,171 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 14:20:00,204 DEBUG OpenAI client created
2024-08-19 14:20:05,378 DEBUG Querying model with image: gpt-4o
2024-08-19 14:20:16,960 DEBUG Query with image successful
2024-08-19 14:20:17,061 DEBUG Querying model with image: gpt-4o
2024-08-19 14:20:27,252 DEBUG Query with image successful
2024-08-19 14:20:27,566 DEBUG Querying model with image: gpt-4o
2024-08-19 14:20:36,028 DEBUG Query with image successful
2024-08-19 14:20:36,129 DEBUG Querying model with image: gpt-4o
2024-08-19 14:20:46,604 DEBUG Query with image successful
2024-08-19 14:20:56,911 DEBUG Querying model with image: gpt-4o
2024-08-19 14:21:06,936 DEBUG Query with image successful
2024-08-19 14:21:07,039 DEBUG Querying model with image: gpt-4o
2024-08-19 14:21:16,107 DEBUG Query with image successful
2024-08-19 14:21:16,438 DEBUG Querying model with image: gpt-4o
2024-08-19 14:21:23,872 DEBUG Query with image successful
2024-08-19 14:21:23,971 DEBUG Querying model with image: gpt-4o
2024-08-19 14:21:31,647 DEBUG Query with image successful
2024-08-19 14:21:31,995 DEBUG Querying model with image: gpt-4o
2024-08-19 14:21:39,714 DEBUG Query with image successful
2024-08-19 14:21:39,819 DEBUG Querying model with image: gpt-4o
2024-08-19 14:21:48,572 DEBUG Query with image successful
2024-08-19 14:21:48,963 DEBUG Querying model with image: gpt-4o
2024-08-19 14:21:59,118 DEBUG Query with image successful
2024-08-19 14:21:59,218 DEBUG Querying model with image: gpt-4o
2024-08-19 14:22:06,827 DEBUG Query with image successful
2024-08-19 14:22:07,167 DEBUG Querying model with image: gpt-4o
2024-08-19 14:22:14,486 DEBUG Query with image successful
2024-08-19 14:22:14,589 DEBUG Querying model with image: gpt-4o
2024-08-19 14:22:21,658 DEBUG Query with image successful
2024-08-19 14:22:21,971 DEBUG Querying model with image: gpt-4o
2024-08-19 14:22:29,258 DEBUG Query with image successful
2024-08-19 14:22:29,365 DEBUG Querying model with image: gpt-4o
2024-08-19 14:22:38,380 DEBUG Query with image successful
2024-08-19 14:22:38,799 DEBUG Querying model with image: gpt-4o
2024-08-19 14:22:47,598 DEBUG Query with image successful
2024-08-19 14:22:47,706 DEBUG Querying model with image: gpt-4o
2024-08-19 14:22:54,509 DEBUG Query with image successful
2024-08-19 14:22:54,846 DEBUG Querying model with image: gpt-4o
2024-08-19 14:23:00,780 DEBUG Query with image successful
2024-08-19 14:23:00,881 DEBUG Querying model with image: gpt-4o
2024-08-19 14:23:11,190 DEBUG Query with image successful
2024-08-19 14:23:11,518 DEBUG Querying model with image: gpt-4o
2024-08-19 14:23:20,714 DEBUG Query with image successful
2024-08-19 14:23:20,813 DEBUG Querying model with image: gpt-4o
2024-08-19 14:23:29,031 DEBUG Query with image successful
2024-08-19 14:23:29,386 DEBUG Querying model with image: gpt-4o
2024-08-19 14:23:36,836 DEBUG Query with image successful
2024-08-19 14:23:36,941 DEBUG Querying model with image: gpt-4o
2024-08-19 14:23:44,862 DEBUG Query with image successful
2024-08-19 14:23:45,184 DEBUG Querying model with image: gpt-4o
2024-08-19 14:24:36,809 DEBUG Query with image successful
2024-08-19 14:24:36,909 DEBUG Querying model with image: gpt-4o
2024-08-19 14:24:50,926 DEBUG Query with image successful
2024-08-19 14:24:51,244 DEBUG Querying model with image: gpt-4o
2024-08-19 14:24:58,314 DEBUG Query with image successful
2024-08-19 14:24:58,423 DEBUG Querying model with image: gpt-4o
2024-08-19 14:25:05,840 DEBUG Query with image successful
2024-08-19 14:25:06,160 DEBUG Querying model with image: gpt-4o
2024-08-19 14:25:15,283 DEBUG Query with image successful
2024-08-19 14:25:15,386 DEBUG Querying model with image: gpt-4o
2024-08-19 14:25:23,188 DEBUG Query with image successful
2024-08-19 14:25:23,502 DEBUG Querying model with image: gpt-4o
2024-08-19 14:25:32,707 DEBUG Query with image successful
2024-08-19 14:25:32,809 DEBUG Querying model with image: gpt-4o
2024-08-19 14:25:41,359 DEBUG Query with image successful
2024-08-19 14:25:41,694 DEBUG Querying model with image: gpt-4o
2024-08-19 14:25:50,698 DEBUG Query with image successful
2024-08-19 14:25:50,811 DEBUG Querying model with image: gpt-4o
2024-08-19 14:25:58,451 DEBUG Query with image successful
2024-08-19 14:25:58,766 DEBUG Querying model with image: gpt-4o
2024-08-19 14:26:06,720 DEBUG Query with image successful
2024-08-19 14:26:06,825 DEBUG Querying model with image: gpt-4o
2024-08-19 14:26:13,908 DEBUG Query with image successful
2024-08-19 14:26:14,229 DEBUG Querying model with image: gpt-4o
2024-08-19 14:26:24,333 DEBUG Query with image successful
2024-08-19 14:26:24,433 DEBUG Querying model with image: gpt-4o
2024-08-19 14:26:32,254 DEBUG Query with image successful
2024-08-19 14:26:32,564 DEBUG Querying model with image: gpt-4o
2024-08-19 14:26:41,839 DEBUG Query with image successful
2024-08-19 14:26:41,942 DEBUG Querying model with image: gpt-4o
2024-08-19 14:26:48,783 DEBUG Query with image successful
2024-08-19 14:26:49,085 DEBUG Querying model with image: gpt-4o
2024-08-19 14:26:57,715 DEBUG Query with image successful
2024-08-19 14:26:57,822 DEBUG Querying model with image: gpt-4o
2024-08-19 14:27:05,507 DEBUG Query with image successful
2024-08-19 14:27:05,800 DEBUG Querying model with image: gpt-4o
2024-08-19 14:27:13,266 DEBUG Query with image successful
2024-08-19 14:27:13,361 DEBUG Querying model with image: gpt-4o
2024-08-19 14:27:20,934 DEBUG Query with image successful
2024-08-19 14:27:21,223 DEBUG Querying model with image: gpt-4o
2024-08-19 14:27:27,974 DEBUG Query with image successful
2024-08-19 14:27:28,074 DEBUG Querying model with image: gpt-4o
2024-08-19 14:27:35,856 DEBUG Query with image successful
2024-08-19 14:27:36,158 DEBUG Querying model with image: gpt-4o
2024-08-19 14:27:44,475 DEBUG Query with image successful
2024-08-19 14:27:44,580 DEBUG Querying model with image: gpt-4o
2024-08-19 14:27:51,961 DEBUG Query with image successful
2024-08-19 14:27:52,246 DEBUG Querying model with image: gpt-4o
2024-08-19 14:28:00,545 DEBUG Query with image successful
2024-08-19 14:28:00,647 DEBUG Querying model with image: gpt-4o
2024-08-19 14:28:08,975 DEBUG Query with image successful
2024-08-19 14:56:32,803 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 14:56:32,830 DEBUG OpenAI client created
2024-08-19 14:56:37,996 DEBUG Querying model with image: gpt-4o
2024-08-19 14:56:47,594 DEBUG Query with image successful
2024-08-19 14:56:47,696 DEBUG Querying model with image: gpt-4o
2024-08-19 14:57:00,059 DEBUG Query with image successful
2024-08-19 14:57:00,356 DEBUG Querying model with image: gpt-4o
2024-08-19 14:57:09,242 DEBUG Query with image successful
2024-08-19 14:57:09,344 DEBUG Querying model with image: gpt-4o
2024-08-19 14:57:16,171 DEBUG Query with image successful
2024-08-19 14:57:26,475 DEBUG Querying model with image: gpt-4o
2024-08-19 14:57:35,579 DEBUG Query with image successful
2024-08-19 14:57:35,679 DEBUG Querying model with image: gpt-4o
2024-08-19 14:57:45,432 DEBUG Query with image successful
2024-08-19 14:57:45,819 DEBUG Querying model with image: gpt-4o
2024-08-19 14:57:55,751 DEBUG Query with image successful
2024-08-19 14:57:55,851 DEBUG Querying model with image: gpt-4o
2024-08-19 14:58:07,151 DEBUG Query with image successful
2024-08-19 14:58:07,476 DEBUG Querying model with image: gpt-4o
2024-08-19 14:58:15,444 DEBUG Query with image successful
2024-08-19 14:58:15,548 DEBUG Querying model with image: gpt-4o
2024-08-19 14:58:24,668 DEBUG Query with image successful
2024-08-19 14:58:25,071 DEBUG Querying model with image: gpt-4o
2024-08-19 14:58:34,453 DEBUG Query with image successful
2024-08-19 14:58:34,552 DEBUG Querying model with image: gpt-4o
2024-08-19 14:58:43,041 DEBUG Query with image successful
2024-08-19 14:58:43,400 DEBUG Querying model with image: gpt-4o
2024-08-19 14:58:51,048 DEBUG Query with image successful
2024-08-19 14:58:51,152 DEBUG Querying model with image: gpt-4o
2024-08-19 14:58:59,559 DEBUG Query with image successful
2024-08-19 14:58:59,900 DEBUG Querying model with image: gpt-4o
2024-08-19 14:59:07,837 DEBUG Query with image successful
2024-08-19 14:59:07,942 DEBUG Querying model with image: gpt-4o
2024-08-19 14:59:16,719 DEBUG Query with image successful
2024-08-19 14:59:17,092 DEBUG Querying model with image: gpt-4o
2024-08-19 14:59:25,281 DEBUG Query with image successful
2024-08-19 14:59:25,384 DEBUG Querying model with image: gpt-4o
2024-08-19 14:59:35,240 DEBUG Query with image successful
2024-08-19 14:59:35,568 DEBUG Querying model with image: gpt-4o
2024-08-19 14:59:42,853 DEBUG Query with image successful
2024-08-19 14:59:42,957 DEBUG Querying model with image: gpt-4o
2024-08-19 14:59:49,820 DEBUG Query with image successful
2024-08-19 14:59:50,171 DEBUG Querying model with image: gpt-4o
2024-08-19 14:59:58,177 DEBUG Query with image successful
2024-08-19 14:59:58,277 DEBUG Querying model with image: gpt-4o
2024-08-19 15:00:08,788 DEBUG Query with image successful
2024-08-19 15:00:09,195 DEBUG Querying model with image: gpt-4o
2024-08-19 15:00:16,352 DEBUG Query with image successful
2024-08-19 15:00:16,455 DEBUG Querying model with image: gpt-4o
2024-08-19 15:00:25,147 DEBUG Query with image successful
2024-08-19 15:00:25,475 DEBUG Querying model with image: gpt-4o
2024-08-19 15:00:32,436 DEBUG Query with image successful
2024-08-19 15:00:32,547 DEBUG Querying model with image: gpt-4o
2024-08-19 15:00:41,738 DEBUG Query with image successful
2024-08-19 15:00:42,059 DEBUG Querying model with image: gpt-4o
2024-08-19 15:00:51,961 DEBUG Query with image successful
2024-08-19 15:00:52,065 DEBUG Querying model with image: gpt-4o
2024-08-19 15:01:01,761 DEBUG Query with image successful
2024-08-19 15:01:02,082 DEBUG Querying model with image: gpt-4o
2024-08-19 15:01:12,270 DEBUG Query with image successful
2024-08-19 15:01:12,375 DEBUG Querying model with image: gpt-4o
2024-08-19 15:01:19,402 DEBUG Query with image successful
2024-08-19 15:01:19,716 DEBUG Querying model with image: gpt-4o
2024-08-19 15:01:26,638 DEBUG Query with image successful
2024-08-19 15:01:26,741 DEBUG Querying model with image: gpt-4o
2024-08-19 15:01:33,379 DEBUG Query with image successful
2024-08-19 15:01:33,704 DEBUG Querying model with image: gpt-4o
2024-08-19 15:01:41,610 DEBUG Query with image successful
2024-08-19 15:01:41,729 DEBUG Querying model with image: gpt-4o
2024-08-19 15:01:51,637 DEBUG Query with image successful
2024-08-19 15:01:52,022 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:01,383 DEBUG Query with image successful
2024-08-19 15:02:01,486 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:07,932 DEBUG Query with image successful
2024-08-19 15:02:08,252 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:17,567 DEBUG Query with image successful
2024-08-19 15:02:17,666 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:23,269 DEBUG Query with image successful
2024-08-19 15:02:23,574 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:30,950 DEBUG Query with image successful
2024-08-19 15:02:31,052 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:37,267 DEBUG Query with image successful
2024-08-19 15:02:37,761 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:46,036 DEBUG Query with image successful
2024-08-19 15:02:46,142 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:56,431 DEBUG Query with image successful
2024-08-19 15:02:56,727 DEBUG Querying model with image: gpt-4o
2024-08-19 15:03:05,772 DEBUG Query with image successful
2024-08-19 15:03:05,870 DEBUG Querying model with image: gpt-4o
2024-08-19 15:03:13,320 DEBUG Query with image successful
2024-08-19 15:03:13,638 DEBUG Querying model with image: gpt-4o
2024-08-19 15:03:19,656 DEBUG Query with image successful
2024-08-19 15:03:19,752 DEBUG Querying model with image: gpt-4o
2024-08-19 15:03:29,764 DEBUG Query with image successful
2024-08-19 15:03:30,078 DEBUG Querying model with image: gpt-4o
2024-08-19 15:03:36,682 DEBUG Query with image successful
2024-08-19 15:03:36,781 DEBUG Querying model with image: gpt-4o
2024-08-19 15:03:44,562 DEBUG Query with image successful
2024-08-19 15:03:44,858 DEBUG Querying model with image: gpt-4o
2024-08-19 15:03:51,601 DEBUG Query with image successful
2024-08-19 15:03:51,697 DEBUG Querying model with image: gpt-4o
2024-08-19 15:04:02,594 DEBUG Query with image successful
2024-08-19 15:19:14,523 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 15:19:14,549 DEBUG OpenAI client created
2024-08-19 15:19:19,728 DEBUG Querying model with image: gpt-4o
2024-08-19 15:19:29,613 DEBUG Query with image successful
2024-08-19 15:19:29,706 DEBUG Querying model with image: gpt-4o
2024-08-19 15:19:39,779 DEBUG Query with image successful
2024-08-19 15:19:40,170 DEBUG Querying model with image: gpt-4o
2024-08-19 15:19:52,961 DEBUG Query with image successful
2024-08-19 15:19:53,059 DEBUG Querying model with image: gpt-4o
2024-08-19 15:20:01,337 DEBUG Query with image successful
2024-08-19 15:20:11,660 DEBUG Querying model with image: gpt-4o
2024-08-19 15:20:21,437 DEBUG Query with image successful
2024-08-19 15:20:21,539 DEBUG Querying model with image: gpt-4o
2024-08-19 15:20:32,497 DEBUG Query with image successful
2024-08-19 15:20:32,830 DEBUG Querying model with image: gpt-4o
2024-08-19 15:20:41,271 DEBUG Query with image successful
2024-08-19 15:20:41,371 DEBUG Querying model with image: gpt-4o
2024-08-19 15:20:49,767 DEBUG Query with image successful
2024-08-19 15:20:50,215 DEBUG Querying model with image: gpt-4o
2024-08-19 15:20:57,965 DEBUG Query with image successful
2024-08-19 15:20:58,067 DEBUG Querying model with image: gpt-4o
2024-08-19 15:21:09,098 DEBUG Query with image successful
2024-08-19 15:21:09,417 DEBUG Querying model with image: gpt-4o
2024-08-19 15:21:19,230 DEBUG Query with image successful
2024-08-19 15:21:19,328 DEBUG Querying model with image: gpt-4o
2024-08-19 15:21:27,469 DEBUG Query with image successful
2024-08-19 15:21:27,778 DEBUG Querying model with image: gpt-4o
2024-08-19 15:21:37,094 DEBUG Query with image successful
2024-08-19 15:21:37,192 DEBUG Querying model with image: gpt-4o
2024-08-19 15:21:46,846 DEBUG Query with image successful
2024-08-19 15:21:47,190 DEBUG Querying model with image: gpt-4o
2024-08-19 15:21:54,571 DEBUG Query with image successful
2024-08-19 15:21:54,671 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:01,967 DEBUG Query with image successful
2024-08-19 15:22:02,377 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:10,765 DEBUG Query with image successful
2024-08-19 15:22:10,891 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:19,610 DEBUG Query with image successful
2024-08-19 15:22:19,943 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:27,649 DEBUG Query with image successful
2024-08-19 15:22:27,746 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:35,977 DEBUG Query with image successful
2024-08-19 15:22:36,299 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:43,206 DEBUG Query with image successful
2024-08-19 15:22:43,305 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:51,398 DEBUG Query with image successful
2024-08-19 15:22:51,724 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:58,682 DEBUG Query with image successful
2024-08-19 15:22:58,792 DEBUG Querying model with image: gpt-4o
2024-08-19 15:23:05,680 DEBUG Query with image successful
2024-08-19 15:23:06,039 DEBUG Querying model with image: gpt-4o
2024-08-19 15:23:14,522 DEBUG Query with image successful
2024-08-19 15:23:14,617 DEBUG Querying model with image: gpt-4o
2024-08-19 15:23:26,819 DEBUG Query with image successful
2024-08-19 15:23:27,145 DEBUG Querying model with image: gpt-4o
2024-08-19 15:23:35,772 DEBUG Query with image successful
2024-08-19 15:23:35,873 DEBUG Querying model with image: gpt-4o
2024-08-19 15:23:43,370 DEBUG Query with image successful
2024-08-19 15:23:43,704 DEBUG Querying model with image: gpt-4o
2024-08-19 15:23:50,226 DEBUG Query with image successful
2024-08-19 15:23:50,331 DEBUG Querying model with image: gpt-4o
2024-08-19 15:23:57,419 DEBUG Query with image successful
2024-08-19 15:23:57,724 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:04,381 DEBUG Query with image successful
2024-08-19 15:24:04,482 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:13,848 DEBUG Query with image successful
2024-08-19 15:24:14,184 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:20,955 DEBUG Query with image successful
2024-08-19 15:24:21,058 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:30,965 DEBUG Query with image successful
2024-08-19 15:24:31,277 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:38,483 DEBUG Query with image successful
2024-08-19 15:24:38,582 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:45,996 DEBUG Query with image successful
2024-08-19 15:24:46,300 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:52,396 DEBUG Query with image successful
2024-08-19 15:24:52,492 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:59,961 DEBUG Query with image successful
2024-08-19 15:25:00,265 DEBUG Querying model with image: gpt-4o
2024-08-19 15:25:07,765 DEBUG Query with image successful
2024-08-19 15:25:07,865 DEBUG Querying model with image: gpt-4o
2024-08-19 15:25:15,523 DEBUG Query with image successful
2024-08-19 15:25:15,828 DEBUG Querying model with image: gpt-4o
2024-08-19 15:25:22,949 DEBUG Query with image successful
2024-08-19 15:25:23,049 DEBUG Querying model with image: gpt-4o
2024-08-19 15:25:34,819 DEBUG Query with image successful
2024-08-19 15:25:35,128 DEBUG Querying model with image: gpt-4o
2024-08-19 15:25:44,922 DEBUG Query with image successful
2024-08-19 15:25:45,027 DEBUG Querying model with image: gpt-4o
2024-08-19 15:25:51,519 DEBUG Query with image successful
2024-08-19 15:25:51,813 DEBUG Querying model with image: gpt-4o
2024-08-19 15:25:58,896 DEBUG Query with image successful
2024-08-19 15:25:59,010 DEBUG Querying model with image: gpt-4o
2024-08-19 15:26:05,847 DEBUG Query with image successful
2024-08-19 15:26:06,148 DEBUG Querying model with image: gpt-4o
2024-08-19 15:26:13,220 DEBUG Query with image successful
2024-08-19 15:26:13,318 DEBUG Querying model with image: gpt-4o
2024-08-19 15:26:19,504 DEBUG Query with image successful
2024-08-19 15:26:19,814 DEBUG Querying model with image: gpt-4o
2024-08-19 15:26:28,572 DEBUG Query with image successful
2024-08-19 15:26:28,673 DEBUG Querying model with image: gpt-4o
2024-08-19 15:26:38,951 DEBUG Query with image successful
2024-08-21 11:01:10,142 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-21 11:01:10,221 DEBUG OpenAI client created
2024-08-21 11:01:15,464 DEBUG Querying model with image: gpt-4o
2024-08-21 11:01:24,505 DEBUG Query with image successful
2024-08-21 11:01:24,602 DEBUG Querying model with image: gpt-4o
2024-08-21 11:01:33,990 DEBUG Query with image successful
2024-08-21 11:01:34,383 DEBUG Querying model with image: gpt-4o
2024-08-21 11:01:41,875 DEBUG Query with image successful
2024-08-21 11:01:41,972 DEBUG Querying model with image: gpt-4o
2024-08-21 11:01:50,450 DEBUG Query with image successful
2024-08-21 11:02:00,760 DEBUG Querying model with image: gpt-4o
2024-08-21 11:02:10,850 DEBUG Query with image successful
2024-08-21 11:02:10,949 DEBUG Querying model with image: gpt-4o
2024-08-21 11:02:17,536 DEBUG Query with image successful
2024-08-21 11:02:17,902 DEBUG Querying model with image: gpt-4o
2024-08-21 11:02:25,564 DEBUG Query with image successful
2024-08-21 11:02:25,662 DEBUG Querying model with image: gpt-4o
2024-08-21 11:02:34,583 DEBUG Query with image successful
2024-08-21 11:02:34,895 DEBUG Querying model with image: gpt-4o
2024-08-21 11:02:42,160 DEBUG Query with image successful
2024-08-21 11:02:42,262 DEBUG Querying model with image: gpt-4o
2024-08-21 11:02:51,349 DEBUG Query with image successful
2024-08-21 11:02:51,688 DEBUG Querying model with image: gpt-4o
2024-08-21 11:03:02,290 DEBUG Query with image successful
2024-08-21 11:03:02,423 DEBUG Querying model with image: gpt-4o
2024-08-21 11:03:09,099 DEBUG Query with image successful
2024-08-21 11:03:09,563 DEBUG Querying model with image: gpt-4o
2024-08-21 11:03:16,444 DEBUG Query with image successful
2024-08-21 11:03:16,549 DEBUG Querying model with image: gpt-4o
2024-08-21 11:03:26,134 DEBUG Query with image successful
2024-08-21 11:03:26,475 DEBUG Querying model with image: gpt-4o
2024-08-21 11:03:34,595 DEBUG Query with image successful
2024-08-21 11:03:34,696 DEBUG Querying model with image: gpt-4o
2024-08-21 11:03:42,429 DEBUG Query with image successful
2024-08-21 11:03:42,741 DEBUG Querying model with image: gpt-4o
2024-08-21 11:03:50,115 DEBUG Query with image successful
2024-08-21 11:03:50,217 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:00,397 DEBUG Query with image successful
2024-08-21 11:04:00,723 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:08,369 DEBUG Query with image successful
2024-08-21 11:04:08,477 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:15,650 DEBUG Query with image successful
2024-08-21 11:04:15,996 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:24,681 DEBUG Query with image successful
2024-08-21 11:04:24,788 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:31,985 DEBUG Query with image successful
2024-08-21 11:04:32,305 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:38,802 DEBUG Query with image successful
2024-08-21 11:04:38,915 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:46,540 DEBUG Query with image successful
2024-08-21 11:04:46,885 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:56,677 DEBUG Query with image successful
2024-08-21 11:04:56,785 DEBUG Querying model with image: gpt-4o
2024-08-21 11:05:04,655 DEBUG Query with image successful
2024-08-21 11:05:04,952 DEBUG Querying model with image: gpt-4o
2024-08-21 11:05:13,523 DEBUG Query with image successful
2024-08-21 11:05:13,625 DEBUG Querying model with image: gpt-4o
2024-08-21 11:05:21,246 DEBUG Query with image successful
2024-08-21 11:05:21,555 DEBUG Querying model with image: gpt-4o
2024-08-21 11:05:29,223 DEBUG Query with image successful
2024-08-21 11:05:29,318 DEBUG Querying model with image: gpt-4o
2024-08-21 11:05:39,139 DEBUG Query with image successful
2024-08-21 11:05:39,454 DEBUG Querying model with image: gpt-4o
2024-08-21 11:05:48,119 DEBUG Query with image successful
2024-08-21 11:05:48,222 DEBUG Querying model with image: gpt-4o
2024-08-21 11:05:58,533 DEBUG Query with image successful
2024-08-21 11:05:58,852 DEBUG Querying model with image: gpt-4o
2024-08-21 11:06:07,295 DEBUG Query with image successful
2024-08-21 11:06:07,404 DEBUG Querying model with image: gpt-4o
2024-08-21 11:06:16,506 DEBUG Query with image successful
2024-08-21 11:06:16,822 DEBUG Querying model with image: gpt-4o
2024-08-21 11:06:26,224 DEBUG Query with image successful
2024-08-21 11:06:26,321 DEBUG Querying model with image: gpt-4o
2024-08-21 11:06:33,562 DEBUG Query with image successful
2024-08-21 11:06:33,915 DEBUG Querying model with image: gpt-4o
2024-08-21 11:06:42,824 DEBUG Query with image successful
2024-08-21 11:06:42,934 DEBUG Querying model with image: gpt-4o
2024-08-21 11:06:49,335 DEBUG Query with image successful
2024-08-21 11:06:49,646 DEBUG Querying model with image: gpt-4o
2024-08-21 11:06:58,073 DEBUG Query with image successful
2024-08-21 11:06:58,176 DEBUG Querying model with image: gpt-4o
2024-08-21 11:07:06,309 DEBUG Query with image successful
2024-08-21 11:07:06,623 DEBUG Querying model with image: gpt-4o
2024-08-21 11:07:15,865 DEBUG Query with image successful
2024-08-21 11:07:15,974 DEBUG Querying model with image: gpt-4o
2024-08-21 11:07:23,750 DEBUG Query with image successful
2024-08-21 11:07:24,078 DEBUG Querying model with image: gpt-4o
2024-08-21 11:07:32,365 DEBUG Query with image successful
2024-08-21 11:07:32,480 DEBUG Querying model with image: gpt-4o
2024-08-21 11:07:40,248 DEBUG Query with image successful
2024-08-21 11:07:40,583 DEBUG Querying model with image: gpt-4o
2024-08-21 11:07:48,622 DEBUG Query with image successful
2024-08-21 11:07:48,724 DEBUG Querying model with image: gpt-4o
2024-08-21 11:07:56,039 DEBUG Query with image successful
2024-08-21 11:07:56,337 DEBUG Querying model with image: gpt-4o
2024-08-21 11:08:04,570 DEBUG Query with image successful
2024-08-21 11:08:04,664 DEBUG Querying model with image: gpt-4o
2024-08-21 11:08:13,565 DEBUG Query with image successful
2024-08-21 11:08:13,848 DEBUG Querying model with image: gpt-4o
2024-08-21 11:08:23,883 DEBUG Query with image successful
2024-08-21 11:08:23,983 DEBUG Querying model with image: gpt-4o
2024-08-21 11:08:31,574 DEBUG Query with image successful
2024-08-22 13:43:07,357 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 13:43:07,496 DEBUG OpenAI client created
2024-08-22 13:50:49,557 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 13:50:49,581 DEBUG OpenAI client created
2024-08-22 13:50:54,752 DEBUG Querying model with image: gpt-4o
2024-08-22 13:51:13,437 DEBUG Query with image successful
2024-08-22 13:51:13,527 DEBUG Querying model with image: gpt-4o
2024-08-22 13:51:22,332 DEBUG Query with image successful
2024-08-22 13:51:22,766 DEBUG Querying model with image: gpt-4o
2024-08-22 13:51:30,681 DEBUG Query with image successful
2024-08-22 13:51:30,781 DEBUG Querying model with image: gpt-4o
2024-08-22 13:51:51,056 DEBUG Query with image successful
2024-08-22 13:52:01,366 DEBUG Querying model with image: gpt-4o
2024-08-22 13:52:10,227 DEBUG Query with image successful
2024-08-22 13:52:10,330 DEBUG Querying model with image: gpt-4o
2024-08-22 13:52:22,871 DEBUG Query with image successful
2024-08-22 13:52:23,205 DEBUG Querying model with image: gpt-4o
2024-08-22 13:52:30,740 DEBUG Query with image successful
2024-08-22 13:52:30,830 DEBUG Querying model with image: gpt-4o
2024-08-22 13:52:44,635 DEBUG Query with image successful
2024-08-22 13:52:44,966 DEBUG Querying model with image: gpt-4o
2024-08-22 13:52:52,713 DEBUG Query with image successful
2024-08-22 13:52:52,817 DEBUG Querying model with image: gpt-4o
2024-08-22 13:53:04,953 DEBUG Query with image successful
2024-08-22 13:53:05,285 DEBUG Querying model with image: gpt-4o
2024-08-22 13:53:13,014 DEBUG Query with image successful
2024-08-22 13:53:13,109 DEBUG Querying model with image: gpt-4o
2024-08-22 13:53:25,340 DEBUG Query with image successful
2024-08-22 13:53:25,659 DEBUG Querying model with image: gpt-4o
2024-08-22 13:53:33,138 DEBUG Query with image successful
2024-08-22 13:53:33,240 DEBUG Querying model with image: gpt-4o
2024-08-22 13:53:40,706 DEBUG Query with image successful
2024-08-22 13:53:41,012 DEBUG Querying model with image: gpt-4o
2024-08-22 13:53:49,485 DEBUG Query with image successful
2024-08-22 13:53:49,577 DEBUG Querying model with image: gpt-4o
2024-08-22 13:53:59,229 DEBUG Query with image successful
2024-08-22 13:53:59,558 DEBUG Querying model with image: gpt-4o
2024-08-22 13:54:08,601 DEBUG Query with image successful
2024-08-22 13:54:08,710 DEBUG Querying model with image: gpt-4o
2024-08-22 13:54:16,049 DEBUG Query with image successful
2024-08-22 13:54:16,360 DEBUG Querying model with image: gpt-4o
2024-08-22 13:54:31,219 DEBUG Query with image successful
2024-08-22 13:54:31,322 DEBUG Querying model with image: gpt-4o
2024-08-22 13:54:38,720 DEBUG Query with image successful
2024-08-22 13:54:39,018 DEBUG Querying model with image: gpt-4o
2024-08-22 13:54:50,196 DEBUG Query with image successful
2024-08-22 13:54:50,289 DEBUG Querying model with image: gpt-4o
2024-08-22 13:54:57,803 DEBUG Query with image successful
2024-08-22 13:54:58,136 DEBUG Querying model with image: gpt-4o
2024-08-22 13:55:07,980 DEBUG Query with image successful
2024-08-22 13:55:08,076 DEBUG Querying model with image: gpt-4o
2024-08-22 13:55:15,745 DEBUG Query with image successful
2024-08-22 13:55:16,061 DEBUG Querying model with image: gpt-4o
2024-08-22 13:55:23,338 DEBUG Query with image successful
2024-08-22 13:55:23,444 DEBUG Querying model with image: gpt-4o
2024-08-22 13:55:31,479 DEBUG Query with image successful
2024-08-22 13:55:31,774 DEBUG Querying model with image: gpt-4o
2024-08-22 13:55:39,063 DEBUG Query with image successful
2024-08-22 13:55:39,166 DEBUG Querying model with image: gpt-4o
2024-08-22 13:55:45,706 DEBUG Query with image successful
2024-08-22 13:55:46,003 DEBUG Querying model with image: gpt-4o
2024-08-22 13:55:54,091 DEBUG Query with image successful
2024-08-22 13:55:54,188 DEBUG Querying model with image: gpt-4o
2024-08-22 13:56:00,707 DEBUG Query with image successful
2024-08-22 13:56:01,020 DEBUG Querying model with image: gpt-4o
2024-08-22 13:56:12,372 DEBUG Query with image successful
2024-08-22 13:56:12,461 DEBUG Querying model with image: gpt-4o
2024-08-22 13:56:20,151 DEBUG Query with image successful
2024-08-22 13:56:20,444 DEBUG Querying model with image: gpt-4o
2024-08-22 13:56:29,989 DEBUG Query with image successful
2024-08-22 13:56:30,083 DEBUG Querying model with image: gpt-4o
2024-08-22 13:56:38,104 DEBUG Query with image successful
2024-08-22 13:56:38,392 DEBUG Querying model with image: gpt-4o
2024-08-22 13:56:47,195 DEBUG Query with image successful
2024-08-22 13:56:47,293 DEBUG Querying model with image: gpt-4o
2024-08-22 13:56:53,584 DEBUG Query with image successful
2024-08-22 13:56:53,872 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:04,144 DEBUG Query with image successful
2024-08-22 13:57:04,240 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:10,911 DEBUG Query with image successful
2024-08-22 13:57:11,210 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:19,881 DEBUG Query with image successful
2024-08-22 13:57:19,975 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:26,096 DEBUG Query with image successful
2024-08-22 13:57:26,381 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:34,714 DEBUG Query with image successful
2024-08-22 13:57:34,812 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:41,250 DEBUG Query with image successful
2024-08-22 13:57:41,544 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:49,770 DEBUG Query with image successful
2024-08-22 13:57:49,864 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:57,241 DEBUG Query with image successful
2024-08-22 13:57:57,554 DEBUG Querying model with image: gpt-4o
2024-08-22 13:58:08,407 DEBUG Query with image successful
2024-08-22 13:58:08,505 DEBUG Querying model with image: gpt-4o
2024-08-22 13:58:15,410 DEBUG Query with image successful
2024-08-22 13:58:15,694 DEBUG Querying model with image: gpt-4o
2024-08-22 13:58:23,352 DEBUG Query with image successful
2024-08-22 13:58:23,439 DEBUG Querying model with image: gpt-4o
2024-08-22 13:58:29,111 DEBUG Query with image successful
2024-08-22 13:58:29,402 DEBUG Querying model with image: gpt-4o
2024-08-22 13:58:36,899 DEBUG Query with image successful
2024-08-22 13:58:36,994 DEBUG Querying model with image: gpt-4o
2024-08-22 13:58:45,714 DEBUG Query with image successful
2024-08-22 14:59:31,403 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 14:59:31,427 DEBUG OpenAI client created
2024-08-22 14:59:36,595 DEBUG Querying model with image: gpt-4o
2024-08-22 14:59:46,457 DEBUG Query with image successful
2024-08-22 14:59:46,540 DEBUG Querying model with image: gpt-4o
2024-08-22 14:59:53,834 DEBUG Query with image successful
2024-08-22 14:59:54,190 DEBUG Querying model with image: gpt-4o
2024-08-22 15:00:02,789 DEBUG Query with image successful
2024-08-22 15:00:02,874 DEBUG Querying model with image: gpt-4o
2024-08-22 15:00:22,854 DEBUG Query with image successful
2024-08-22 15:00:33,126 DEBUG Querying model with image: gpt-4o
2024-08-22 15:00:43,876 DEBUG Query with image successful
2024-08-22 15:00:43,961 DEBUG Querying model with image: gpt-4o
2024-08-22 15:00:58,427 DEBUG Query with image successful
2024-08-22 15:00:58,739 DEBUG Querying model with image: gpt-4o
2024-08-22 15:01:16,677 DEBUG Query with image successful
2024-08-22 15:01:16,768 DEBUG Querying model with image: gpt-4o
2024-08-22 15:01:30,105 DEBUG Query with image successful
2024-08-22 15:01:30,587 DEBUG Querying model with image: gpt-4o
2024-08-22 15:01:49,708 DEBUG Query with image successful
2024-08-22 15:01:49,811 DEBUG Querying model with image: gpt-4o
2024-08-22 15:02:07,903 DEBUG Query with image successful
2024-08-22 15:02:08,236 DEBUG Querying model with image: gpt-4o
2024-08-22 15:02:22,446 DEBUG Query with image successful
2024-08-22 15:02:22,542 DEBUG Querying model with image: gpt-4o
2024-08-22 15:02:40,554 DEBUG Query with image successful
2024-08-22 15:02:40,994 DEBUG Querying model with image: gpt-4o
2024-08-22 15:03:00,094 DEBUG Query with image successful
2024-08-22 15:03:00,212 DEBUG Querying model with image: gpt-4o
2024-08-22 15:03:12,294 DEBUG Query with image successful
2024-08-22 15:03:12,716 DEBUG Querying model with image: gpt-4o
2024-08-22 15:03:28,540 DEBUG Query with image successful
2024-08-22 15:03:28,635 DEBUG Querying model with image: gpt-4o
2024-08-22 15:03:47,904 DEBUG Query with image successful
2024-08-22 15:03:48,222 DEBUG Querying model with image: gpt-4o
2024-08-22 15:04:03,569 DEBUG Query with image successful
2024-08-22 15:04:03,672 DEBUG Querying model with image: gpt-4o
2024-08-22 15:04:18,477 DEBUG Query with image successful
2024-08-22 15:04:18,783 DEBUG Querying model with image: gpt-4o
2024-08-22 15:04:34,009 DEBUG Query with image successful
2024-08-22 15:04:34,109 DEBUG Querying model with image: gpt-4o
2024-08-22 15:04:44,300 DEBUG Query with image successful
2024-08-22 15:04:44,641 DEBUG Querying model with image: gpt-4o
2024-08-22 15:05:12,196 DEBUG Query with image successful
2024-08-22 15:05:12,289 DEBUG Querying model with image: gpt-4o
2024-08-22 15:05:21,241 DEBUG Query with image successful
2024-08-22 15:05:21,548 DEBUG Querying model with image: gpt-4o
2024-08-22 15:05:33,159 DEBUG Query with image successful
2024-08-22 15:05:33,256 DEBUG Querying model with image: gpt-4o
2024-08-22 15:05:42,865 DEBUG Query with image successful
2024-08-22 15:05:43,156 DEBUG Querying model with image: gpt-4o
2024-08-22 15:05:54,560 DEBUG Query with image successful
2024-08-22 15:05:54,662 DEBUG Querying model with image: gpt-4o
2024-08-22 15:06:04,193 DEBUG Query with image successful
2024-08-22 15:06:04,500 DEBUG Querying model with image: gpt-4o
2024-08-22 15:06:18,695 DEBUG Query with image successful
2024-08-22 15:06:18,794 DEBUG Querying model with image: gpt-4o
2024-08-22 15:06:31,782 DEBUG Query with image successful
2024-08-22 15:06:32,102 DEBUG Querying model with image: gpt-4o
2024-08-22 15:06:40,665 DEBUG Query with image successful
2024-08-22 15:06:40,764 DEBUG Querying model with image: gpt-4o
2024-08-22 15:06:47,264 DEBUG Query with image successful
2024-08-22 15:06:47,556 DEBUG Querying model with image: gpt-4o
2024-08-22 15:06:54,216 DEBUG Query with image successful
2024-08-22 15:06:54,307 DEBUG Querying model with image: gpt-4o
2024-08-22 15:07:02,644 DEBUG Query with image successful
2024-08-22 15:07:02,945 DEBUG Querying model with image: gpt-4o
2024-08-22 15:07:13,257 DEBUG Query with image successful
2024-08-22 15:07:13,351 DEBUG Querying model with image: gpt-4o
2024-08-22 15:07:20,861 DEBUG Query with image successful
2024-08-22 15:07:21,206 DEBUG Querying model with image: gpt-4o
2024-08-22 15:07:33,095 DEBUG Query with image successful
2024-08-22 15:07:33,194 DEBUG Querying model with image: gpt-4o
2024-08-22 15:07:46,081 DEBUG Query with image successful
2024-08-22 15:07:46,374 DEBUG Querying model with image: gpt-4o
2024-08-22 15:07:58,276 DEBUG Query with image successful
2024-08-22 15:07:58,382 DEBUG Querying model with image: gpt-4o
2024-08-22 15:08:10,746 DEBUG Query with image successful
2024-08-22 15:08:11,036 DEBUG Querying model with image: gpt-4o
2024-08-22 15:08:27,192 DEBUG Query with image successful
2024-08-22 15:08:27,290 DEBUG Querying model with image: gpt-4o
2024-08-22 15:08:37,197 DEBUG Query with image successful
2024-08-22 15:08:37,508 DEBUG Querying model with image: gpt-4o
2024-08-22 15:08:50,046 DEBUG Query with image successful
2024-08-22 15:08:50,145 DEBUG Querying model with image: gpt-4o
2024-08-22 15:09:03,702 DEBUG Query with image successful
2024-08-22 15:09:04,012 DEBUG Querying model with image: gpt-4o
2024-08-22 15:09:17,781 DEBUG Query with image successful
2024-08-22 15:09:17,878 DEBUG Querying model with image: gpt-4o
2024-08-22 15:09:35,290 DEBUG Query with image successful
2024-08-22 15:09:35,580 DEBUG Querying model with image: gpt-4o
2024-08-22 15:09:47,765 DEBUG Query with image successful
2024-08-22 15:09:47,854 DEBUG Querying model with image: gpt-4o
2024-08-22 15:10:04,300 DEBUG Query with image successful
2024-08-22 15:10:04,598 DEBUG Querying model with image: gpt-4o
2024-08-22 15:10:18,752 DEBUG Query with image successful
2024-08-22 15:10:18,842 DEBUG Querying model with image: gpt-4o
2024-08-22 15:10:35,696 DEBUG Query with image successful
2024-08-22 15:10:36,024 DEBUG Querying model with image: gpt-4o
2024-08-22 15:10:49,376 DEBUG Query with image successful
2024-08-22 15:10:49,467 DEBUG Querying model with image: gpt-4o
2024-08-22 15:11:04,491 DEBUG Query with image successful
2024-08-22 15:41:24,465 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 15:41:24,490 DEBUG OpenAI client created
2024-08-22 15:41:29,649 DEBUG Querying model with image: gpt-4o
2024-08-22 15:41:39,341 DEBUG Query with image successful
2024-08-22 15:41:39,428 DEBUG Querying model with image: gpt-4o
2024-08-22 15:41:45,878 DEBUG Query with image successful
2024-08-22 15:41:46,220 DEBUG Querying model with image: gpt-4o
2024-08-22 15:41:53,055 DEBUG Query with image successful
2024-08-22 15:41:53,147 DEBUG Querying model with image: gpt-4o
2024-08-22 15:42:00,919 DEBUG Query with image successful
2024-08-22 15:42:11,235 DEBUG Querying model with image: gpt-4o
2024-08-22 15:42:19,494 DEBUG Query with image successful
2024-08-22 15:42:19,589 DEBUG Querying model with image: gpt-4o
2024-08-22 15:42:29,030 DEBUG Query with image successful
2024-08-22 15:42:29,343 DEBUG Querying model with image: gpt-4o
2024-08-22 15:42:35,945 DEBUG Query with image successful
2024-08-22 15:42:36,038 DEBUG Querying model with image: gpt-4o
2024-08-22 15:42:42,013 DEBUG Query with image successful
2024-08-22 15:42:42,389 DEBUG Querying model with image: gpt-4o
2024-08-22 15:42:48,832 DEBUG Query with image successful
2024-08-22 15:42:48,931 DEBUG Querying model with image: gpt-4o
2024-08-22 15:42:59,752 DEBUG Query with image successful
2024-08-22 15:43:00,082 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:06,762 DEBUG Query with image successful
2024-08-22 15:43:06,857 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:13,158 DEBUG Query with image successful
2024-08-22 15:43:13,524 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:20,403 DEBUG Query with image successful
2024-08-22 15:43:20,500 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:28,731 DEBUG Query with image successful
2024-08-22 15:43:29,043 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:37,093 DEBUG Query with image successful
2024-08-22 15:43:37,187 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:45,161 DEBUG Query with image successful
2024-08-22 15:43:45,482 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:52,437 DEBUG Query with image successful
2024-08-22 15:43:52,533 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:59,168 DEBUG Query with image successful
2024-08-22 15:43:59,474 DEBUG Querying model with image: gpt-4o
2024-08-22 15:44:10,642 DEBUG Query with image successful
2024-08-22 15:44:10,738 DEBUG Querying model with image: gpt-4o
2024-08-22 15:44:16,911 DEBUG Query with image successful
2024-08-22 15:44:17,219 DEBUG Querying model with image: gpt-4o
2024-08-22 15:44:24,784 DEBUG Query with image successful
2024-08-22 15:44:24,880 DEBUG Querying model with image: gpt-4o
2024-08-22 15:44:32,152 DEBUG Query with image successful
2024-08-22 15:44:32,464 DEBUG Querying model with image: gpt-4o
2024-08-22 15:44:40,426 DEBUG Query with image successful
2024-08-22 15:44:40,523 DEBUG Querying model with image: gpt-4o
2024-08-22 15:44:47,749 DEBUG Query with image successful
2024-08-22 15:44:48,080 DEBUG Querying model with image: gpt-4o
2024-08-22 15:44:56,235 DEBUG Query with image successful
2024-08-22 15:44:56,327 DEBUG Querying model with image: gpt-4o
2024-08-22 15:45:06,042 DEBUG Query with image successful
2024-08-22 15:45:06,357 DEBUG Querying model with image: gpt-4o
2024-08-22 15:45:13,587 DEBUG Query with image successful
2024-08-22 15:45:13,679 DEBUG Querying model with image: gpt-4o
2024-08-22 15:45:20,414 DEBUG Query with image successful
2024-08-22 15:45:20,717 DEBUG Querying model with image: gpt-4o
2024-08-22 15:45:26,846 DEBUG Query with image successful
2024-08-22 15:45:26,940 DEBUG Querying model with image: gpt-4o
2024-08-22 15:45:33,400 DEBUG Query with image successful
2024-08-22 15:45:33,757 DEBUG Querying model with image: gpt-4o
2024-08-22 15:45:43,745 DEBUG Query with image successful
2024-08-22 15:45:43,843 DEBUG Querying model with image: gpt-4o
2024-08-22 15:45:51,625 DEBUG Query with image successful
2024-08-22 15:45:51,936 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:00,281 DEBUG Query with image successful
2024-08-22 15:46:00,376 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:08,210 DEBUG Query with image successful
2024-08-22 15:46:08,555 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:17,906 DEBUG Query with image successful
2024-08-22 15:46:18,011 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:24,125 DEBUG Query with image successful
2024-08-22 15:46:24,431 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:30,061 DEBUG Query with image successful
2024-08-22 15:46:30,153 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:35,679 DEBUG Query with image successful
2024-08-22 15:46:35,970 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:42,943 DEBUG Query with image successful
2024-08-22 15:46:43,038 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:51,791 DEBUG Query with image successful
2024-08-22 15:46:52,090 DEBUG Querying model with image: gpt-4o
2024-08-22 15:47:00,211 DEBUG Query with image successful
2024-08-22 15:47:00,305 DEBUG Querying model with image: gpt-4o
2024-08-22 15:47:10,314 DEBUG Query with image successful
2024-08-22 15:47:10,619 DEBUG Querying model with image: gpt-4o
2024-08-22 15:47:19,537 DEBUG Query with image successful
2024-08-22 15:47:19,641 DEBUG Querying model with image: gpt-4o
2024-08-22 15:47:31,791 DEBUG Query with image successful
2024-08-22 15:47:32,127 DEBUG Querying model with image: gpt-4o
2024-08-22 15:47:45,034 DEBUG Query with image successful
2024-08-22 15:47:45,131 DEBUG Querying model with image: gpt-4o
2024-08-22 15:47:58,194 DEBUG Query with image successful
2024-08-22 15:47:58,493 DEBUG Querying model with image: gpt-4o
2024-08-22 15:48:14,791 DEBUG Query with image successful
2024-08-22 15:48:14,888 DEBUG Querying model with image: gpt-4o
2024-08-22 15:48:28,157 DEBUG Query with image successful
2024-08-22 15:48:28,470 DEBUG Querying model with image: gpt-4o
2024-08-22 15:48:38,935 DEBUG Query with image successful
2024-08-22 15:48:39,028 DEBUG Querying model with image: gpt-4o
2024-08-22 15:48:48,645 DEBUG Query with image successful
2024-08-22 16:05:44,756 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 16:05:44,794 DEBUG OpenAI client created
2024-08-22 16:05:49,906 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:00,460 DEBUG Query with image successful
2024-08-22 16:06:00,546 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:07,305 DEBUG Query with image successful
2024-08-22 16:06:07,661 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:14,374 DEBUG Query with image successful
2024-08-22 16:06:14,451 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:21,120 DEBUG Query with image successful
2024-08-22 16:06:31,396 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:40,029 DEBUG Query with image successful
2024-08-22 16:06:40,093 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:45,949 DEBUG Query with image successful
2024-08-22 16:06:46,179 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:51,836 DEBUG Query with image successful
2024-08-22 16:06:51,896 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:58,112 DEBUG Query with image successful
2024-08-22 16:06:58,370 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:03,464 DEBUG Query with image successful
2024-08-22 16:07:03,537 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:10,455 DEBUG Query with image successful
2024-08-22 16:07:10,697 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:17,040 DEBUG Query with image successful
2024-08-22 16:07:17,104 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:23,981 DEBUG Query with image successful
2024-08-22 16:07:24,208 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:31,556 DEBUG Query with image successful
2024-08-22 16:07:31,625 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:39,761 DEBUG Query with image successful
2024-08-22 16:07:39,997 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:45,556 DEBUG Query with image successful
2024-08-22 16:07:45,619 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:51,593 DEBUG Query with image successful
2024-08-22 16:07:51,836 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:58,567 DEBUG Query with image successful
2024-08-22 16:07:58,625 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:03,630 DEBUG Query with image successful
2024-08-22 16:08:03,864 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:11,896 DEBUG Query with image successful
2024-08-22 16:08:11,960 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:19,072 DEBUG Query with image successful
2024-08-22 16:08:19,313 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:25,774 DEBUG Query with image successful
2024-08-22 16:08:25,845 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:31,913 DEBUG Query with image successful
2024-08-22 16:08:32,147 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:38,307 DEBUG Query with image successful
2024-08-22 16:08:38,367 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:44,324 DEBUG Query with image successful
2024-08-22 16:08:44,560 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:50,116 DEBUG Query with image successful
2024-08-22 16:08:50,175 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:56,662 DEBUG Query with image successful
2024-08-22 16:08:56,897 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:03,380 DEBUG Query with image successful
2024-08-22 16:09:03,437 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:08,682 DEBUG Query with image successful
2024-08-22 16:09:08,930 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:15,832 DEBUG Query with image successful
2024-08-22 16:09:15,892 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:20,998 DEBUG Query with image successful
2024-08-22 16:09:21,236 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:28,587 DEBUG Query with image successful
2024-08-22 16:09:28,654 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:35,244 DEBUG Query with image successful
2024-08-22 16:09:35,471 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:41,281 DEBUG Query with image successful
2024-08-22 16:09:41,341 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:48,127 DEBUG Query with image successful
2024-08-22 16:09:48,355 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:56,289 DEBUG Query with image successful
2024-08-22 16:09:56,350 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:01,063 DEBUG Query with image successful
2024-08-22 16:10:01,291 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:07,791 DEBUG Query with image successful
2024-08-22 16:10:07,853 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:13,290 DEBUG Query with image successful
2024-08-22 16:10:13,521 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:18,713 DEBUG Query with image successful
2024-08-22 16:10:18,776 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:24,014 DEBUG Query with image successful
2024-08-22 16:10:24,239 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:29,735 DEBUG Query with image successful
2024-08-22 16:10:29,797 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:39,342 DEBUG Query with image successful
2024-08-22 16:10:39,571 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:45,183 DEBUG Query with image successful
2024-08-22 16:10:45,246 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:50,322 DEBUG Query with image successful
2024-08-22 16:10:50,544 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:55,223 DEBUG Query with image successful
2024-08-22 16:10:55,287 DEBUG Querying model with image: gpt-4o
2024-08-22 16:11:00,301 DEBUG Query with image successful
2024-08-22 16:11:00,526 DEBUG Querying model with image: gpt-4o
2024-08-22 16:11:07,127 DEBUG Query with image successful
2024-08-22 16:11:07,186 DEBUG Querying model with image: gpt-4o
2024-08-22 16:11:12,335 DEBUG Query with image successful
2024-08-22 16:11:12,556 DEBUG Querying model with image: gpt-4o
2024-08-22 16:11:17,787 DEBUG Query with image successful
2024-08-22 16:11:17,850 DEBUG Querying model with image: gpt-4o
2024-08-22 16:11:22,839 DEBUG Query with image successful
2024-08-22 16:30:43,888 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 16:30:43,919 DEBUG OpenAI client created
2024-08-22 16:30:49,029 DEBUG Querying model with image: gpt-4o
2024-08-22 16:30:54,929 DEBUG Query with image successful
2024-08-22 16:30:54,998 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:00,275 DEBUG Query with image successful
2024-08-22 16:31:00,522 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:06,740 DEBUG Query with image successful
2024-08-22 16:31:06,801 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:13,275 DEBUG Query with image successful
2024-08-22 16:31:23,668 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:30,096 DEBUG Query with image successful
2024-08-22 16:31:30,160 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:35,766 DEBUG Query with image successful
2024-08-22 16:31:36,094 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:43,415 DEBUG Query with image successful
2024-08-22 16:31:43,477 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:49,125 DEBUG Query with image successful
2024-08-22 16:31:49,375 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:54,468 DEBUG Query with image successful
2024-08-22 16:31:54,528 DEBUG Querying model with image: gpt-4o
2024-08-22 16:32:01,814 DEBUG Query with image successful
2024-08-22 16:32:02,062 DEBUG Querying model with image: gpt-4o
2024-08-22 16:32:08,697 DEBUG Query with image successful
2024-08-22 16:32:08,760 DEBUG Querying model with image: gpt-4o
2024-08-22 16:32:16,339 DEBUG Query with image successful
2024-08-22 16:32:16,612 DEBUG Querying model with image: gpt-4o
2024-08-22 16:32:25,826 DEBUG Query with image successful
2024-08-22 16:32:25,885 DEBUG Querying model with image: gpt-4o
2024-08-22 16:32:31,420 DEBUG Query with image successful
2024-08-22 16:32:31,673 DEBUG Querying model with image: gpt-4o
2024-08-22 16:32:37,227 DEBUG Query with image successful
2024-08-22 16:32:37,288 DEBUG Querying model with image: gpt-4o
2024-08-22 16:32:43,144 DEBUG Query with image successful
2024-08-22 16:32:43,390 DEBUG Querying model with image: gpt-4o
2024-08-22 16:33:19,222 DEBUG Query with image successful
2024-08-22 16:33:19,289 DEBUG Querying model with image: gpt-4o
2024-08-22 16:33:24,599 DEBUG Query with image successful
2024-08-22 16:33:24,911 DEBUG Querying model with image: gpt-4o
2024-08-22 16:33:35,666 DEBUG Query with image successful
2024-08-22 16:33:35,727 DEBUG Querying model with image: gpt-4o
2024-08-22 16:33:42,226 DEBUG Query with image successful
2024-08-22 16:33:42,462 DEBUG Querying model with image: gpt-4o
2024-08-22 16:33:48,885 DEBUG Query with image successful
2024-08-22 16:33:48,946 DEBUG Querying model with image: gpt-4o
2024-08-22 16:33:56,144 DEBUG Query with image successful
2024-08-22 16:33:56,413 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:03,987 DEBUG Query with image successful
2024-08-22 16:34:04,047 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:09,903 DEBUG Query with image successful
2024-08-22 16:34:10,166 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:15,892 DEBUG Query with image successful
2024-08-22 16:34:15,955 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:21,627 DEBUG Query with image successful
2024-08-22 16:34:21,861 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:28,283 DEBUG Query with image successful
2024-08-22 16:34:28,344 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:34,088 DEBUG Query with image successful
2024-08-22 16:34:34,319 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:39,009 DEBUG Query with image successful
2024-08-22 16:34:39,071 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:45,136 DEBUG Query with image successful
2024-08-22 16:34:45,369 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:50,824 DEBUG Query with image successful
2024-08-22 16:34:50,897 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:10,693 DEBUG Query with image successful
2024-08-22 16:35:10,930 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:15,960 DEBUG Query with image successful
2024-08-22 16:35:16,028 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:22,266 DEBUG Query with image successful
2024-08-22 16:35:22,504 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:27,307 DEBUG Query with image successful
2024-08-22 16:35:27,368 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:32,804 DEBUG Query with image successful
2024-08-22 16:35:33,042 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:38,063 DEBUG Query with image successful
2024-08-22 16:35:38,122 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:43,784 DEBUG Query with image successful
2024-08-22 16:35:44,012 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:49,486 DEBUG Query with image successful
2024-08-22 16:35:49,542 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:56,692 DEBUG Query with image successful
2024-08-22 16:35:56,929 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:01,932 DEBUG Query with image successful
2024-08-22 16:36:01,994 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:07,409 DEBUG Query with image successful
2024-08-22 16:36:07,653 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:13,191 DEBUG Query with image successful
2024-08-22 16:36:13,255 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:18,940 DEBUG Query with image successful
2024-08-22 16:36:19,167 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:23,766 DEBUG Query with image successful
2024-08-22 16:36:23,831 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:29,492 DEBUG Query with image successful
2024-08-22 16:36:29,723 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:33,932 DEBUG Query with image successful
2024-08-22 16:36:33,995 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:39,231 DEBUG Query with image successful
2024-08-22 16:36:39,449 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:44,456 DEBUG Query with image successful
2024-08-22 16:36:44,516 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:49,886 DEBUG Query with image successful
2024-08-22 16:46:36,065 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 16:46:36,102 DEBUG OpenAI client created
2024-08-22 16:46:41,267 DEBUG Querying model with image: gpt-4o
2024-08-22 16:46:57,082 DEBUG Query with image successful
2024-08-22 16:46:57,171 DEBUG Querying model with image: gpt-4o
2024-08-22 16:47:06,028 DEBUG Query with image successful
2024-08-22 16:47:06,341 DEBUG Querying model with image: gpt-4o
2024-08-22 16:47:21,311 DEBUG Query with image successful
2024-08-22 16:47:21,411 DEBUG Querying model with image: gpt-4o
2024-08-22 16:47:35,643 DEBUG Query with image successful
2024-08-22 16:47:45,953 DEBUG Querying model with image: gpt-4o
2024-08-22 16:47:56,960 DEBUG Query with image successful
2024-08-22 16:47:57,066 DEBUG Querying model with image: gpt-4o
2024-08-22 16:48:04,031 DEBUG Query with image successful
2024-08-22 16:48:04,406 DEBUG Querying model with image: gpt-4o
2024-08-22 16:48:12,536 DEBUG Query with image successful
2024-08-22 16:48:12,627 DEBUG Querying model with image: gpt-4o
2024-08-22 16:48:20,786 DEBUG Query with image successful
2024-08-22 16:48:21,197 DEBUG Querying model with image: gpt-4o
2024-08-22 16:48:30,586 DEBUG Query with image successful
2024-08-22 16:48:30,684 DEBUG Querying model with image: gpt-4o
2024-08-22 16:48:37,650 DEBUG Query with image successful
2024-08-22 16:48:37,960 DEBUG Querying model with image: gpt-4o
2024-08-22 16:48:48,649 DEBUG Query with image successful
2024-08-22 16:48:48,753 DEBUG Querying model with image: gpt-4o
2024-08-22 16:48:56,856 DEBUG Query with image successful
2024-08-22 16:48:57,257 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:05,161 DEBUG Query with image successful
2024-08-22 16:49:05,268 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:12,005 DEBUG Query with image successful
2024-08-22 16:49:12,322 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:20,045 DEBUG Query with image successful
2024-08-22 16:49:20,141 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:26,505 DEBUG Query with image successful
2024-08-22 16:49:26,804 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:34,953 DEBUG Query with image successful
2024-08-22 16:49:35,045 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:44,755 DEBUG Query with image successful
2024-08-22 16:49:45,076 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:51,911 DEBUG Query with image successful
2024-08-22 16:49:52,010 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:59,558 DEBUG Query with image successful
2024-08-22 16:49:59,931 DEBUG Querying model with image: gpt-4o
2024-08-22 16:50:08,645 DEBUG Query with image successful
2024-08-22 16:50:08,753 DEBUG Querying model with image: gpt-4o
2024-08-22 16:50:14,589 DEBUG Query with image successful
2024-08-22 16:50:14,935 DEBUG Querying model with image: gpt-4o
2024-08-22 16:50:25,601 DEBUG Query with image successful
2024-08-22 16:50:25,701 DEBUG Querying model with image: gpt-4o
2024-08-22 16:50:35,047 DEBUG Query with image successful
2024-08-22 16:50:35,383 DEBUG Querying model with image: gpt-4o
2024-08-22 16:50:43,321 DEBUG Query with image successful
2024-08-22 16:50:43,407 DEBUG Querying model with image: gpt-4o
2024-08-22 16:50:51,640 DEBUG Query with image successful
2024-08-22 16:50:51,947 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:01,007 DEBUG Query with image successful
2024-08-22 16:51:01,100 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:08,446 DEBUG Query with image successful
2024-08-22 16:51:08,743 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:14,997 DEBUG Query with image successful
2024-08-22 16:51:15,090 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:21,825 DEBUG Query with image successful
2024-08-22 16:51:22,145 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:28,832 DEBUG Query with image successful
2024-08-22 16:51:28,923 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:36,209 DEBUG Query with image successful
2024-08-22 16:51:36,512 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:44,517 DEBUG Query with image successful
2024-08-22 16:51:44,611 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:53,039 DEBUG Query with image successful
2024-08-22 16:51:53,358 DEBUG Querying model with image: gpt-4o
2024-08-22 16:52:00,216 DEBUG Query with image successful
2024-08-22 16:52:00,305 DEBUG Querying model with image: gpt-4o
2024-08-22 16:52:06,819 DEBUG Query with image successful
2024-08-22 16:52:07,112 DEBUG Querying model with image: gpt-4o
2024-08-22 16:52:14,651 DEBUG Query with image successful
2024-08-22 16:52:14,739 DEBUG Querying model with image: gpt-4o
2024-08-22 16:52:23,469 DEBUG Query with image successful
2024-08-22 16:52:23,791 DEBUG Querying model with image: gpt-4o
2024-08-22 16:52:31,864 DEBUG Query with image successful
2024-08-22 16:52:31,956 DEBUG Querying model with image: gpt-4o
2024-08-22 16:52:39,839 DEBUG Query with image successful
2024-08-22 16:52:40,122 DEBUG Querying model with image: gpt-4o
2024-08-22 16:52:50,416 DEBUG Query with image successful
2024-08-22 16:52:50,505 DEBUG Querying model with image: gpt-4o
2024-08-22 16:53:02,238 DEBUG Query with image successful
2024-08-22 16:53:02,524 DEBUG Querying model with image: gpt-4o
2024-08-22 16:53:14,489 DEBUG Query with image successful
2024-08-22 16:53:14,581 DEBUG Querying model with image: gpt-4o
2024-08-22 16:53:22,173 DEBUG Query with image successful
2024-08-22 16:53:22,454 DEBUG Querying model with image: gpt-4o
2024-08-22 16:53:28,975 DEBUG Query with image successful
2024-08-22 16:53:29,072 DEBUG Querying model with image: gpt-4o
2024-08-22 16:53:36,259 DEBUG Query with image successful
2024-08-22 16:53:36,556 DEBUG Querying model with image: gpt-4o
2024-08-22 16:53:43,141 DEBUG Query with image successful
2024-08-22 16:53:43,235 DEBUG Querying model with image: gpt-4o
2024-08-22 16:54:23,734 DEBUG Query with image successful
2024-08-22 16:54:24,012 DEBUG Querying model with image: gpt-4o
2024-08-22 16:54:31,212 DEBUG Query with image successful
2024-08-22 16:54:31,300 DEBUG Querying model with image: gpt-4o
2024-08-22 16:54:38,715 DEBUG Query with image successful
2024-08-22 19:14:00,827 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 19:14:00,888 DEBUG OpenAI client created
2024-08-22 20:47:46,812 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 20:47:46,839 DEBUG OpenAI client created
2024-08-22 20:53:53,324 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 20:53:53,355 DEBUG OpenAI client created
2024-08-22 21:14:41,848 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 21:14:41,881 DEBUG OpenAI client created
2024-08-22 21:14:47,054 DEBUG Querying model with image: gpt-4o
2024-08-22 21:15:00,538 DEBUG Query with image successful
2024-08-22 21:15:00,629 DEBUG Querying model with image: gpt-4o
2024-08-22 21:15:14,831 DEBUG Query with image successful
2024-08-22 21:15:15,275 DEBUG Querying model with image: gpt-4o
2024-08-22 21:15:29,715 DEBUG Query with image successful
2024-08-22 21:15:29,820 DEBUG Querying model with image: gpt-4o
2024-08-22 21:15:49,379 DEBUG Query with image successful
2024-08-22 21:15:59,673 DEBUG Querying model with image: gpt-4o
2024-08-22 21:16:11,435 DEBUG Query with image successful
2024-08-22 21:16:11,538 DEBUG Querying model with image: gpt-4o
2024-08-22 21:16:22,083 DEBUG Query with image successful
2024-08-22 21:16:22,421 DEBUG Querying model with image: gpt-4o
2024-08-22 21:16:30,751 DEBUG Query with image successful
2024-08-22 21:16:30,844 DEBUG Querying model with image: gpt-4o
2024-08-22 21:16:42,406 DEBUG Query with image successful
2024-08-22 21:16:42,731 DEBUG Querying model with image: gpt-4o
2024-08-22 21:16:52,421 DEBUG Query with image successful
2024-08-22 21:16:52,526 DEBUG Querying model with image: gpt-4o
2024-08-22 21:17:01,793 DEBUG Query with image successful
2024-08-22 21:17:02,130 DEBUG Querying model with image: gpt-4o
2024-08-22 21:17:11,178 DEBUG Query with image successful
2024-08-22 21:17:11,275 DEBUG Querying model with image: gpt-4o
2024-08-22 21:17:20,999 DEBUG Query with image successful
2024-08-22 21:17:21,346 DEBUG Querying model with image: gpt-4o
2024-08-22 21:17:30,514 DEBUG Query with image successful
2024-08-22 21:17:30,609 DEBUG Querying model with image: gpt-4o
2024-08-22 21:17:39,469 DEBUG Query with image successful
2024-08-22 21:17:39,898 DEBUG Querying model with image: gpt-4o
2024-08-22 21:17:47,908 DEBUG Query with image successful
2024-08-22 21:17:48,004 DEBUG Querying model with image: gpt-4o
2024-08-22 21:17:55,762 DEBUG Query with image successful
2024-08-22 21:17:56,088 DEBUG Querying model with image: gpt-4o
2024-08-22 21:18:04,520 DEBUG Query with image successful
2024-08-22 21:18:04,620 DEBUG Querying model with image: gpt-4o
2024-08-22 21:18:12,655 DEBUG Query with image successful
2024-08-22 21:18:13,011 DEBUG Querying model with image: gpt-4o
2024-08-22 21:18:21,224 DEBUG Query with image successful
2024-08-22 21:18:21,322 DEBUG Querying model with image: gpt-4o
2024-08-22 21:18:29,821 DEBUG Query with image successful
2024-08-22 21:18:30,187 DEBUG Querying model with image: gpt-4o
2024-08-22 21:18:40,158 DEBUG Query with image successful
2024-08-22 21:18:40,257 DEBUG Querying model with image: gpt-4o
2024-08-22 21:18:49,371 DEBUG Query with image successful
2024-08-22 21:18:49,728 DEBUG Querying model with image: gpt-4o
2024-08-22 21:18:58,118 DEBUG Query with image successful
2024-08-22 21:18:58,211 DEBUG Querying model with image: gpt-4o
2024-08-22 21:19:05,378 DEBUG Query with image successful
2024-08-22 21:19:05,705 DEBUG Querying model with image: gpt-4o
2024-08-22 21:19:13,820 DEBUG Query with image successful
2024-08-22 21:19:13,915 DEBUG Querying model with image: gpt-4o
2024-08-22 21:19:21,733 DEBUG Query with image successful
2024-08-22 21:19:22,049 DEBUG Querying model with image: gpt-4o
2024-08-22 21:19:30,809 DEBUG Query with image successful
2024-08-22 21:19:30,913 DEBUG Querying model with image: gpt-4o
2024-08-22 21:19:37,972 DEBUG Query with image successful
2024-08-22 21:19:38,287 DEBUG Querying model with image: gpt-4o
2024-08-22 21:19:47,333 DEBUG Query with image successful
2024-08-22 21:19:47,427 DEBUG Querying model with image: gpt-4o
2024-08-22 21:19:56,581 DEBUG Query with image successful
2024-08-22 21:19:56,904 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:05,351 DEBUG Query with image successful
2024-08-22 21:20:05,452 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:11,141 DEBUG Query with image successful
2024-08-22 21:20:11,481 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:19,793 DEBUG Query with image successful
2024-08-22 21:20:19,887 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:26,727 DEBUG Query with image successful
2024-08-22 21:20:27,096 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:34,017 DEBUG Query with image successful
2024-08-22 21:20:34,113 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:41,679 DEBUG Query with image successful
2024-08-22 21:20:41,997 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:49,210 DEBUG Query with image successful
2024-08-22 21:20:49,306 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:56,281 DEBUG Query with image successful
2024-08-22 21:20:56,610 DEBUG Querying model with image: gpt-4o
2024-08-22 21:21:03,519 DEBUG Query with image successful
2024-08-22 21:21:03,616 DEBUG Querying model with image: gpt-4o
2024-08-22 21:21:09,769 DEBUG Query with image successful
2024-08-22 21:21:10,086 DEBUG Querying model with image: gpt-4o
2024-08-22 21:21:17,793 DEBUG Query with image successful
2024-08-22 21:21:17,897 DEBUG Querying model with image: gpt-4o
2024-08-22 21:21:29,080 DEBUG Query with image successful
2024-08-22 21:21:29,399 DEBUG Querying model with image: gpt-4o
2024-08-22 21:21:37,585 DEBUG Query with image successful
2024-08-22 21:21:37,681 DEBUG Querying model with image: gpt-4o
2024-08-22 21:21:44,782 DEBUG Query with image successful
2024-08-22 21:21:45,090 DEBUG Querying model with image: gpt-4o
2024-08-22 21:21:54,051 DEBUG Query with image successful
2024-08-22 21:21:54,150 DEBUG Querying model with image: gpt-4o
2024-08-22 21:22:03,181 DEBUG Query with image successful
2024-08-22 21:22:03,488 DEBUG Querying model with image: gpt-4o
2024-08-22 21:22:10,911 DEBUG Query with image successful
2024-08-22 21:22:11,003 DEBUG Querying model with image: gpt-4o
2024-08-22 21:22:17,501 DEBUG Query with image successful
2024-08-22 21:22:17,839 DEBUG Querying model with image: gpt-4o
2024-08-22 21:22:30,681 DEBUG Query with image successful
2024-08-22 21:22:30,778 DEBUG Querying model with image: gpt-4o
2024-08-22 21:22:38,385 DEBUG Query with image successful
2024-08-23 13:14:17,094 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-23 13:14:17,126 DEBUG OpenAI client created
2024-08-23 13:14:22,243 DEBUG Querying model with image: gpt-4o
2024-08-23 13:14:29,240 DEBUG Query with image successful
2024-08-23 13:14:29,303 DEBUG Querying model with image: gpt-4o
2024-08-23 13:14:36,092 DEBUG Query with image successful
2024-08-23 13:14:36,409 DEBUG Querying model with image: gpt-4o
2024-08-23 13:14:42,758 DEBUG Query with image successful
2024-08-23 13:14:42,816 DEBUG Querying model with image: gpt-4o
2024-08-23 13:14:49,296 DEBUG Query with image successful
2024-08-23 13:14:59,525 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:04,883 DEBUG Query with image successful
2024-08-23 13:15:04,940 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:11,650 DEBUG Query with image successful
2024-08-23 13:15:11,970 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:19,442 DEBUG Query with image successful
2024-08-23 13:15:19,498 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:25,306 DEBUG Query with image successful
2024-08-23 13:15:25,606 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:32,698 DEBUG Query with image successful
2024-08-23 13:15:32,764 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:38,322 DEBUG Query with image successful
2024-08-23 13:15:38,660 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:45,647 DEBUG Query with image successful
2024-08-23 13:15:45,705 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:51,089 DEBUG Query with image successful
2024-08-23 13:15:51,339 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:57,425 DEBUG Query with image successful
2024-08-23 13:15:57,489 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:04,238 DEBUG Query with image successful
2024-08-23 13:16:04,486 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:11,144 DEBUG Query with image successful
2024-08-23 13:16:11,201 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:17,536 DEBUG Query with image successful
2024-08-23 13:16:17,808 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:24,833 DEBUG Query with image successful
2024-08-23 13:16:24,891 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:32,612 DEBUG Query with image successful
2024-08-23 13:16:32,912 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:39,129 DEBUG Query with image successful
2024-08-23 13:16:39,188 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:44,355 DEBUG Query with image successful
2024-08-23 13:16:44,611 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:50,478 DEBUG Query with image successful
2024-08-23 13:16:50,533 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:55,553 DEBUG Query with image successful
2024-08-23 13:16:55,846 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:01,150 DEBUG Query with image successful
2024-08-23 13:17:01,210 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:07,089 DEBUG Query with image successful
2024-08-23 13:17:07,337 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:12,123 DEBUG Query with image successful
2024-08-23 13:17:12,180 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:18,083 DEBUG Query with image successful
2024-08-23 13:17:18,378 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:24,110 DEBUG Query with image successful
2024-08-23 13:17:24,167 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:29,251 DEBUG Query with image successful
2024-08-23 13:17:29,491 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:35,780 DEBUG Query with image successful
2024-08-23 13:17:35,837 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:43,722 DEBUG Query with image successful
2024-08-23 13:17:43,959 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:49,657 DEBUG Query with image successful
2024-08-23 13:17:49,717 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:57,292 DEBUG Query with image successful
2024-08-23 13:17:57,522 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:03,808 DEBUG Query with image successful
2024-08-23 13:18:03,872 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:08,198 DEBUG Query with image successful
2024-08-23 13:18:08,422 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:13,212 DEBUG Query with image successful
2024-08-23 13:18:13,273 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:17,061 DEBUG Query with image successful
2024-08-23 13:18:17,298 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:23,543 DEBUG Query with image successful
2024-08-23 13:18:23,601 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:28,989 DEBUG Query with image successful
2024-08-23 13:18:29,229 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:34,567 DEBUG Query with image successful
2024-08-23 13:18:34,625 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:42,072 DEBUG Query with image successful
2024-08-23 13:18:42,310 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:47,487 DEBUG Query with image successful
2024-08-23 13:18:47,554 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:54,023 DEBUG Query with image successful
2024-08-23 13:18:54,271 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:59,968 DEBUG Query with image successful
2024-08-23 13:19:00,039 DEBUG Querying model with image: gpt-4o
2024-08-23 13:19:04,853 DEBUG Query with image successful
2024-08-23 13:19:05,092 DEBUG Querying model with image: gpt-4o
2024-08-23 13:19:11,788 DEBUG Query with image successful
2024-08-23 13:19:11,846 DEBUG Querying model with image: gpt-4o
2024-08-23 13:19:17,059 DEBUG Query with image successful
2024-08-23 13:19:17,304 DEBUG Querying model with image: gpt-4o
2024-08-23 13:19:22,057 DEBUG Query with image successful
2024-08-23 13:19:22,117 DEBUG Querying model with image: gpt-4o
2024-08-23 13:19:28,818 DEBUG Query with image successful
2024-08-23 13:19:29,044 DEBUG Querying model with image: gpt-4o
2024-08-23 13:19:33,658 DEBUG Query with image successful
2024-08-23 13:19:33,716 DEBUG Querying model with image: gpt-4o
2024-08-23 13:19:39,459 DEBUG Query with image successful
2024-08-23 13:34:05,643 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-23 13:34:05,672 DEBUG OpenAI client created
2024-08-23 13:34:10,876 DEBUG Querying model with image: gpt-4o
2024-08-23 13:34:29,299 DEBUG Query with image successful
2024-08-23 13:34:29,384 DEBUG Querying model with image: gpt-4o
2024-08-23 13:34:35,975 DEBUG Query with image successful
2024-08-23 13:34:36,326 DEBUG Querying model with image: gpt-4o
2024-08-23 13:34:43,379 DEBUG Query with image successful
2024-08-23 13:34:43,461 DEBUG Querying model with image: gpt-4o
2024-08-23 13:34:51,630 DEBUG Query with image successful
2024-08-23 13:35:01,917 DEBUG Querying model with image: gpt-4o
2024-08-23 13:35:09,325 DEBUG Query with image successful
2024-08-23 13:35:09,417 DEBUG Querying model with image: gpt-4o
2024-08-23 13:35:15,512 DEBUG Query with image successful
2024-08-23 13:35:15,828 DEBUG Querying model with image: gpt-4o
2024-08-23 13:35:22,234 DEBUG Query with image successful
2024-08-23 13:35:22,319 DEBUG Querying model with image: gpt-4o
2024-08-23 13:35:28,249 DEBUG Query with image successful
2024-08-23 13:35:28,634 DEBUG Querying model with image: gpt-4o
2024-08-23 13:35:35,125 DEBUG Query with image successful
2024-08-23 13:35:35,215 DEBUG Querying model with image: gpt-4o
2024-08-23 13:35:43,819 DEBUG Query with image successful
2024-08-23 13:35:44,124 DEBUG Querying model with image: gpt-4o
2024-08-23 13:35:55,823 DEBUG Query with image successful
2024-08-23 13:35:55,914 DEBUG Querying model with image: gpt-4o
2024-08-23 13:36:04,707 DEBUG Query with image successful
2024-08-23 13:36:05,019 DEBUG Querying model with image: gpt-4o
2024-08-23 13:36:26,782 DEBUG Query with image successful
2024-08-23 13:36:26,878 DEBUG Querying model with image: gpt-4o
2024-08-23 13:36:34,669 DEBUG Query with image successful
2024-08-23 13:36:35,056 DEBUG Querying model with image: gpt-4o
2024-08-23 13:37:27,460 DEBUG Query with image successful
2024-08-23 13:37:27,560 DEBUG Querying model with image: gpt-4o
2024-08-23 13:37:34,876 DEBUG Query with image successful
2024-08-23 13:37:35,208 DEBUG Querying model with image: gpt-4o
2024-08-23 13:37:42,269 DEBUG Query with image successful
2024-08-23 13:37:42,367 DEBUG Querying model with image: gpt-4o
2024-08-23 13:37:48,913 DEBUG Query with image successful
2024-08-23 13:37:49,308 DEBUG Querying model with image: gpt-4o
2024-08-23 13:37:55,609 DEBUG Query with image successful
2024-08-23 13:37:55,711 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:02,089 DEBUG Query with image successful
2024-08-23 13:38:02,415 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:08,340 DEBUG Query with image successful
2024-08-23 13:38:08,440 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:14,905 DEBUG Query with image successful
2024-08-23 13:38:15,209 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:22,277 DEBUG Query with image successful
2024-08-23 13:38:22,372 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:28,223 DEBUG Query with image successful
2024-08-23 13:38:28,518 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:35,162 DEBUG Query with image successful
2024-08-23 13:38:35,260 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:42,316 DEBUG Query with image successful
2024-08-23 13:38:42,634 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:49,198 DEBUG Query with image successful
2024-08-23 13:38:49,298 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:55,741 DEBUG Query with image successful
2024-08-23 13:38:56,070 DEBUG Querying model with image: gpt-4o
2024-08-23 13:39:43,698 DEBUG Query with image successful
2024-08-23 13:39:43,793 DEBUG Querying model with image: gpt-4o
2024-08-23 13:39:50,089 DEBUG Query with image successful
2024-08-23 13:39:50,500 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:02,545 DEBUG Query with image successful
2024-08-23 13:40:02,640 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:08,118 DEBUG Query with image successful
2024-08-23 13:40:08,417 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:15,619 DEBUG Query with image successful
2024-08-23 13:40:15,718 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:21,255 DEBUG Query with image successful
2024-08-23 13:40:21,552 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:27,291 DEBUG Query with image successful
2024-08-23 13:40:27,382 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:33,128 DEBUG Query with image successful
2024-08-23 13:40:33,471 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:41,201 DEBUG Query with image successful
2024-08-23 13:40:41,292 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:46,386 DEBUG Query with image successful
2024-08-23 13:40:46,680 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:51,738 DEBUG Query with image successful
2024-08-23 13:40:51,834 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:57,067 DEBUG Query with image successful
2024-08-23 13:40:57,361 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:03,941 DEBUG Query with image successful
2024-08-23 13:41:04,050 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:10,515 DEBUG Query with image successful
2024-08-23 13:41:10,824 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:16,681 DEBUG Query with image successful
2024-08-23 13:41:16,778 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:23,271 DEBUG Query with image successful
2024-08-23 13:41:23,579 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:30,199 DEBUG Query with image successful
2024-08-23 13:41:30,290 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:36,007 DEBUG Query with image successful
2024-08-23 13:41:36,298 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:44,253 DEBUG Query with image successful
2024-08-23 13:41:44,353 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:49,817 DEBUG Query with image successful
2024-08-23 13:41:50,106 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:55,132 DEBUG Query with image successful
2024-08-23 13:41:55,229 DEBUG Querying model with image: gpt-4o
2024-08-23 13:42:00,162 DEBUG Query with image successful
2024-08-23 13:55:21,089 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-23 13:55:21,136 DEBUG OpenAI client created
2024-08-23 13:55:26,310 DEBUG Querying model with image: gpt-4o
2024-08-23 13:55:33,478 DEBUG Query with image successful
2024-08-23 13:55:33,579 DEBUG Querying model with image: gpt-4o
2024-08-23 13:55:41,588 DEBUG Query with image successful
2024-08-23 13:55:41,955 DEBUG Querying model with image: gpt-4o
2024-08-23 13:55:49,034 DEBUG Query with image successful
2024-08-23 13:55:49,138 DEBUG Querying model with image: gpt-4o
2024-08-23 13:55:56,068 DEBUG Query with image successful
2024-08-23 13:56:06,347 DEBUG Querying model with image: gpt-4o
2024-08-23 13:56:14,242 DEBUG Query with image successful
2024-08-23 13:56:14,341 DEBUG Querying model with image: gpt-4o
2024-08-23 13:56:20,867 DEBUG Query with image successful
2024-08-23 13:56:21,259 DEBUG Querying model with image: gpt-4o
2024-08-23 13:56:27,647 DEBUG Query with image successful
2024-08-23 13:56:27,748 DEBUG Querying model with image: gpt-4o
2024-08-23 13:56:34,349 DEBUG Query with image successful
2024-08-23 13:56:34,818 DEBUG Querying model with image: gpt-4o
2024-08-23 13:56:42,373 DEBUG Query with image successful
2024-08-23 13:56:42,469 DEBUG Querying model with image: gpt-4o
2024-08-23 13:56:50,040 DEBUG Query with image successful
2024-08-23 13:56:50,361 DEBUG Querying model with image: gpt-4o
2024-08-23 13:56:58,243 DEBUG Query with image successful
2024-08-23 13:56:58,347 DEBUG Querying model with image: gpt-4o
2024-08-23 13:57:05,191 DEBUG Query with image successful
2024-08-23 13:57:05,491 DEBUG Querying model with image: gpt-4o
2024-08-23 13:57:16,233 DEBUG Query with image successful
2024-08-23 13:57:16,337 DEBUG Querying model with image: gpt-4o
2024-08-23 13:57:22,483 DEBUG Query with image successful
2024-08-23 13:57:22,909 DEBUG Querying model with image: gpt-4o
2024-08-23 13:57:30,299 DEBUG Query with image successful
2024-08-23 13:57:30,395 DEBUG Querying model with image: gpt-4o
2024-08-23 13:58:19,504 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-23 13:58:19,542 DEBUG OpenAI client created
2024-08-23 13:58:24,727 DEBUG Querying model with image: gpt-4o
2024-08-23 13:58:32,146 DEBUG Query with image successful
2024-08-23 13:58:32,238 DEBUG Querying model with image: gpt-4o
2024-08-23 13:58:38,486 DEBUG Query with image successful
2024-08-23 13:58:38,935 DEBUG Querying model with image: gpt-4o
2024-08-23 13:58:47,752 DEBUG Query with image successful
2024-08-23 13:58:47,841 DEBUG Querying model with image: gpt-4o
2024-08-23 13:58:55,637 DEBUG Query with image successful
2024-08-23 13:59:05,927 DEBUG Querying model with image: gpt-4o
2024-08-23 13:59:15,182 DEBUG Query with image successful
2024-08-23 13:59:15,271 DEBUG Querying model with image: gpt-4o
2024-08-23 13:59:24,860 DEBUG Query with image successful
2024-08-23 13:59:25,250 DEBUG Querying model with image: gpt-4o
2024-08-23 13:59:32,731 DEBUG Query with image successful
2024-08-23 13:59:32,831 DEBUG Querying model with image: gpt-4o
2024-08-23 13:59:41,522 DEBUG Query with image successful
2024-08-23 13:59:41,825 DEBUG Querying model with image: gpt-4o
2024-08-23 13:59:50,631 DEBUG Query with image successful
2024-08-23 13:59:50,729 DEBUG Querying model with image: gpt-4o
2024-08-23 13:59:59,142 DEBUG Query with image successful
2024-08-23 13:59:59,446 DEBUG Querying model with image: gpt-4o
2024-08-23 14:00:07,125 DEBUG Query with image successful
2024-08-23 14:00:07,224 DEBUG Querying model with image: gpt-4o
2024-08-23 14:00:13,919 DEBUG Query with image successful
2024-08-23 14:00:14,281 DEBUG Querying model with image: gpt-4o
2024-08-23 14:00:21,216 DEBUG Query with image successful
2024-08-23 14:00:21,312 DEBUG Querying model with image: gpt-4o
2024-08-23 14:00:28,426 DEBUG Query with image successful
2024-08-23 14:00:28,742 DEBUG Querying model with image: gpt-4o
2024-08-23 14:00:35,805 DEBUG Query with image successful
2024-08-23 14:00:35,902 DEBUG Querying model with image: gpt-4o
2024-08-23 14:00:43,817 DEBUG Query with image successful
2024-08-23 14:00:44,132 DEBUG Querying model with image: gpt-4o
2024-08-23 14:00:52,520 DEBUG Query with image successful
2024-08-23 14:00:52,611 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:00,359 DEBUG Query with image successful
2024-08-23 14:01:00,657 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:07,937 DEBUG Query with image successful
2024-08-23 14:01:08,033 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:15,703 DEBUG Query with image successful
2024-08-23 14:01:16,045 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:23,028 DEBUG Query with image successful
2024-08-23 14:01:23,129 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:29,930 DEBUG Query with image successful
2024-08-23 14:01:30,313 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:36,744 DEBUG Query with image successful
2024-08-23 14:01:36,841 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:44,380 DEBUG Query with image successful
2024-08-23 14:01:44,721 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:50,816 DEBUG Query with image successful
2024-08-23 14:01:50,908 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:59,773 DEBUG Query with image successful
2024-08-23 14:02:00,087 DEBUG Querying model with image: gpt-4o
2024-08-23 14:02:08,902 DEBUG Query with image successful
2024-08-23 14:02:09,006 DEBUG Querying model with image: gpt-4o
2024-08-23 14:02:17,516 DEBUG Query with image successful
2024-08-23 14:02:17,814 DEBUG Querying model with image: gpt-4o
2024-08-23 14:02:25,341 DEBUG Query with image successful
2024-08-23 14:02:25,439 DEBUG Querying model with image: gpt-4o
2024-08-23 14:02:34,381 DEBUG Query with image successful
2024-08-23 14:02:34,705 DEBUG Querying model with image: gpt-4o
2024-08-23 14:02:42,687 DEBUG Query with image successful
2024-08-23 14:02:42,776 DEBUG Querying model with image: gpt-4o
2024-08-23 14:02:49,566 DEBUG Query with image successful
2024-08-23 14:02:49,866 DEBUG Querying model with image: gpt-4o
2024-08-23 14:02:55,380 DEBUG Query with image successful
2024-08-23 14:02:55,482 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:02,909 DEBUG Query with image successful
2024-08-23 14:03:03,211 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:09,570 DEBUG Query with image successful
2024-08-23 14:03:09,660 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:15,463 DEBUG Query with image successful
2024-08-23 14:03:15,768 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:23,959 DEBUG Query with image successful
2024-08-23 14:03:24,052 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:29,993 DEBUG Query with image successful
2024-08-23 14:03:30,301 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:36,804 DEBUG Query with image successful
2024-08-23 14:03:36,899 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:42,179 DEBUG Query with image successful
2024-08-23 14:03:42,474 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:47,981 DEBUG Query with image successful
2024-08-23 14:03:48,074 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:54,339 DEBUG Query with image successful
2024-08-23 14:03:54,631 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:00,843 DEBUG Query with image successful
2024-08-23 14:04:00,945 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:07,932 DEBUG Query with image successful
2024-08-23 14:04:08,225 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:14,685 DEBUG Query with image successful
2024-08-23 14:04:14,775 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:20,984 DEBUG Query with image successful
2024-08-23 14:04:21,274 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:26,966 DEBUG Query with image successful
2024-08-23 14:04:27,058 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:32,882 DEBUG Query with image successful
2024-08-23 14:04:33,164 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:39,715 DEBUG Query with image successful
2024-08-23 14:04:39,805 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:46,553 DEBUG Query with image successful
2024-08-23 14:20:09,840 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-23 14:20:09,882 DEBUG OpenAI client created
2024-08-23 14:20:14,969 DEBUG Querying model with image: gpt-4o
2024-08-23 14:20:20,597 DEBUG Query with image successful
2024-08-23 14:20:20,641 DEBUG Querying model with image: gpt-4o
2024-08-23 14:20:25,595 DEBUG Query with image successful
2024-08-23 14:20:25,794 DEBUG Querying model with image: gpt-4o
2024-08-23 14:20:32,967 DEBUG Query with image successful
2024-08-23 14:20:33,008 DEBUG Querying model with image: gpt-4o
2024-08-23 14:20:38,274 DEBUG Query with image successful
2024-08-23 14:20:48,488 DEBUG Querying model with image: gpt-4o
2024-08-23 14:20:54,150 DEBUG Query with image successful
2024-08-23 14:20:54,202 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:00,829 DEBUG Query with image successful
2024-08-23 14:21:01,105 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:05,802 DEBUG Query with image successful
2024-08-23 14:21:05,850 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:11,395 DEBUG Query with image successful
2024-08-23 14:21:11,651 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:17,340 DEBUG Query with image successful
2024-08-23 14:21:17,393 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:23,341 DEBUG Query with image successful
2024-08-23 14:21:23,676 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:30,842 DEBUG Query with image successful
2024-08-23 14:21:30,887 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:35,311 DEBUG Query with image successful
2024-08-23 14:21:35,521 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:41,773 DEBUG Query with image successful
2024-08-23 14:21:41,816 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:49,082 DEBUG Query with image successful
2024-08-23 14:21:49,421 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:55,401 DEBUG Query with image successful
2024-08-23 14:21:55,445 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:00,128 DEBUG Query with image successful
2024-08-23 14:22:00,511 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:09,583 DEBUG Query with image successful
2024-08-23 14:22:09,644 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:14,371 DEBUG Query with image successful
2024-08-23 14:22:14,638 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:19,191 DEBUG Query with image successful
2024-08-23 14:22:19,248 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:23,556 DEBUG Query with image successful
2024-08-23 14:22:23,779 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:29,976 DEBUG Query with image successful
2024-08-23 14:22:30,019 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:34,323 DEBUG Query with image successful
2024-08-23 14:22:34,667 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:40,376 DEBUG Query with image successful
2024-08-23 14:22:40,422 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:47,257 DEBUG Query with image successful
2024-08-23 14:22:47,501 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:52,380 DEBUG Query with image successful
2024-08-23 14:22:52,429 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:56,512 DEBUG Query with image successful
2024-08-23 14:22:56,739 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:02,790 DEBUG Query with image successful
2024-08-23 14:23:02,840 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:06,916 DEBUG Query with image successful
2024-08-23 14:23:07,168 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:12,191 DEBUG Query with image successful
2024-08-23 14:23:12,236 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:16,308 DEBUG Query with image successful
2024-08-23 14:23:16,533 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:21,582 DEBUG Query with image successful
2024-08-23 14:23:21,632 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:26,147 DEBUG Query with image successful
2024-08-23 14:23:26,382 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:31,056 DEBUG Query with image successful
2024-08-23 14:23:31,099 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:35,750 DEBUG Query with image successful
2024-08-23 14:23:35,965 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:40,686 DEBUG Query with image successful
2024-08-23 14:23:40,729 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:44,608 DEBUG Query with image successful
2024-08-23 14:23:44,811 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:49,285 DEBUG Query with image successful
2024-08-23 14:23:49,329 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:54,468 DEBUG Query with image successful
2024-08-23 14:23:54,749 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:58,907 DEBUG Query with image successful
2024-08-23 14:23:58,961 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:02,884 DEBUG Query with image successful
2024-08-23 14:24:03,099 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:07,790 DEBUG Query with image successful
2024-08-23 14:24:07,838 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:12,696 DEBUG Query with image successful
2024-08-23 14:24:12,908 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:17,587 DEBUG Query with image successful
2024-08-23 14:24:17,628 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:21,628 DEBUG Query with image successful
2024-08-23 14:24:21,829 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:26,093 DEBUG Query with image successful
2024-08-23 14:24:26,145 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:29,426 DEBUG Query with image successful
2024-08-23 14:24:29,636 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:33,538 DEBUG Query with image successful
2024-08-23 14:24:33,581 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:37,341 DEBUG Query with image successful
2024-08-23 14:24:37,540 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:42,071 DEBUG Query with image successful
2024-08-23 14:24:42,125 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:45,761 DEBUG Query with image successful
2024-08-26 16:05:31,274 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:05:31,454 DEBUG OpenAI client created
2024-08-26 16:05:36,520 DEBUG Querying model with image: gpt-4o
2024-08-26 16:05:42,632 DEBUG Query with image successful
2024-08-26 16:05:42,671 DEBUG Querying model with image: gpt-4o
2024-08-26 16:05:47,665 DEBUG Query with image successful
2024-08-26 16:05:47,846 DEBUG Querying model with image: gpt-4o
2024-08-26 16:05:52,406 DEBUG Query with image successful
2024-08-26 16:05:52,440 DEBUG Querying model with image: gpt-4o
2024-08-26 16:05:57,092 DEBUG Query with image successful
2024-08-26 16:06:08,001 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:13,747 DEBUG Query with image successful
2024-08-26 16:06:13,780 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:18,085 DEBUG Query with image successful
2024-08-26 16:06:18,287 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:22,824 DEBUG Query with image successful
2024-08-26 16:06:22,863 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:27,705 DEBUG Query with image successful
2024-08-26 16:06:27,970 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:32,987 DEBUG Query with image successful
2024-08-26 16:06:33,022 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:38,012 DEBUG Query with image successful
2024-08-26 16:06:38,266 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:42,184 DEBUG Query with image successful
2024-08-26 16:06:42,219 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:47,988 DEBUG Query with image successful
2024-08-26 16:06:48,202 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:53,142 DEBUG Query with image successful
2024-08-26 16:06:53,174 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:57,857 DEBUG Query with image successful
2024-08-26 16:06:58,069 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:04,586 DEBUG Query with image successful
2024-08-26 16:07:04,624 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:08,884 DEBUG Query with image successful
2024-08-26 16:07:09,090 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:14,303 DEBUG Query with image successful
2024-08-26 16:07:14,344 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:22,950 DEBUG Query with image successful
2024-08-26 16:07:23,198 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:28,681 DEBUG Query with image successful
2024-08-26 16:07:28,716 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:33,237 DEBUG Query with image successful
2024-08-26 16:07:33,430 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:38,565 DEBUG Query with image successful
2024-08-26 16:07:38,599 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:43,071 DEBUG Query with image successful
2024-08-26 16:07:43,270 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:47,959 DEBUG Query with image successful
2024-08-26 16:07:47,992 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:52,672 DEBUG Query with image successful
2024-08-26 16:07:52,865 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:58,217 DEBUG Query with image successful
2024-08-26 16:07:58,249 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:03,124 DEBUG Query with image successful
2024-08-26 16:08:03,318 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:07,634 DEBUG Query with image successful
2024-08-26 16:08:07,667 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:12,435 DEBUG Query with image successful
2024-08-26 16:08:13,110 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:17,299 DEBUG Query with image successful
2024-08-26 16:08:17,333 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:23,450 DEBUG Query with image successful
2024-08-26 16:08:23,648 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:28,341 DEBUG Query with image successful
2024-08-26 16:08:28,378 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:33,131 DEBUG Query with image successful
2024-08-26 16:08:33,330 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:38,115 DEBUG Query with image successful
2024-08-26 16:08:38,149 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:42,439 DEBUG Query with image successful
2024-08-26 16:08:42,628 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:48,234 DEBUG Query with image successful
2024-08-26 16:08:48,268 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:54,205 DEBUG Query with image successful
2024-08-26 16:08:54,391 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:01,944 DEBUG Query with image successful
2024-08-26 16:09:01,975 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:06,781 DEBUG Query with image successful
2024-08-26 16:09:06,983 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:10,871 DEBUG Query with image successful
2024-08-26 16:09:10,905 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:15,233 DEBUG Query with image successful
2024-08-26 16:09:15,465 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:20,548 DEBUG Query with image successful
2024-08-26 16:09:20,587 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:25,489 DEBUG Query with image successful
2024-08-26 16:09:25,676 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:30,692 DEBUG Query with image successful
2024-08-26 16:09:30,729 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:34,835 DEBUG Query with image successful
2024-08-26 16:09:35,015 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:39,626 DEBUG Query with image successful
2024-08-26 16:09:39,660 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:43,500 DEBUG Query with image successful
2024-08-26 16:09:43,672 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:47,717 DEBUG Query with image successful
2024-08-26 16:09:47,752 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:50,837 DEBUG Query with image successful
2024-08-26 16:09:51,014 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:58,620 DEBUG Query with image successful
2024-08-26 16:09:58,654 DEBUG Querying model with image: gpt-4o
2024-08-26 16:10:03,072 DEBUG Query with image successful
2024-08-26 16:23:39,291 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:23:39,314 DEBUG OpenAI client created
2024-08-26 16:23:44,378 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:23:53,976 DEBUG Query with image successful
2024-08-26 16:23:54,011 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:24:02,258 DEBUG Query with image successful
2024-08-26 16:24:02,432 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:24:11,333 DEBUG Query with image successful
2024-08-26 16:24:11,366 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:24:24,086 DEBUG Query with image successful
2024-08-26 16:24:34,265 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:24:43,010 DEBUG Query with image successful
2024-08-26 16:24:43,043 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:24:51,821 DEBUG Query with image successful
2024-08-26 16:24:52,003 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:25:01,292 DEBUG Query with image successful
2024-08-26 16:25:01,327 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:25:12,011 DEBUG Query with image successful
2024-08-26 16:25:12,190 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:25:21,285 DEBUG Query with image successful
2024-08-26 16:25:21,321 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:25:32,623 DEBUG Query with image successful
2024-08-26 16:25:32,803 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:25:41,155 DEBUG Query with image successful
2024-08-26 16:25:41,188 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:25:49,014 DEBUG Query with image successful
2024-08-26 16:25:49,195 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:25:59,797 DEBUG Query with image successful
2024-08-26 16:25:59,833 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:26:08,925 DEBUG Query with image successful
2024-08-26 16:26:09,243 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:26:17,737 DEBUG Query with image successful
2024-08-26 16:26:17,772 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:26:26,313 DEBUG Query with image successful
2024-08-26 16:26:26,499 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:26:35,553 DEBUG Query with image successful
2024-08-26 16:26:35,590 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:26:44,897 DEBUG Query with image successful
2024-08-26 16:26:45,081 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:26:53,049 DEBUG Query with image successful
2024-08-26 16:26:53,080 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:04,640 DEBUG Query with image successful
2024-08-26 16:27:04,868 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:12,884 DEBUG Query with image successful
2024-08-26 16:27:12,915 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:19,340 DEBUG Query with image successful
2024-08-26 16:27:19,536 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:26,588 DEBUG Query with image successful
2024-08-26 16:27:26,627 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:34,074 DEBUG Query with image successful
2024-08-26 16:27:34,264 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:42,484 DEBUG Query with image successful
2024-08-26 16:27:42,521 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:49,475 DEBUG Query with image successful
2024-08-26 16:27:49,671 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:59,612 DEBUG Query with image successful
2024-08-26 16:27:59,644 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:28:08,297 DEBUG Query with image successful
2024-08-26 16:28:08,522 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:28:17,937 DEBUG Query with image successful
2024-08-26 16:28:17,971 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:28:25,846 DEBUG Query with image successful
2024-08-26 16:28:26,046 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:28:33,937 DEBUG Query with image successful
2024-08-26 16:28:33,972 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:28:41,705 DEBUG Query with image successful
2024-08-26 16:28:41,892 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:28:51,394 DEBUG Query with image successful
2024-08-26 16:28:51,428 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:28:57,521 DEBUG Query with image successful
2024-08-26 16:28:57,711 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:29:07,067 DEBUG Query with image successful
2024-08-26 16:29:07,103 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:29:20,070 DEBUG Query with image successful
2024-08-26 16:29:20,274 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:29:33,627 DEBUG Query with image successful
2024-08-26 16:29:33,660 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:29:42,864 DEBUG Query with image successful
2024-08-26 16:29:43,197 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:29:52,606 DEBUG Query with image successful
2024-08-26 16:29:52,645 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:29:59,043 DEBUG Query with image successful
2024-08-26 16:29:59,225 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:30:08,670 DEBUG Query with image successful
2024-08-26 16:30:08,704 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:30:18,340 DEBUG Query with image successful
2024-08-26 16:30:18,538 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:30:28,869 DEBUG Query with image successful
2024-08-26 16:30:28,900 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:30:35,623 DEBUG Query with image successful
2024-08-26 16:30:35,813 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:30:45,110 DEBUG Query with image successful
2024-08-26 16:30:45,149 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:30:50,558 DEBUG Query with image successful
2024-08-26 16:30:50,750 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:30:57,778 DEBUG Query with image successful
2024-08-26 16:30:57,812 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:31:03,062 DEBUG Query with image successful
2024-08-26 16:31:03,238 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:31:10,262 DEBUG Query with image successful
2024-08-26 16:31:10,295 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:31:16,053 DEBUG Query with image successful
2024-08-26 16:43:12,670 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:43:12,691 DEBUG OpenAI client created
2024-08-26 16:43:17,757 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:43:28,341 DEBUG Query with image successful
2024-08-26 16:43:28,379 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:43:42,484 DEBUG Query with image successful
2024-08-26 16:43:42,661 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:43:50,602 DEBUG Query with image successful
2024-08-26 16:43:50,635 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:44:05,275 DEBUG Query with image successful
2024-08-26 16:44:15,450 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:44:24,531 DEBUG Query with image successful
2024-08-26 16:44:24,569 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:44:37,493 DEBUG Query with image successful
2024-08-26 16:44:37,682 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:44:46,592 DEBUG Query with image successful
2024-08-26 16:44:46,624 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:45:04,791 DEBUG Query with image successful
2024-08-26 16:45:04,971 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:45:18,815 DEBUG Query with image successful
2024-08-26 16:45:18,853 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:56:10,101 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:56:10,128 DEBUG OpenAI client created
2024-08-26 16:56:15,187 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:56:26,885 DEBUG Query with image successful
2024-08-26 16:56:26,918 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:56:40,927 DEBUG Query with image successful
2024-08-26 16:56:41,100 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:56:52,644 DEBUG Query with image successful
2024-08-26 16:56:52,681 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:57:09,606 DEBUG Query with image successful
2024-08-26 16:57:19,779 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:57:33,185 DEBUG Query with image successful
2024-08-26 16:57:33,225 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:57:51,253 DEBUG Query with image successful
2024-08-26 16:57:51,446 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:58:14,814 DEBUG Query with image successful
2024-08-26 16:58:14,848 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:58:54,453 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:58:54,477 DEBUG OpenAI client created
2024-08-26 16:58:59,540 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:59:15,576 DEBUG Query with image successful
2024-08-26 16:59:15,609 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:59:40,315 DEBUG Query with image successful
2024-08-26 16:59:40,508 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:59:50,286 DEBUG Query with image successful
2024-08-26 16:59:50,324 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:00:25,110 DEBUG Query with image successful
2024-08-26 17:00:35,397 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:00:51,921 DEBUG Query with image successful
2024-08-26 17:00:51,968 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:01:29,966 DEBUG Query with image successful
2024-08-26 17:01:30,138 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:01:37,870 DEBUG Query with image successful
2024-08-26 17:01:37,903 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:03:31,136 DEBUG Query with image successful
2024-08-26 17:03:31,335 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:03:48,564 DEBUG Query with image successful
2024-08-26 17:03:48,602 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:04:18,401 DEBUG Query with image successful
2024-08-26 17:04:18,585 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:04:35,620 DEBUG Query with image successful
2024-08-26 17:04:35,654 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:04:59,340 DEBUG Query with image successful
2024-08-26 17:04:59,517 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:05:08,478 DEBUG Query with image successful
2024-08-26 17:05:08,518 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:05:29,369 DEBUG Query with image successful
2024-08-26 17:05:29,599 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:05:40,834 DEBUG Query with image successful
2024-08-26 17:05:40,871 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:06:07,788 DEBUG Query with image successful
2024-08-26 17:06:08,032 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:06:17,760 DEBUG Query with image successful
2024-08-26 17:06:17,796 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:06:48,218 DEBUG Query with image successful
2024-08-26 17:06:48,432 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:06:59,644 DEBUG Query with image successful
2024-08-26 17:06:59,681 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:07:27,582 DEBUG Query with image successful
2024-08-26 17:07:27,770 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:07:39,028 DEBUG Query with image successful
2024-08-26 17:07:39,063 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:08:02,199 DEBUG Query with image successful
2024-08-26 17:08:02,457 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:08:09,706 DEBUG Query with image successful
2024-08-26 17:08:09,739 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:08:40,514 DEBUG Query with image successful
2024-08-26 17:08:40,729 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:08:49,661 DEBUG Query with image successful
2024-08-26 17:08:49,694 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:09:18,494 DEBUG Query with image successful
2024-08-26 17:09:18,678 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:09:37,040 DEBUG Query with image successful
2024-08-26 17:09:37,075 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:09:56,563 DEBUG Query with image successful
2024-08-26 17:09:56,744 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:10:07,137 DEBUG Query with image successful
2024-08-26 17:10:07,169 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:10:31,209 DEBUG Query with image successful
2024-08-26 17:10:31,388 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:10:50,170 DEBUG Query with image successful
2024-08-26 17:10:50,203 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:11:34,176 DEBUG Query with image successful
2024-08-26 17:11:34,424 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:11:44,906 DEBUG Query with image successful
2024-08-26 17:11:44,939 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:12:06,065 DEBUG Query with image successful
2024-08-26 17:12:06,242 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:12:15,726 DEBUG Query with image successful
2024-08-26 17:12:15,757 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:12:32,773 DEBUG Query with image successful
2024-08-26 17:12:32,947 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:12:42,790 DEBUG Query with image successful
2024-08-26 17:12:42,823 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:13:00,607 DEBUG Query with image successful
2024-08-26 17:13:00,787 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:13:12,436 DEBUG Query with image successful
2024-08-26 17:13:12,472 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:13:31,536 DEBUG Query with image successful
2024-08-26 17:13:31,725 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:13:45,520 DEBUG Query with image successful
2024-08-26 17:13:45,552 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:14:02,414 DEBUG Query with image successful
2024-08-26 17:14:02,600 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:14:12,228 DEBUG Query with image successful
2024-08-26 17:14:12,260 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:14:34,317 DEBUG Query with image successful
2024-08-26 17:14:34,493 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:14:43,035 DEBUG Query with image successful
2024-08-26 17:14:43,069 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:14:59,327 DEBUG Query with image successful
2024-08-26 17:14:59,510 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:15:11,130 DEBUG Query with image successful
2024-08-26 17:15:11,163 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:15:28,158 DEBUG Query with image successful
2024-08-26 17:15:28,331 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:15:37,090 DEBUG Query with image successful
2024-08-26 17:15:37,124 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:15:52,909 DEBUG Query with image successful
2024-08-26 17:51:12,607 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 17:51:12,634 DEBUG OpenAI client created
2024-08-26 17:51:12,634 DEBUG Model set to: gpt-4-turbo
2024-08-26 17:51:17,699 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:52:03,744 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 17:52:03,771 DEBUG OpenAI client created
2024-08-26 17:52:03,771 DEBUG Model set to: gpt-4-turbo
2024-08-26 17:52:08,847 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:52:18,780 DEBUG Query with image successful
2024-08-26 17:52:18,813 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:52:34,001 DEBUG Query with image successful
2024-08-26 17:52:34,276 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:52:44,155 DEBUG Query with image successful
2024-08-26 17:52:44,188 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:52:54,537 DEBUG Query with image successful
2024-08-26 17:53:04,712 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:53:16,892 DEBUG Query with image successful
2024-08-26 17:53:16,927 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:53:29,157 DEBUG Query with image successful
2024-08-26 17:53:29,402 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:53:39,221 DEBUG Query with image successful
2024-08-26 17:53:39,254 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:53:50,081 DEBUG Query with image successful
2024-08-26 17:53:50,350 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:54:02,223 DEBUG Query with image successful
2024-08-26 17:54:02,260 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:54:12,495 DEBUG Query with image successful
2024-08-26 17:54:12,728 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:54:23,076 DEBUG Query with image successful
2024-08-26 17:54:23,108 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:54:33,459 DEBUG Query with image successful
2024-08-26 17:54:33,654 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:54:42,569 DEBUG Query with image successful
2024-08-26 17:54:42,603 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:54:52,334 DEBUG Query with image successful
2024-08-26 17:54:52,528 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:55:04,721 DEBUG Query with image successful
2024-08-26 17:55:04,755 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:55:16,230 DEBUG Query with image successful
2024-08-26 17:55:16,483 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:55:29,935 DEBUG Query with image successful
2024-08-26 17:55:29,972 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:55:36,942 DEBUG Query with image successful
2024-08-26 17:55:37,134 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:55:46,346 DEBUG Query with image successful
2024-08-26 17:55:46,379 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:55:55,744 DEBUG Query with image successful
2024-08-26 17:55:55,970 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:56:04,924 DEBUG Query with image successful
2024-08-26 17:56:04,957 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:56:13,768 DEBUG Query with image successful
2024-08-26 17:56:13,996 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:56:23,095 DEBUG Query with image successful
2024-08-26 17:56:23,128 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:56:31,022 DEBUG Query with image successful
2024-08-26 17:56:31,197 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:56:42,010 DEBUG Query with image successful
2024-08-26 17:56:42,042 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:56:48,941 DEBUG Query with image successful
2024-08-26 17:56:49,125 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:56:59,019 DEBUG Query with image successful
2024-08-26 17:56:59,051 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:57:05,040 DEBUG Query with image successful
2024-08-26 17:57:05,228 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:57:15,178 DEBUG Query with image successful
2024-08-26 17:57:15,212 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:57:22,690 DEBUG Query with image successful
2024-08-26 17:57:22,873 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:57:31,085 DEBUG Query with image successful
2024-08-26 17:57:31,118 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:57:38,454 DEBUG Query with image successful
2024-08-26 17:57:38,638 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:57:48,903 DEBUG Query with image successful
2024-08-26 17:57:48,933 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:57:54,076 DEBUG Query with image successful
2024-08-26 17:57:54,249 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:02,917 DEBUG Query with image successful
2024-08-26 17:58:02,947 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:08,611 DEBUG Query with image successful
2024-08-26 17:58:08,782 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:17,704 DEBUG Query with image successful
2024-08-26 17:58:17,737 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:24,040 DEBUG Query with image successful
2024-08-26 17:58:24,215 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:34,083 DEBUG Query with image successful
2024-08-26 17:58:34,114 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:39,756 DEBUG Query with image successful
2024-08-26 17:58:39,933 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:49,002 DEBUG Query with image successful
2024-08-26 17:58:49,035 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:53,810 DEBUG Query with image successful
2024-08-26 17:58:54,000 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:02,857 DEBUG Query with image successful
2024-08-26 17:59:02,893 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:10,969 DEBUG Query with image successful
2024-08-26 17:59:11,141 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:20,709 DEBUG Query with image successful
2024-08-26 17:59:20,744 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:27,426 DEBUG Query with image successful
2024-08-26 17:59:27,597 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:36,698 DEBUG Query with image successful
2024-08-26 17:59:36,731 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:41,160 DEBUG Query with image successful
2024-08-26 17:59:41,340 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:51,300 DEBUG Query with image successful
2024-08-26 17:59:51,332 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:56,430 DEBUG Query with image successful
2024-08-28 20:43:19,108 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-28 20:43:19,150 DEBUG OpenAI client created
2024-08-28 20:43:19,151 DEBUG Model set to: gpt-4-turbo
2024-08-28 20:43:24,184 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:43:28,789 DEBUG Query with image successful
2024-08-28 20:43:28,801 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:43:35,605 DEBUG Query with image successful
2024-08-28 20:43:35,756 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:43:45,166 DEBUG Query with image successful
2024-08-28 20:43:45,183 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:43:53,430 DEBUG Query with image successful
2024-08-28 20:44:15,404 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-28 20:44:15,437 DEBUG OpenAI client created
2024-08-28 20:44:15,437 DEBUG Model set to: gpt-4-turbo
2024-08-28 20:44:20,466 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:44:30,612 DEBUG Query with image successful
2024-08-28 20:44:30,624 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:44:40,237 DEBUG Query with image successful
2024-08-28 20:44:40,447 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:44:47,680 DEBUG Query with image successful
2024-08-28 20:44:47,698 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:44:57,058 DEBUG Query with image successful
2024-08-28 20:45:07,201 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:47:35,398 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-28 20:47:35,424 DEBUG OpenAI client created
2024-08-28 20:47:35,425 DEBUG Model set to: gpt-4-turbo
2024-08-28 20:47:40,462 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:47:52,176 DEBUG Query with image successful
2024-08-28 20:47:52,188 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:48:05,303 DEBUG Query with image successful
2024-08-28 20:48:05,469 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:48:15,631 DEBUG Query with image successful
2024-08-28 20:48:15,647 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:48:24,295 DEBUG Query with image successful
2024-08-28 20:48:34,440 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:48:46,175 DEBUG Query with image successful
2024-08-28 20:48:46,192 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:49:00,185 DEBUG Query with image successful
2024-08-28 20:49:00,459 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:49:15,028 DEBUG Query with image successful
2024-08-28 20:49:15,045 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:49:29,271 DEBUG Query with image successful
2024-08-28 20:49:29,429 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:49:38,464 DEBUG Query with image successful
2024-08-28 20:49:38,486 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:49:51,934 DEBUG Query with image successful
2024-08-28 20:49:52,092 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:50:05,391 DEBUG Query with image successful
2024-08-28 20:50:05,411 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:50:15,471 DEBUG Query with image successful
2024-08-28 20:50:15,712 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:50:25,540 DEBUG Query with image successful
2024-08-28 20:50:25,556 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:50:37,316 DEBUG Query with image successful
2024-08-28 20:50:37,584 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:50:49,238 DEBUG Query with image successful
2024-08-28 20:50:49,268 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:50:58,338 DEBUG Query with image successful
2024-08-28 20:50:58,493 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:51:11,631 DEBUG Query with image successful
2024-08-28 20:51:11,647 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:51:22,497 DEBUG Query with image successful
2024-08-28 20:51:22,657 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:51:33,044 DEBUG Query with image successful
2024-08-28 20:51:33,064 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:51:53,391 DEBUG Query with image successful
2024-08-28 20:51:53,596 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:52:01,954 DEBUG Query with image successful
2024-08-28 20:52:01,983 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:52:10,766 DEBUG Query with image successful
2024-08-28 20:52:10,983 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:52:20,556 DEBUG Query with image successful
2024-08-28 20:52:20,574 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:52:28,485 DEBUG Query with image successful
2024-08-28 20:52:28,699 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:52:36,472 DEBUG Query with image successful
2024-08-28 20:52:36,497 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:52:45,274 DEBUG Query with image successful
2024-08-28 20:52:45,441 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:52:53,902 DEBUG Query with image successful
2024-08-28 20:52:53,930 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:53:01,645 DEBUG Query with image successful
2024-08-28 20:53:01,830 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:53:10,388 DEBUG Query with image successful
2024-08-28 20:53:10,410 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:53:17,397 DEBUG Query with image successful
2024-08-28 20:53:17,565 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:53:28,751 DEBUG Query with image successful
2024-08-28 20:53:28,772 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:53:35,630 DEBUG Query with image successful
2024-08-28 20:53:35,778 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:53:45,594 DEBUG Query with image successful
2024-08-28 20:53:45,614 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:53:53,741 DEBUG Query with image successful
2024-08-28 20:53:53,886 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:54:03,637 DEBUG Query with image successful
2024-08-28 20:54:03,659 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:54:10,129 DEBUG Query with image successful
2024-08-28 20:54:10,290 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:54:20,663 DEBUG Query with image successful
2024-08-28 20:54:20,681 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:54:27,457 DEBUG Query with image successful
2024-08-28 20:54:27,634 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:54:38,316 DEBUG Query with image successful
2024-08-28 20:54:38,336 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:54:45,523 DEBUG Query with image successful
2024-08-28 20:54:45,682 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:54:54,741 DEBUG Query with image successful
2024-08-28 20:54:54,760 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:55:00,140 DEBUG Query with image successful
2024-08-28 20:55:00,301 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:55:06,225 DEBUG Query with image successful
2024-08-28 20:55:06,242 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:55:15,183 DEBUG Query with image successful
2024-08-28 20:55:15,333 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:55:23,265 DEBUG Query with image successful
2024-08-28 20:55:23,284 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:55:33,444 DEBUG Query with image successful
2024-08-28 20:55:33,590 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:55:45,005 DEBUG Query with image successful
2024-08-28 20:55:45,023 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:55:55,500 DEBUG Query with image successful
2024-08-28 20:55:55,658 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:56:06,205 DEBUG Query with image successful
2024-08-28 20:56:06,222 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:56:16,917 DEBUG Query with image successful
2024-08-28 20:58:04,344 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-28 20:58:04,367 DEBUG OpenAI client created
2024-08-28 20:58:04,367 DEBUG Model set to: gpt-4-turbo
2024-08-28 20:58:09,400 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:58:12,814 DEBUG Query with image successful
2024-08-28 20:58:12,831 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:58:21,441 DEBUG Query with image successful
2024-08-28 20:58:21,583 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:58:31,989 DEBUG Query with image successful
2024-08-28 20:58:32,006 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:58:45,256 DEBUG Query with image successful
2024-08-28 20:58:55,401 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:59:04,881 DEBUG Query with image successful
2024-08-28 20:59:04,901 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:59:13,584 DEBUG Query with image successful
2024-08-28 20:59:13,741 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:59:25,476 DEBUG Query with image successful
2024-08-28 20:59:25,493 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:59:37,047 DEBUG Query with image successful
2024-08-28 20:59:37,298 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:59:43,737 DEBUG Query with image successful
2024-08-28 20:59:43,757 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:59:52,096 DEBUG Query with image successful
2024-08-28 20:59:52,255 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:00:01,019 DEBUG Query with image successful
2024-08-28 21:00:01,040 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:00:10,971 DEBUG Query with image successful
2024-08-28 21:00:11,131 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:00:26,173 DEBUG Query with image successful
2024-08-28 21:00:26,191 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:00:40,640 DEBUG Query with image successful
2024-08-28 21:00:40,800 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:00:48,256 DEBUG Query with image successful
2024-08-28 21:00:48,271 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:00:59,931 DEBUG Query with image successful
2024-08-28 21:01:00,122 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:01:09,535 DEBUG Query with image successful
2024-08-28 21:01:09,553 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:01:18,521 DEBUG Query with image successful
2024-08-28 21:01:18,695 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:01:28,761 DEBUG Query with image successful
2024-08-28 21:01:28,779 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:01:42,384 DEBUG Query with image successful
2024-08-28 21:01:42,611 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:01:51,436 DEBUG Query with image successful
2024-08-28 21:01:51,452 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:02:06,210 DEBUG Query with image successful
2024-08-28 21:02:06,371 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:02:15,167 DEBUG Query with image successful
2024-08-28 21:02:15,183 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:02:29,237 DEBUG Query with image successful
2024-08-28 21:02:29,421 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:02:36,993 DEBUG Query with image successful
2024-08-28 21:02:37,009 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:02:49,348 DEBUG Query with image successful
2024-08-28 21:02:49,501 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:03:09,855 DEBUG Query with image successful
2024-08-28 21:03:09,872 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:03:20,031 DEBUG Query with image successful
2024-08-28 21:03:20,191 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:03:29,765 DEBUG Query with image successful
2024-08-28 21:03:29,782 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:03:42,905 DEBUG Query with image successful
2024-08-28 21:03:43,063 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:03:57,574 DEBUG Query with image successful
2024-08-28 21:03:57,591 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:04:07,417 DEBUG Query with image successful
2024-08-28 21:04:07,568 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:04:13,983 DEBUG Query with image successful
2024-08-28 21:04:14,004 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:04:22,335 DEBUG Query with image successful
2024-08-28 21:04:22,501 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:04:29,942 DEBUG Query with image successful
2024-08-28 21:04:29,958 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:04:41,750 DEBUG Query with image successful
2024-08-28 21:04:41,903 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:04:49,230 DEBUG Query with image successful
2024-08-28 21:04:49,251 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:00,679 DEBUG Query with image successful
2024-08-28 21:05:00,838 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:06,573 DEBUG Query with image successful
2024-08-28 21:05:06,589 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:13,257 DEBUG Query with image successful
2024-08-28 21:05:13,424 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:20,950 DEBUG Query with image successful
2024-08-28 21:05:20,966 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:27,979 DEBUG Query with image successful
2024-08-28 21:05:28,132 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:37,152 DEBUG Query with image successful
2024-08-28 21:05:37,168 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:43,957 DEBUG Query with image successful
2024-08-28 21:05:44,104 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:51,221 DEBUG Query with image successful
2024-08-28 21:05:51,240 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:59,630 DEBUG Query with image successful
2024-08-28 21:05:59,821 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:06:08,485 DEBUG Query with image successful
2024-08-28 21:06:08,500 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:06:21,203 DEBUG Query with image successful
2024-08-28 21:06:21,348 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:06:33,064 DEBUG Query with image successful
2024-08-28 21:06:33,080 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:06:37,941 DEBUG Query with image successful
2024-08-29 14:02:29,242 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-29 14:02:29,264 DEBUG OpenAI client created
2024-08-29 14:02:29,264 DEBUG Model set to: gpt-4-turbo
2024-08-29 14:02:34,299 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:02:44,565 DEBUG Query with image successful
2024-08-29 14:02:44,578 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:02:58,306 DEBUG Query with image successful
2024-08-29 14:02:58,512 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:03:11,529 DEBUG Query with image successful
2024-08-29 14:03:11,547 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:03:22,832 DEBUG Query with image successful
2024-08-29 14:03:32,977 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:03:42,219 DEBUG Query with image successful
2024-08-29 14:03:42,243 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:03:53,364 DEBUG Query with image successful
2024-08-29 14:03:53,605 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:04:02,032 DEBUG Query with image successful
2024-08-29 14:04:02,049 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:04:13,854 DEBUG Query with image successful
2024-08-29 14:04:14,107 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:04:24,851 DEBUG Query with image successful
2024-08-29 14:04:24,876 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:04:38,800 DEBUG Query with image successful
2024-08-29 14:04:38,973 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:04:52,201 DEBUG Query with image successful
2024-08-29 14:04:52,218 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:05:02,524 DEBUG Query with image successful
2024-08-29 14:05:02,689 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:05:10,597 DEBUG Query with image successful
2024-08-29 14:05:10,613 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:05:22,561 DEBUG Query with image successful
2024-08-29 14:05:22,723 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:05:30,997 DEBUG Query with image successful
2024-08-29 14:05:31,017 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:05:42,547 DEBUG Query with image successful
2024-08-29 14:05:42,727 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:05:51,478 DEBUG Query with image successful
2024-08-29 14:05:51,497 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:06:00,635 DEBUG Query with image successful
2024-08-29 14:06:00,838 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:06:08,245 DEBUG Query with image successful
2024-08-29 14:06:08,264 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:06:22,642 DEBUG Query with image successful
2024-08-29 14:06:22,792 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:06:32,631 DEBUG Query with image successful
2024-08-29 14:06:32,646 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:06:44,369 DEBUG Query with image successful
2024-08-29 14:06:44,562 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:06:49,871 DEBUG Query with image successful
2024-08-29 14:06:49,889 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:06:57,149 DEBUG Query with image successful
2024-08-29 14:06:57,303 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:07:04,988 DEBUG Query with image successful
2024-08-29 14:07:05,005 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:07:13,081 DEBUG Query with image successful
2024-08-29 14:07:13,234 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:07:24,741 DEBUG Query with image successful
2024-08-29 14:07:24,760 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:07:34,095 DEBUG Query with image successful
2024-08-29 14:07:34,249 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:07:42,185 DEBUG Query with image successful
2024-08-29 14:07:42,205 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:07:53,248 DEBUG Query with image successful
2024-08-29 14:07:53,416 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:01,342 DEBUG Query with image successful
2024-08-29 14:08:01,360 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:10,155 DEBUG Query with image successful
2024-08-29 14:08:10,309 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:17,336 DEBUG Query with image successful
2024-08-29 14:08:17,353 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:26,002 DEBUG Query with image successful
2024-08-29 14:08:26,178 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:33,879 DEBUG Query with image successful
2024-08-29 14:08:33,899 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:40,606 DEBUG Query with image successful
2024-08-29 14:08:40,761 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:47,046 DEBUG Query with image successful
2024-08-29 14:08:47,063 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:56,275 DEBUG Query with image successful
2024-08-29 14:08:56,428 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:04,493 DEBUG Query with image successful
2024-08-29 14:09:04,510 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:11,639 DEBUG Query with image successful
2024-08-29 14:09:11,790 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:19,226 DEBUG Query with image successful
2024-08-29 14:09:19,242 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:25,585 DEBUG Query with image successful
2024-08-29 14:09:25,751 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:35,448 DEBUG Query with image successful
2024-08-29 14:09:35,466 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:40,800 DEBUG Query with image successful
2024-08-29 14:09:40,949 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:49,858 DEBUG Query with image successful
2024-08-29 14:09:49,877 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:55,618 DEBUG Query with image successful
2024-08-29 14:09:55,761 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:10:03,957 DEBUG Query with image successful
2024-08-29 14:10:03,976 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:10:14,767 DEBUG Query with image successful
2024-08-29 14:10:14,910 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:10:25,629 DEBUG Query with image successful
2024-08-29 14:10:25,645 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:10:30,283 DEBUG Query with image successful
2024-09-18 19:51:46,794 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-18 19:51:46,838 DEBUG OpenAI client created
2024-09-18 19:51:46,838 DEBUG Model set to: gpt-4-turbo
2024-09-18 19:51:51,997 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:52:08,160 DEBUG Query with image successful
2024-09-18 19:52:08,238 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:52:23,934 DEBUG Query with image successful
2024-09-18 19:52:24,305 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:52:39,490 DEBUG Query with image successful
2024-09-18 19:52:39,571 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:52:56,512 DEBUG Query with image successful
2024-09-18 19:53:06,778 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:53:20,464 DEBUG Query with image successful
2024-09-18 19:53:20,547 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:53:35,377 DEBUG Query with image successful
2024-09-18 19:53:35,644 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:53:57,314 DEBUG Query with image successful
2024-09-18 19:53:57,395 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:54:15,168 DEBUG Query with image successful
2024-09-18 19:54:15,456 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:54:30,211 DEBUG Query with image successful
2024-09-18 19:54:30,293 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:54:43,898 DEBUG Query with image successful
2024-09-18 19:54:44,187 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:54:59,708 DEBUG Query with image successful
2024-09-18 19:54:59,789 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:55:14,171 DEBUG Query with image successful
2024-09-18 19:55:14,564 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:55:28,858 DEBUG Query with image successful
2024-09-18 19:55:28,948 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:55:43,182 DEBUG Query with image successful
2024-09-18 19:55:43,492 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:56:02,942 DEBUG Query with image successful
2024-09-18 19:56:03,040 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:56:23,454 DEBUG Query with image successful
2024-09-18 19:56:23,766 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:56:41,959 DEBUG Query with image successful
2024-09-18 19:56:42,047 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:56:59,576 DEBUG Query with image successful
2024-09-18 19:56:59,923 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:57:18,008 DEBUG Query with image successful
2024-09-18 19:57:18,099 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:57:39,898 DEBUG Query with image successful
2024-09-18 19:57:40,215 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:57:56,068 DEBUG Query with image successful
2024-09-18 19:57:56,172 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:58:12,833 DEBUG Query with image successful
2024-09-18 19:58:13,166 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:58:28,251 DEBUG Query with image successful
2024-09-18 19:58:28,344 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:58:44,413 DEBUG Query with image successful
2024-09-18 19:58:44,728 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:58:59,141 DEBUG Query with image successful
2024-09-18 19:58:59,229 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:59:14,384 DEBUG Query with image successful
2024-09-18 19:59:14,685 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:59:30,278 DEBUG Query with image successful
2024-09-18 19:59:30,369 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:59:45,250 DEBUG Query with image successful
2024-09-18 19:59:45,557 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:00:00,459 DEBUG Query with image successful
2024-09-18 20:00:00,545 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:00:16,494 DEBUG Query with image successful
2024-09-18 20:00:16,798 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:00:31,492 DEBUG Query with image successful
2024-09-18 20:00:31,579 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:00:48,246 DEBUG Query with image successful
2024-09-18 20:00:48,540 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:01:06,863 DEBUG Query with image successful
2024-09-18 20:01:06,966 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:01:21,519 DEBUG Query with image successful
2024-09-18 20:01:21,817 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:01:38,883 DEBUG Query with image successful
2024-09-18 20:01:38,970 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:01:53,294 DEBUG Query with image successful
2024-09-18 20:01:53,587 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:02:10,558 DEBUG Query with image successful
2024-09-18 20:02:10,644 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:02:23,200 DEBUG Query with image successful
2024-09-18 20:02:23,513 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:02:40,055 DEBUG Query with image successful
2024-09-18 20:02:40,148 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:02:53,589 DEBUG Query with image successful
2024-09-18 20:02:53,869 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:03:07,266 DEBUG Query with image successful
2024-09-18 20:03:07,352 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:03:21,637 DEBUG Query with image successful
2024-09-18 20:03:21,931 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:03:36,590 DEBUG Query with image successful
2024-09-18 20:03:36,677 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:03:54,167 DEBUG Query with image successful
2024-09-18 20:03:54,466 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:04:08,781 DEBUG Query with image successful
2024-09-18 20:04:08,872 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:04:22,724 DEBUG Query with image successful
2024-09-18 20:04:23,048 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:04:37,103 DEBUG Query with image successful
2024-09-18 20:04:37,186 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:04:52,017 DEBUG Query with image successful
2024-09-18 20:04:52,315 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:05:03,302 DEBUG Query with image successful
2024-09-18 20:05:03,397 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:05:18,634 DEBUG Query with image successful
2024-09-29 19:22:41,921 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-29 19:22:41,949 DEBUG OpenAI client created
2024-09-29 19:22:41,949 DEBUG Model set to: gpt-4-turbo
2024-09-29 19:49:57,584 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-29 19:49:57,608 DEBUG OpenAI client created
2024-09-29 19:49:57,609 DEBUG Model set to: gpt-4-turbo
2024-09-29 19:50:02,777 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:50:13,779 DEBUG Query successful
2024-09-29 19:50:13,874 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:50:24,093 DEBUG Query successful
2024-09-29 19:50:24,376 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:50:35,039 DEBUG Query successful
2024-09-29 19:50:35,129 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:50:44,376 DEBUG Query successful
2024-09-29 19:50:54,662 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:51:05,043 DEBUG Query successful
2024-09-29 19:51:05,135 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:51:26,274 DEBUG Query successful
2024-09-29 19:51:26,554 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:51:37,499 DEBUG Query successful
2024-09-29 19:51:37,588 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:51:46,281 DEBUG Query successful
2024-09-29 19:51:46,552 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:51:54,996 DEBUG Query successful
2024-09-29 19:51:55,086 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:52:07,327 DEBUG Query successful
2024-09-29 19:52:07,712 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:52:17,782 DEBUG Query successful
2024-09-29 19:52:17,869 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:52:35,895 DEBUG Query successful
2024-09-29 19:52:36,283 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:52:49,784 DEBUG Query successful
2024-09-29 19:52:49,880 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:53:05,701 DEBUG Query successful
2024-09-29 19:53:05,982 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:53:24,803 DEBUG Query successful
2024-09-29 19:53:24,894 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:53:34,480 DEBUG Query successful
2024-09-29 19:53:34,781 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:53:50,503 DEBUG Query successful
2024-09-29 19:53:50,595 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:54:01,596 DEBUG Query successful
2024-09-29 19:54:01,892 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:54:12,699 DEBUG Query successful
2024-09-29 19:54:12,789 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:54:25,553 DEBUG Query successful
2024-09-29 19:54:25,854 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:54:39,551 DEBUG Query successful
2024-09-29 19:54:39,645 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:54:51,798 DEBUG Query successful
2024-09-29 19:54:52,094 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:55:03,654 DEBUG Query successful
2024-09-29 19:55:03,758 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:55:12,181 DEBUG Query successful
2024-09-29 19:55:12,478 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:55:28,570 DEBUG Query successful
2024-09-29 19:55:28,664 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:55:36,874 DEBUG Query successful
2024-09-29 19:55:37,172 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:55:46,855 DEBUG Query successful
2024-09-29 19:55:46,946 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:55:54,240 DEBUG Query successful
2024-09-29 19:55:54,528 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:56:10,943 DEBUG Query successful
2024-09-29 19:56:11,038 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:56:18,194 DEBUG Query successful
2024-09-29 19:56:18,485 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:56:34,610 DEBUG Query successful
2024-09-29 19:56:34,709 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:56:41,048 DEBUG Query successful
2024-09-29 19:56:41,347 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:56:53,257 DEBUG Query successful
2024-09-29 19:56:53,353 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:57:01,438 DEBUG Query successful
2024-09-29 19:57:01,726 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:57:15,211 DEBUG Query successful
2024-09-29 19:57:15,304 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:57:23,595 DEBUG Query successful
2024-09-29 19:57:23,880 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:57:35,426 DEBUG Query successful
2024-09-29 19:57:35,524 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:57:41,022 DEBUG Query successful
2024-09-29 19:57:41,307 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:57:51,142 DEBUG Query successful
2024-09-29 19:57:51,244 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:57:58,239 DEBUG Query successful
2024-09-29 19:57:58,532 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:58:07,414 DEBUG Query successful
2024-09-29 19:58:07,509 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:58:16,565 DEBUG Query successful
2024-09-29 19:58:16,856 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:58:30,282 DEBUG Query successful
2024-09-29 19:58:30,380 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:58:38,664 DEBUG Query successful
2024-09-29 19:58:38,951 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:58:49,572 DEBUG Query successful
2024-09-29 19:58:49,669 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:58:56,339 DEBUG Query successful
2024-09-29 19:58:56,629 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:59:05,960 DEBUG Query successful
2024-09-29 19:59:06,052 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:59:13,689 DEBUG Query successful
2024-09-29 19:59:13,977 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:59:22,557 DEBUG Query successful
2024-09-29 19:59:22,662 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:59:29,587 DEBUG Query successful
2024-10-04 19:30:26,025 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:30:26,175 DEBUG OpenAI client created
2024-10-04 19:30:26,175 DEBUG Model set to: gpt-4-turbo
2024-10-04 19:33:20,914 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:33:20,945 DEBUG OpenAI client created
2024-10-04 19:33:20,945 DEBUG Model set to: gpt-4-turbo
2024-10-04 19:36:09,352 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:36:09,381 DEBUG OpenAI client created
2024-10-04 19:36:09,382 DEBUG Model set to: gpt-4-turbo
2024-10-04 19:36:09,397 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:36:09,421 DEBUG OpenAI client created
2024-10-04 19:36:09,421 DEBUG Model set to: gpt-4o
2024-10-04 19:36:14,974 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:36:26,776 DEBUG Query successful
2024-10-04 19:36:26,778 DEBUG Querying model: gpt-4o
2024-10-04 19:36:27,641 DEBUG Query successful
2024-10-04 19:36:27,663 DEBUG Querying model: gpt-4o
2024-10-04 19:36:33,099 DEBUG Query successful
2024-10-04 19:36:33,181 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:37:33,106 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:37:33,135 DEBUG OpenAI client created
2024-10-04 19:37:33,135 DEBUG Model set to: gpt-4-turbo
2024-10-04 19:37:33,142 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:37:33,168 DEBUG OpenAI client created
2024-10-04 19:37:33,168 DEBUG Model set to: gpt-4o
2024-10-04 19:37:38,323 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:37:46,450 DEBUG Query successful
2024-10-04 19:37:46,452 DEBUG Querying model: gpt-4o
2024-10-04 19:37:47,202 DEBUG Query successful
2024-10-04 19:37:47,202 DEBUG Querying model: gpt-4o
2024-10-04 19:37:50,640 DEBUG Query successful
2024-10-04 19:37:50,725 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:38:01,153 DEBUG Query successful
2024-10-04 19:38:01,471 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:38:15,148 DEBUG Query successful
2024-10-04 19:38:15,151 DEBUG Querying model: gpt-4o
2024-10-04 19:38:15,974 DEBUG Query successful
2024-10-04 19:38:15,990 DEBUG Querying model: gpt-4o
2024-10-04 19:38:20,670 DEBUG Query successful
2024-10-04 19:38:20,760 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:38:35,510 DEBUG Query successful
2024-10-04 19:38:45,782 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:38:59,286 DEBUG Query successful
2024-10-04 19:38:59,296 DEBUG Querying model: gpt-4o
2024-10-04 19:39:00,215 DEBUG Query successful
2024-10-04 19:39:00,216 DEBUG Querying model: gpt-4o
2024-10-04 19:39:05,118 DEBUG Query successful
2024-10-04 19:39:05,203 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:39:18,292 DEBUG Query successful
2024-10-04 19:39:18,592 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:39:31,175 DEBUG Query successful
2024-10-04 19:39:31,181 DEBUG Querying model: gpt-4o
2024-10-04 19:39:32,145 DEBUG Query successful
2024-10-04 19:39:32,146 DEBUG Querying model: gpt-4o
2024-10-04 19:39:36,318 DEBUG Query successful
2024-10-04 19:39:36,412 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:39:47,651 DEBUG Query successful
2024-10-04 19:39:47,945 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:40:01,055 DEBUG Query successful
2024-10-04 19:40:01,058 DEBUG Querying model: gpt-4o
2024-10-04 19:40:01,994 DEBUG Query successful
2024-10-04 19:40:01,994 DEBUG Querying model: gpt-4o
2024-10-04 19:40:06,584 DEBUG Query successful
2024-10-04 19:40:06,669 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:40:15,428 DEBUG Query successful
2024-10-04 19:40:15,729 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:40:27,866 DEBUG Query successful
2024-10-04 19:40:27,868 DEBUG Querying model: gpt-4o
2024-10-04 19:40:28,835 DEBUG Query successful
2024-10-04 19:40:28,835 DEBUG Querying model: gpt-4o
2024-10-04 19:40:39,981 DEBUG Query successful
2024-10-04 19:40:40,068 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:40:50,492 DEBUG Query successful
2024-10-04 19:40:50,788 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:41:09,356 DEBUG Query successful
2024-10-04 19:41:09,359 DEBUG Querying model: gpt-4o
2024-10-04 19:41:10,510 DEBUG Query successful
2024-10-04 19:41:10,510 DEBUG Querying model: gpt-4o
2024-10-04 19:41:15,932 DEBUG Query successful
2024-10-04 19:41:16,028 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:41:31,571 DEBUG Query successful
2024-10-04 19:41:31,857 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:41:46,623 DEBUG Query successful
2024-10-04 19:41:46,629 DEBUG Querying model: gpt-4o
2024-10-04 19:41:47,603 DEBUG Query successful
2024-10-04 19:41:47,603 DEBUG Querying model: gpt-4o
2024-10-04 19:41:53,492 DEBUG Query successful
2024-10-04 19:41:53,571 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:42:05,125 DEBUG Query successful
2024-10-04 19:42:05,409 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:42:17,832 DEBUG Query successful
2024-10-04 19:42:17,834 DEBUG Querying model: gpt-4o
2024-10-04 19:42:19,058 DEBUG Query successful
2024-10-04 19:42:19,058 DEBUG Querying model: gpt-4o
2024-10-04 19:42:24,663 DEBUG Query successful
2024-10-04 19:42:24,748 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:42:35,120 DEBUG Query successful
2024-10-04 19:42:35,425 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:42:48,968 DEBUG Query successful
2024-10-04 19:42:48,970 DEBUG Querying model: gpt-4o
2024-10-04 19:42:50,315 DEBUG Query successful
2024-10-04 19:42:50,315 DEBUG Querying model: gpt-4o
2024-10-04 19:42:57,197 DEBUG Query successful
2024-10-04 19:42:57,280 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:43:09,623 DEBUG Query successful
2024-10-04 19:43:09,913 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:43:22,947 DEBUG Query successful
2024-10-04 19:43:22,949 DEBUG Querying model: gpt-4o
2024-10-04 19:43:24,235 DEBUG Query successful
2024-10-04 19:43:24,235 DEBUG Querying model: gpt-4o
2024-10-04 19:43:29,968 DEBUG Query successful
2024-10-04 19:43:30,058 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:43:39,683 DEBUG Query successful
2024-10-04 19:43:39,964 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:43:52,628 DEBUG Query successful
2024-10-04 19:43:52,631 DEBUG Querying model: gpt-4o
2024-10-04 19:43:53,952 DEBUG Query successful
2024-10-04 19:43:53,952 DEBUG Querying model: gpt-4o
2024-10-04 19:43:59,808 DEBUG Query successful
2024-10-04 19:43:59,893 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:44:18,159 DEBUG Query successful
2024-10-04 19:44:18,438 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:44:30,762 DEBUG Query successful
2024-10-04 19:44:30,764 DEBUG Querying model: gpt-4o
2024-10-04 19:44:32,077 DEBUG Query successful
2024-10-04 19:44:32,077 DEBUG Querying model: gpt-4o
2024-10-04 19:44:38,584 DEBUG Query successful
2024-10-04 19:44:38,675 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:44:50,471 DEBUG Query successful
2024-10-04 19:44:50,761 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:45:07,000 DEBUG Query successful
2024-10-04 19:45:07,002 DEBUG Querying model: gpt-4o
2024-10-04 19:45:08,502 DEBUG Query successful
2024-10-04 19:45:08,502 DEBUG Querying model: gpt-4o
2024-10-04 19:45:13,549 DEBUG Query successful
2024-10-04 19:45:13,638 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:45:22,785 DEBUG Query successful
2024-10-04 19:45:23,104 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:45:36,551 DEBUG Query successful
2024-10-04 19:45:36,554 DEBUG Querying model: gpt-4o
2024-10-04 19:45:38,399 DEBUG Query successful
2024-10-04 19:45:38,399 DEBUG Querying model: gpt-4o
2024-10-04 19:45:45,519 DEBUG Query successful
2024-10-04 19:45:45,622 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:45:55,052 DEBUG Query successful
2024-10-04 19:45:55,339 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:46:07,944 DEBUG Query successful
2024-10-04 19:46:07,949 DEBUG Querying model: gpt-4o
2024-10-04 19:46:09,508 DEBUG Query successful
2024-10-04 19:46:09,509 DEBUG Querying model: gpt-4o
2024-10-04 19:46:14,626 DEBUG Query successful
2024-10-04 19:46:14,713 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:46:22,499 DEBUG Query successful
2024-10-04 19:46:22,771 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:46:36,040 DEBUG Query successful
2024-10-04 19:46:36,043 DEBUG Querying model: gpt-4o
2024-10-04 19:46:37,706 DEBUG Query successful
2024-10-04 19:46:37,706 DEBUG Querying model: gpt-4o
2024-10-04 19:46:43,477 DEBUG Query successful
2024-10-04 19:46:43,564 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:46:52,802 DEBUG Query successful
2024-10-04 19:46:53,083 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:47:07,910 DEBUG Query successful
2024-10-04 19:47:07,913 DEBUG Querying model: gpt-4o
2024-10-04 19:47:09,549 DEBUG Query successful
2024-10-04 19:47:09,549 DEBUG Querying model: gpt-4o
2024-10-04 19:47:16,468 DEBUG Query successful
2024-10-04 19:47:16,555 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:47:25,297 DEBUG Query successful
2024-10-04 19:47:25,578 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:47:37,530 DEBUG Query successful
2024-10-04 19:47:37,532 DEBUG Querying model: gpt-4o
2024-10-04 19:47:39,097 DEBUG Query successful
2024-10-04 19:47:39,098 DEBUG Querying model: gpt-4o
2024-10-04 19:47:44,212 DEBUG Query successful
2024-10-04 19:47:44,307 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:47:54,225 DEBUG Query successful
2024-10-04 19:47:54,505 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:48:09,163 DEBUG Query successful
2024-10-04 19:48:09,170 DEBUG Querying model: gpt-4o
2024-10-04 19:48:11,016 DEBUG Query successful
2024-10-04 19:48:11,016 DEBUG Querying model: gpt-4o
2024-10-04 19:48:16,939 DEBUG Query successful
2024-10-04 19:48:17,028 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:48:31,002 DEBUG Query successful
2024-10-04 19:48:31,276 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:48:45,716 DEBUG Query successful
2024-10-04 19:48:45,718 DEBUG Querying model: gpt-4o
2024-10-04 19:48:47,270 DEBUG Query successful
2024-10-04 19:48:47,271 DEBUG Querying model: gpt-4o
2024-10-04 19:48:52,843 DEBUG Query successful
2024-10-04 19:48:52,929 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:49:03,407 DEBUG Query successful
2024-10-04 19:49:03,696 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:49:19,286 DEBUG Query successful
2024-10-04 19:49:19,289 DEBUG Querying model: gpt-4o
2024-10-04 19:49:21,116 DEBUG Query successful
2024-10-04 19:49:21,116 DEBUG Querying model: gpt-4o
2024-10-04 19:49:27,772 DEBUG Query successful
2024-10-04 19:49:27,853 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:49:38,023 DEBUG Query successful
2024-10-04 19:49:38,318 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:49:52,132 DEBUG Query successful
2024-10-04 19:49:52,135 DEBUG Querying model: gpt-4o
2024-10-04 19:49:53,888 DEBUG Query successful
2024-10-04 19:49:53,896 DEBUG Querying model: gpt-4o
2024-10-04 19:50:02,007 DEBUG Query successful
2024-10-04 19:50:02,098 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:50:11,416 DEBUG Query successful
2024-10-04 19:50:11,696 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:50:25,255 DEBUG Query successful
2024-10-04 19:50:25,262 DEBUG Querying model: gpt-4o
2024-10-04 19:50:27,217 DEBUG Query successful
2024-10-04 19:50:27,217 DEBUG Querying model: gpt-4o
2024-10-04 19:50:36,154 DEBUG Query successful
2024-10-04 19:50:36,246 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:50:44,335 DEBUG Query successful
2024-10-04 19:50:44,622 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:50:57,978 DEBUG Query successful
2024-10-04 19:50:57,981 DEBUG Querying model: gpt-4o
2024-10-04 19:50:59,959 DEBUG Query successful
2024-10-04 19:50:59,959 DEBUG Querying model: gpt-4o
2024-10-04 19:51:10,906 DEBUG Query successful
2024-10-04 19:51:10,995 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:51:34,705 DEBUG Query successful
2024-10-04 19:58:23,181 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:58:23,206 DEBUG OpenAI client created
2024-10-04 19:58:23,206 DEBUG Model set to: gpt-4-turbo
2024-10-04 19:58:23,213 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:58:23,235 DEBUG OpenAI client created
2024-10-04 19:58:23,235 DEBUG Model set to: gpt-4o
2024-10-04 19:58:28,332 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:58:34,862 DEBUG Query successful
2024-10-04 19:58:34,863 DEBUG Querying model: gpt-4o
2024-10-04 19:58:35,672 DEBUG Query successful
2024-10-04 19:58:35,672 DEBUG Querying model: gpt-4o
2024-10-04 19:58:42,231 DEBUG Query successful
2024-10-04 19:58:42,241 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:58:56,042 DEBUG Query successful
2024-10-04 19:58:56,192 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:59:01,641 DEBUG Query successful
2024-10-04 19:59:01,642 DEBUG Querying model: gpt-4o
2024-10-04 19:59:02,453 DEBUG Query successful
2024-10-04 19:59:02,460 DEBUG Querying model: gpt-4o
2024-10-04 19:59:08,389 DEBUG Query successful
2024-10-04 19:59:08,410 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:59:17,175 DEBUG Query successful
2024-10-04 19:59:27,436 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:59:36,818 DEBUG Query successful
2024-10-04 19:59:36,824 DEBUG Querying model: gpt-4o
2024-10-04 19:59:37,991 DEBUG Query successful
2024-10-04 19:59:37,991 DEBUG Querying model: gpt-4o
2024-10-04 19:59:43,730 DEBUG Query successful
2024-10-04 19:59:43,807 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:59:55,601 DEBUG Query successful
2024-10-04 19:59:55,881 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:00:12,554 DEBUG Query successful
2024-10-04 20:00:12,556 DEBUG Querying model: gpt-4o
2024-10-04 20:00:13,474 DEBUG Query successful
2024-10-04 20:00:13,475 DEBUG Querying model: gpt-4o
2024-10-04 20:00:19,073 DEBUG Query successful
2024-10-04 20:00:19,157 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:00:33,341 DEBUG Query successful
2024-10-04 20:00:33,617 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:00:48,558 DEBUG Query successful
2024-10-04 20:00:48,563 DEBUG Querying model: gpt-4o
2024-10-04 20:00:49,572 DEBUG Query successful
2024-10-04 20:00:49,572 DEBUG Querying model: gpt-4o
2024-10-04 20:00:58,822 DEBUG Query successful
2024-10-04 20:00:58,915 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:01:11,722 DEBUG Query successful
2024-10-04 20:01:12,010 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:01:26,167 DEBUG Query successful
2024-10-04 20:01:26,169 DEBUG Querying model: gpt-4o
2024-10-04 20:01:27,173 DEBUG Query successful
2024-10-04 20:01:27,174 DEBUG Querying model: gpt-4o
2024-10-04 20:01:36,955 DEBUG Query successful
2024-10-04 20:01:37,051 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:01:52,991 DEBUG Query successful
2024-10-04 20:01:53,269 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:02:07,297 DEBUG Query successful
2024-10-04 20:02:07,305 DEBUG Querying model: gpt-4o
2024-10-04 20:02:08,531 DEBUG Query successful
2024-10-04 20:02:08,531 DEBUG Querying model: gpt-4o
2024-10-04 20:58:09,922 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 20:58:09,952 DEBUG OpenAI client created
2024-10-04 20:58:09,952 DEBUG Model set to: gpt-4-turbo
2024-10-04 20:58:09,952 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 20:58:09,975 DEBUG OpenAI client created
2024-10-04 20:58:09,976 DEBUG Model set to: gpt-4
2024-10-04 20:58:15,147 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:58:27,286 DEBUG Query successful
2024-10-04 20:58:27,289 DEBUG Querying model: gpt-4
2024-10-04 20:58:29,589 DEBUG Query successful
2024-10-04 20:58:29,614 DEBUG Querying model: gpt-4
2024-10-04 20:59:02,783 DEBUG Query successful
2024-10-04 20:59:02,862 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:59:13,039 DEBUG Query successful
2024-10-04 20:59:13,043 DEBUG Querying model: gpt-4
2024-10-04 20:59:21,741 DEBUG Query successful
2024-10-04 20:59:22,050 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:59:39,367 DEBUG Query successful
2024-10-04 20:59:39,371 DEBUG Querying model: gpt-4
2024-10-04 20:59:41,820 DEBUG Query successful
2024-10-04 20:59:41,820 DEBUG Querying model: gpt-4
2024-10-04 21:00:10,220 DEBUG Query successful
2024-10-04 21:00:10,304 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:25:04,895 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 14:25:04,928 DEBUG OpenAI client created
2024-10-05 14:25:04,928 DEBUG Model set to: gpt-4-turbo
2024-10-05 14:25:04,928 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 14:25:04,954 DEBUG OpenAI client created
2024-10-05 14:25:04,954 DEBUG Model set to: gpt-4
2024-10-05 14:25:10,011 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:25:16,158 DEBUG Query successful
2024-10-05 14:25:16,159 DEBUG Querying model: gpt-4
2024-10-05 14:25:17,796 DEBUG Query successful
2024-10-05 14:25:17,848 DEBUG Querying model: gpt-4
2024-10-05 14:25:37,099 DEBUG Query successful
2024-10-05 14:25:37,112 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:25:43,949 DEBUG Query successful
2024-10-05 14:25:43,954 DEBUG Querying model: gpt-4
2024-10-05 14:25:50,724 DEBUG Query successful
2024-10-05 14:25:50,866 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:25:56,430 DEBUG Query successful
2024-10-05 14:25:56,431 DEBUG Querying model: gpt-4
2024-10-05 14:25:58,644 DEBUG Query successful
2024-10-05 14:25:58,644 DEBUG Querying model: gpt-4
2024-10-05 14:26:19,681 DEBUG Query successful
2024-10-05 14:26:19,700 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:26:32,253 DEBUG Query successful
2024-10-05 14:26:32,253 DEBUG Querying model: gpt-4
2024-10-05 14:26:33,442 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8617 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8617 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:26:43,890 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:26:52,293 DEBUG Query successful
2024-10-05 14:26:52,298 DEBUG Querying model: gpt-4
2024-10-05 14:26:52,900 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8735 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8735 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:26:52,901 DEBUG Querying model: gpt-4
2024-10-05 14:26:53,438 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:26:53,526 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:27:05,243 DEBUG Query successful
2024-10-05 14:27:05,246 DEBUG Querying model: gpt-4
2024-10-05 14:27:05,795 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8617 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8617 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:27:06,080 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:27:32,415 DEBUG Query successful
2024-10-05 14:27:32,417 DEBUG Querying model: gpt-4
2024-10-05 14:27:33,068 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8763 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8763 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:27:33,070 DEBUG Querying model: gpt-4
2024-10-05 14:27:33,728 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:27:33,823 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:27:47,149 DEBUG Query successful
2024-10-05 14:27:47,150 DEBUG Querying model: gpt-4
2024-10-05 14:27:47,789 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8623 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8623 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:27:48,069 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:27:56,434 DEBUG Query successful
2024-10-05 14:27:56,436 DEBUG Querying model: gpt-4
2024-10-05 14:27:57,034 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8747 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8747 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:27:57,035 DEBUG Querying model: gpt-4
2024-10-05 14:27:57,522 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:27:57,613 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:28:15,546 DEBUG Query successful
2024-10-05 14:28:15,552 DEBUG Querying model: gpt-4
2024-10-05 14:28:16,114 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8615 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8615 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:28:16,390 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:28:25,513 DEBUG Query successful
2024-10-05 14:28:25,518 DEBUG Querying model: gpt-4
2024-10-05 14:28:26,205 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8811 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8811 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:28:26,206 DEBUG Querying model: gpt-4
2024-10-05 14:28:26,657 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:28:26,744 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:28:44,076 DEBUG Query successful
2024-10-05 14:28:44,077 DEBUG Querying model: gpt-4
2024-10-05 14:28:44,715 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8623 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8623 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:28:44,984 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:28:52,755 DEBUG Query successful
2024-10-05 14:28:52,761 DEBUG Querying model: gpt-4
2024-10-05 14:28:53,358 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8736 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8736 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:28:53,359 DEBUG Querying model: gpt-4
2024-10-05 14:28:53,856 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:28:53,942 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:29:22,363 DEBUG Query successful
2024-10-05 14:29:22,365 DEBUG Querying model: gpt-4
2024-10-05 14:29:22,979 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8623 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8623 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:29:23,379 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:29:31,983 DEBUG Query successful
2024-10-05 14:29:31,984 DEBUG Querying model: gpt-4
2024-10-05 14:29:32,535 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8679 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8679 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:29:32,536 DEBUG Querying model: gpt-4
2024-10-05 14:29:33,021 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:29:33,105 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:30:00,237 DEBUG Query successful
2024-10-05 14:30:00,238 DEBUG Querying model: gpt-4
2024-10-05 14:30:00,832 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8602 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8602 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:30:01,117 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:30:11,630 DEBUG Query successful
2024-10-05 14:30:11,632 DEBUG Querying model: gpt-4
2024-10-05 14:30:12,211 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8762 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8762 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:30:12,212 DEBUG Querying model: gpt-4
2024-10-05 14:30:12,716 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:30:12,800 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:30:37,752 DEBUG Query successful
2024-10-05 14:30:37,755 DEBUG Querying model: gpt-4
2024-10-05 14:30:38,334 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8602 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8602 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:30:38,601 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:30:47,510 DEBUG Query successful
2024-10-05 14:30:47,512 DEBUG Querying model: gpt-4
2024-10-05 14:30:48,128 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8753 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8753 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:30:48,130 DEBUG Querying model: gpt-4
2024-10-05 14:30:48,578 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:30:48,668 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:31:14,993 DEBUG Query successful
2024-10-05 14:31:14,994 DEBUG Querying model: gpt-4
2024-10-05 14:31:15,597 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8602 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8602 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:31:15,861 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:31:24,075 DEBUG Query successful
2024-10-05 14:31:24,081 DEBUG Querying model: gpt-4
2024-10-05 14:31:24,657 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8741 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8741 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:31:24,658 DEBUG Querying model: gpt-4
2024-10-05 14:31:25,106 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:31:25,184 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:31:53,485 DEBUG Query successful
2024-10-05 14:31:53,486 DEBUG Querying model: gpt-4
2024-10-05 14:31:54,061 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8569 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8569 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:31:54,326 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:32:02,333 DEBUG Query successful
2024-10-05 14:32:02,335 DEBUG Querying model: gpt-4
2024-10-05 14:32:02,890 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8700 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8700 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:32:02,891 DEBUG Querying model: gpt-4
2024-10-05 14:32:03,337 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:32:03,417 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:32:32,469 DEBUG Query successful
2024-10-05 14:32:32,471 DEBUG Querying model: gpt-4
2024-10-05 14:32:33,030 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8569 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8569 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:32:33,300 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:32:40,590 DEBUG Query successful
2024-10-05 14:32:40,592 DEBUG Querying model: gpt-4
2024-10-05 14:32:41,170 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8659 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8659 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:32:41,171 DEBUG Querying model: gpt-4
2024-10-05 14:32:41,615 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:32:41,694 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:33:12,887 DEBUG Query successful
2024-10-05 14:33:12,888 DEBUG Querying model: gpt-4
2024-10-05 14:33:13,503 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8534 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8534 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:33:13,782 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:33:21,307 DEBUG Query successful
2024-10-05 14:33:21,311 DEBUG Querying model: gpt-4
2024-10-05 14:33:21,901 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8654 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8654 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:33:21,903 DEBUG Querying model: gpt-4
2024-10-05 14:33:22,366 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:33:22,457 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:33:54,378 DEBUG Query successful
2024-10-05 14:33:54,379 DEBUG Querying model: gpt-4
2024-10-05 14:33:54,962 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8526 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8526 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:33:55,233 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:34:03,664 DEBUG Query successful
2024-10-05 14:34:03,671 DEBUG Querying model: gpt-4
2024-10-05 14:34:04,484 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8676 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8676 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:34:04,485 DEBUG Querying model: gpt-4
2024-10-05 14:34:04,895 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:34:04,977 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:34:38,269 DEBUG Query successful
2024-10-05 14:34:38,273 DEBUG Querying model: gpt-4
2024-10-05 14:34:38,901 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8526 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8526 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:34:39,163 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:34:49,374 DEBUG Query successful
2024-10-05 14:34:49,378 DEBUG Querying model: gpt-4
2024-10-05 14:34:49,967 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8637 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8637 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:34:49,968 DEBUG Querying model: gpt-4
2024-10-05 14:34:50,430 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:34:50,510 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:35:21,355 DEBUG Query successful
2024-10-05 14:35:21,356 DEBUG Querying model: gpt-4
2024-10-05 14:35:21,951 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8493 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8493 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:35:22,223 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:35:29,776 DEBUG Query successful
2024-10-05 14:35:29,779 DEBUG Querying model: gpt-4
2024-10-05 14:35:30,384 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8608 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8608 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:35:30,385 DEBUG Querying model: gpt-4
2024-10-05 14:35:30,859 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:35:30,950 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:36:10,593 DEBUG Query successful
2024-10-05 14:36:10,598 DEBUG Querying model: gpt-4
2024-10-05 14:36:11,187 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8478 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8478 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:36:11,466 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:36:18,726 DEBUG Query successful
2024-10-05 14:36:18,728 DEBUG Querying model: gpt-4
2024-10-05 14:36:19,336 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8627 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8627 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:36:19,337 DEBUG Querying model: gpt-4
2024-10-05 14:36:19,838 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:36:19,933 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:36:59,437 DEBUG Query successful
2024-10-05 14:36:59,439 DEBUG Querying model: gpt-4
2024-10-05 14:37:00,372 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8478 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8478 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:37:00,668 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:37:11,291 DEBUG Query successful
2024-10-05 14:37:11,297 DEBUG Querying model: gpt-4
2024-10-05 14:37:11,910 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8686 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8686 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:37:11,911 DEBUG Querying model: gpt-4
2024-10-05 14:37:12,363 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:37:12,452 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:37:50,583 DEBUG Query successful
2024-10-05 14:37:50,587 DEBUG Querying model: gpt-4
2024-10-05 14:37:51,242 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8470 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8470 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:37:51,521 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:37:59,871 DEBUG Query successful
2024-10-05 14:37:59,873 DEBUG Querying model: gpt-4
2024-10-05 14:38:00,428 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8648 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8648 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:38:00,430 DEBUG Querying model: gpt-4
2024-10-05 14:38:00,915 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:38:01,000 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:38:40,278 DEBUG Query successful
2024-10-05 14:38:40,280 DEBUG Querying model: gpt-4
2024-10-05 14:38:40,981 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8470 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8470 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:38:41,282 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:38:47,737 DEBUG Query successful
2024-10-05 14:38:47,739 DEBUG Querying model: gpt-4
2024-10-05 14:38:48,330 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8556 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8556 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:38:48,331 DEBUG Querying model: gpt-4
2024-10-05 14:38:48,801 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:38:48,891 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:39:30,393 DEBUG Query successful
2024-10-05 14:39:30,400 DEBUG Querying model: gpt-4
2024-10-05 14:39:31,006 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8447 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8447 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:39:31,290 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:39:42,062 DEBUG Query successful
2024-10-05 14:39:42,066 DEBUG Querying model: gpt-4
2024-10-05 14:39:42,605 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8611 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8611 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:39:42,606 DEBUG Querying model: gpt-4
2024-10-05 14:39:43,074 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:39:43,164 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:40:36,943 DEBUG Query successful
2024-10-05 14:40:36,946 DEBUG Querying model: gpt-4
2024-10-05 14:40:37,543 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8447 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8447 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:40:37,841 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:40:45,319 DEBUG Query successful
2024-10-05 14:40:45,325 DEBUG Querying model: gpt-4
2024-10-05 14:40:45,900 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8566 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8566 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:40:45,901 DEBUG Querying model: gpt-4
2024-10-05 14:40:46,392 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:40:46,478 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:41:36,911 DEBUG Query successful
2024-10-05 14:41:36,912 DEBUG Querying model: gpt-4
2024-10-05 14:41:37,532 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8447 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8447 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:41:37,834 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:41:51,010 DEBUG Query successful
2024-10-05 14:41:51,012 DEBUG Querying model: gpt-4
2024-10-05 14:41:51,613 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8527 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8527 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:41:51,614 DEBUG Querying model: gpt-4
2024-10-05 14:41:52,089 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:41:52,177 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:42:43,748 DEBUG Query successful
2024-10-05 14:42:43,750 DEBUG Querying model: gpt-4
2024-10-05 14:42:44,352 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8414 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8414 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:42:44,627 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:42:50,938 DEBUG Query successful
2024-10-05 14:42:50,940 DEBUG Querying model: gpt-4
2024-10-05 14:42:51,515 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8536 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8536 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:42:51,517 DEBUG Querying model: gpt-4
2024-10-05 14:42:51,960 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:42:52,050 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:43:38,430 DEBUG Query successful
2024-10-05 14:43:38,436 DEBUG Querying model: gpt-4
2024-10-05 14:43:39,044 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8422 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8422 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:55:32,713 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 14:55:32,737 DEBUG OpenAI client created
2024-10-05 14:55:32,737 DEBUG Model set to: gpt-4-turbo
2024-10-05 14:55:32,737 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 14:55:32,762 DEBUG OpenAI client created
2024-10-05 14:55:32,762 DEBUG Model set to: gpt-4-turbo
2024-10-05 15:13:03,033 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 15:13:03,061 DEBUG OpenAI client created
2024-10-05 15:13:03,061 DEBUG Model set to: gpt-4-turbo
2024-10-05 15:13:03,061 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 15:13:03,084 DEBUG OpenAI client created
2024-10-05 15:13:03,085 DEBUG Model set to: gpt-4-turbo
2024-10-05 15:13:08,249 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:13:17,196 DEBUG Query successful
2024-10-05 15:22:07,855 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 15:22:07,899 DEBUG OpenAI client created
2024-10-05 15:22:07,899 DEBUG Model set to: gpt-4-turbo
2024-10-05 15:22:07,899 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 15:22:07,924 DEBUG OpenAI client created
2024-10-05 15:22:07,924 DEBUG Model set to: gpt-4-turbo
2024-10-05 15:22:13,093 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:22:24,385 DEBUG Query successful
2024-10-05 15:22:24,387 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:22:25,697 DEBUG Query successful
2024-10-05 15:22:25,714 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:22:44,870 DEBUG Query successful
2024-10-05 15:22:44,959 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:22:54,002 DEBUG Query successful
2024-10-05 15:22:54,003 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:23:02,606 DEBUG Query successful
2024-10-05 15:23:02,870 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:23:15,200 DEBUG Query successful
2024-10-05 15:23:15,206 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:23:17,036 DEBUG Query successful
2024-10-05 15:23:17,051 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:23:34,844 DEBUG Query successful
2024-10-05 15:23:34,932 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:23:52,006 DEBUG Query successful
2024-10-05 15:23:52,008 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:24:01,236 DEBUG Query successful
2024-10-05 15:24:11,509 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:24:21,807 DEBUG Query successful
2024-10-05 15:24:21,810 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:24:24,253 DEBUG Query successful
2024-10-05 15:24:24,254 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:24:47,188 DEBUG Query successful
2024-10-05 15:24:47,276 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:25:10,575 DEBUG Query successful
2024-10-05 15:25:10,577 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:25:23,063 DEBUG Query successful
2024-10-05 15:25:23,338 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:25:36,967 DEBUG Query successful
2024-10-05 15:25:36,968 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:25:42,624 DEBUG Query successful
2024-10-05 15:25:42,624 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:26:06,166 DEBUG Query successful
2024-10-05 15:26:06,261 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:26:35,541 DEBUG Query successful
2024-10-05 15:26:35,546 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:26:45,561 DEBUG Query successful
2024-10-05 15:26:45,846 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:26:54,482 DEBUG Query successful
2024-10-05 15:26:54,486 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:26:58,789 DEBUG Query successful
2024-10-05 15:26:58,790 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:27:19,252 DEBUG Query successful
2024-10-05 15:27:19,342 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:27:37,963 DEBUG Query successful
2024-10-05 15:27:37,965 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:27:49,580 DEBUG Query successful
2024-10-05 15:27:49,862 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:28:05,361 DEBUG Query successful
2024-10-05 15:28:05,366 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:28:08,739 DEBUG Query successful
2024-10-05 15:28:08,741 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:28:33,992 DEBUG Query successful
2024-10-05 15:28:34,092 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:28:59,377 DEBUG Query successful
2024-10-05 15:28:59,382 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:29:12,163 DEBUG Query successful
2024-10-05 15:29:12,449 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:29:20,751 DEBUG Query successful
2024-10-05 15:29:20,752 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:29:26,686 DEBUG Query successful
2024-10-05 15:29:26,691 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:29:47,872 DEBUG Query successful
2024-10-05 15:29:47,957 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:30:10,761 DEBUG Query successful
2024-10-05 15:30:10,763 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:30:23,802 DEBUG Query successful
2024-10-05 15:30:24,092 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:30:35,149 DEBUG Query successful
2024-10-05 15:30:35,151 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:30:39,024 DEBUG Query successful
2024-10-05 15:30:39,024 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:31:08,387 DEBUG Query successful
2024-10-05 15:31:08,493 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:31:41,959 DEBUG Query successful
2024-10-05 15:31:41,961 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:31:57,781 DEBUG Query successful
2024-10-05 15:31:58,079 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:32:08,493 DEBUG Query successful
2024-10-05 15:32:08,496 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:32:11,113 DEBUG Query successful
2024-10-05 15:32:11,115 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:32:45,189 DEBUG Query successful
2024-10-05 15:32:45,282 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:33:19,618 DEBUG Query successful
2024-10-05 15:33:19,619 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:33:35,090 DEBUG Query successful
2024-10-05 15:33:35,368 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:33:42,723 DEBUG Query successful
2024-10-05 15:33:42,728 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:33:46,429 DEBUG Query successful
2024-10-05 15:33:46,429 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:34:18,555 DEBUG Query successful
2024-10-05 15:34:18,649 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:34:51,811 DEBUG Query successful
2024-10-05 15:34:51,818 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:35:07,056 DEBUG Query successful
2024-10-05 15:35:07,351 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:35:15,412 DEBUG Query successful
2024-10-05 15:35:15,413 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:35:18,279 DEBUG Query successful
2024-10-05 15:35:18,284 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:35:57,589 DEBUG Query successful
2024-10-05 15:35:57,702 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:36:35,037 DEBUG Query successful
2024-10-05 15:36:35,039 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:36:53,567 DEBUG Query successful
2024-10-05 15:36:53,849 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:37:01,687 DEBUG Query successful
2024-10-05 15:37:01,689 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:37:05,492 DEBUG Query successful
2024-10-05 15:37:05,493 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:37:41,761 DEBUG Query successful
2024-10-05 15:37:41,862 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:38:15,395 DEBUG Query successful
2024-10-05 15:38:15,396 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:38:33,096 DEBUG Query successful
2024-10-05 15:38:33,393 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:38:41,821 DEBUG Query successful
2024-10-05 15:38:41,823 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:38:44,595 DEBUG Query successful
2024-10-05 15:38:44,599 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:39:11,646 DEBUG Query successful
2024-10-05 15:39:11,739 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:39:56,508 DEBUG Query successful
2024-10-05 15:39:56,510 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:40:09,904 DEBUG Query successful
2024-10-05 15:40:10,180 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:40:19,062 DEBUG Query successful
2024-10-05 15:40:19,067 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:40:22,938 DEBUG Query successful
2024-10-05 15:40:22,938 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:40:58,405 DEBUG Query successful
2024-10-05 15:40:58,495 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:41:31,694 DEBUG Query successful
2024-10-05 15:41:31,700 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:41:50,525 DEBUG Query successful
2024-10-05 15:41:50,805 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:41:58,601 DEBUG Query successful
2024-10-05 15:41:58,605 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:42:02,009 DEBUG Query successful
2024-10-05 15:42:02,010 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:42:36,638 DEBUG Query successful
2024-10-05 15:42:36,733 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:43:14,659 DEBUG Query successful
2024-10-05 15:43:14,661 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:43:29,725 DEBUG Query successful
2024-10-05 15:43:30,000 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:43:39,379 DEBUG Query successful
2024-10-05 15:43:39,381 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:43:42,631 DEBUG Query successful
2024-10-05 15:43:42,632 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:44:19,048 DEBUG Query successful
2024-10-05 15:44:19,145 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:44:56,195 DEBUG Query successful
2024-10-05 15:44:56,196 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:46:18,232 DEBUG Query successful
2024-10-05 15:46:18,516 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:46:27,879 DEBUG Query successful
2024-10-05 15:46:27,881 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:46:37,385 DEBUG Query successful
2024-10-05 15:46:37,386 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:47:15,169 DEBUG Query successful
2024-10-05 15:47:15,261 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:47:56,211 DEBUG Query successful
2024-10-05 15:47:56,212 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:48:14,836 DEBUG Query successful
2024-10-05 15:48:15,118 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:48:24,371 DEBUG Query successful
2024-10-05 15:48:24,377 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:48:29,253 DEBUG Query successful
2024-10-05 15:48:29,261 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:49:05,148 DEBUG Query successful
2024-10-05 15:49:05,244 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:49:49,974 DEBUG Query successful
2024-10-05 15:49:49,981 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:50:04,816 DEBUG Query successful
2024-10-05 15:50:05,095 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:50:14,887 DEBUG Query successful
2024-10-05 15:50:14,889 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:50:19,976 DEBUG Query successful
2024-10-05 15:50:19,982 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:51:12,553 DEBUG Query successful
2024-10-05 15:51:12,649 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:51:58,333 DEBUG Query successful
2024-10-05 15:51:58,334 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:52:12,249 DEBUG Query successful
2024-10-05 15:52:12,543 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:52:20,224 DEBUG Query successful
2024-10-05 15:52:20,229 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:52:30,053 DEBUG Query successful
2024-10-05 15:52:30,054 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:53:11,631 DEBUG Query successful
2024-10-05 15:53:11,746 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:54:06,226 DEBUG Query successful
2024-10-05 15:54:06,227 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:54:22,142 DEBUG Query successful
2024-10-05 15:54:22,426 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:54:29,943 DEBUG Query successful
2024-10-05 15:54:29,946 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:54:34,748 DEBUG Query successful
2024-10-05 15:54:34,749 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:55:18,372 DEBUG Query successful
2024-10-05 15:55:18,453 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:56:08,838 DEBUG Query successful
2024-10-05 15:56:08,840 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:56:23,738 DEBUG Query successful
2024-10-05 15:56:24,007 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:56:30,812 DEBUG Query successful
2024-10-05 15:56:30,816 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:56:36,632 DEBUG Query successful
2024-10-05 15:56:36,632 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:57:22,873 DEBUG Query successful
2024-10-05 15:57:22,973 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:58:08,767 DEBUG Query successful
2024-10-05 15:58:08,772 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:58:26,440 DEBUG Query successful
2024-10-05 15:58:26,715 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:58:35,828 DEBUG Query successful
2024-10-05 15:58:35,829 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:58:40,740 DEBUG Query successful
2024-10-05 15:58:40,745 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:59:21,221 DEBUG Query successful
2024-10-05 15:59:21,314 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:01:16,878 DEBUG Query successful
2024-10-05 16:01:16,880 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:01:35,823 DEBUG Query successful
2024-10-05 16:01:36,101 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:01:45,811 DEBUG Query successful
2024-10-05 16:01:45,814 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:01:58,947 DEBUG Query successful
2024-10-05 16:01:58,948 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:02:39,587 DEBUG Query successful
2024-10-05 16:02:39,671 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:03:34,312 DEBUG Query successful
2024-10-05 16:03:34,314 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:03:51,925 DEBUG Query successful
2024-10-05 16:03:52,220 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:04:00,277 DEBUG Query successful
2024-10-05 16:04:00,279 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:04:05,555 DEBUG Query successful
2024-10-05 16:04:05,556 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:04:53,508 DEBUG Query successful
2024-10-05 16:04:53,601 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:05:48,993 DEBUG Query successful
2024-10-05 16:05:48,994 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:06:11,468 DEBUG Query successful
2024-10-06 14:43:44,182 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 14:43:44,340 DEBUG OpenAI client created
2024-10-06 14:43:44,340 DEBUG Model set to: gpt-4-turbo
2024-10-06 14:43:44,340 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 14:43:44,362 DEBUG OpenAI client created
2024-10-06 14:43:44,362 DEBUG Model set to: gpt-4-turbo
2024-10-06 14:43:49,529 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:44:00,675 DEBUG Query successful
2024-10-06 14:44:00,679 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:44:01,807 DEBUG Query successful
2024-10-06 14:44:01,831 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:44:21,027 DEBUG Query successful
2024-10-06 14:44:21,108 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:44:30,429 DEBUG Query successful
2024-10-06 14:44:30,435 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:44:40,460 DEBUG Query successful
2024-10-06 14:44:40,729 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:44:53,120 DEBUG Query successful
2024-10-06 14:44:53,123 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:44:54,535 DEBUG Query successful
2024-10-06 14:44:54,536 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:45:07,667 DEBUG Query successful
2024-10-06 14:45:07,752 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:45:19,680 DEBUG Query successful
2024-10-06 14:45:19,681 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:45:28,981 DEBUG Query successful
2024-10-06 14:45:39,281 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:45:48,871 DEBUG Query successful
2024-10-06 14:45:48,874 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:45:50,612 DEBUG Query successful
2024-10-06 14:45:50,612 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:46:01,852 DEBUG Query successful
2024-10-06 14:46:01,937 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:46:15,259 DEBUG Query successful
2024-10-06 14:46:15,260 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:46:25,316 DEBUG Query successful
2024-10-06 14:46:25,590 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:46:34,758 DEBUG Query successful
2024-10-06 14:46:34,759 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:46:36,308 DEBUG Query successful
2024-10-06 14:46:36,313 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:46:49,895 DEBUG Query successful
2024-10-06 14:46:49,976 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:47:05,076 DEBUG Query successful
2024-10-06 14:47:05,078 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:47:16,195 DEBUG Query successful
2024-10-06 14:47:16,479 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:47:25,494 DEBUG Query successful
2024-10-06 14:47:25,500 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:47:27,732 DEBUG Query successful
2024-10-06 14:47:27,734 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:47:46,292 DEBUG Query successful
2024-10-06 14:47:46,381 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:48:04,506 DEBUG Query successful
2024-10-06 14:48:04,511 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:48:14,662 DEBUG Query successful
2024-10-06 14:48:14,936 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:48:24,130 DEBUG Query successful
2024-10-06 14:48:24,132 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:48:26,702 DEBUG Query successful
2024-10-06 14:48:26,703 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:48:44,169 DEBUG Query successful
2024-10-06 14:48:44,255 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:49:04,765 DEBUG Query successful
2024-10-06 14:49:04,767 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:49:17,255 DEBUG Query successful
2024-10-06 14:49:17,564 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:49:26,410 DEBUG Query successful
2024-10-06 14:49:26,413 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:49:28,839 DEBUG Query successful
2024-10-06 14:49:28,840 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:49:51,675 DEBUG Query successful
2024-10-06 14:49:51,765 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:50:12,959 DEBUG Query successful
2024-10-06 14:50:12,962 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:50:27,317 DEBUG Query successful
2024-10-06 14:50:27,606 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:50:35,093 DEBUG Query successful
2024-10-06 14:50:35,095 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:50:37,044 DEBUG Query successful
2024-10-06 14:50:37,048 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:50:57,004 DEBUG Query successful
2024-10-06 14:50:57,115 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:51:22,081 DEBUG Query successful
2024-10-06 14:51:22,084 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:51:34,183 DEBUG Query successful
2024-10-06 14:51:34,512 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:51:42,760 DEBUG Query successful
2024-10-06 14:51:42,763 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:51:45,328 DEBUG Query successful
2024-10-06 14:51:45,329 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:52:05,906 DEBUG Query successful
2024-10-06 14:52:06,017 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:52:34,737 DEBUG Query successful
2024-10-06 14:52:34,739 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:52:48,484 DEBUG Query successful
2024-10-06 14:52:48,767 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:52:56,336 DEBUG Query successful
2024-10-06 14:52:56,339 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:52:58,942 DEBUG Query successful
2024-10-06 14:52:58,943 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:53:20,200 DEBUG Query successful
2024-10-06 14:53:20,287 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:53:50,131 DEBUG Query successful
2024-10-06 14:53:50,132 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:54:06,826 DEBUG Query successful
2024-10-06 14:54:07,109 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:54:17,310 DEBUG Query successful
2024-10-06 14:54:17,311 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:54:21,650 DEBUG Query successful
2024-10-06 14:54:21,651 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:54:44,696 DEBUG Query successful
2024-10-06 14:54:44,783 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:55:16,716 DEBUG Query successful
2024-10-06 14:55:16,717 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:55:28,605 DEBUG Query successful
2024-10-06 14:55:28,950 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:55:38,435 DEBUG Query successful
2024-10-06 14:55:38,437 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:55:41,309 DEBUG Query successful
2024-10-06 14:55:41,311 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:56:05,378 DEBUG Query successful
2024-10-06 14:56:05,466 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:56:38,658 DEBUG Query successful
2024-10-06 14:56:38,659 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:56:56,132 DEBUG Query successful
2024-10-06 14:56:56,404 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:57:04,967 DEBUG Query successful
2024-10-06 14:57:04,972 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:57:08,059 DEBUG Query successful
2024-10-06 14:57:08,060 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:57:36,430 DEBUG Query successful
2024-10-06 14:57:36,511 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:58:17,084 DEBUG Query successful
2024-10-06 14:58:17,090 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:58:28,870 DEBUG Query successful
2024-10-06 14:58:29,147 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:58:40,991 DEBUG Query successful
2024-10-06 14:58:40,993 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:58:43,480 DEBUG Query successful
2024-10-06 14:58:43,481 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:59:13,590 DEBUG Query successful
2024-10-06 14:59:13,677 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:59:47,148 DEBUG Query successful
2024-10-06 14:59:47,150 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:00:01,667 DEBUG Query successful
2024-10-06 15:00:01,947 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:00:09,738 DEBUG Query successful
2024-10-06 15:00:09,739 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:00:12,976 DEBUG Query successful
2024-10-06 15:00:12,977 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:00:38,077 DEBUG Query successful
2024-10-06 15:00:38,165 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:01:19,801 DEBUG Query successful
2024-10-06 15:01:19,802 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:01:35,373 DEBUG Query successful
2024-10-06 15:01:35,716 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:01:46,893 DEBUG Query successful
2024-10-06 15:01:46,895 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:01:50,616 DEBUG Query successful
2024-10-06 15:01:50,620 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:02:23,459 DEBUG Query successful
2024-10-06 15:02:23,544 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:03:00,737 DEBUG Query successful
2024-10-06 15:03:00,738 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:03:12,347 DEBUG Query successful
2024-10-06 15:03:12,725 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:03:21,365 DEBUG Query successful
2024-10-06 15:03:21,371 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:03:24,166 DEBUG Query successful
2024-10-06 15:03:24,167 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:03:56,808 DEBUG Query successful
2024-10-06 15:03:56,893 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:04:33,761 DEBUG Query successful
2024-10-06 15:04:33,766 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:04:47,200 DEBUG Query successful
2024-10-06 15:04:47,480 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:04:55,601 DEBUG Query successful
2024-10-06 15:04:55,602 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:04:59,069 DEBUG Query successful
2024-10-06 15:04:59,069 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:05:32,116 DEBUG Query successful
2024-10-06 15:05:32,205 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:00:29,170 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 17:00:29,312 DEBUG OpenAI client created
2024-10-06 17:00:29,312 DEBUG Model set to: gpt-4-turbo
2024-10-06 17:00:29,312 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 17:00:29,333 DEBUG OpenAI client created
2024-10-06 17:00:29,333 DEBUG Model set to: gpt-4-turbo
2024-10-06 17:00:34,509 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:00:46,913 DEBUG Query successful
2024-10-06 17:00:46,914 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:00:48,138 DEBUG Query successful
2024-10-06 17:00:48,176 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:01:05,074 DEBUG Query successful
2024-10-06 17:01:05,159 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:01:17,776 DEBUG Query successful
2024-10-06 17:01:17,778 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:01:28,046 DEBUG Query successful
2024-10-06 17:01:28,311 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:01:36,080 DEBUG Query successful
2024-10-06 17:01:36,081 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:01:38,447 DEBUG Query successful
2024-10-06 17:01:38,451 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:01:53,406 DEBUG Query successful
2024-10-06 17:01:53,486 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:02:06,310 DEBUG Query successful
2024-10-06 17:02:06,311 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:02:14,785 DEBUG Query successful
2024-10-06 17:02:25,049 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:02:34,094 DEBUG Query successful
2024-10-06 17:02:34,096 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:02:35,719 DEBUG Query successful
2024-10-06 17:02:35,721 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:02:50,628 DEBUG Query successful
2024-10-06 17:02:50,718 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:03:06,388 DEBUG Query successful
2024-10-06 17:03:06,389 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:03:16,331 DEBUG Query successful
2024-10-06 17:03:16,619 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:03:25,086 DEBUG Query successful
2024-10-06 17:03:25,088 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:03:27,408 DEBUG Query successful
2024-10-06 17:03:27,409 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:03:41,324 DEBUG Query successful
2024-10-06 17:03:41,418 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:03:58,174 DEBUG Query successful
2024-10-06 17:03:58,176 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:04:09,110 DEBUG Query successful
2024-10-06 17:04:09,375 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:04:18,134 DEBUG Query successful
2024-10-06 17:04:18,136 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:04:20,935 DEBUG Query successful
2024-10-06 17:04:20,936 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:04:37,228 DEBUG Query successful
2024-10-06 17:04:37,314 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:04:59,714 DEBUG Query successful
2024-10-06 17:04:59,715 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:05:10,645 DEBUG Query successful
2024-10-06 17:05:10,930 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:05:31,309 DEBUG Query successful
2024-10-06 17:05:31,312 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:05:33,607 DEBUG Query successful
2024-10-06 17:05:33,611 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:05:51,842 DEBUG Query successful
2024-10-06 17:05:51,928 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:06:11,514 DEBUG Query successful
2024-10-06 17:06:11,516 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:06:22,796 DEBUG Query successful
2024-10-06 17:06:23,062 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:06:32,666 DEBUG Query successful
2024-10-06 17:06:32,671 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:06:34,712 DEBUG Query successful
2024-10-06 17:06:34,713 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:06:53,806 DEBUG Query successful
2024-10-06 17:06:53,893 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:07:15,857 DEBUG Query successful
2024-10-06 17:07:15,862 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:07:27,844 DEBUG Query successful
2024-10-06 17:07:28,118 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:07:35,627 DEBUG Query successful
2024-10-06 17:07:35,628 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:07:39,373 DEBUG Query successful
2024-10-06 17:07:39,373 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:07:59,371 DEBUG Query successful
2024-10-06 17:07:59,454 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:08:26,063 DEBUG Query successful
2024-10-06 17:08:26,064 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:08:38,058 DEBUG Query successful
2024-10-06 17:08:38,330 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:08:48,365 DEBUG Query successful
2024-10-06 17:08:48,368 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:08:50,689 DEBUG Query successful
2024-10-06 17:08:50,690 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:09:14,581 DEBUG Query successful
2024-10-06 17:09:14,668 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:09:44,345 DEBUG Query successful
2024-10-06 17:09:44,347 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:09:59,525 DEBUG Query successful
2024-10-06 17:09:59,796 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:10:07,125 DEBUG Query successful
2024-10-06 17:10:07,131 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:10:10,080 DEBUG Query successful
2024-10-06 17:10:10,084 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:10:28,940 DEBUG Query successful
2024-10-06 17:10:29,044 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:11:01,092 DEBUG Query successful
2024-10-06 17:11:01,093 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:11:16,083 DEBUG Query successful
2024-10-06 17:11:16,347 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:11:26,307 DEBUG Query successful
2024-10-06 17:11:26,313 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:11:29,965 DEBUG Query successful
2024-10-06 17:11:29,969 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:11:52,410 DEBUG Query successful
2024-10-06 17:11:52,494 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:12:25,587 DEBUG Query successful
2024-10-06 17:12:25,592 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:12:39,338 DEBUG Query successful
2024-10-06 17:12:39,607 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:12:46,795 DEBUG Query successful
2024-10-06 17:12:46,797 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:12:49,725 DEBUG Query successful
2024-10-06 17:12:49,726 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:13:14,430 DEBUG Query successful
2024-10-06 17:13:14,515 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:13:46,949 DEBUG Query successful
2024-10-06 17:13:46,950 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:14:01,005 DEBUG Query successful
2024-10-06 17:14:01,294 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:14:09,464 DEBUG Query successful
2024-10-06 17:14:09,466 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:14:12,653 DEBUG Query successful
2024-10-06 17:14:12,654 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:14:39,858 DEBUG Query successful
2024-10-06 17:14:39,939 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:15:13,063 DEBUG Query successful
2024-10-06 17:15:13,064 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:15:25,567 DEBUG Query successful
2024-10-06 17:15:25,910 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:15:34,408 DEBUG Query successful
2024-10-06 17:15:34,409 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:15:36,982 DEBUG Query successful
2024-10-06 17:15:36,987 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:16:01,397 DEBUG Query successful
2024-10-06 17:16:01,491 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:16:33,696 DEBUG Query successful
2024-10-06 17:16:33,698 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:16:50,680 DEBUG Query successful
2024-10-06 17:16:51,024 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:16:59,747 DEBUG Query successful
2024-10-06 17:16:59,749 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:17:03,371 DEBUG Query successful
2024-10-06 17:17:03,372 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:17:31,420 DEBUG Query successful
2024-10-06 17:17:31,513 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:18:17,927 DEBUG Query successful
2024-10-06 17:18:17,932 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:18:36,405 DEBUG Query successful
2024-10-06 17:18:36,679 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:18:48,033 DEBUG Query successful
2024-10-06 17:18:48,036 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:18:51,419 DEBUG Query successful
2024-10-06 17:18:51,420 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:19:20,659 DEBUG Query successful
2024-10-06 17:19:20,747 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:20:02,094 DEBUG Query successful
2024-10-06 17:20:02,097 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:20:21,292 DEBUG Query successful
2024-10-06 17:20:21,577 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:20:33,504 DEBUG Query successful
2024-10-06 17:20:33,506 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:20:37,228 DEBUG Query successful
2024-10-06 17:20:37,229 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:21:04,868 DEBUG Query successful
2024-10-06 17:21:04,958 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:21:43,834 DEBUG Query successful
2024-10-06 17:21:43,837 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:22:00,562 DEBUG Query successful
2024-10-06 17:22:00,841 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:22:09,560 DEBUG Query successful
2024-10-06 17:22:09,562 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:22:12,504 DEBUG Query successful
2024-10-06 17:22:12,508 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:22:43,576 DEBUG Query successful
2024-10-06 17:22:43,663 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:23:28,642 DEBUG Query successful
2024-10-06 17:23:28,643 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:23:44,457 DEBUG Query successful
2024-10-06 17:23:44,732 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:23:51,030 DEBUG Query successful
2024-10-06 17:23:51,031 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:23:53,744 DEBUG Query successful
2024-10-06 17:23:53,745 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:24:27,969 DEBUG Query successful
2024-10-06 17:24:28,058 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:25:17,233 DEBUG Query successful
2024-10-06 17:25:17,238 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:25:33,558 DEBUG Query successful
2024-10-06 17:25:33,833 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:25:41,655 DEBUG Query successful
2024-10-06 17:25:41,657 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:25:44,847 DEBUG Query successful
2024-10-06 17:25:44,848 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:26:19,328 DEBUG Query successful
2024-10-06 17:26:19,413 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:27:02,742 DEBUG Query successful
2024-10-06 17:27:02,744 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:27:19,605 DEBUG Query successful
2024-10-06 17:27:19,952 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:27:26,343 DEBUG Query successful
2024-10-06 17:27:26,345 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:27:31,200 DEBUG Query successful
2024-10-06 17:27:31,200 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:28:02,725 DEBUG Query successful
2024-10-06 17:28:02,814 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:28:50,811 DEBUG Query successful
2024-10-06 17:28:50,813 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:29:07,627 DEBUG Query successful
2024-10-06 17:29:07,907 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:29:15,156 DEBUG Query successful
2024-10-06 17:29:15,159 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:29:18,702 DEBUG Query successful
2024-10-06 17:29:18,707 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:29:44,912 DEBUG Query successful
2024-10-06 17:29:45,000 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:30:31,761 DEBUG Query successful
2024-10-06 17:30:31,762 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:30:53,314 DEBUG Query successful
2024-10-06 17:30:53,599 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:31:01,607 DEBUG Query successful
2024-10-06 17:31:01,613 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:31:05,417 DEBUG Query successful
2024-10-06 17:31:05,418 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:31:42,369 DEBUG Query successful
2024-10-06 17:31:42,456 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:33:34,004 DEBUG Query successful
2024-10-06 17:33:34,006 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:34:02,655 DEBUG Query successful
2024-10-06 17:34:02,959 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:34:13,235 DEBUG Query successful
2024-10-06 17:34:13,237 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:34:16,976 DEBUG Query successful
2024-10-06 17:34:16,976 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:34:45,129 DEBUG Query successful
2024-10-06 17:34:45,210 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:35:37,556 DEBUG Query successful
2024-10-06 17:35:37,557 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:35:59,120 DEBUG Query successful
2024-10-06 17:35:59,390 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:36:06,509 DEBUG Query successful
2024-10-06 17:36:06,511 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:36:10,122 DEBUG Query successful
2024-10-06 17:36:10,122 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:36:34,240 DEBUG Query successful
2024-10-06 17:36:34,326 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:37:30,796 DEBUG Query successful
2024-10-06 17:37:30,798 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:37:52,635 DEBUG Query successful
2024-10-06 17:49:56,546 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 17:49:56,576 DEBUG OpenAI client created
2024-10-06 17:49:56,577 DEBUG Model set to: gpt-4-turbo
2024-10-06 17:49:56,577 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 17:49:56,598 DEBUG OpenAI client created
2024-10-06 17:49:56,599 DEBUG Model set to: gpt-4-turbo
2024-10-06 17:50:01,763 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:50:12,152 DEBUG Query successful
2024-10-06 17:50:12,154 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:50:13,864 DEBUG Query successful
2024-10-06 17:50:13,876 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:50:25,300 DEBUG Query successful
2024-10-06 17:50:25,382 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:50:35,538 DEBUG Query successful
2024-10-06 17:50:35,539 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:50:40,988 DEBUG Query successful
2024-10-06 17:50:41,255 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:50:52,819 DEBUG Query successful
2024-10-06 17:50:52,824 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:50:55,306 DEBUG Query successful
2024-10-06 17:50:55,308 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:51:10,910 DEBUG Query successful
2024-10-06 17:51:10,989 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:51:22,709 DEBUG Query successful
2024-10-06 17:51:22,714 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:51:31,244 DEBUG Query successful
2024-10-06 17:51:41,552 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:51:50,409 DEBUG Query successful
2024-10-06 17:51:50,411 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:51:53,838 DEBUG Query successful
2024-10-06 17:51:53,839 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:52:09,132 DEBUG Query successful
2024-10-06 17:52:09,227 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:52:22,440 DEBUG Query successful
2024-10-06 17:52:22,442 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:52:30,921 DEBUG Query successful
2024-10-06 17:52:31,196 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:52:38,873 DEBUG Query successful
2024-10-06 17:52:38,874 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:52:41,589 DEBUG Query successful
2024-10-06 17:52:41,591 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:52:55,458 DEBUG Query successful
2024-10-06 17:52:55,546 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:53:10,365 DEBUG Query successful
2024-10-06 17:53:10,367 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:53:18,531 DEBUG Query successful
2024-10-06 17:53:18,808 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:53:26,871 DEBUG Query successful
2024-10-06 17:53:26,872 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:53:29,641 DEBUG Query successful
2024-10-06 17:53:29,663 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:53:44,968 DEBUG Query successful
2024-10-06 17:53:45,055 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:54:03,913 DEBUG Query successful
2024-10-06 17:54:03,915 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:54:13,287 DEBUG Query successful
2024-10-06 17:54:13,671 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:54:22,275 DEBUG Query successful
2024-10-06 17:54:22,281 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:54:26,208 DEBUG Query successful
2024-10-06 17:54:26,209 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:54:44,523 DEBUG Query successful
2024-10-06 17:54:44,612 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:55:05,774 DEBUG Query successful
2024-10-06 17:55:05,779 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:55:20,689 DEBUG Query successful
2024-10-06 17:55:21,018 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:55:30,403 DEBUG Query successful
2024-10-06 17:55:30,407 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:55:33,372 DEBUG Query successful
2024-10-06 17:55:33,374 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:55:57,849 DEBUG Query successful
2024-10-06 17:55:57,931 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:56:20,070 DEBUG Query successful
2024-10-06 17:56:20,072 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:56:29,826 DEBUG Query successful
2024-10-06 17:56:30,099 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:56:38,661 DEBUG Query successful
2024-10-06 17:56:38,662 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:56:42,552 DEBUG Query successful
2024-10-06 17:56:42,554 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:57:03,790 DEBUG Query successful
2024-10-06 17:57:03,872 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:57:30,123 DEBUG Query successful
2024-10-06 17:57:30,125 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:57:40,340 DEBUG Query successful
2024-10-06 17:57:40,618 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:57:48,825 DEBUG Query successful
2024-10-06 17:57:48,826 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:57:52,027 DEBUG Query successful
2024-10-06 17:57:52,032 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:58:12,545 DEBUG Query successful
2024-10-06 17:58:12,632 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:58:39,834 DEBUG Query successful
2024-10-06 17:58:39,836 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:58:53,714 DEBUG Query successful
2024-10-06 17:58:53,992 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:59:05,121 DEBUG Query successful
2024-10-06 17:59:05,123 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:59:09,517 DEBUG Query successful
2024-10-06 17:59:09,518 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:59:30,038 DEBUG Query successful
2024-10-06 17:59:30,140 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:59:59,979 DEBUG Query successful
2024-10-06 17:59:59,985 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:00:13,274 DEBUG Query successful
2024-10-06 18:00:13,546 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:00:23,131 DEBUG Query successful
2024-10-06 18:00:23,133 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:00:26,845 DEBUG Query successful
2024-10-06 18:00:26,850 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:00:55,431 DEBUG Query successful
2024-10-06 18:00:55,540 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:01:25,304 DEBUG Query successful
2024-10-06 18:01:25,305 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:01:37,483 DEBUG Query successful
2024-10-06 18:01:37,770 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:01:47,495 DEBUG Query successful
2024-10-06 18:01:47,497 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:01:53,979 DEBUG Query successful
2024-10-06 18:01:53,980 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:02:22,370 DEBUG Query successful
2024-10-06 18:02:22,476 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:02:56,301 DEBUG Query successful
2024-10-06 18:02:56,303 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:03:08,778 DEBUG Query successful
2024-10-06 18:03:09,045 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:03:19,023 DEBUG Query successful
2024-10-06 18:03:19,025 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:03:26,887 DEBUG Query successful
2024-10-06 18:03:26,898 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:03:54,472 DEBUG Query successful
2024-10-06 18:03:54,553 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:04:31,856 DEBUG Query successful
2024-10-06 18:04:31,858 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:04:41,663 DEBUG Query successful
2024-10-06 18:04:41,931 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:04:54,264 DEBUG Query successful
2024-10-06 18:04:54,266 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:05:01,473 DEBUG Query successful
2024-10-06 18:05:01,475 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:05:26,737 DEBUG Query successful
2024-10-06 18:05:26,818 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:06:12,224 DEBUG Query successful
2024-10-06 18:06:12,229 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:06:27,067 DEBUG Query successful
2024-10-06 18:06:27,340 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:06:36,994 DEBUG Query successful
2024-10-06 18:06:36,996 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:06:45,107 DEBUG Query successful
2024-10-06 18:06:45,108 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:07:14,515 DEBUG Query successful
2024-10-06 18:07:14,595 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:07:53,052 DEBUG Query successful
2024-10-06 18:07:53,055 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:08:05,609 DEBUG Query successful
2024-10-06 18:08:05,897 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:08:14,941 DEBUG Query successful
2024-10-06 18:08:14,943 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:08:19,531 DEBUG Query successful
2024-10-06 18:08:19,532 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:08:49,972 DEBUG Query successful
2024-10-06 18:08:50,057 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:09:28,807 DEBUG Query successful
2024-10-06 18:09:28,808 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:09:43,026 DEBUG Query successful
2024-10-06 18:09:43,296 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:09:50,573 DEBUG Query successful
2024-10-06 18:09:50,575 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:09:55,636 DEBUG Query successful
2024-10-06 18:09:55,641 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:10:26,137 DEBUG Query successful
2024-10-06 18:10:26,217 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:11:05,677 DEBUG Query successful
2024-10-06 18:11:05,679 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:11:17,817 DEBUG Query successful
2024-10-06 18:11:18,086 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:11:26,728 DEBUG Query successful
2024-10-06 18:11:26,730 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:11:36,615 DEBUG Query successful
2024-10-06 18:11:36,617 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:12:09,781 DEBUG Query successful
2024-10-06 18:12:09,867 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:12:48,760 DEBUG Query successful
2024-10-06 18:12:48,761 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:12:59,820 DEBUG Query successful
2024-10-06 18:13:00,090 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:13:08,069 DEBUG Query successful
2024-10-06 18:13:08,071 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:13:13,863 DEBUG Query successful
2024-10-06 18:13:13,865 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:13:43,415 DEBUG Query successful
2024-10-06 18:13:43,501 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:14:22,278 DEBUG Query successful
2024-10-06 18:14:22,281 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:14:37,151 DEBUG Query successful
2024-10-06 18:14:37,425 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:14:46,493 DEBUG Query successful
2024-10-06 18:14:46,495 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:14:52,950 DEBUG Query successful
2024-10-06 18:14:52,951 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:15:20,296 DEBUG Query successful
2024-10-06 18:15:20,383 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:15:58,920 DEBUG Query successful
2024-10-06 18:15:58,922 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:16:10,656 DEBUG Query successful
2024-10-06 18:16:10,929 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:16:19,398 DEBUG Query successful
2024-10-06 18:16:19,400 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:16:25,660 DEBUG Query successful
2024-10-06 18:16:25,665 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:16:56,262 DEBUG Query successful
2024-10-06 18:16:56,341 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:17:46,223 DEBUG Query successful
2024-10-06 18:17:46,223 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:17:57,390 DEBUG Query successful
2024-10-06 18:17:57,689 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:18:21,522 DEBUG Query successful
2024-10-06 18:18:21,526 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:18:27,185 DEBUG Query successful
2024-10-06 18:18:27,188 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:18:50,789 DEBUG Query successful
2024-10-06 18:18:50,870 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:20:38,460 DEBUG Query successful
2024-10-06 18:20:38,483 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:20:50,003 DEBUG Query successful
2024-10-06 18:20:50,299 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:20:57,342 DEBUG Query successful
2024-10-06 18:20:57,345 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:21:06,381 DEBUG Query successful
2024-10-06 18:21:06,382 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:21:24,111 DEBUG Query successful
2024-10-06 18:21:24,193 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:22:10,447 DEBUG Query successful
2024-10-06 18:22:10,448 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:22:23,978 DEBUG Query successful
2024-10-06 18:22:24,272 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:22:31,586 DEBUG Query successful
2024-10-06 18:22:31,589 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:22:37,587 DEBUG Query successful
2024-10-06 18:22:37,589 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:23:00,563 DEBUG Query successful
2024-10-06 18:23:00,642 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:23:47,266 DEBUG Query successful
2024-10-06 18:23:47,268 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:24:00,731 DEBUG Query successful
2024-10-06 18:24:01,001 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:24:10,480 DEBUG Query successful
2024-10-06 18:24:10,482 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:24:16,191 DEBUG Query successful
2024-10-06 18:24:16,196 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:24:42,308 DEBUG Query successful
2024-10-06 18:24:42,386 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:25:35,760 DEBUG Query successful
2024-10-06 18:25:35,761 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:25:44,176 DEBUG Query successful
2024-10-06 20:09:32,903 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 20:09:32,932 DEBUG OpenAI client created
2024-10-06 20:09:32,932 DEBUG Model set to: gpt-4-turbo
2024-10-06 20:09:38,092 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:09:47,051 DEBUG Query successful
2024-10-06 20:09:47,141 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:09:55,326 DEBUG Query successful
2024-10-06 20:09:55,593 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:10:06,147 DEBUG Query successful
2024-10-06 20:10:06,231 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:10:16,641 DEBUG Query successful
2024-10-06 20:10:26,911 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:10:37,181 DEBUG Query successful
2024-10-06 20:10:37,271 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:10:47,698 DEBUG Query successful
2024-10-06 20:10:47,977 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:10:56,687 DEBUG Query successful
2024-10-06 20:10:56,776 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:11:16,574 DEBUG Query successful
2024-10-06 20:11:16,856 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:11:30,205 DEBUG Query successful
2024-10-06 20:11:30,296 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:11:38,249 DEBUG Query successful
2024-10-06 20:11:38,551 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:11:49,377 DEBUG Query successful
2024-10-06 20:11:49,474 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:11:57,555 DEBUG Query successful
2024-10-06 20:11:57,834 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:12:07,627 DEBUG Query successful
2024-10-06 20:12:07,717 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:12:14,833 DEBUG Query successful
2024-10-06 20:12:15,111 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:12:28,104 DEBUG Query successful
2024-10-06 20:12:28,195 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:12:39,463 DEBUG Query successful
2024-10-06 20:12:39,743 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:12:52,015 DEBUG Query successful
2024-10-06 20:12:52,103 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:12:59,911 DEBUG Query successful
2024-10-06 20:13:00,199 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:13:10,070 DEBUG Query successful
2024-10-06 20:13:10,161 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:13:19,988 DEBUG Query successful
2024-10-06 20:13:20,271 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:13:30,834 DEBUG Query successful
2024-10-06 20:13:30,931 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:13:39,289 DEBUG Query successful
2024-10-06 20:13:39,566 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:13:52,820 DEBUG Query successful
2024-10-06 20:13:52,908 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:14:00,568 DEBUG Query successful
2024-10-06 20:14:00,855 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:14:12,329 DEBUG Query successful
2024-10-06 20:14:12,427 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:14:20,069 DEBUG Query successful
2024-10-06 20:14:20,357 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:15:10,476 DEBUG Query successful
2024-10-06 20:15:10,559 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:15:19,715 DEBUG Query successful
2024-10-06 20:15:19,997 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:15:30,573 DEBUG Query successful
2024-10-06 20:15:30,656 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:15:38,415 DEBUG Query successful
2024-10-06 20:15:38,689 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:15:52,241 DEBUG Query successful
2024-10-06 20:15:52,333 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:16:05,990 DEBUG Query successful
2024-10-06 20:16:06,261 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:16:16,243 DEBUG Query successful
2024-10-06 20:16:16,329 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:16:22,442 DEBUG Query successful
2024-10-06 20:16:22,726 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:16:31,193 DEBUG Query successful
2024-10-06 20:16:31,281 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:16:37,495 DEBUG Query successful
2024-10-06 20:16:37,767 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:16:48,780 DEBUG Query successful
2024-10-06 20:16:48,866 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:16:56,675 DEBUG Query successful
2024-10-06 20:16:56,959 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:17:06,066 DEBUG Query successful
2024-10-06 20:17:06,158 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:17:13,927 DEBUG Query successful
2024-10-06 20:17:14,197 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:17:27,138 DEBUG Query successful
2024-10-06 20:17:27,227 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:17:36,211 DEBUG Query successful
2024-10-06 20:17:36,493 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:17:46,545 DEBUG Query successful
2024-10-06 20:17:46,632 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:17:54,291 DEBUG Query successful
2024-10-06 20:17:54,585 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:18:04,648 DEBUG Query successful
2024-10-06 20:18:04,739 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:18:11,947 DEBUG Query successful
2024-10-06 20:18:12,234 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:18:21,999 DEBUG Query successful
2024-10-06 20:18:22,095 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:18:29,022 DEBUG Query successful
2024-10-06 20:18:29,293 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:18:40,422 DEBUG Query successful
2024-10-06 20:18:40,511 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:18:45,550 DEBUG Query successful
2024-10-06 20:26:22,140 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 20:26:22,164 DEBUG OpenAI client created
2024-10-06 20:26:22,164 DEBUG Model set to: gpt-4-turbo
2024-10-06 20:26:22,164 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 20:26:22,188 DEBUG OpenAI client created
2024-10-06 20:26:22,189 DEBUG Model set to: gpt-4-turbo
2024-10-06 20:26:27,231 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:26:32,520 DEBUG Query successful
2024-10-06 20:26:32,522 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:26:34,305 DEBUG Query successful
2024-10-06 20:26:34,307 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:26:49,188 DEBUG Query successful
2024-10-06 20:26:49,202 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:26:58,176 DEBUG Query successful
2024-10-06 20:26:58,177 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:27:04,138 DEBUG Query successful
2024-10-06 20:27:04,160 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:27:10,881 DEBUG Query successful
2024-10-06 20:27:10,882 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:27:18,355 DEBUG Query successful
2024-10-06 20:27:18,525 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:27:28,856 DEBUG Query successful
2024-10-06 20:27:28,879 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:27:31,918 DEBUG Query successful
2024-10-06 20:27:31,919 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:27:44,096 DEBUG Query successful
2024-10-06 20:27:44,116 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:27:56,541 DEBUG Query successful
2024-10-06 20:27:56,542 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:28:02,792 DEBUG Query successful
2024-10-06 20:28:13,051 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:28:20,779 DEBUG Query successful
2024-10-06 20:28:20,781 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:28:24,422 DEBUG Query successful
2024-10-06 20:28:24,433 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:28:38,780 DEBUG Query successful
2024-10-06 20:28:38,859 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:28:52,280 DEBUG Query successful
2024-10-06 20:28:52,281 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:28:59,337 DEBUG Query successful
2024-10-06 20:28:59,609 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:29:09,124 DEBUG Query successful
2024-10-06 20:29:09,128 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:29:13,559 DEBUG Query successful
2024-10-06 20:29:13,560 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:29:29,745 DEBUG Query successful
2024-10-06 20:29:29,825 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:29:43,489 DEBUG Query successful
2024-10-06 20:29:43,491 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:29:51,780 DEBUG Query successful
2024-10-06 20:29:52,068 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:30:04,007 DEBUG Query successful
2024-10-06 20:30:04,008 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:30:07,934 DEBUG Query successful
2024-10-06 20:30:07,938 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:30:23,713 DEBUG Query successful
2024-10-06 20:30:23,807 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:30:39,150 DEBUG Query successful
2024-10-06 20:30:39,151 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:30:47,446 DEBUG Query successful
2024-10-06 20:30:47,865 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:30:57,829 DEBUG Query successful
2024-10-06 20:30:57,832 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:31:03,061 DEBUG Query successful
2024-10-06 20:31:03,063 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:31:19,242 DEBUG Query successful
2024-10-06 20:31:19,335 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:31:43,736 DEBUG Query successful
2024-10-06 20:31:43,741 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:31:53,554 DEBUG Query successful
2024-10-06 20:31:53,913 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:32:02,489 DEBUG Query successful
2024-10-06 20:32:02,492 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:32:06,906 DEBUG Query successful
2024-10-06 20:32:06,907 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:32:27,081 DEBUG Query successful
2024-10-06 20:32:27,176 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:32:48,866 DEBUG Query successful
2024-10-06 20:32:48,867 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:32:58,449 DEBUG Query successful
2024-10-06 20:32:58,737 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:33:08,158 DEBUG Query successful
2024-10-06 20:33:08,161 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:33:13,450 DEBUG Query successful
2024-10-06 20:33:13,451 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:33:34,011 DEBUG Query successful
2024-10-06 20:33:34,115 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:33:56,055 DEBUG Query successful
2024-10-06 20:33:56,056 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:33:56,470 ERROR An error occurred: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.PermissionDeniedError: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}
2024-10-06 20:33:56,576 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:33:56,795 ERROR An error occurred: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.PermissionDeniedError: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}
2024-10-06 20:33:56,800 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:33:56,969 ERROR An error occurred: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.PermissionDeniedError: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}
2024-10-06 20:33:57,254 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:34:06,456 DEBUG Query successful
2024-10-06 20:34:06,458 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:34:12,851 DEBUG Query successful
2024-10-06 20:34:12,852 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:34:38,674 DEBUG Query successful
2024-10-06 20:34:38,756 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:35:04,411 DEBUG Query successful
2024-10-06 20:35:04,412 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:35:14,173 DEBUG Query successful
2024-10-06 20:35:14,515 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:35:23,031 DEBUG Query successful
2024-10-06 20:35:23,032 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:35:31,259 DEBUG Query successful
2024-10-06 20:35:31,260 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:35:55,105 DEBUG Query successful
2024-10-06 20:35:55,197 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:36:22,542 DEBUG Query successful
2024-10-06 20:36:22,544 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:36:32,125 DEBUG Query successful
2024-10-06 20:36:32,439 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:36:41,052 DEBUG Query successful
2024-10-06 20:36:41,054 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:36:47,375 DEBUG Query successful
2024-10-06 20:36:47,380 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:37:09,520 DEBUG Query successful
2024-10-06 20:37:09,610 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:37:37,971 DEBUG Query successful
2024-10-06 20:37:37,972 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:37:49,261 DEBUG Query successful
2024-10-06 20:37:49,619 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:38:01,359 DEBUG Query successful
2024-10-06 20:38:01,361 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:38:10,228 DEBUG Query successful
2024-10-06 20:38:10,228 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:38:31,560 DEBUG Query successful
2024-10-06 20:38:31,647 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:39:14,925 DEBUG Query successful
2024-10-06 20:39:14,930 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:39:28,736 DEBUG Query successful
2024-10-06 20:39:29,029 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:39:37,883 DEBUG Query successful
2024-10-06 20:39:37,886 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:39:46,379 DEBUG Query successful
2024-10-06 20:39:46,391 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:40:08,764 DEBUG Query successful
2024-10-06 20:40:08,852 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:40:42,712 DEBUG Query successful
2024-10-06 20:40:42,713 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:40:54,691 DEBUG Query successful
2024-10-06 20:40:54,983 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:41:06,159 DEBUG Query successful
2024-10-06 20:41:06,161 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:41:13,205 DEBUG Query successful
2024-10-06 20:41:13,208 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:41:31,755 DEBUG Query successful
2024-10-06 20:41:31,842 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:42:04,603 DEBUG Query successful
2024-10-06 20:42:04,605 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:42:19,779 DEBUG Query successful
2024-10-06 20:42:20,069 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:42:29,366 DEBUG Query successful
2024-10-06 20:42:29,369 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:42:36,489 DEBUG Query successful
2024-10-06 20:42:36,493 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:42:57,403 DEBUG Query successful
2024-10-06 20:42:57,493 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:43:33,923 DEBUG Query successful
2024-10-06 20:43:33,925 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:43:45,704 DEBUG Query successful
2024-10-06 20:43:45,991 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:43:54,310 DEBUG Query successful
2024-10-06 20:43:54,313 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:44:01,000 DEBUG Query successful
2024-10-06 20:44:01,001 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:44:20,619 DEBUG Query successful
2024-10-06 20:44:20,709 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:45:03,103 DEBUG Query successful
2024-10-06 20:45:03,105 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:45:16,971 DEBUG Query successful
2024-10-06 20:45:17,262 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:45:27,990 DEBUG Query successful
2024-10-06 20:45:27,992 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:45:35,660 DEBUG Query successful
2024-10-06 20:45:35,661 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:45:59,682 DEBUG Query successful
2024-10-06 20:45:59,773 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:46:54,045 DEBUG Query successful
2024-10-06 20:46:54,047 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:47:08,738 DEBUG Query successful
2024-10-06 20:47:09,033 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:47:16,983 DEBUG Query successful
2024-10-06 20:47:16,985 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:47:24,325 DEBUG Query successful
2024-10-06 20:47:24,326 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:47:43,897 DEBUG Query successful
2024-10-06 20:47:43,980 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:48:28,985 DEBUG Query successful
2024-10-06 20:48:28,986 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:48:41,376 DEBUG Query successful
2024-10-06 20:48:41,667 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:48:50,923 DEBUG Query successful
2024-10-06 20:48:50,924 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:48:58,806 DEBUG Query successful
2024-10-06 20:48:58,808 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:49:19,828 DEBUG Query successful
2024-10-06 20:49:19,918 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:50:02,139 DEBUG Query successful
2024-10-06 20:50:02,141 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:50:15,560 DEBUG Query successful
2024-10-06 20:50:15,844 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:51:24,021 DEBUG Query successful
2024-10-06 20:51:24,027 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:51:31,117 DEBUG Query successful
2024-10-06 20:51:31,119 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:51:55,333 DEBUG Query successful
2024-10-06 20:51:55,418 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:52:40,616 DEBUG Query successful
2024-10-06 20:52:40,622 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:52:56,140 DEBUG Query successful
2024-10-06 20:52:56,433 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:53:05,519 DEBUG Query successful
2024-10-06 20:53:05,522 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:53:14,074 DEBUG Query successful
2024-10-06 20:53:14,074 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:53:43,547 DEBUG Query successful
2024-10-06 20:53:43,637 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:54:28,275 DEBUG Query successful
2024-10-06 20:54:28,278 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:54:41,598 DEBUG Query successful
2024-10-06 20:54:41,890 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:54:47,939 DEBUG Query successful
2024-10-06 20:54:47,940 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:54:55,590 DEBUG Query successful
2024-10-06 20:54:55,592 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:55:22,141 DEBUG Query successful
2024-10-06 20:55:22,232 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:56:21,009 DEBUG Query successful
2024-10-06 20:56:21,011 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:56:33,363 DEBUG Query successful
2024-10-06 20:56:33,653 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:56:40,960 DEBUG Query successful
2024-10-06 20:56:40,962 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:56:48,354 DEBUG Query successful
2024-10-06 20:56:48,358 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:57:16,516 DEBUG Query successful
2024-10-06 20:57:16,600 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:58:07,404 DEBUG Query successful
2024-10-06 20:58:07,406 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:58:21,471 DEBUG Query successful
2024-10-06 20:58:21,772 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:58:30,237 DEBUG Query successful
2024-10-06 20:58:30,239 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:58:39,182 DEBUG Query successful
2024-10-06 20:58:39,184 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:59:11,949 DEBUG Query successful
2024-10-06 20:59:12,039 DEBUG Querying model: gpt-4-turbo
2024-10-06 21:00:08,128 DEBUG Query successful
2024-10-06 21:00:08,131 DEBUG Querying model: gpt-4-turbo
2024-10-06 21:00:23,819 DEBUG Query successful
2024-10-06 21:00:24,216 DEBUG Querying model: gpt-4-turbo
2024-10-06 21:00:32,138 DEBUG Query successful
2024-10-06 21:00:32,140 DEBUG Querying model: gpt-4-turbo
2024-10-06 21:00:40,539 DEBUG Query successful
2024-10-06 21:00:40,540 DEBUG Querying model: gpt-4-turbo
2024-10-06 21:01:05,936 DEBUG Query successful
2024-10-06 21:01:06,027 DEBUG Querying model: gpt-4-turbo
2024-10-06 21:03:06,126 DEBUG Query successful
2024-10-06 21:03:06,128 DEBUG Querying model: gpt-4-turbo
2024-10-06 21:03:25,027 DEBUG Query successful
2024-10-06 22:08:03,054 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 22:08:03,080 DEBUG OpenAI client created
2024-10-06 22:08:03,080 DEBUG Model set to: gpt-4-turbo
2024-10-06 22:08:03,080 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 22:08:03,102 DEBUG OpenAI client created
2024-10-06 22:08:03,102 DEBUG Model set to: gpt-4-turbo
2024-10-06 22:08:08,142 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:08:16,849 DEBUG Query successful
2024-10-06 22:08:16,851 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:08:18,291 DEBUG Query successful
2024-10-06 22:08:18,293 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:08:28,934 DEBUG Query successful
2024-10-06 22:08:28,948 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:08:34,570 DEBUG Query successful
2024-10-06 22:08:34,572 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:08:41,100 DEBUG Query successful
2024-10-06 22:08:41,104 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:08:46,999 DEBUG Query successful
2024-10-06 22:08:47,004 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:08:54,020 DEBUG Query successful
2024-10-06 22:08:54,169 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:09:01,337 DEBUG Query successful
2024-10-06 22:09:01,338 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:09:03,307 DEBUG Query successful
2024-10-06 22:09:03,308 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:09:13,108 DEBUG Query successful
2024-10-06 22:09:13,128 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:09:21,251 DEBUG Query successful
2024-10-06 22:09:21,252 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:09:29,249 DEBUG Query successful
2024-10-06 22:09:29,250 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:09:37,492 DEBUG Query successful
2024-10-06 22:09:37,493 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:09:45,009 DEBUG Query successful
2024-10-06 22:09:55,283 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:10:05,689 DEBUG Query successful
2024-10-06 22:10:05,691 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:10:07,993 DEBUG Query successful
2024-10-06 22:10:07,994 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:10:18,809 DEBUG Query successful
2024-10-06 22:10:18,884 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:10:32,627 DEBUG Query successful
2024-10-06 22:10:32,632 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:10:41,571 DEBUG Query successful
2024-10-06 22:10:41,574 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:10:48,390 DEBUG Query successful
2024-10-06 22:10:48,390 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:10:57,209 DEBUG Query successful
2024-10-06 22:10:57,482 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:11:05,409 DEBUG Query successful
2024-10-06 22:11:05,410 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:11:08,129 DEBUG Query successful
2024-10-06 22:11:08,134 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:11:19,694 DEBUG Query successful
2024-10-06 22:11:19,770 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:11:32,865 DEBUG Query successful
2024-10-06 22:11:32,871 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:11:42,298 DEBUG Query successful
2024-10-06 22:11:42,302 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:11:52,363 DEBUG Query successful
2024-10-06 22:11:52,364 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:12:00,904 DEBUG Query successful
2024-10-06 22:12:01,180 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:12:21,705 DEBUG Query successful
2024-10-06 22:12:21,708 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:12:24,853 DEBUG Query successful
2024-10-06 22:12:24,854 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:12:41,460 DEBUG Query successful
2024-10-06 22:12:41,549 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:12:56,891 DEBUG Query successful
2024-10-06 22:12:56,892 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:13:10,343 DEBUG Query successful
2024-10-06 22:13:10,344 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:13:24,332 DEBUG Query successful
2024-10-06 22:13:24,336 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:13:33,118 DEBUG Query successful
2024-10-06 22:13:33,397 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:13:42,793 DEBUG Query successful
2024-10-06 22:13:42,795 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:13:45,666 DEBUG Query successful
2024-10-06 22:13:45,667 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:14:02,223 DEBUG Query successful
2024-10-06 22:14:02,307 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:14:19,018 DEBUG Query successful
2024-10-06 22:14:19,019 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:14:29,793 DEBUG Query successful
2024-10-06 22:14:29,794 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:14:39,881 DEBUG Query successful
2024-10-06 22:14:39,883 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:14:49,557 DEBUG Query successful
2024-10-06 22:14:49,846 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:14:58,567 DEBUG Query successful
2024-10-06 22:14:58,574 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:15:02,297 DEBUG Query successful
2024-10-06 22:15:02,297 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:15:19,437 DEBUG Query successful
2024-10-06 22:15:19,525 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:15:37,207 DEBUG Query successful
2024-10-06 22:15:37,212 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:15:49,016 DEBUG Query successful
2024-10-06 22:15:49,017 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:16:01,269 DEBUG Query successful
2024-10-06 22:16:01,292 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:16:13,252 DEBUG Query successful
2024-10-06 22:16:13,692 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:16:25,371 DEBUG Query successful
2024-10-06 22:16:25,373 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:16:27,990 DEBUG Query successful
2024-10-06 22:16:27,995 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:16:48,305 DEBUG Query successful
2024-10-06 22:16:48,407 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:17:09,675 DEBUG Query successful
2024-10-06 22:17:09,676 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:17:24,193 DEBUG Query successful
2024-10-06 22:17:24,215 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:17:36,191 DEBUG Query successful
2024-10-06 22:17:36,191 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:17:46,975 DEBUG Query successful
2024-10-06 22:17:47,252 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:17:56,162 DEBUG Query successful
2024-10-06 22:17:56,163 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:17:59,280 DEBUG Query successful
2024-10-06 22:17:59,283 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:18:25,210 DEBUG Query successful
2024-10-06 22:18:25,295 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:18:50,686 DEBUG Query successful
2024-10-06 22:18:50,688 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:19:02,553 DEBUG Query successful
2024-10-06 22:19:02,554 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:19:14,938 DEBUG Query successful
2024-10-06 22:19:14,960 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:19:28,462 DEBUG Query successful
2024-10-06 22:19:28,733 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:19:41,447 DEBUG Query successful
2024-10-06 22:19:41,448 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:19:45,638 DEBUG Query successful
2024-10-06 22:19:45,639 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:20:08,975 DEBUG Query successful
2024-10-06 22:20:09,061 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:20:35,585 DEBUG Query successful
2024-10-06 22:20:35,588 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:20:50,045 DEBUG Query successful
2024-10-06 22:20:50,046 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:21:07,347 DEBUG Query successful
2024-10-06 22:21:07,347 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:21:21,182 DEBUG Query successful
2024-10-06 22:21:21,476 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:21:30,731 DEBUG Query successful
2024-10-06 22:21:30,754 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:21:38,262 DEBUG Query successful
2024-10-06 22:21:38,263 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:22:03,504 DEBUG Query successful
2024-10-06 22:22:03,590 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:22:37,768 DEBUG Query successful
2024-10-06 22:22:37,773 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:22:52,311 DEBUG Query successful
2024-10-06 22:22:52,313 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:23:07,809 DEBUG Query successful
2024-10-06 22:23:07,810 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:23:23,821 DEBUG Query successful
2024-10-06 22:23:24,097 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:23:35,205 DEBUG Query successful
2024-10-06 22:23:35,208 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:23:42,191 DEBUG Query successful
2024-10-06 22:23:42,193 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:24:09,036 DEBUG Query successful
2024-10-06 22:24:09,118 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:24:40,608 DEBUG Query successful
2024-10-06 22:24:40,610 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:24:54,220 DEBUG Query successful
2024-10-06 22:24:54,225 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:25:10,364 DEBUG Query successful
2024-10-06 22:25:10,365 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:25:24,751 DEBUG Query successful
2024-10-06 22:25:25,087 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:25:33,766 DEBUG Query successful
2024-10-06 22:25:33,767 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:25:38,074 DEBUG Query successful
2024-10-06 22:25:38,075 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:26:06,252 DEBUG Query successful
2024-10-06 22:26:06,339 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:26:37,256 DEBUG Query successful
2024-10-06 22:26:37,257 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:26:50,062 DEBUG Query successful
2024-10-06 22:26:50,062 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:27:02,801 DEBUG Query successful
2024-10-06 22:27:02,824 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:27:15,755 DEBUG Query successful
2024-10-06 22:27:16,029 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:27:26,077 DEBUG Query successful
2024-10-06 22:27:26,080 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:27:30,122 DEBUG Query successful
2024-10-06 22:27:30,123 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:28:01,817 DEBUG Query successful
2024-10-06 22:28:01,899 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:28:33,191 DEBUG Query successful
2024-10-06 22:28:33,193 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:28:48,387 DEBUG Query successful
2024-10-06 22:28:48,388 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:29:01,993 DEBUG Query successful
2024-10-06 22:29:01,994 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:29:17,249 DEBUG Query successful
2024-10-06 22:29:17,525 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:29:27,455 DEBUG Query successful
2024-10-06 22:29:27,460 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:29:30,934 DEBUG Query successful
2024-10-06 22:29:30,935 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:30:00,141 DEBUG Query successful
2024-10-06 22:30:00,222 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:30:32,922 DEBUG Query successful
2024-10-06 22:30:32,928 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:30:49,364 DEBUG Query successful
2024-10-06 22:30:49,365 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:31:12,074 DEBUG Query successful
2024-10-06 22:31:12,075 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:31:29,255 DEBUG Query successful
2024-10-06 22:31:29,541 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:31:38,050 DEBUG Query successful
2024-10-06 22:31:38,051 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:31:44,784 DEBUG Query successful
2024-10-06 22:31:44,786 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:32:21,283 DEBUG Query successful
2024-10-06 22:32:21,365 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:32:57,069 DEBUG Query successful
2024-10-06 22:32:57,072 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:33:13,810 DEBUG Query successful
2024-10-06 22:33:13,814 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:33:30,495 DEBUG Query successful
2024-10-06 22:33:30,497 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:33:45,574 DEBUG Query successful
2024-10-06 22:33:45,885 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:33:58,539 DEBUG Query successful
2024-10-06 22:33:58,541 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:34:08,299 DEBUG Query successful
2024-10-06 22:34:08,301 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:34:45,361 DEBUG Query successful
2024-10-06 22:34:45,447 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:35:33,252 DEBUG Query successful
2024-10-06 22:35:33,254 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:35:47,940 DEBUG Query successful
2024-10-06 22:35:47,941 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:36:02,609 DEBUG Query successful
2024-10-06 22:36:02,614 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:36:17,489 DEBUG Query successful
2024-10-06 22:36:17,832 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:36:26,104 DEBUG Query successful
2024-10-06 22:36:26,105 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:36:31,177 DEBUG Query successful
2024-10-06 22:36:31,178 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:37:03,214 DEBUG Query successful
2024-10-06 22:37:03,297 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:37:41,752 DEBUG Query successful
2024-10-06 22:37:41,754 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:37:56,842 DEBUG Query successful
2024-10-06 22:37:56,843 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:38:30,076 DEBUG Query successful
2024-10-06 22:38:30,078 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:38:44,367 DEBUG Query successful
2024-10-06 22:38:44,671 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:38:55,174 DEBUG Query successful
2024-10-06 22:38:55,180 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:39:00,686 DEBUG Query successful
2024-10-06 22:39:00,687 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:39:42,240 DEBUG Query successful
2024-10-06 22:39:42,324 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:40:20,336 DEBUG Query successful
2024-10-06 22:40:20,341 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:40:34,859 DEBUG Query successful
2024-10-06 22:40:34,860 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:40:49,330 DEBUG Query successful
2024-10-06 22:40:49,332 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:41:03,049 DEBUG Query successful
2024-10-06 22:41:03,320 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:41:10,190 DEBUG Query successful
2024-10-06 22:41:10,191 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:41:15,243 DEBUG Query successful
2024-10-06 22:41:15,248 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:41:57,819 DEBUG Query successful
2024-10-06 22:41:57,901 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:42:42,392 DEBUG Query successful
2024-10-06 22:42:42,393 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:42:59,587 DEBUG Query successful
2024-10-06 22:42:59,610 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:43:15,735 DEBUG Query successful
2024-10-06 22:43:15,737 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:43:31,588 DEBUG Query successful
2024-10-06 22:43:31,865 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:43:40,690 DEBUG Query successful
2024-10-06 22:43:40,692 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:43:46,207 DEBUG Query successful
2024-10-06 22:43:46,208 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:44:31,682 DEBUG Query successful
2024-10-06 22:44:31,767 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:45:21,374 DEBUG Query successful
2024-10-06 22:45:21,376 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:45:37,838 DEBUG Query successful
2024-10-06 22:45:37,839 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:45:54,888 DEBUG Query successful
2024-10-06 22:45:54,893 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:46:11,208 DEBUG Query successful
2024-10-06 22:46:11,486 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:46:22,217 DEBUG Query successful
2024-10-06 22:46:22,218 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:46:27,854 DEBUG Query successful
2024-10-06 22:46:27,855 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:47:01,243 DEBUG Query successful
2024-10-06 22:47:01,343 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:47:45,203 DEBUG Query successful
2024-10-06 22:47:45,205 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:48:11,340 DEBUG Query successful
2024-10-06 22:48:11,341 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:48:29,174 DEBUG Query successful
2024-10-06 22:48:29,176 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:48:45,285 DEBUG Query successful
2024-10-06 22:48:45,564 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:48:58,227 DEBUG Query successful
2024-10-06 22:48:58,232 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:49:04,038 DEBUG Query successful
2024-10-06 22:49:04,039 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:49:41,455 DEBUG Query successful
2024-10-06 22:49:41,536 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:50:30,040 DEBUG Query successful
2024-10-06 22:50:30,046 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:50:48,719 DEBUG Query successful
2024-10-06 22:50:48,720 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:51:06,374 DEBUG Query successful
2024-10-06 22:51:06,375 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:51:30,357 DEBUG Query successful
2024-10-06 22:51:30,630 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:51:39,772 DEBUG Query successful
2024-10-06 22:51:39,775 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:51:55,298 DEBUG Query successful
2024-10-06 22:51:55,299 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:52:40,170 DEBUG Query successful
2024-10-06 22:52:40,251 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:54:10,173 DEBUG Query successful
2024-10-06 22:54:10,177 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:54:27,376 DEBUG Query successful
2024-10-06 22:54:27,380 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:54:45,651 DEBUG Query successful
2024-10-06 22:54:45,653 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:55:11,185 DEBUG Query successful
2024-10-06 22:55:11,466 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:55:20,942 DEBUG Query successful
2024-10-06 22:55:20,943 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:55:26,736 DEBUG Query successful
2024-10-06 22:55:26,738 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:56:03,260 DEBUG Query successful
2024-10-06 22:56:03,347 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:56:59,096 DEBUG Query successful
2024-10-06 22:56:59,097 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:57:14,858 DEBUG Query successful
2024-10-06 22:57:14,859 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:57:25,735 DEBUG Query successful
2024-10-06 22:57:25,757 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:57:36,551 DEBUG Query successful
2024-10-10 12:26:13,659 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-10 12:26:13,831 DEBUG OpenAI client created
2024-10-10 12:26:13,831 DEBUG Model set to: gpt-4-turbo
2024-10-10 12:26:13,832 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-10 12:26:13,853 DEBUG OpenAI client created
2024-10-10 12:26:13,853 DEBUG Model set to: gpt-4-turbo
2024-10-10 12:26:19,019 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:26:32,396 DEBUG Query successful
2024-10-10 12:26:32,412 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:26:43,146 DEBUG Query successful
2024-10-10 12:26:43,180 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:27:03,349 DEBUG Query successful
2024-10-10 12:27:03,434 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:27:15,821 DEBUG Query successful
2024-10-10 12:27:15,823 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:27:25,479 DEBUG Query successful
2024-10-10 12:27:25,479 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:27:32,075 DEBUG Query successful
2024-10-10 12:27:32,076 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:27:39,448 DEBUG Query successful
2024-10-10 12:27:39,802 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:27:49,171 DEBUG Query successful
2024-10-10 12:27:49,177 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:28:03,030 DEBUG Query successful
2024-10-10 12:28:03,035 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:28:22,893 DEBUG Query successful
2024-10-10 12:28:22,993 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:28:42,771 DEBUG Query successful
2024-10-10 12:28:42,777 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:28:53,892 DEBUG Query successful
2024-10-10 12:29:04,180 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:29:13,915 DEBUG Query successful
2024-10-10 12:29:13,918 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:29:25,835 DEBUG Query successful
2024-10-10 12:29:25,842 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:29:41,247 DEBUG Query successful
2024-10-10 12:29:41,333 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:29:56,701 DEBUG Query successful
2024-10-10 12:29:56,703 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:30:09,263 DEBUG Query successful
2024-10-10 12:30:09,562 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:30:21,740 DEBUG Query successful
2024-10-10 12:30:21,742 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:30:33,801 DEBUG Query successful
2024-10-10 12:30:33,816 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:30:53,327 DEBUG Query successful
2024-10-10 12:30:53,422 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:31:14,133 DEBUG Query successful
2024-10-10 12:31:14,135 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:31:23,805 DEBUG Query successful
2024-10-10 12:31:24,100 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:31:33,005 DEBUG Query successful
2024-10-10 12:31:33,006 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:31:44,820 DEBUG Query successful
2024-10-10 12:31:44,829 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:32:11,312 DEBUG Query successful
2024-10-10 12:32:11,403 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:32:33,321 DEBUG Query successful
2024-10-10 12:32:33,323 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:32:44,613 DEBUG Query successful
2024-10-10 12:32:44,913 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:32:54,914 DEBUG Query successful
2024-10-10 12:32:54,918 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:33:08,631 DEBUG Query successful
2024-10-10 12:33:08,635 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:33:26,543 DEBUG Query successful
2024-10-10 12:33:26,632 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:33:47,599 DEBUG Query successful
2024-10-10 12:33:47,604 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:33:58,760 DEBUG Query successful
2024-10-10 12:33:59,044 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:34:14,075 DEBUG Query successful
2024-10-10 12:34:14,077 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:34:26,826 DEBUG Query successful
2024-10-10 12:34:26,830 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:34:44,488 DEBUG Query successful
2024-10-10 12:34:44,585 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:35:11,084 DEBUG Query successful
2024-10-10 12:35:11,085 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:35:23,379 DEBUG Query successful
2024-10-10 12:35:23,681 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:35:34,340 DEBUG Query successful
2024-10-10 12:35:34,342 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:35:48,732 DEBUG Query successful
2024-10-10 12:35:48,735 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:36:07,807 DEBUG Query successful
2024-10-10 12:36:07,896 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:36:37,416 DEBUG Query successful
2024-10-10 12:36:37,418 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:36:48,591 DEBUG Query successful
2024-10-10 12:36:48,899 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:37:01,242 DEBUG Query successful
2024-10-10 12:37:01,244 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:37:18,735 DEBUG Query successful
2024-10-10 12:37:18,744 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:37:39,954 DEBUG Query successful
2024-10-10 12:37:40,044 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:38:11,398 DEBUG Query successful
2024-10-10 12:38:11,400 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:38:22,782 DEBUG Query successful
2024-10-10 12:38:23,089 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:38:31,694 DEBUG Query successful
2024-10-10 12:38:31,700 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:38:46,310 DEBUG Query successful
2024-10-10 12:38:46,313 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:39:10,278 DEBUG Query successful
2024-10-10 12:39:10,376 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:39:41,032 DEBUG Query successful
2024-10-10 12:39:41,034 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:39:56,752 DEBUG Query successful
2024-10-10 12:39:57,051 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:40:06,181 DEBUG Query successful
2024-10-10 12:40:06,183 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:40:22,201 DEBUG Query successful
2024-10-10 12:40:22,213 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:40:41,969 DEBUG Query successful
2024-10-10 12:40:42,060 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:42:20,267 DEBUG Query successful
2024-10-10 12:42:20,268 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:42:32,732 DEBUG Query successful
2024-10-10 12:42:33,018 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:42:43,864 DEBUG Query successful
2024-10-10 12:42:43,867 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:43:00,186 DEBUG Query successful
2024-10-10 12:43:00,197 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:43:23,963 DEBUG Query successful
2024-10-10 12:43:24,070 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:44:08,085 DEBUG Query successful
2024-10-10 12:44:08,087 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:44:19,783 DEBUG Query successful
2024-10-10 12:44:20,069 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:44:29,763 DEBUG Query successful
2024-10-10 12:44:29,765 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:44:51,121 DEBUG Query successful
2024-10-10 12:44:51,128 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:45:13,167 DEBUG Query successful
2024-10-10 12:45:13,254 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:45:46,763 DEBUG Query successful
2024-10-10 12:45:46,765 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:46:00,358 DEBUG Query successful
2024-10-10 12:46:00,642 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:46:09,180 DEBUG Query successful
2024-10-10 12:46:09,185 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:46:29,491 DEBUG Query successful
2024-10-10 12:46:29,493 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:46:59,400 DEBUG Query successful
2024-10-10 12:46:59,513 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:47:40,481 DEBUG Query successful
2024-10-10 12:47:40,486 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:47:57,958 DEBUG Query successful
2024-10-10 12:47:58,243 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:48:14,327 DEBUG Query successful
2024-10-10 12:48:14,329 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:48:33,604 DEBUG Query successful
2024-10-10 12:48:33,611 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:49:02,175 DEBUG Query successful
2024-10-10 12:49:02,263 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:49:45,967 DEBUG Query successful
2024-10-10 12:49:45,969 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:49:57,946 DEBUG Query successful
2024-10-10 12:49:58,234 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:50:23,747 DEBUG Query successful
2024-10-10 12:50:23,749 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:50:40,480 DEBUG Query successful
2024-10-10 12:50:40,484 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:51:09,600 DEBUG Query successful
2024-10-10 12:51:09,693 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:51:59,100 DEBUG Query successful
2024-10-10 12:51:59,101 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:52:10,963 DEBUG Query successful
2024-10-10 12:52:11,256 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:52:21,314 DEBUG Query successful
2024-10-10 12:52:21,319 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:52:42,911 DEBUG Query successful
2024-10-10 12:52:42,920 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:53:16,964 DEBUG Query successful
2024-10-10 12:53:17,056 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:54:02,526 DEBUG Query successful
2024-10-10 12:54:02,528 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:54:17,626 DEBUG Query successful
2024-10-10 12:54:17,917 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:54:28,564 DEBUG Query successful
2024-10-10 12:54:28,567 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:54:43,799 DEBUG Query successful
2024-10-10 12:54:43,809 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:55:16,403 DEBUG Query successful
2024-10-10 12:55:16,495 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:56:09,607 DEBUG Query successful
2024-10-10 12:56:09,613 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:56:23,393 DEBUG Query successful
2024-10-10 12:56:23,680 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:56:31,833 DEBUG Query successful
2024-10-10 12:56:31,834 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:56:49,832 DEBUG Query successful
2024-10-10 12:56:49,838 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:57:25,834 DEBUG Query successful
2024-10-10 12:57:25,927 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:58:17,252 DEBUG Query successful
2024-10-10 12:58:17,257 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:58:36,794 DEBUG Query successful
2024-10-10 12:58:37,080 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:58:49,281 DEBUG Query successful
2024-10-10 12:58:49,283 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:59:14,347 DEBUG Query successful
2024-10-10 12:59:14,351 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:00:16,243 DEBUG Query successful
2024-10-10 13:00:16,334 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:01:14,971 DEBUG Query successful
2024-10-10 13:01:14,973 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:01:26,971 DEBUG Query successful
2024-10-10 13:01:27,255 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:01:37,007 DEBUG Query successful
2024-10-10 13:01:37,009 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:02:01,020 DEBUG Query successful
2024-10-10 13:02:01,026 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:02:32,896 DEBUG Query successful
2024-10-10 13:02:32,987 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:03:32,192 DEBUG Query successful
2024-10-10 13:03:32,193 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:03:43,539 DEBUG Query successful
2024-10-10 13:03:43,825 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:03:55,357 DEBUG Query successful
2024-10-10 13:03:55,359 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:04:21,408 DEBUG Query successful
2024-10-10 13:04:21,414 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:04:53,280 DEBUG Query successful
2024-10-10 13:04:53,370 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:05:49,159 DEBUG Query successful
2024-10-10 13:05:49,166 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:06:00,387 DEBUG Query successful
2024-10-10 13:06:00,791 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:06:12,545 DEBUG Query successful
2024-10-10 13:06:12,546 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:06:31,059 DEBUG Query successful
2024-10-10 13:06:31,064 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:06:56,396 DEBUG Query successful
2024-10-10 13:06:56,490 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:08:51,912 DEBUG Query successful
2024-10-10 13:08:51,916 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:09:11,846 DEBUG Query successful
2024-10-10 13:09:12,138 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:09:19,123 DEBUG Query successful
2024-10-10 13:09:19,125 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:09:45,253 DEBUG Query successful
2024-10-10 13:09:45,255 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:10:14,356 DEBUG Query successful
2024-10-10 13:10:14,444 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:11:10,182 DEBUG Query successful
2024-10-10 13:11:10,184 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:11:38,413 DEBUG Query successful
2024-10-10 13:11:38,699 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:11:47,455 DEBUG Query successful
2024-10-10 13:11:47,457 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:12:12,089 DEBUG Query successful
2024-10-10 13:12:12,106 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:12:46,590 DEBUG Query successful
2024-10-10 13:12:46,678 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:13:43,860 DEBUG Query successful
2024-10-10 13:13:43,866 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:14:02,865 DEBUG Query successful
2024-10-10 13:57:29,611 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-10 13:57:29,646 DEBUG OpenAI client created
2024-10-10 13:57:29,646 DEBUG Model set to: gpt-4-turbo
2024-10-10 13:57:29,647 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-10 13:57:29,668 DEBUG OpenAI client created
2024-10-10 13:57:29,668 DEBUG Model set to: gpt-4-turbo
2024-10-10 13:57:34,908 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:57:46,775 DEBUG Query successful
2024-10-10 13:57:46,776 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:57:53,499 DEBUG Query successful
2024-10-10 13:57:53,501 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:58:01,448 DEBUG Query successful
2024-10-10 13:58:01,449 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:58:08,102 DEBUG Query successful
2024-10-10 13:58:08,505 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:59:05,046 DEBUG Query successful
2024-10-10 13:59:05,047 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:59:12,602 DEBUG Query successful
2024-10-10 13:59:22,956 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:59:38,398 DEBUG Query successful
2024-10-10 13:59:38,401 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:59:45,766 DEBUG Query successful
2024-10-10 13:59:46,112 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:00:07,259 DEBUG Query successful
2024-10-10 14:00:07,261 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:00:15,568 DEBUG Query successful
2024-10-10 14:00:15,906 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:00:38,436 DEBUG Query successful
2024-10-10 14:00:38,437 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:00:46,105 DEBUG Query successful
2024-10-10 14:00:46,452 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:01:14,419 DEBUG Query successful
2024-10-10 14:01:14,421 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:01:21,633 DEBUG Query successful
2024-10-10 14:01:21,985 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:01:46,128 DEBUG Query successful
2024-10-10 14:01:46,130 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:01:53,902 DEBUG Query successful
2024-10-10 14:01:54,254 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:02:20,670 DEBUG Query successful
2024-10-10 14:02:20,676 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:02:28,089 DEBUG Query successful
2024-10-10 14:02:28,431 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:03:01,494 DEBUG Query successful
2024-10-10 14:03:01,496 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:03:08,938 DEBUG Query successful
2024-10-10 14:03:09,300 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:03:59,651 DEBUG Query successful
2024-10-10 14:03:59,652 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:04:05,803 DEBUG Query successful
2024-10-10 14:04:06,254 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:04:39,388 DEBUG Query successful
2024-10-10 14:04:39,390 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:04:46,489 DEBUG Query successful
2024-10-10 14:04:46,833 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:05:28,354 DEBUG Query successful
2024-10-10 14:05:28,359 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:05:35,410 DEBUG Query successful
2024-10-10 14:05:35,759 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:06:09,866 DEBUG Query successful
2024-10-10 14:06:09,868 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:06:15,540 DEBUG Query successful
2024-10-10 14:06:15,882 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:07:02,605 DEBUG Query successful
2024-10-10 14:07:02,606 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:07:09,100 DEBUG Query successful
2024-10-10 14:07:09,468 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:08:49,828 DEBUG Query successful
2024-10-10 14:08:49,831 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:08:55,333 DEBUG Query successful
2024-10-10 14:08:55,685 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:09:41,121 DEBUG Query successful
2024-10-10 14:09:41,126 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:09:49,171 DEBUG Query successful
2024-10-10 14:09:49,545 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:10:38,350 DEBUG Query successful
2024-10-10 14:10:38,351 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:10:47,692 DEBUG Query successful
2024-10-10 14:10:48,039 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:12:32,504 DEBUG Query successful
2024-10-10 14:12:32,506 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:12:39,129 DEBUG Query successful
2024-10-10 14:12:39,487 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:13:26,441 DEBUG Query successful
2024-10-10 14:13:26,443 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:13:32,196 DEBUG Query successful
2024-10-10 14:13:32,541 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:14:30,280 DEBUG Query successful
2024-10-10 14:14:30,285 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:14:35,468 DEBUG Query successful
2024-10-10 14:14:35,816 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:15:21,054 DEBUG Query successful
2024-10-10 14:15:21,055 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:15:25,652 DEBUG Query successful
2024-10-10 14:15:26,012 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:16:17,990 DEBUG Query successful
2024-10-10 14:16:17,991 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:16:24,102 DEBUG Query successful
2024-10-10 14:16:24,455 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:17:16,925 DEBUG Query successful
2024-10-10 14:17:16,927 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:17:21,742 DEBUG Query successful
2024-10-10 14:17:22,086 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:18:20,897 DEBUG Query successful
2024-10-10 14:18:20,899 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:18:25,539 DEBUG Query successful
2024-10-10 14:18:25,882 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:19:22,312 DEBUG Query successful
2024-10-10 14:19:22,313 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:19:26,750 DEBUG Query successful
2024-10-10 14:52:33,001 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-10 14:52:33,027 DEBUG OpenAI client created
2024-10-10 14:52:33,028 DEBUG Model set to: gpt-4-turbo
2024-10-10 14:52:33,028 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-10 14:52:33,049 DEBUG OpenAI client created
2024-10-10 14:52:33,049 DEBUG Model set to: gpt-4-turbo
2024-10-10 14:52:38,333 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:52:50,978 DEBUG Query successful
2024-10-10 14:52:50,980 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:52:59,374 DEBUG Query successful
2024-10-10 14:52:59,375 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:53:07,179 DEBUG Query successful
2024-10-10 14:53:07,184 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:53:15,551 DEBUG Query successful
2024-10-10 14:53:15,970 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:53:32,806 DEBUG Query successful
2024-10-10 14:53:32,808 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:53:40,920 DEBUG Query successful
2024-10-10 14:53:51,271 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:54:09,249 DEBUG Query successful
2024-10-10 14:54:09,250 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:54:17,512 DEBUG Query successful
2024-10-10 14:54:17,912 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:54:35,228 DEBUG Query successful
2024-10-10 14:54:35,229 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:54:43,183 DEBUG Query successful
2024-10-10 14:54:43,555 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:55:04,839 DEBUG Query successful
2024-10-10 14:55:04,842 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:55:10,625 DEBUG Query successful
2024-10-10 14:55:11,079 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:55:35,365 DEBUG Query successful
2024-10-10 14:55:35,368 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:55:43,079 DEBUG Query successful
2024-10-10 14:55:43,435 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:56:21,733 DEBUG Query successful
2024-10-10 14:56:21,735 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:56:27,817 DEBUG Query successful
2024-10-10 14:56:28,175 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:56:55,188 DEBUG Query successful
2024-10-10 14:56:55,190 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:57:03,685 DEBUG Query successful
2024-10-10 14:57:04,229 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:57:38,331 DEBUG Query successful
2024-10-10 14:57:38,337 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:57:45,536 DEBUG Query successful
2024-10-10 14:57:45,887 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:58:20,637 DEBUG Query successful
2024-10-10 14:58:20,640 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:58:25,259 DEBUG Query successful
2024-10-10 14:58:25,615 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:58:56,586 DEBUG Query successful
2024-10-10 14:58:56,588 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:59:03,997 DEBUG Query successful
2024-10-10 14:59:04,364 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:59:37,006 DEBUG Query successful
2024-10-10 14:59:37,006 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:59:43,613 DEBUG Query successful
2024-10-10 14:59:43,971 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:00:34,555 DEBUG Query successful
2024-10-10 15:00:34,557 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:00:43,309 DEBUG Query successful
2024-10-10 15:00:43,686 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:01:23,267 DEBUG Query successful
2024-10-10 15:01:23,273 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:01:31,333 DEBUG Query successful
2024-10-10 15:01:31,684 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:03:11,838 DEBUG Query successful
2024-10-10 15:03:11,841 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:03:21,372 DEBUG Query successful
2024-10-10 15:03:21,735 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:04:03,575 DEBUG Query successful
2024-10-10 15:04:03,580 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:04:11,143 DEBUG Query successful
2024-10-10 15:04:11,489 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:04:56,194 DEBUG Query successful
2024-10-10 15:04:56,199 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:05:03,538 DEBUG Query successful
2024-10-10 15:05:03,897 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:06:54,802 DEBUG Query successful
2024-10-10 15:06:54,804 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:07:01,036 DEBUG Query successful
2024-10-10 15:07:01,388 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:07:58,852 DEBUG Query successful
2024-10-10 15:07:58,853 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:08:26,042 DEBUG Query successful
2024-10-10 15:08:26,392 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:09:22,590 DEBUG Query successful
2024-10-10 15:09:22,591 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:09:30,911 DEBUG Query successful
2024-10-10 15:09:31,276 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:10:27,512 DEBUG Query successful
2024-10-10 15:10:27,517 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:10:35,982 DEBUG Query successful
2024-10-17 20:35:42,129 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-17 20:35:42,258 DEBUG OpenAI client created
2024-10-17 20:35:42,258 DEBUG Model set to: gpt-4-turbo
2024-10-17 20:35:42,259 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-17 20:35:42,279 DEBUG OpenAI client created
2024-10-17 20:35:42,279 DEBUG Model set to: gpt-4-turbo
2024-10-17 20:35:47,435 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:36:03,983 DEBUG Query successful
2024-10-17 20:36:03,984 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:36:11,310 DEBUG Query successful
2024-10-17 20:36:11,311 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:36:19,937 DEBUG Query successful
2024-10-17 20:36:19,938 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:36:29,146 DEBUG Query successful
2024-10-17 20:36:29,360 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:36:57,740 DEBUG Query successful
2024-10-17 20:36:57,741 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:37:04,036 DEBUG Query successful
2024-10-17 20:37:14,234 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:37:56,643 DEBUG Query successful
2024-10-17 20:37:56,644 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:38:02,739 DEBUG Query successful
2024-10-17 20:38:02,996 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:38:34,450 DEBUG Query successful
2024-10-17 20:38:34,456 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:38:42,262 DEBUG Query successful
2024-10-17 20:38:42,637 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:39:22,055 DEBUG Query successful
2024-10-17 20:39:22,056 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:39:46,519 DEBUG Query successful
2024-10-17 20:39:46,840 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:40:34,950 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-17 20:40:35,703 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:40:43,591 DEBUG Query successful
2024-10-17 20:40:43,832 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:42:08,374 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-17 20:42:08,401 DEBUG OpenAI client created
2024-10-17 20:42:08,402 DEBUG Model set to: gpt-4-turbo
2024-10-17 20:42:08,402 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-17 20:42:08,423 DEBUG OpenAI client created
2024-10-17 20:42:08,423 DEBUG Model set to: gpt-4-turbo
2024-10-17 20:42:13,478 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:42:38,624 DEBUG Query successful
2024-10-17 20:42:38,625 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:42:44,356 DEBUG Query successful
2024-10-17 20:42:44,357 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:42:50,114 DEBUG Query successful
2024-10-17 20:42:50,117 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:42:59,462 DEBUG Query successful
2024-10-17 20:42:59,610 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:43:16,743 DEBUG Query successful
2024-10-17 20:43:16,743 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:43:24,366 DEBUG Query successful
2024-10-17 20:43:34,717 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:44:27,324 DEBUG Query successful
2024-10-17 20:44:27,326 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:44:36,094 DEBUG Query successful
2024-10-17 20:44:36,495 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:45:42,591 DEBUG Query successful
2024-10-17 20:45:42,593 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:45:50,887 DEBUG Query successful
2024-10-17 20:45:51,252 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:47:19,215 DEBUG Query successful
2024-10-17 20:47:19,217 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:47:25,589 DEBUG Query successful
2024-10-17 20:47:25,956 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:49:31,959 DEBUG Query successful
2024-10-17 20:49:31,961 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:49:40,382 DEBUG Query successful
2024-10-17 20:49:40,810 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:52:00,988 DEBUG Query successful
2024-10-17 20:52:00,990 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:52:09,510 DEBUG Query successful
2024-10-17 20:52:09,941 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:54:45,407 DEBUG Query successful
2024-10-17 20:54:45,413 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:54:53,991 DEBUG Query successful
2024-10-17 20:54:54,362 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:57:59,422 DEBUG Query successful
2024-10-17 20:57:59,423 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:58:08,864 DEBUG Query successful
2024-10-17 20:58:09,277 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:01:43,714 DEBUG Query successful
2024-10-17 21:01:43,716 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:01:53,927 DEBUG Query successful
2024-10-17 21:01:54,340 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:13:56,197 DEBUG Query successful
2024-10-17 21:13:56,199 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:14:07,089 DEBUG Query successful
2024-10-17 21:14:07,505 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:18:18,752 DEBUG Query successful
2024-10-17 21:18:18,758 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:18:26,822 DEBUG Query successful
2024-10-17 21:18:27,176 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:23:16,538 DEBUG Query successful
2024-10-17 21:23:16,539 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:23:23,520 DEBUG Query successful
2024-10-17 21:23:23,884 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:28:37,202 DEBUG Query successful
2024-10-17 21:28:37,204 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:28:44,372 DEBUG Query successful
2024-10-17 21:28:44,732 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:34:05,983 DEBUG Query successful
2024-10-17 21:34:05,987 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:34:13,298 DEBUG Query successful
2024-10-17 21:34:13,672 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:40:05,446 DEBUG Query successful
2024-10-17 21:40:05,448 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:40:12,822 DEBUG Query successful
2024-10-17 21:40:13,207 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:58:46,849 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-17 21:58:46,876 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:58:54,872 DEBUG Query successful
2024-10-17 21:58:55,236 DEBUG Querying model: gpt-4-turbo
2024-10-17 22:11:26,356 DEBUG Query successful
2024-10-17 22:11:26,359 DEBUG Querying model: gpt-4-turbo
2024-10-17 22:11:35,451 DEBUG Query successful
2024-10-17 22:11:35,844 DEBUG Querying model: gpt-4-turbo
2024-10-17 22:25:45,521 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-17 22:25:45,545 DEBUG Querying model: gpt-4-turbo
2024-10-17 22:25:55,553 DEBUG Query successful
2024-10-17 22:25:55,921 DEBUG Querying model: gpt-4-turbo
2024-10-17 22:47:30,268 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-17 22:47:30,293 DEBUG Querying model: gpt-4-turbo
2024-10-17 22:47:39,657 DEBUG Query successful
2024-10-17 22:47:40,006 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:01:11,060 DEBUG Query successful
2024-10-17 23:01:11,063 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:01:19,954 DEBUG Query successful
2024-10-17 23:01:20,305 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:23:24,016 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-17 23:23:24,036 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:23:33,559 DEBUG Query successful
2024-10-17 23:23:33,914 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:30:58,928 DEBUG Query successful
2024-10-17 23:30:58,930 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:31:08,177 DEBUG Query successful
2024-10-17 23:31:08,534 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:39:00,116 DEBUG Query successful
2024-10-17 23:39:00,122 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:39:10,591 DEBUG Query successful
2024-10-17 23:39:10,940 DEBUG Querying model: gpt-4-turbo
2024-10-18 00:01:39,566 DEBUG Query successful
2024-10-18 00:01:39,570 DEBUG Querying model: gpt-4-turbo
2024-10-18 00:01:48,802 DEBUG Query successful
2024-10-18 20:10:06,209 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-18 20:10:06,374 DEBUG OpenAI client created
2024-10-18 20:10:06,374 DEBUG Model set to: gpt-4-turbo
2024-10-18 20:10:06,375 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-18 20:10:06,395 DEBUG OpenAI client created
2024-10-18 20:10:06,395 DEBUG Model set to: gpt-4-turbo
2024-10-18 20:10:23,663 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:10:36,583 DEBUG Query successful
2024-10-18 20:10:36,585 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:10:38,438 DEBUG Query successful
2024-10-18 20:10:38,439 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:10:56,919 DEBUG Query successful
2024-10-18 20:10:57,012 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:11:12,963 DEBUG Query successful
2024-10-18 20:11:12,965 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:11:21,186 DEBUG Query successful
2024-10-18 20:11:21,190 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:11:31,250 DEBUG Query successful
2024-10-18 20:11:31,251 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:11:40,722 DEBUG Query successful
2024-10-18 20:11:41,248 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:11:54,616 DEBUG Query successful
2024-10-18 20:11:54,618 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:11:56,326 DEBUG Query successful
2024-10-18 20:11:56,327 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:12:10,996 DEBUG Query successful
2024-10-18 20:12:11,092 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:12:28,386 DEBUG Query successful
2024-10-18 20:12:28,387 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:12:40,914 DEBUG Query successful
2024-10-18 20:12:40,915 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:12:51,199 DEBUG Query successful
2024-10-18 20:12:51,200 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:12:59,866 DEBUG Query successful
2024-10-18 20:13:10,366 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:18:54,354 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-18 20:18:54,378 DEBUG OpenAI client created
2024-10-18 20:18:54,378 DEBUG Model set to: gpt-4-turbo
2024-10-18 20:18:54,378 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-18 20:18:54,400 DEBUG OpenAI client created
2024-10-18 20:18:54,400 DEBUG Model set to: gpt-4-turbo
2024-10-18 20:19:09,616 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:19:20,850 DEBUG Query successful
2024-10-18 20:19:20,852 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:19:26,418 DEBUG Query successful
2024-10-18 20:19:26,426 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:19:41,629 DEBUG Query successful
2024-10-18 20:19:41,719 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:20:04,094 DEBUG Query successful
2024-10-18 20:20:04,096 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:20:12,129 DEBUG Query successful
2024-10-18 20:20:12,134 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:20:18,541 DEBUG Query successful
2024-10-18 20:20:18,542 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:20:25,500 DEBUG Query successful
2024-10-18 20:20:26,171 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:20:36,939 DEBUG Query successful
2024-10-18 20:20:36,941 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:20:44,340 DEBUG Query successful
2024-10-18 20:20:44,343 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:21:03,308 DEBUG Query successful
2024-10-18 20:21:03,411 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:21:22,888 DEBUG Query successful
2024-10-18 20:21:22,890 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:21:31,937 DEBUG Query successful
2024-10-18 20:21:31,937 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:21:41,649 DEBUG Query successful
2024-10-18 20:21:41,650 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:21:49,247 DEBUG Query successful
2024-10-18 20:21:59,722 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:22:09,389 DEBUG Query successful
2024-10-18 20:22:09,391 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:22:16,254 DEBUG Query successful
2024-10-18 20:22:16,262 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:22:34,120 DEBUG Query successful
2024-10-18 20:22:34,227 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:22:51,896 DEBUG Query successful
2024-10-18 20:22:51,897 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:35:34,479 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-18 20:35:34,502 DEBUG OpenAI client created
2024-10-18 20:35:34,502 DEBUG Model set to: gpt-4-turbo
2024-10-18 20:35:34,502 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-18 20:35:34,525 DEBUG OpenAI client created
2024-10-18 20:35:34,525 DEBUG Model set to: gpt-4-turbo
2024-10-18 20:35:51,700 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:36:04,213 DEBUG Query successful
2024-10-18 20:36:04,214 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:36:05,886 DEBUG Query successful
2024-10-18 20:36:05,888 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:36:24,453 DEBUG Query successful
2024-10-18 20:36:24,546 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:36:41,064 DEBUG Query successful
2024-10-18 20:36:41,067 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:36:49,788 DEBUG Query successful
2024-10-18 20:36:49,789 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:36:56,564 DEBUG Query successful
2024-10-18 20:36:56,564 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:37:04,292 DEBUG Query successful
2024-10-18 20:37:04,755 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:37:17,760 DEBUG Query successful
2024-10-18 20:37:17,762 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:37:19,742 DEBUG Query successful
2024-10-18 20:37:19,743 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:37:40,215 DEBUG Query successful
2024-10-18 20:37:40,336 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:04:02,352 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:04:02,488 DEBUG OpenAI client created
2024-10-19 14:04:02,488 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:04:02,489 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:04:02,511 DEBUG OpenAI client created
2024-10-19 14:04:02,511 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:04:18,583 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:10:12,971 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:10:12,994 DEBUG OpenAI client created
2024-10-19 14:10:12,994 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:10:12,994 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:10:13,016 DEBUG OpenAI client created
2024-10-19 14:10:13,016 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:10:13,051 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:10:13,074 DEBUG OpenAI client created
2024-10-19 14:10:13,074 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:10:13,074 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:10:13,096 DEBUG OpenAI client created
2024-10-19 14:10:13,096 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:10:29,252 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:10:30,723 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:10:40,168 DEBUG Query successful
2024-10-19 14:10:40,169 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:10:43,567 DEBUG Query successful
2024-10-19 14:10:43,569 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:10:45,610 DEBUG Query successful
2024-10-19 14:10:46,210 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:10:48,344 DEBUG Query successful
2024-10-19 14:10:48,344 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:10:53,593 DEBUG Query successful
2024-10-19 14:10:53,594 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:00,789 DEBUG Query successful
2024-10-19 14:11:01,318 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:02,738 DEBUG Query successful
2024-10-19 14:11:02,743 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:08,278 DEBUG Query successful
2024-10-19 14:11:15,488 DEBUG Query successful
2024-10-19 14:11:15,490 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:18,889 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:22,605 DEBUG Query successful
2024-10-19 14:11:33,176 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:37,369 DEBUG Query successful
2024-10-19 14:11:37,372 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:47,265 DEBUG Query successful
2024-10-19 14:11:47,871 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:49,062 DEBUG Query successful
2024-10-19 14:11:49,065 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:55,048 DEBUG Query successful
2024-10-19 14:11:55,575 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:10,406 DEBUG Query successful
2024-10-19 14:12:10,412 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:12,885 DEBUG Query successful
2024-10-19 14:12:12,887 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:16,412 DEBUG Query successful
2024-10-19 14:12:17,004 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:18,426 DEBUG Query successful
2024-10-19 14:12:19,246 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:38,598 DEBUG Query successful
2024-10-19 14:12:38,600 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:39,042 DEBUG Query successful
2024-10-19 14:12:39,043 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:44,274 DEBUG Query successful
2024-10-19 14:12:44,682 DEBUG Query successful
2024-10-19 14:12:44,847 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:45,343 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:09,134 DEBUG Query successful
2024-10-19 14:13:09,139 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:14,076 DEBUG Query successful
2024-10-19 14:13:14,415 DEBUG Query successful
2024-10-19 14:13:14,420 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:14,584 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:21,630 DEBUG Query successful
2024-10-19 14:13:22,116 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:38,379 DEBUG Query successful
2024-10-19 14:13:38,381 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:44,522 DEBUG Query successful
2024-10-19 14:13:45,047 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:51,360 DEBUG Query successful
2024-10-19 14:13:51,362 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:57,816 DEBUG Query successful
2024-10-19 14:13:58,395 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:14:11,556 DEBUG Query successful
2024-10-19 14:14:11,558 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:14:16,801 DEBUG Query successful
2024-10-19 14:14:17,329 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:14:32,488 DEBUG Query successful
2024-10-19 14:14:32,490 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:14:38,530 DEBUG Query successful
2024-10-19 14:14:39,165 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:14:44,106 DEBUG Query successful
2024-10-19 14:14:44,107 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:14:50,020 DEBUG Query successful
2024-10-19 14:14:50,640 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:15:11,181 DEBUG Query successful
2024-10-19 14:15:11,183 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:15:16,495 DEBUG Query successful
2024-10-19 14:15:17,034 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:15:26,298 DEBUG Query successful
2024-10-19 14:15:26,305 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:15:32,934 DEBUG Query successful
2024-10-19 14:15:33,593 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:15:51,259 DEBUG Query successful
2024-10-19 14:15:51,262 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:15:57,222 DEBUG Query successful
2024-10-19 14:15:57,750 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:16:09,113 DEBUG Query successful
2024-10-19 14:16:09,114 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:16:13,581 DEBUG Query successful
2024-10-19 14:16:14,059 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:16:32,526 DEBUG Query successful
2024-10-19 14:16:32,528 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:16:39,334 DEBUG Query successful
2024-10-19 14:16:39,833 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:16:52,512 DEBUG Query successful
2024-10-19 14:16:52,513 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:16:57,321 DEBUG Query successful
2024-10-19 14:16:57,819 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:17:13,246 DEBUG Query successful
2024-10-19 14:17:13,248 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:17:19,011 DEBUG Query successful
2024-10-19 14:17:19,529 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:17:39,642 DEBUG Query successful
2024-10-19 14:17:39,643 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:17:44,882 DEBUG Query successful
2024-10-19 14:17:45,404 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:18:04,329 DEBUG Query successful
2024-10-19 14:18:04,330 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:18:10,003 DEBUG Query successful
2024-10-19 14:18:10,518 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:18:25,746 DEBUG Query successful
2024-10-19 14:18:25,769 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:18:30,723 DEBUG Query successful
2024-10-19 14:18:31,203 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:18:57,600 DEBUG Query successful
2024-10-19 14:18:57,602 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:19:04,149 DEBUG Query successful
2024-10-19 14:19:04,667 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:19:52,641 DEBUG Query successful
2024-10-19 14:19:52,642 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:19:58,605 DEBUG Query successful
2024-10-19 14:19:59,097 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:20:12,370 DEBUG Query successful
2024-10-19 14:20:12,371 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:20:17,933 DEBUG Query successful
2024-10-19 14:20:18,422 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:20:45,361 DEBUG Query successful
2024-10-19 14:20:45,364 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:20:51,015 DEBUG Query successful
2024-10-19 14:20:51,611 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:21:08,120 DEBUG Query successful
2024-10-19 14:21:08,121 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:21:12,693 DEBUG Query successful
2024-10-19 14:21:13,237 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:22:02,570 DEBUG Query successful
2024-10-19 14:22:02,573 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:22:09,946 DEBUG Query successful
2024-10-19 14:22:10,422 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:22:38,026 DEBUG Query successful
2024-10-19 14:22:38,029 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:22:45,002 DEBUG Query successful
2024-10-19 14:22:45,465 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:23:35,862 DEBUG Query successful
2024-10-19 14:23:35,864 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:23:42,595 DEBUG Query successful
2024-10-19 14:23:43,089 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:23:59,542 DEBUG Query successful
2024-10-19 14:23:59,544 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:24:05,180 DEBUG Query successful
2024-10-19 14:24:05,687 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:24:39,615 DEBUG Query successful
2024-10-19 14:24:39,616 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:24:46,237 DEBUG Query successful
2024-10-19 14:24:46,745 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:24:58,755 DEBUG Query successful
2024-10-19 14:24:58,756 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:25:05,346 DEBUG Query successful
2024-10-19 14:25:05,967 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:26:44,702 DEBUG Query successful
2024-10-19 14:26:44,705 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:26:50,940 DEBUG Query successful
2024-10-19 14:26:51,429 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:27:52,562 DEBUG Query successful
2024-10-19 14:27:52,563 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:27:58,944 DEBUG Query successful
2024-10-19 14:27:59,453 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:28:06,346 DEBUG Query successful
2024-10-19 14:28:06,349 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:28:11,107 DEBUG Query successful
2024-10-19 14:28:11,594 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:30:02,554 DEBUG Query successful
2024-10-19 14:30:02,556 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:30:08,760 DEBUG Query successful
2024-10-19 14:30:09,296 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:31:11,829 DEBUG Query successful
2024-10-19 14:31:11,835 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:31:18,207 DEBUG Query successful
2024-10-19 14:31:18,687 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:32:13,753 DEBUG Query successful
2024-10-19 14:32:13,755 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:32:14,445 DEBUG Query successful
2024-10-19 14:32:14,447 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:32:19,399 DEBUG Query successful
2024-10-19 14:32:19,944 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:32:21,643 DEBUG Query successful
2024-10-19 14:35:29,378 DEBUG Query successful
2024-10-19 14:35:29,379 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:35:35,162 DEBUG Query successful
2024-10-19 14:35:38,411 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:35:38,437 DEBUG OpenAI client created
2024-10-19 14:35:38,437 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:35:38,437 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:35:38,460 DEBUG OpenAI client created
2024-10-19 14:35:38,460 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:35:38,493 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:35:38,516 DEBUG OpenAI client created
2024-10-19 14:35:38,516 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:35:38,516 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:35:38,538 DEBUG OpenAI client created
2024-10-19 14:35:38,538 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:35:55,280 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:35:57,244 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:06,315 DEBUG Query successful
2024-10-19 14:36:06,319 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:11,168 DEBUG Query successful
2024-10-19 14:36:11,306 DEBUG Query successful
2024-10-19 14:36:11,309 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:11,724 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:16,069 DEBUG Query successful
2024-10-19 14:36:16,070 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:21,103 DEBUG Query successful
2024-10-19 14:36:21,103 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:24,889 DEBUG Query successful
2024-10-19 14:36:24,890 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:26,252 DEBUG Query successful
2024-10-19 14:36:26,887 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:29,577 DEBUG Query successful
2024-10-19 14:36:40,112 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:40,289 DEBUG Query successful
2024-10-19 14:36:40,290 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:47,136 DEBUG Query successful
2024-10-19 14:36:47,137 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:52,807 DEBUG Query successful
2024-10-19 14:36:52,808 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:58,601 DEBUG Query successful
2024-10-19 14:36:58,603 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:58,656 DEBUG Query successful
2024-10-19 14:37:04,039 DEBUG Query successful
2024-10-19 14:37:04,617 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:09,204 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:22,760 DEBUG Query successful
2024-10-19 14:37:22,762 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:26,189 DEBUG Query successful
2024-10-19 14:37:26,192 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:29,410 DEBUG Query successful
2024-10-19 14:37:30,055 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:32,524 DEBUG Query successful
2024-10-19 14:37:32,524 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:38,551 DEBUG Query successful
2024-10-19 14:37:38,555 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:46,187 DEBUG Query successful
2024-10-19 14:37:46,654 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:51,388 DEBUG Query successful
2024-10-19 14:37:51,389 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:57,269 DEBUG Query successful
2024-10-19 14:37:57,835 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:06,079 DEBUG Query successful
2024-10-19 14:38:06,081 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:13,066 DEBUG Query successful
2024-10-19 14:38:13,068 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:19,548 DEBUG Query successful
2024-10-19 14:38:19,553 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:19,858 DEBUG Query successful
2024-10-19 14:38:19,860 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:26,142 DEBUG Query successful
2024-10-19 14:38:26,856 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:27,349 DEBUG Query successful
2024-10-19 14:38:28,052 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:51,631 DEBUG Query successful
2024-10-19 14:38:51,632 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:53,921 DEBUG Query successful
2024-10-19 14:38:53,922 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:59,955 DEBUG Query successful
2024-10-19 14:39:00,496 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:00,691 DEBUG Query successful
2024-10-19 14:39:00,695 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:06,928 DEBUG Query successful
2024-10-19 14:39:06,929 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:13,890 DEBUG Query successful
2024-10-19 14:39:14,487 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:29,733 DEBUG Query successful
2024-10-19 14:39:29,734 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:36,055 DEBUG Query successful
2024-10-19 14:39:36,558 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:41,462 DEBUG Query successful
2024-10-19 14:39:41,464 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:50,161 DEBUG Query successful
2024-10-19 14:39:50,166 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:57,899 DEBUG Query successful
2024-10-19 14:39:57,900 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:05,842 DEBUG Query successful
2024-10-19 14:40:06,388 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:09,757 DEBUG Query successful
2024-10-19 14:40:09,759 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:15,401 DEBUG Query successful
2024-10-19 14:40:15,911 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:30,686 DEBUG Query successful
2024-10-19 14:40:30,687 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:38,184 DEBUG Query successful
2024-10-19 14:40:38,185 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:45,983 DEBUG Query successful
2024-10-19 14:40:45,983 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:53,787 DEBUG Query successful
2024-10-19 14:40:54,449 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:55,230 DEBUG Query successful
2024-10-19 14:40:55,236 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:41:00,503 DEBUG Query successful
2024-10-19 14:41:01,052 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:41:20,499 DEBUG Query successful
2024-10-19 14:41:20,500 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:41:29,127 DEBUG Query successful
2024-10-19 14:41:29,131 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:41:37,171 DEBUG Query successful
2024-10-19 14:41:37,172 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:41:43,926 DEBUG Query successful
2024-10-19 14:41:43,927 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:41:45,023 DEBUG Query successful
2024-10-19 14:41:45,521 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:41:48,925 DEBUG Query successful
2024-10-19 14:41:49,407 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:42:26,838 DEBUG Query successful
2024-10-19 14:42:26,839 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:42:30,012 DEBUG Query successful
2024-10-19 14:42:30,013 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:42:32,904 DEBUG Query successful
2024-10-19 14:42:33,515 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:42:37,873 DEBUG Query successful
2024-10-19 14:42:37,874 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:42:46,744 DEBUG Query successful
2024-10-19 14:42:46,745 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:42:55,158 DEBUG Query successful
2024-10-19 14:42:55,798 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:43:13,161 DEBUG Query successful
2024-10-19 14:43:13,162 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:43:18,851 DEBUG Query successful
2024-10-19 14:43:19,375 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:43:33,837 DEBUG Query successful
2024-10-19 14:43:33,842 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:43:43,149 DEBUG Query successful
2024-10-19 14:43:43,150 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:43:50,988 DEBUG Query successful
2024-10-19 14:43:50,993 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:43:59,513 DEBUG Query successful
2024-10-19 14:44:00,019 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:44:04,168 DEBUG Query successful
2024-10-19 14:44:04,170 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:44:08,255 DEBUG Query successful
2024-10-19 14:44:08,784 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:44:36,902 DEBUG Query successful
2024-10-19 14:44:36,908 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:44:44,188 DEBUG Query successful
2024-10-19 14:44:44,188 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:44:51,620 DEBUG Query successful
2024-10-19 14:44:51,621 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:44:57,896 DEBUG Query successful
2024-10-19 14:44:57,897 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:44:59,252 DEBUG Query successful
2024-10-19 14:44:59,743 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:45:02,062 DEBUG Query successful
2024-10-19 14:45:02,628 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:45:39,363 DEBUG Query successful
2024-10-19 14:45:39,364 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:45:48,906 DEBUG Query successful
2024-10-19 14:45:48,906 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:45:55,628 DEBUG Query successful
2024-10-19 14:45:55,629 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:46:03,414 DEBUG Query successful
2024-10-19 14:46:03,937 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:46:43,925 DEBUG Query successful
2024-10-19 14:46:43,926 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:46:53,235 DEBUG Query successful
2024-10-19 14:46:53,238 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:46:53,524 DEBUG Query successful
2024-10-19 14:46:53,528 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:46:57,177 DEBUG Query successful
2024-10-19 14:46:57,724 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:47:00,809 DEBUG Query successful
2024-10-19 14:47:00,809 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:47:07,993 DEBUG Query successful
2024-10-19 14:47:08,498 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:47:46,176 DEBUG Query successful
2024-10-19 14:47:46,177 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:47:49,824 DEBUG Query successful
2024-10-19 14:47:50,348 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:47:52,595 DEBUG Query successful
2024-10-19 14:47:52,601 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:48:01,674 DEBUG Query successful
2024-10-19 14:48:01,675 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:48:10,055 DEBUG Query successful
2024-10-19 14:48:10,055 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:48:17,512 DEBUG Query successful
2024-10-19 14:48:18,047 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:48:58,681 DEBUG Query successful
2024-10-19 14:48:58,682 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:49:06,665 DEBUG Query successful
2024-10-19 14:49:06,667 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:49:14,323 DEBUG Query successful
2024-10-19 14:49:14,324 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:49:21,469 DEBUG Query successful
2024-10-19 14:49:22,063 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:49:41,564 DEBUG Query successful
2024-10-19 14:49:41,567 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:49:45,621 DEBUG Query successful
2024-10-19 14:49:46,115 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:50:36,650 DEBUG Query successful
2024-10-19 14:50:36,652 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:50:40,072 DEBUG Query successful
2024-10-19 14:50:40,627 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:51:04,164 DEBUG Query successful
2024-10-19 14:51:04,166 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:51:10,012 DEBUG Query successful
2024-10-19 14:51:10,013 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:51:16,197 DEBUG Query successful
2024-10-19 14:51:16,198 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:51:21,994 DEBUG Query successful
2024-10-19 14:51:22,543 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:52:09,106 DEBUG Query successful
2024-10-19 14:52:09,107 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:52:15,292 DEBUG Query successful
2024-10-19 14:52:15,296 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:52:21,332 DEBUG Query successful
2024-10-19 14:52:21,333 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:52:27,351 DEBUG Query successful
2024-10-19 14:52:27,853 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:52:32,339 DEBUG Query successful
2024-10-19 14:52:32,344 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:52:37,090 DEBUG Query successful
2024-10-19 14:52:37,565 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:53:08,418 DEBUG Query successful
2024-10-19 14:53:08,419 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:53:14,289 DEBUG Query successful
2024-10-19 14:53:14,311 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:53:20,822 DEBUG Query successful
2024-10-19 14:53:20,822 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:53:26,732 DEBUG Query successful
2024-10-19 14:53:26,733 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:53:28,101 DEBUG Query successful
2024-10-19 14:53:28,601 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:53:31,185 DEBUG Query successful
2024-10-19 14:53:31,642 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:54:22,810 DEBUG Query successful
2024-10-19 14:54:22,812 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:54:25,422 DEBUG Query successful
2024-10-19 14:54:25,425 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:54:30,115 DEBUG Query successful
2024-10-19 14:54:30,116 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:54:30,417 DEBUG Query successful
2024-10-19 14:54:30,882 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:54:37,602 DEBUG Query successful
2024-10-19 14:54:37,602 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:54:45,358 DEBUG Query successful
2024-10-19 14:54:45,906 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:55:32,246 DEBUG Query successful
2024-10-19 14:55:32,248 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:55:39,692 DEBUG Query successful
2024-10-19 14:55:39,693 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:55:47,555 DEBUG Query successful
2024-10-19 14:55:47,556 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:55:55,574 DEBUG Query successful
2024-10-19 14:55:56,103 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:57:41,841 DEBUG Query successful
2024-10-19 14:57:41,843 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:57:47,831 DEBUG Query successful
2024-10-19 14:57:48,329 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:57:52,025 DEBUG Query successful
2024-10-19 14:57:52,028 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:58:00,306 DEBUG Query successful
2024-10-19 14:58:00,311 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:58:08,401 DEBUG Query successful
2024-10-19 14:58:08,402 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:58:20,328 DEBUG Query successful
2024-10-19 14:58:20,795 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:58:51,033 DEBUG Query successful
2024-10-19 14:58:51,036 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:58:57,338 DEBUG Query successful
2024-10-19 14:58:57,813 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:59:18,850 DEBUG Query successful
2024-10-19 14:59:18,852 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:59:32,460 DEBUG Query successful
2024-10-19 14:59:32,461 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:59:40,620 DEBUG Query successful
2024-10-19 14:59:40,621 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:59:53,996 DEBUG Query successful
2024-10-19 14:59:54,489 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:01:52,812 DEBUG Query successful
2024-10-19 15:01:52,814 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:02:05,420 DEBUG Query successful
2024-10-19 15:02:05,421 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:02:10,379 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:02:10,491 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:02:15,164 DEBUG Query successful
2024-10-19 15:02:15,165 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:02:16,018 DEBUG Query successful
2024-10-19 15:02:16,508 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:02:30,934 DEBUG Query successful
2024-10-19 15:02:31,468 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:04:16,442 DEBUG Query successful
2024-10-19 15:04:16,445 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:04:22,103 DEBUG Query successful
2024-10-19 15:04:22,531 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:05:43,195 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:05:43,235 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:05:53,231 DEBUG Query successful
2024-10-19 15:05:53,232 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:06:01,822 DEBUG Query successful
2024-10-19 15:06:01,823 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:06:14,397 DEBUG Query successful
2024-10-19 15:06:14,817 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:06:28,692 DEBUG Query successful
2024-10-19 15:06:28,693 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:06:35,881 DEBUG Query successful
2024-10-19 15:06:36,306 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:07:39,755 DEBUG Query successful
2024-10-19 15:07:39,757 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:07:45,827 DEBUG Query successful
2024-10-19 15:07:46,309 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:08:20,985 DEBUG Query successful
2024-10-19 15:08:20,990 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:08:34,801 DEBUG Query successful
2024-10-19 15:08:34,803 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:08:43,014 DEBUG Query successful
2024-10-19 15:08:43,016 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:08:52,064 DEBUG Query successful
2024-10-19 15:08:52,579 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:11:00,535 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:11:00,577 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:11:05,968 DEBUG Query successful
2024-10-19 15:11:06,397 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:12:03,346 DEBUG Query successful
2024-10-19 15:12:03,353 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:12:15,719 DEBUG Query successful
2024-10-19 15:12:15,719 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:12:29,924 DEBUG Query successful
2024-10-19 15:12:29,924 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:12:40,689 DEBUG Query successful
2024-10-19 15:12:41,148 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:13:43,120 DEBUG Query successful
2024-10-19 15:13:43,121 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:13:56,882 DEBUG Query successful
2024-10-19 15:13:56,886 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:14:06,873 DEBUG Query successful
2024-10-19 15:14:06,874 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:14:16,172 DEBUG Query successful
2024-10-19 15:14:16,617 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:14:19,737 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:14:19,774 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:14:26,059 DEBUG Query successful
2024-10-19 15:14:26,505 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:17:29,015 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:17:29,050 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:17:36,084 DEBUG Query successful
2024-10-19 15:17:36,085 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:17:39,644 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:17:39,683 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:17:42,734 DEBUG Query successful
2024-10-19 15:17:42,735 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:17:45,104 DEBUG Query successful
2024-10-19 15:17:45,361 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:17:50,087 DEBUG Query successful
2024-10-19 15:17:50,304 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:20:58,737 DEBUG Query successful
2024-10-19 15:20:58,740 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:20:58,817 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:20:58,852 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:21:04,203 DEBUG Query successful
2024-10-19 15:21:04,425 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:21:07,334 DEBUG Query successful
2024-10-19 15:21:07,335 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:21:13,762 DEBUG Query successful
2024-10-19 15:21:13,763 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:21:28,638 DEBUG Query successful
2024-10-19 15:21:28,855 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:22:07,316 DEBUG Query successful
2024-10-19 15:22:07,317 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:22:13,569 DEBUG Query successful
2024-10-19 15:22:13,742 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:23:15,571 DEBUG Query successful
2024-10-19 15:23:15,573 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:23:21,395 DEBUG Query successful
2024-10-19 15:23:21,588 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:24:41,142 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:24:41,164 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:24:53,214 DEBUG Query successful
2024-10-19 15:24:53,215 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:25:01,066 DEBUG Query successful
2024-10-19 15:25:01,066 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:25:09,460 DEBUG Query successful
2024-10-19 15:25:09,738 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:26:34,510 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:26:34,537 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:26:38,276 DEBUG Query successful
2024-10-19 15:26:38,492 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:28:21,589 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:28:21,626 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:28:29,638 DEBUG Query successful
2024-10-19 15:28:29,640 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:28:37,713 DEBUG Query successful
2024-10-19 15:28:37,713 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:28:43,702 DEBUG Query successful
2024-10-19 15:28:43,707 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:28:46,786 DEBUG Query successful
2024-10-19 15:28:47,667 DEBUG Query successful
2024-10-19 15:28:47,847 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:28:51,597 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:32:00,820 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:32:00,850 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:32:21,898 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:32:21,922 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:32:21,978 DEBUG Query successful
2024-10-19 15:32:22,179 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:32:40,791 DEBUG Query successful
2024-10-19 15:32:40,798 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:32:54,906 DEBUG Query successful
2024-10-19 15:32:54,907 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:33:15,475 DEBUG Query successful
2024-10-19 15:33:16,025 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:35:36,460 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:35:36,489 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:35:40,423 DEBUG Query successful
2024-10-19 15:35:40,636 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:36:28,658 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:36:28,689 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:36:46,212 DEBUG Query successful
2024-10-19 15:36:46,213 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:37:02,281 DEBUG Query successful
2024-10-19 15:37:02,282 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:37:18,731 DEBUG Query successful
2024-10-19 15:37:19,270 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:38:53,188 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:38:53,215 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:38:56,570 DEBUG Query successful
2024-10-19 15:39:01,032 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:40:31,571 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:40:31,603 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:40:48,998 DEBUG Query successful
2024-10-19 15:40:49,001 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:41:05,797 DEBUG Query successful
2024-10-19 15:41:05,797 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:41:23,029 DEBUG Query successful
2024-10-19 15:41:23,515 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:42:14,409 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:42:14,442 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:42:21,435 DEBUG Query successful
2024-10-19 15:42:21,988 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:44:36,617 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:44:36,647 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:44:54,645 DEBUG Query successful
2024-10-19 15:44:54,647 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:45:16,427 DEBUG Query successful
2024-10-19 15:45:16,429 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:45:35,045 DEBUG Query successful
2024-10-19 15:45:35,498 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:45:35,537 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:45:35,701 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:45:43,261 DEBUG Query successful
2024-10-19 15:45:43,804 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:48:49,439 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:48:49,481 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:48:58,679 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:48:58,707 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:49:06,726 DEBUG Query successful
2024-10-19 15:49:06,726 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:49:08,123 DEBUG Query successful
2024-10-19 15:49:08,723 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:49:27,645 DEBUG Query successful
2024-10-19 15:49:27,646 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:49:46,707 DEBUG Query successful
2024-10-19 15:49:47,312 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:51:22,032 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:51:22,073 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:51:32,007 DEBUG Query successful
2024-10-19 15:51:32,696 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:52:59,784 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:52:59,823 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:53:18,927 DEBUG Query successful
2024-10-19 15:53:18,929 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:53:38,686 DEBUG Query successful
2024-10-19 15:53:38,687 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:54:00,103 DEBUG Query successful
2024-10-19 15:54:00,675 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:54:46,006 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:54:46,044 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:54:54,416 DEBUG Query successful
2024-10-19 15:54:54,982 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:57:15,087 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:57:15,117 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:57:34,049 DEBUG Query successful
2024-10-19 15:57:34,050 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:57:53,357 DEBUG Query successful
2024-10-19 15:57:53,358 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:58:08,158 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:58:08,184 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:58:13,094 DEBUG Query successful
2024-10-19 15:58:13,770 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:58:16,908 DEBUG Query successful
2024-10-19 15:58:17,439 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:01:26,532 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:01:26,568 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:01:31,409 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:01:31,450 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:01:39,687 DEBUG Query successful
2024-10-19 16:01:40,289 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:01:54,293 DEBUG Query successful
2024-10-19 16:01:54,294 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:02:15,511 DEBUG Query successful
2024-10-19 16:02:15,516 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:02:34,966 DEBUG Query successful
2024-10-19 16:02:35,472 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:04:54,151 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:04:54,193 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:05:01,201 DEBUG Query successful
2024-10-19 16:05:01,789 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:05:48,409 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:05:48,447 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:06:09,156 DEBUG Query successful
2024-10-19 16:06:09,157 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:06:31,131 DEBUG Query successful
2024-10-19 16:06:31,136 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:06:54,434 DEBUG Query successful
2024-10-19 16:06:54,927 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:08:14,875 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:08:14,914 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:08:24,006 DEBUG Query successful
2024-10-19 16:08:24,652 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:10:07,740 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:10:07,773 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:10:31,187 DEBUG Query successful
2024-10-19 16:10:31,187 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:10:54,395 DEBUG Query successful
2024-10-19 16:10:54,396 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:11:19,164 DEBUG Query successful
2024-10-19 16:11:19,625 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:11:38,320 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:11:38,377 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:11:46,210 DEBUG Query successful
2024-10-19 16:11:46,657 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:14:32,774 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:14:32,805 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:14:53,998 DEBUG Query successful
2024-10-19 16:14:54,000 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:15:00,122 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:15:00,159 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:15:08,369 DEBUG Query successful
2024-10-19 16:15:08,803 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:15:16,737 DEBUG Query successful
2024-10-19 16:15:16,738 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:15:37,272 DEBUG Query successful
2024-10-19 16:15:37,735 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:18:22,138 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:18:22,178 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:18:29,725 DEBUG Query successful
2024-10-19 16:18:30,156 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:18:51,300 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:18:51,342 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:19:11,283 DEBUG Query successful
2024-10-19 16:19:11,288 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:19:31,747 DEBUG Query successful
2024-10-19 16:19:31,748 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:19:55,276 DEBUG Query successful
2024-10-19 16:19:55,743 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:21:43,846 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:21:43,885 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:21:51,803 DEBUG Query successful
2024-10-19 16:21:52,259 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:23:09,215 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:23:09,255 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:23:26,906 DEBUG Query successful
2024-10-19 16:23:26,906 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:23:45,794 DEBUG Query successful
2024-10-19 16:23:45,795 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:24:00,842 DEBUG Query successful
2024-10-19 16:24:01,297 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:25:06,494 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:25:06,525 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:25:16,662 DEBUG Query successful
2024-10-19 16:25:17,099 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:27:15,216 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:27:15,251 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:27:31,347 DEBUG Query successful
2024-10-19 16:27:31,352 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:27:51,031 DEBUG Query successful
2024-10-19 16:27:51,031 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:28:07,433 DEBUG Query successful
2024-10-19 16:28:07,875 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:28:30,193 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:28:30,227 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:28:37,698 DEBUG Query successful
2024-10-19 16:28:38,116 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:31:20,505 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:31:20,539 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:31:38,422 DEBUG Query successful
2024-10-19 16:31:38,426 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:31:50,833 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:31:50,868 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:31:57,304 DEBUG Query successful
2024-10-19 16:31:57,305 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:31:58,983 DEBUG Query successful
2024-10-19 16:31:59,433 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:32:16,864 DEBUG Query successful
2024-10-19 16:32:17,275 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:35:12,426 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:35:12,467 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:35:22,526 DEBUG Query successful
2024-10-19 16:35:22,972 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:35:30,478 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:35:30,512 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:35:45,378 DEBUG Query successful
2024-10-19 16:35:45,379 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:36:00,077 DEBUG Query successful
2024-10-19 16:36:00,079 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:36:13,022 DEBUG Query successful
2024-10-19 16:36:13,455 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:38:35,922 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:38:35,956 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:38:46,182 DEBUG Query successful
2024-10-19 16:38:46,613 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:39:26,763 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:39:26,801 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:39:42,483 DEBUG Query successful
2024-10-19 16:39:42,484 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:39:57,947 DEBUG Query successful
2024-10-19 16:39:57,947 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:40:11,349 DEBUG Query successful
2024-10-19 16:40:11,787 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:41:59,701 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:41:59,747 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:42:10,646 DEBUG Query successful
2024-10-19 16:42:11,133 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:43:25,305 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:43:25,339 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:43:41,406 DEBUG Query successful
2024-10-19 16:43:41,407 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:43:58,940 DEBUG Query successful
2024-10-19 16:43:58,941 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:44:14,527 DEBUG Query successful
2024-10-19 16:44:14,958 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:45:28,609 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:45:28,656 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:45:40,625 DEBUG Query successful
2024-10-19 16:45:41,162 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:47:29,594 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:47:29,643 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:47:46,057 DEBUG Query successful
2024-10-19 16:47:46,058 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:48:01,973 DEBUG Query successful
2024-10-19 16:48:01,973 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:48:17,891 DEBUG Query successful
2024-10-19 16:48:18,323 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:48:55,365 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:48:55,409 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:49:06,194 DEBUG Query successful
2024-10-19 16:49:06,678 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:51:31,595 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:51:31,642 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:51:46,746 DEBUG Query successful
2024-10-19 16:51:46,748 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:52:03,039 DEBUG Query successful
2024-10-19 16:52:03,040 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:52:20,384 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:52:20,430 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:52:21,864 DEBUG Query successful
2024-10-19 16:52:22,304 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:52:36,468 DEBUG Query successful
2024-10-19 16:52:36,975 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:55:35,821 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:55:35,860 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:55:50,519 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:55:50,557 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:55:53,201 DEBUG Query successful
2024-10-19 16:55:53,202 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:56:02,608 DEBUG Query successful
2024-10-19 16:56:03,084 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:56:09,517 DEBUG Query successful
2024-10-19 16:56:09,518 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:56:28,670 DEBUG Query successful
2024-10-19 16:56:29,103 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:59:16,731 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:59:16,776 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:59:28,926 DEBUG Query successful
2024-10-19 16:59:29,420 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:59:42,412 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:59:42,449 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:59:59,610 DEBUG Query successful
2024-10-19 16:59:59,611 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:00:18,124 DEBUG Query successful
2024-10-19 17:00:18,124 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:00:34,545 DEBUG Query successful
2024-10-19 17:00:35,025 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:02:45,627 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:02:45,670 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:02:58,091 DEBUG Query successful
2024-10-19 17:02:58,608 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:03:50,252 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:03:50,297 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:04:08,682 DEBUG Query successful
2024-10-19 17:04:08,682 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:04:29,318 DEBUG Query successful
2024-10-19 17:04:29,319 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:04:59,338 DEBUG Query successful
2024-10-19 17:04:59,759 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:06:12,153 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:06:12,189 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:06:18,803 DEBUG Query successful
2024-10-19 17:06:19,236 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:08:13,150 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:08:13,191 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:08:26,232 DEBUG Query successful
2024-10-19 17:08:26,232 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:08:41,515 DEBUG Query successful
2024-10-19 17:08:41,515 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:08:56,863 DEBUG Query successful
2024-10-19 17:08:57,274 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:09:32,650 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:09:32,700 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:09:38,796 DEBUG Query successful
2024-10-19 17:12:09,958 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:12:09,996 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:12:24,755 DEBUG Query successful
2024-10-19 17:12:24,756 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:12:39,509 DEBUG Query successful
2024-10-19 17:12:39,510 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:12:55,577 DEBUG Query successful
2024-10-19 17:13:06,907 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 17:13:06,908 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 17:13:06,945 DEBUG OpenAI client created
2024-10-19 17:13:06,945 DEBUG Model set to: gpt-4-turbo
2024-10-19 17:13:06,945 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 17:13:06,946 DEBUG OpenAI client created
2024-10-19 17:13:06,946 DEBUG Model set to: gpt-4-turbo
2024-10-19 17:13:06,946 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 17:13:06,967 DEBUG OpenAI client created
2024-10-19 17:13:06,967 DEBUG Model set to: gpt-4-turbo
2024-10-19 17:13:06,969 DEBUG OpenAI client created
2024-10-19 17:13:06,969 DEBUG Model set to: gpt-4-turbo
2024-10-19 17:13:26,502 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:26,611 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:36,580 DEBUG Query successful
2024-10-19 17:13:36,581 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:39,249 DEBUG Query successful
2024-10-19 17:13:39,252 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:43,181 DEBUG Query successful
2024-10-19 17:13:43,747 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:45,048 DEBUG Query successful
2024-10-19 17:13:45,049 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:51,316 DEBUG Query successful
2024-10-19 17:13:51,317 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:57,744 DEBUG Query successful
2024-10-19 17:13:57,750 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:58,516 DEBUG Query successful
2024-10-19 17:13:59,082 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:04,015 DEBUG Query successful
2024-10-19 17:14:13,363 DEBUG Query successful
2024-10-19 17:14:13,365 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:14,534 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:22,008 DEBUG Query successful
2024-10-19 17:14:22,009 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:28,995 DEBUG Query successful
2024-10-19 17:14:28,997 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:32,097 DEBUG Query successful
2024-10-19 17:14:32,098 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:36,813 DEBUG Query successful
2024-10-19 17:14:38,798 DEBUG Query successful
2024-10-19 17:14:39,312 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:47,287 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:57,295 DEBUG Query successful
2024-10-19 17:14:57,296 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:03,632 DEBUG Query successful
2024-10-19 17:15:04,237 DEBUG Query successful
2024-10-19 17:15:04,238 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:04,297 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:11,064 DEBUG Query successful
2024-10-19 17:15:11,065 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:18,591 DEBUG Query successful
2024-10-19 17:15:18,592 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:25,956 DEBUG Query successful
2024-10-19 17:15:26,454 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:32,049 DEBUG Query successful
2024-10-19 17:15:32,051 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:39,188 DEBUG Query successful
2024-10-19 17:15:39,762 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:52,909 DEBUG Query successful
2024-10-19 17:15:52,910 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:01,140 DEBUG Query successful
2024-10-19 17:16:01,141 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:04,904 DEBUG Query successful
2024-10-19 17:16:04,907 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:08,909 DEBUG Query successful
2024-10-19 17:16:08,910 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:12,626 DEBUG Query successful
2024-10-19 17:16:13,182 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:17,134 DEBUG Query successful
2024-10-19 17:16:17,663 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:44,907 DEBUG Query successful
2024-10-19 17:16:44,909 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:46,323 DEBUG Query successful
2024-10-19 17:16:46,326 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:52,047 DEBUG Query successful
2024-10-19 17:16:52,513 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:55,291 DEBUG Query successful
2024-10-19 17:16:55,293 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:17:02,841 DEBUG Query successful
2024-10-19 17:17:02,841 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:17:12,650 DEBUG Query successful
2024-10-19 17:17:13,283 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:17:22,410 DEBUG Query successful
2024-10-19 17:17:22,411 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:17:29,472 DEBUG Query successful
2024-10-19 17:17:29,986 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:17:44,656 DEBUG Query successful
2024-10-19 17:17:44,657 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:17:52,812 DEBUG Query successful
2024-10-19 17:17:52,817 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:01,777 DEBUG Query successful
2024-10-19 17:18:01,778 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:03,753 DEBUG Query successful
2024-10-19 17:18:03,756 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:10,557 DEBUG Query successful
2024-10-19 17:18:10,708 DEBUG Query successful
2024-10-19 17:18:11,097 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:11,310 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:42,586 DEBUG Query successful
2024-10-19 17:18:42,588 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:51,159 DEBUG Query successful
2024-10-19 17:18:51,160 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:52,167 DEBUG Query successful
2024-10-19 17:18:52,167 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:57,743 DEBUG Query successful
2024-10-19 17:18:58,267 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:59,827 DEBUG Query successful
2024-10-19 17:18:59,829 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:19:08,969 DEBUG Query successful
2024-10-19 17:19:09,570 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:19:38,030 DEBUG Query successful
2024-10-19 17:19:38,031 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:19:38,783 DEBUG Query successful
2024-10-19 17:19:38,788 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:19:44,128 DEBUG Query successful
2024-10-19 17:19:44,661 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:19:46,000 DEBUG Query successful
2024-10-19 17:19:46,001 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:19:54,901 DEBUG Query successful
2024-10-19 17:19:54,902 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:20:03,476 DEBUG Query successful
2024-10-19 17:20:03,978 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:20:19,516 DEBUG Query successful
2024-10-19 17:20:19,517 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:20:26,671 DEBUG Query successful
2024-10-19 17:20:27,165 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:20:40,811 DEBUG Query successful
2024-10-19 17:20:40,817 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:20:50,379 DEBUG Query successful
2024-10-19 17:20:50,380 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:20:58,786 DEBUG Query successful
2024-10-19 17:20:58,787 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:21:04,786 DEBUG Query successful
2024-10-19 17:21:04,789 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:21:07,909 DEBUG Query successful
2024-10-19 17:21:08,533 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:21:09,920 DEBUG Query successful
2024-10-19 17:21:10,602 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:21:44,288 DEBUG Query successful
2024-10-19 17:21:44,289 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:21:52,795 DEBUG Query successful
2024-10-19 17:21:52,797 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:22:03,062 DEBUG Query successful
2024-10-19 17:22:03,068 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:22:12,193 DEBUG Query successful
2024-10-19 17:22:12,738 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:03,152 DEBUG Query successful
2024-10-19 17:23:03,155 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:04,143 DEBUG Query successful
2024-10-19 17:23:04,148 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:08,395 DEBUG Query successful
2024-10-19 17:23:08,974 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:11,858 DEBUG Query successful
2024-10-19 17:23:11,860 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:19,990 DEBUG Query successful
2024-10-19 17:23:19,991 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:28,273 DEBUG Query successful
2024-10-19 17:23:28,759 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:50,696 DEBUG Query successful
2024-10-19 17:23:50,698 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:55,259 DEBUG Query successful
2024-10-19 17:23:55,764 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:24:05,937 DEBUG Query successful
2024-10-19 17:24:05,942 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:24:15,955 DEBUG Query successful
2024-10-19 17:24:15,955 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:24:24,071 DEBUG Query successful
2024-10-19 17:24:24,072 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:24:33,670 DEBUG Query successful
2024-10-19 17:24:34,279 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:24:43,183 DEBUG Query successful
2024-10-19 17:24:43,186 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:24:48,082 DEBUG Query successful
2024-10-19 17:24:48,691 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:25:23,791 DEBUG Query successful
2024-10-19 17:25:23,794 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:25:32,575 DEBUG Query successful
2024-10-19 17:25:32,579 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:25:40,800 DEBUG Query successful
2024-10-19 17:25:40,800 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:25:42,445 DEBUG Query successful
2024-10-19 17:25:42,446 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:25:47,664 DEBUG Query successful
2024-10-19 17:25:48,164 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:25:48,689 DEBUG Query successful
2024-10-19 17:25:49,228 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:26:39,293 DEBUG Query successful
2024-10-19 17:26:39,294 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:26:46,933 DEBUG Query successful
2024-10-19 17:26:46,933 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:26:49,066 DEBUG Query successful
2024-10-19 17:26:49,068 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:26:53,662 DEBUG Query successful
2024-10-19 17:26:54,197 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:26:56,819 DEBUG Query successful
2024-10-19 17:26:56,822 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:27:05,529 DEBUG Query successful
2024-10-19 17:27:06,118 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:27:58,102 DEBUG Query successful
2024-10-19 17:27:58,107 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:28:06,878 DEBUG Query successful
2024-10-19 17:28:06,878 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:28:17,723 DEBUG Query successful
2024-10-19 17:28:17,725 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:28:26,641 DEBUG Query successful
2024-10-19 17:28:27,140 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:28:52,298 DEBUG Query successful
2024-10-19 17:28:52,300 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:28:56,889 DEBUG Query successful
2024-10-19 17:28:57,389 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:29:14,192 DEBUG Query successful
2024-10-19 17:29:14,194 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:29:23,606 DEBUG Query successful
2024-10-19 17:29:23,606 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:29:33,144 DEBUG Query successful
2024-10-19 17:29:33,144 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:29:42,832 DEBUG Query successful
2024-10-19 17:29:43,368 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:30:31,235 DEBUG Query successful
2024-10-19 17:30:31,237 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:30:39,015 DEBUG Query successful
2024-10-19 17:30:39,016 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:30:45,766 DEBUG Query successful
2024-10-19 17:30:45,768 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:30:53,566 DEBUG Query successful
2024-10-19 17:30:54,176 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:31:46,385 DEBUG Query successful
2024-10-19 17:31:46,387 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:31:57,807 DEBUG Query successful
2024-10-19 17:31:57,808 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:32:02,766 DEBUG Query successful
2024-10-19 17:32:02,773 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:32:06,961 DEBUG Query successful
2024-10-19 17:32:06,961 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:32:08,232 DEBUG Query successful
2024-10-19 17:32:08,760 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:32:15,766 DEBUG Query successful
2024-10-19 17:32:16,385 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:33:06,327 DEBUG Query successful
2024-10-19 17:33:06,330 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:33:10,513 DEBUG Query successful
2024-10-19 17:33:10,981 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:34:12,843 DEBUG Query successful
2024-10-19 17:34:12,846 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:34:21,757 DEBUG Query successful
2024-10-19 17:34:21,762 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:34:29,947 DEBUG Query successful
2024-10-19 17:34:29,947 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:34:38,651 DEBUG Query successful
2024-10-19 17:34:39,120 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:35:10,160 DEBUG Query successful
2024-10-19 17:35:10,162 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:35:15,379 DEBUG Query successful
2024-10-19 17:35:15,905 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:35:30,794 DEBUG Query successful
2024-10-19 17:35:30,795 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:35:38,910 DEBUG Query successful
2024-10-19 17:35:38,916 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:35:46,582 DEBUG Query successful
2024-10-19 17:35:46,583 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:35:56,361 DEBUG Query successful
2024-10-19 17:35:56,844 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:36:17,222 DEBUG Query successful
2024-10-19 17:36:17,222 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:36:21,891 DEBUG Query successful
2024-10-19 17:36:22,435 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:36:53,602 DEBUG Query successful
2024-10-19 17:36:53,604 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:37:05,479 DEBUG Query successful
2024-10-19 17:37:05,485 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:37:15,630 DEBUG Query successful
2024-10-19 17:37:15,630 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:37:23,162 DEBUG Query successful
2024-10-19 17:37:23,163 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:37:24,987 DEBUG Query successful
2024-10-19 17:37:25,532 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:37:26,992 DEBUG Query successful
2024-10-19 17:37:27,680 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:40:37,319 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:40:37,379 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:40:42,039 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:40:42,078 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:40:45,944 DEBUG Query successful
2024-10-19 17:40:46,431 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:40:47,974 DEBUG Query successful
2024-10-19 17:40:47,976 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:40:58,553 DEBUG Query successful
2024-10-19 17:40:58,556 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:41:08,576 DEBUG Query successful
2024-10-19 17:41:09,083 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:43:58,699 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:43:58,737 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:44:03,166 DEBUG Query successful
2024-10-19 17:44:03,717 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:44:20,708 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:44:20,737 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:44:31,713 DEBUG Query successful
2024-10-19 17:44:31,714 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:44:42,602 DEBUG Query successful
2024-10-19 17:44:42,603 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:44:55,738 DEBUG Query successful
2024-10-19 17:44:56,331 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:45:55,073 DEBUG Query successful
2024-10-19 17:45:55,074 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:46:03,844 DEBUG Query successful
2024-10-19 17:46:03,850 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:46:12,181 DEBUG Query successful
2024-10-19 17:46:12,182 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:46:19,475 DEBUG Query successful
2024-10-19 17:46:19,985 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:47:16,331 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:47:16,371 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:47:21,469 DEBUG Query successful
2024-10-19 17:47:21,987 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:49:34,387 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:49:34,415 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:49:46,714 DEBUG Query successful
2024-10-19 17:49:46,715 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:49:56,923 DEBUG Query successful
2024-10-19 17:49:56,923 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:50:04,025 DEBUG Query successful
2024-10-19 17:50:04,500 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:50:34,906 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:50:34,943 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:50:40,849 DEBUG Query successful
2024-10-19 17:50:41,344 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:51:03,101 DEBUG Query successful
2024-10-19 17:51:03,102 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:51:11,656 DEBUG Query successful
2024-10-19 17:51:11,657 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:51:25,368 DEBUG Query successful
2024-10-19 17:51:25,370 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:51:33,154 DEBUG Query successful
2024-10-19 17:51:33,662 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:52:31,858 DEBUG Query successful
2024-10-19 17:52:31,860 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:52:41,082 DEBUG Query successful
2024-10-19 17:52:41,082 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:52:50,444 DEBUG Query successful
2024-10-19 17:52:50,445 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:53:01,897 DEBUG Query successful
2024-10-19 17:53:02,348 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:53:53,552 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:53:53,587 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:53:58,774 DEBUG Query successful
2024-10-19 17:53:59,232 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:56:15,676 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:56:15,705 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:56:24,087 DEBUG Query successful
2024-10-19 17:56:24,088 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:56:31,547 DEBUG Query successful
2024-10-19 17:56:31,548 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:56:43,198 DEBUG Query successful
2024-10-19 17:56:43,667 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:57:11,615 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:57:11,644 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:57:17,541 DEBUG Query successful
2024-10-19 17:57:18,009 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:59:55,853 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:59:55,878 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:00:05,830 DEBUG Query successful
2024-10-19 18:00:05,831 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:00:15,972 DEBUG Query successful
2024-10-19 18:00:15,976 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:00:25,901 DEBUG Query successful
2024-10-19 18:00:26,384 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:00:30,094 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:00:30,134 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:00:35,342 DEBUG Query successful
2024-10-19 18:00:35,845 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:03:38,691 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:03:38,724 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:03:49,258 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:03:49,296 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:03:49,762 DEBUG Query successful
2024-10-19 18:03:49,766 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:03:53,597 DEBUG Query successful
2024-10-19 18:03:54,092 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:04:04,126 DEBUG Query successful
2024-10-19 18:04:04,129 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:04:14,797 DEBUG Query successful
2024-10-19 18:04:15,265 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:07:06,078 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:07:06,109 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:07:09,049 DEBUG Query successful
2024-10-19 18:07:09,512 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:07:27,837 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:07:27,880 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:07:36,577 DEBUG Query successful
2024-10-19 18:07:36,578 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:07:46,552 DEBUG Query successful
2024-10-19 18:07:46,554 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:07:55,321 DEBUG Query successful
2024-10-19 18:07:55,779 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:10:21,773 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:10:21,796 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:10:24,543 DEBUG Query successful
2024-10-19 18:10:24,996 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:11:08,219 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:11:08,249 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:11:19,092 DEBUG Query successful
2024-10-19 18:11:19,096 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:11:27,756 DEBUG Query successful
2024-10-19 18:11:27,756 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:11:38,361 DEBUG Query successful
2024-10-19 18:13:36,627 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:13:36,661 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:13:41,051 DEBUG Query successful
2024-10-19 21:51:41,963 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 21:51:41,987 DEBUG OpenAI client created
2024-10-19 21:51:41,988 DEBUG Model set to: gpt-4-turbo
2024-10-19 21:51:41,988 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 21:51:42,009 DEBUG OpenAI client created
2024-10-19 21:51:42,009 DEBUG Model set to: gpt-4-turbo
2024-10-19 21:51:58,616 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:09,498 DEBUG Query successful
2024-10-19 21:52:09,500 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:10,486 DEBUG Query successful
2024-10-19 21:52:10,517 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:19,288 DEBUG Query successful
2024-10-19 21:52:19,385 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:30,017 DEBUG Query successful
2024-10-19 21:52:30,019 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:34,610 DEBUG Query successful
2024-10-19 21:52:34,611 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:41,081 DEBUG Query successful
2024-10-19 21:52:41,082 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:47,037 DEBUG Query successful
2024-10-19 21:52:47,542 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:58,394 DEBUG Query successful
2024-10-19 21:52:58,396 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:59,921 DEBUG Query successful
2024-10-19 21:52:59,931 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:53:08,903 DEBUG Query successful
2024-10-19 21:53:09,001 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:53:24,659 DEBUG Query successful
2024-10-19 21:53:24,660 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:53:32,071 DEBUG Query successful
2024-10-19 21:53:32,073 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:53:39,751 DEBUG Query successful
2024-10-19 21:53:39,753 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:53:48,009 DEBUG Query successful
2024-10-19 21:53:58,415 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:54:10,909 DEBUG Query successful
2024-10-19 21:54:10,911 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:54:12,918 DEBUG Query successful
2024-10-19 21:54:12,920 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:54:25,130 DEBUG Query successful
2024-10-19 21:54:25,230 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:54:47,898 DEBUG Query successful
2024-10-19 21:54:47,900 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:54:56,552 DEBUG Query successful
2024-10-19 21:54:56,553 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:55:05,265 DEBUG Query successful
2024-10-19 21:55:05,266 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:55:13,779 DEBUG Query successful
2024-10-19 21:55:14,251 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:55:23,225 DEBUG Query successful
2024-10-19 21:55:23,227 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:55:24,885 DEBUG Query successful
2024-10-19 21:55:24,887 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:55:38,608 DEBUG Query successful
2024-10-19 21:55:38,703 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:56:01,170 DEBUG Query successful
2024-10-19 21:56:01,174 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:56:10,109 DEBUG Query successful
2024-10-19 21:56:10,110 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:56:20,331 DEBUG Query successful
2024-10-19 21:56:20,332 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:56:30,148 DEBUG Query successful
2024-10-19 21:56:30,634 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:56:40,308 DEBUG Query successful
2024-10-19 21:56:40,310 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:56:42,425 DEBUG Query successful
2024-10-19 21:56:42,427 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:56:59,675 DEBUG Query successful
2024-10-19 21:56:59,778 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:57:29,058 DEBUG Query successful
2024-10-19 21:57:29,060 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:57:39,235 DEBUG Query successful
2024-10-19 21:57:39,237 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:57:50,391 DEBUG Query successful
2024-10-19 21:57:50,392 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:58:00,272 DEBUG Query successful
2024-10-19 21:58:00,883 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:58:11,475 DEBUG Query successful
2024-10-19 21:58:11,477 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:58:13,638 DEBUG Query successful
2024-10-19 21:58:13,639 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:58:31,594 DEBUG Query successful
2024-10-19 21:58:31,705 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:59:08,185 DEBUG Query successful
2024-10-19 21:59:08,186 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:59:19,454 DEBUG Query successful
2024-10-19 21:59:19,455 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:59:31,679 DEBUG Query successful
2024-10-19 21:59:31,679 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:59:44,004 DEBUG Query successful
2024-10-19 21:59:44,542 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:59:54,543 DEBUG Query successful
2024-10-19 21:59:54,545 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:59:56,635 DEBUG Query successful
2024-10-19 21:59:56,636 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:00:14,594 DEBUG Query successful
2024-10-19 22:00:14,706 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:00:51,359 DEBUG Query successful
2024-10-19 22:00:51,361 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:01:06,367 DEBUG Query successful
2024-10-19 22:01:06,368 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:01:19,202 DEBUG Query successful
2024-10-19 22:01:19,208 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:01:32,467 DEBUG Query successful
2024-10-19 22:01:33,003 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:01:43,885 DEBUG Query successful
2024-10-19 22:01:43,890 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:01:46,105 DEBUG Query successful
2024-10-19 22:01:46,107 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:02:07,779 DEBUG Query successful
2024-10-19 22:02:07,882 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:02:49,618 DEBUG Query successful
2024-10-19 22:02:49,620 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:03:04,775 DEBUG Query successful
2024-10-19 22:03:04,776 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:03:21,830 DEBUG Query successful
2024-10-19 22:03:21,831 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:03:34,350 DEBUG Query successful
2024-10-19 22:03:34,804 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:03:47,572 DEBUG Query successful
2024-10-19 22:03:47,574 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:03:53,128 DEBUG Query successful
2024-10-19 22:03:53,130 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:04:14,755 DEBUG Query successful
2024-10-19 22:04:14,850 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:04:46,033 DEBUG Query successful
2024-10-19 22:04:46,035 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:05:00,399 DEBUG Query successful
2024-10-19 22:05:00,400 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:05:13,646 DEBUG Query successful
2024-10-19 22:05:13,647 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:05:28,499 DEBUG Query successful
2024-10-19 22:05:28,920 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:05:48,587 DEBUG Query successful
2024-10-19 22:05:48,589 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:05:53,968 DEBUG Query successful
2024-10-19 22:05:53,968 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:06:14,268 DEBUG Query successful
2024-10-19 22:06:14,362 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:07:00,121 DEBUG Query successful
2024-10-19 22:07:00,123 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:07:13,261 DEBUG Query successful
2024-10-19 22:07:13,262 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:07:30,302 DEBUG Query successful
2024-10-19 22:07:30,302 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:07:46,965 DEBUG Query successful
2024-10-19 22:07:47,424 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:07:59,130 DEBUG Query successful
2024-10-19 22:07:59,131 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:08:04,626 DEBUG Query successful
2024-10-19 22:08:04,627 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:08:28,314 DEBUG Query successful
2024-10-19 22:08:28,417 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:09:06,059 DEBUG Query successful
2024-10-19 22:09:06,064 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:09:20,879 DEBUG Query successful
2024-10-19 22:09:20,880 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:09:35,676 DEBUG Query successful
2024-10-19 22:09:35,677 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:09:51,980 DEBUG Query successful
2024-10-19 22:09:52,445 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:10:14,812 DEBUG Query successful
2024-10-19 22:10:14,814 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:10:18,562 DEBUG Query successful
2024-10-19 22:10:18,565 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:10:41,955 DEBUG Query successful
2024-10-19 22:10:42,059 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:11:21,554 DEBUG Query successful
2024-10-19 22:11:21,556 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:11:37,572 DEBUG Query successful
2024-10-19 22:11:37,573 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:11:51,994 DEBUG Query successful
2024-10-19 22:11:51,994 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:12:07,118 DEBUG Query successful
2024-10-19 22:12:07,588 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:12:20,719 DEBUG Query successful
2024-10-19 22:12:20,724 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:12:25,224 DEBUG Query successful
2024-10-19 22:12:25,224 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:12:54,183 DEBUG Query successful
2024-10-19 22:12:54,285 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:14:42,666 DEBUG Query successful
2024-10-19 22:14:42,670 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:14:55,351 DEBUG Query successful
2024-10-19 22:14:55,352 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:15:09,337 DEBUG Query successful
2024-10-19 22:15:09,342 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:15:23,943 DEBUG Query successful
2024-10-19 22:15:24,433 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:15:35,580 DEBUG Query successful
2024-10-19 22:15:35,584 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:15:38,948 DEBUG Query successful
2024-10-19 22:15:38,949 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:16:05,723 DEBUG Query successful
2024-10-19 22:16:05,822 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:16:55,670 DEBUG Query successful
2024-10-19 22:16:55,672 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:17:08,963 DEBUG Query successful
2024-10-19 22:17:08,964 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:17:23,740 DEBUG Query successful
2024-10-19 22:17:23,741 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:17:37,355 DEBUG Query successful
2024-10-19 22:17:37,841 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:17:50,289 DEBUG Query successful
2024-10-19 22:17:50,293 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:17:59,068 DEBUG Query successful
2024-10-19 22:17:59,070 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:18:32,265 DEBUG Query successful
2024-10-19 22:18:32,372 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:20:27,766 DEBUG Query successful
2024-10-19 22:20:27,769 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:20:43,564 DEBUG Query successful
2024-10-19 22:20:43,565 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:20:58,857 DEBUG Query successful
2024-10-19 22:20:58,858 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:21:15,316 DEBUG Query successful
2024-10-19 22:21:15,768 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:21:25,592 DEBUG Query successful
2024-10-19 22:21:25,594 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:21:29,281 DEBUG Query successful
2024-10-19 22:21:29,287 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:22:04,654 DEBUG Query successful
2024-10-19 22:22:04,758 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:22:57,777 DEBUG Query successful
2024-10-19 22:22:57,779 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:23:14,643 DEBUG Query successful
2024-10-19 22:23:14,644 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:23:32,293 DEBUG Query successful
2024-10-19 22:23:32,294 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:23:54,793 DEBUG Query successful
2024-10-19 22:23:55,252 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:24:09,024 DEBUG Query successful
2024-10-19 22:24:09,026 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:24:12,823 DEBUG Query successful
2024-10-19 22:24:12,824 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:24:45,291 DEBUG Query successful
2024-10-19 22:24:45,394 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:25:34,954 DEBUG Query successful
2024-10-19 22:25:34,955 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:25:48,166 DEBUG Query successful
2024-10-19 22:25:48,168 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:26:02,659 DEBUG Query successful
2024-10-19 22:26:02,660 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:26:16,426 DEBUG Query successful
2024-10-19 22:26:17,085 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:26:27,800 DEBUG Query successful
2024-10-19 22:26:27,804 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:26:37,421 DEBUG Query successful
2024-10-19 22:26:37,422 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:27:19,451 DEBUG Query successful
2024-10-19 22:27:19,562 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:28:09,228 DEBUG Query successful
2024-10-19 22:28:09,231 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:28:22,248 DEBUG Query successful
2024-10-19 22:28:22,249 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:28:36,119 DEBUG Query successful
2024-10-19 22:28:36,120 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:28:49,173 DEBUG Query successful
2024-10-19 22:28:49,692 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:29:02,060 DEBUG Query successful
2024-10-19 22:29:02,063 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:29:06,591 DEBUG Query successful
2024-10-19 22:29:06,592 DEBUG Querying model: gpt-4-turbo
2024-10-24 16:18:33,667 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:33,775 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:33,817 DEBUG OpenAI client created
2024-10-24 16:18:33,817 DEBUG Model set to: gpt-4-turbo
2024-10-24 16:18:33,817 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:33,822 DEBUG OpenAI client created
2024-10-24 16:18:33,822 DEBUG Model set to: gpt-4-turbo
2024-10-24 16:18:33,822 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:33,845 DEBUG OpenAI client created
2024-10-24 16:18:33,845 DEBUG Model set to: gpt-4-turbo
2024-10-24 16:18:33,850 DEBUG OpenAI client created
2024-10-24 16:18:33,850 DEBUG Model set to: gpt-4-turbo
2024-10-24 16:18:57,901 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:57,933 DEBUG OpenAI client created
2024-10-24 16:18:57,933 DEBUG Model set to: gpt-4-turbo
2024-10-24 16:18:57,933 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:57,956 DEBUG OpenAI client created
2024-10-24 16:18:57,958 DEBUG Model set to: gpt-4-turbo
2024-10-24 16:18:58,004 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:58,033 DEBUG OpenAI client created
2024-10-24 16:18:58,033 DEBUG Model set to: gpt-4-turbo
2024-10-24 16:18:58,033 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:58,058 DEBUG OpenAI client created
2024-10-24 16:18:58,058 DEBUG Model set to: gpt-4-turbo
2024-10-24 20:27:12,243 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 20:27:12,385 DEBUG OpenAI client created
2024-10-24 20:27:12,385 DEBUG Model set to: gpt-4-turbo
2024-10-24 20:27:12,386 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 20:27:12,408 DEBUG OpenAI client created
2024-10-24 20:27:12,408 DEBUG Model set to: gpt-4-turbo
2024-10-24 20:28:25,882 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 20:28:25,908 DEBUG OpenAI client created
2024-10-24 20:28:25,908 DEBUG Model set to: gpt-4-turbo
2024-10-24 20:28:25,908 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 20:28:25,932 DEBUG OpenAI client created
2024-10-24 20:28:25,933 DEBUG Model set to: gpt-4-turbo
2024-10-24 20:28:42,298 DEBUG Querying model: gpt-4-turbo
2024-10-24 20:28:56,504 DEBUG Query successful
2024-10-24 20:28:56,593 DEBUG Querying model: gpt-4-turbo
2024-10-24 20:29:17,340 DEBUG Query successful
2024-10-24 20:29:17,362 DEBUG Querying model: gpt-4-turbo
2024-10-24 20:29:41,815 DEBUG Query successful
2024-10-24 20:29:41,818 DEBUG Querying model: gpt-4-turbo
2024-10-24 20:29:51,580 DEBUG Query successful
2024-10-24 20:29:52,164 DEBUG Querying model: gpt-4-turbo
2024-10-25 12:13:59,464 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 12:13:59,500 DEBUG OpenAI client created
2024-10-25 12:13:59,500 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 12:13:59,500 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 12:13:59,528 DEBUG OpenAI client created
2024-10-25 12:13:59,528 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 12:14:26,131 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:14:35,254 DEBUG Query successful
2024-10-25 12:14:35,332 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:14:52,593 DEBUG Query successful
2024-10-25 12:14:52,618 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:15:01,590 DEBUG Query successful
2024-10-25 12:15:01,591 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:15:04,323 DEBUG Query successful
2024-10-25 12:15:04,863 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:15:16,765 DEBUG Query successful
2024-10-25 12:15:16,843 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:15:34,683 DEBUG Query successful
2024-10-25 12:15:34,714 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:15:45,913 DEBUG Query successful
2024-10-25 12:15:45,914 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:15:49,363 DEBUG Query successful
2024-10-25 12:15:49,793 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:15:57,741 DEBUG Query successful
2024-10-25 12:15:57,817 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:16:10,683 DEBUG Query successful
2024-10-25 12:16:10,711 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:16:24,524 DEBUG Query successful
2024-10-25 12:16:24,524 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:16:28,684 DEBUG Query successful
2024-10-25 12:16:29,146 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:16:38,586 DEBUG Query successful
2024-10-25 12:16:38,662 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:16:55,622 DEBUG Query successful
2024-10-25 12:16:55,649 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:17:10,045 DEBUG Query successful
2024-10-25 12:17:10,046 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:17:13,294 DEBUG Query successful
2024-10-25 12:17:13,806 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:17:21,173 DEBUG Query successful
2024-10-25 12:17:21,256 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:17:38,520 DEBUG Query successful
2024-10-25 12:17:38,542 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:17:54,943 DEBUG Query successful
2024-10-25 12:17:54,945 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:18:00,252 DEBUG Query successful
2024-10-25 12:18:00,827 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:18:08,729 DEBUG Query successful
2024-10-25 12:18:08,803 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:18:23,421 DEBUG Query successful
2024-10-25 12:18:23,449 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:18:43,013 DEBUG Query successful
2024-10-25 12:18:43,015 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:18:46,803 DEBUG Query successful
2024-10-25 12:18:47,355 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:18:54,978 DEBUG Query successful
2024-10-25 12:18:55,056 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:19:11,873 DEBUG Query successful
2024-10-25 12:19:11,899 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:19:35,318 DEBUG Query successful
2024-10-25 12:19:35,320 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:19:40,328 DEBUG Query successful
2024-10-25 12:19:40,850 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:19:48,220 DEBUG Query successful
2024-10-25 12:19:48,297 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:20:20,305 DEBUG Query successful
2024-10-25 12:20:20,333 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:20:42,655 DEBUG Query successful
2024-10-25 12:20:42,656 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:20:46,945 DEBUG Query successful
2024-10-25 12:20:47,423 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:20:54,904 DEBUG Query successful
2024-10-25 12:20:54,983 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:21:43,787 DEBUG Query successful
2024-10-25 12:21:43,814 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:22:06,830 DEBUG Query successful
2024-10-25 12:22:06,831 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:22:10,690 DEBUG Query successful
2024-10-25 12:22:11,184 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:22:18,930 DEBUG Query successful
2024-10-25 12:22:19,007 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:22:38,220 DEBUG Query successful
2024-10-25 12:22:38,242 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:23:07,485 DEBUG Query successful
2024-10-25 12:23:07,486 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:23:11,470 DEBUG Query successful
2024-10-25 12:23:11,978 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:23:20,029 DEBUG Query successful
2024-10-25 12:23:20,104 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:23:40,891 DEBUG Query successful
2024-10-25 12:23:40,922 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:24:09,340 DEBUG Query successful
2024-10-25 12:24:09,341 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:24:13,513 DEBUG Query successful
2024-10-25 12:24:13,972 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:24:22,356 DEBUG Query successful
2024-10-25 12:24:22,438 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:24:37,420 DEBUG Query successful
2024-10-25 12:24:37,444 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:34:52,113 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:34:52,377 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:35:10,089 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:35:10,095 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:35:28,054 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:35:28,061 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:35:45,934 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:35:46,432 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:36:03,922 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:36:04,009 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:36:21,837 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:36:21,860 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:36:39,893 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:36:39,915 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:36:57,594 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:36:57,600 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:37:15,349 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:37:15,351 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:37:33,193 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:37:33,661 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:37:51,370 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:37:51,455 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:38:09,319 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:38:09,349 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:38:27,354 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:38:27,373 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:38:44,870 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:38:44,874 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:39:02,383 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:39:02,388 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:39:19,836 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:39:20,363 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:39:37,910 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:39:37,990 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:39:55,674 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:39:55,703 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:40:14,157 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:40:14,179 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:40:32,056 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:40:32,058 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:40:49,896 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:40:49,901 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:41:06,141 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: EOF occurred in violation of protocol (_ssl.c:997)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: EOF occurred in violation of protocol (_ssl.c:997)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:41:06,668 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:41:16,261 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:41:16,341 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:41:25,893 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:41:25,922 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:41:43,425 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:41:43,447 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:41:56,557 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:41:56,560 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:42:11,574 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:42:11,580 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:42:25,726 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:42:26,269 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:42:38,997 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:42:39,098 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:42:56,986 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:42:57,013 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:43:11,313 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:43:11,332 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:43:24,851 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:43:24,857 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:43:42,426 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:43:42,431 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:44:00,392 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:44:00,897 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:44:18,694 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:44:18,773 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:44:36,355 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:44:36,383 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:44:54,297 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:44:54,313 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:45:11,726 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:45:11,733 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:45:29,314 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:45:29,322 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:45:45,592 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:45:46,064 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:45:58,627 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:45:58,709 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:46:16,326 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:46:16,355 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:46:30,865 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:46:30,882 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:46:48,507 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:46:48,513 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:47:06,173 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:47:06,181 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:47:24,175 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:47:24,708 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:47:42,438 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:47:42,518 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:48:00,313 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:48:00,339 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:48:18,679 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:48:18,698 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:48:32,603 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:48:32,609 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:48:45,509 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:48:45,515 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:48:58,237 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:48:58,723 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:49:16,727 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:49:16,811 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:49:30,830 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:49:30,861 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:49:45,210 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:49:45,228 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:49:58,140 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:49:58,145 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:50:10,623 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:50:10,626 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:50:25,569 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:50:26,095 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:50:43,873 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:50:43,956 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:50:58,562 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:50:58,592 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:51:16,762 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:51:16,785 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:51:30,895 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:51:30,898 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:51:44,943 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:51:44,950 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:51:58,212 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:51:58,711 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:52:11,583 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:52:11,668 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:52:25,238 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:52:25,269 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:52:38,911 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:52:38,928 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:52:46,586 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:52:46,593 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:52:59,390 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:52:59,395 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:53:12,213 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:53:12,749 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:53:31,438 DEBUG Query successful
2024-10-25 12:53:31,520 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:53:42,681 DEBUG Query successful
2024-10-25 12:53:42,714 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:54:19,872 DEBUG Query successful
2024-10-25 12:54:19,873 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:54:26,738 DEBUG Query successful
2024-10-25 12:54:27,268 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:54:35,467 DEBUG Query successful
2024-10-25 12:54:35,547 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:54:46,352 DEBUG Query successful
2024-10-25 12:54:46,380 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:55:25,284 DEBUG Query successful
2024-10-25 12:55:25,285 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:55:30,407 DEBUG Query successful
2024-10-25 12:55:30,861 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:55:41,023 DEBUG Query successful
2024-10-25 12:55:41,103 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:55:52,642 DEBUG Query successful
2024-10-25 12:55:52,667 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:56:34,281 DEBUG Query successful
2024-10-25 12:56:34,282 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:56:38,878 DEBUG Query successful
2024-10-25 12:56:39,337 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:56:47,689 DEBUG Query successful
2024-10-25 12:56:47,767 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:56:59,333 DEBUG Query successful
2024-10-25 12:56:59,362 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:57:45,869 DEBUG Query successful
2024-10-25 12:57:45,871 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:57:53,558 DEBUG Query successful
2024-10-25 12:57:54,039 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:58:00,519 DEBUG Query successful
2024-10-25 12:58:00,602 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:58:12,621 DEBUG Query successful
2024-10-25 12:58:12,649 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:58:58,638 DEBUG Query successful
2024-10-25 12:58:58,641 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:59:03,348 DEBUG Query successful
2024-10-25 12:59:03,861 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:59:10,758 DEBUG Query successful
2024-10-25 12:59:10,833 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:59:24,329 DEBUG Query successful
2024-10-25 12:59:24,353 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:00:10,920 DEBUG Query successful
2024-10-25 13:00:10,922 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:00:16,230 DEBUG Query successful
2024-10-25 13:00:16,675 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:00:26,090 DEBUG Query successful
2024-10-25 13:00:26,167 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:00:39,491 DEBUG Query successful
2024-10-25 13:00:39,513 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:02:29,796 DEBUG Query successful
2024-10-25 13:02:29,799 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:02:34,647 DEBUG Query successful
2024-10-25 13:02:35,125 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:02:44,005 DEBUG Query successful
2024-10-25 13:02:44,080 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:02:57,687 DEBUG Query successful
2024-10-25 13:02:57,710 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:03:49,010 DEBUG Query successful
2024-10-25 13:03:49,012 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:03:54,640 DEBUG Query successful
2024-10-25 13:03:58,930 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:04:07,421 DEBUG Query successful
2024-10-25 13:04:07,501 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:04:22,101 DEBUG Query successful
2024-10-25 13:04:22,123 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:05:12,947 DEBUG Query successful
2024-10-25 13:05:12,950 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:05:18,445 DEBUG Query successful
2024-10-25 13:05:18,970 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:05:28,587 DEBUG Query successful
2024-10-25 13:05:28,663 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:05:43,518 DEBUG Query successful
2024-10-25 13:05:43,545 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:06:43,775 DEBUG Query successful
2024-10-25 13:06:43,777 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:06:49,364 DEBUG Query successful
2024-10-25 13:06:49,873 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:07:00,679 DEBUG Query successful
2024-10-25 13:07:00,765 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:07:15,577 DEBUG Query successful
2024-10-25 13:07:15,600 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:13:02,546 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 13:13:02,572 DEBUG OpenAI client created
2024-10-25 13:13:02,572 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 13:13:02,572 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 13:13:02,599 DEBUG OpenAI client created
2024-10-25 13:13:02,599 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 13:13:20,875 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 13:13:20,899 DEBUG OpenAI client created
2024-10-25 13:13:20,899 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 13:13:20,899 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 13:13:20,921 DEBUG OpenAI client created
2024-10-25 13:13:20,922 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 13:13:39,694 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:13:48,419 DEBUG Query successful
2024-10-25 13:13:48,511 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:13:57,236 DEBUG Query successful
2024-10-25 13:13:57,261 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:05,256 DEBUG Query successful
2024-10-25 13:14:05,258 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:07,747 DEBUG Query successful
2024-10-25 13:14:08,318 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:15,508 DEBUG Query successful
2024-10-25 13:14:15,587 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:21,849 DEBUG Query successful
2024-10-25 13:14:21,876 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:32,131 DEBUG Query successful
2024-10-25 13:14:32,133 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:35,044 DEBUG Query successful
2024-10-25 13:14:35,489 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:45,370 DEBUG Query successful
2024-10-25 13:14:45,448 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:53,551 DEBUG Query successful
2024-10-25 13:14:53,573 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:15:05,541 DEBUG Query successful
2024-10-25 13:15:05,542 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:15:08,298 DEBUG Query successful
2024-10-25 13:15:08,796 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:15:16,703 DEBUG Query successful
2024-10-25 13:15:16,784 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:15:26,500 DEBUG Query successful
2024-10-25 13:15:26,536 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:15:41,320 DEBUG Query successful
2024-10-25 13:15:41,322 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:15:44,259 DEBUG Query successful
2024-10-25 13:15:44,750 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:15:53,341 DEBUG Query successful
2024-10-25 13:15:53,423 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:16:03,087 DEBUG Query successful
2024-10-25 13:16:03,110 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:17:13,625 DEBUG Query successful
2024-10-25 13:17:13,627 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:17:16,613 DEBUG Query successful
2024-10-25 13:17:17,371 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:17:24,529 DEBUG Query successful
2024-10-25 13:17:24,610 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:17:35,699 DEBUG Query successful
2024-10-25 13:17:35,729 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:17:54,337 DEBUG Query successful
2024-10-25 13:17:54,341 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:17:57,852 DEBUG Query successful
2024-10-25 13:17:58,359 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:18:08,147 DEBUG Query successful
2024-10-25 13:18:08,225 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:18:22,682 DEBUG Query successful
2024-10-25 13:18:22,711 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:18:42,568 DEBUG Query successful
2024-10-25 13:18:42,569 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:18:45,902 DEBUG Query successful
2024-10-25 13:18:46,444 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:18:54,452 DEBUG Query successful
2024-10-25 13:18:54,532 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:19:08,923 DEBUG Query successful
2024-10-25 13:19:08,961 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:19:32,780 DEBUG Query successful
2024-10-25 13:19:32,783 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:19:35,974 DEBUG Query successful
2024-10-25 13:19:36,480 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:19:43,830 DEBUG Query successful
2024-10-25 13:19:43,923 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:19:57,721 DEBUG Query successful
2024-10-25 13:19:57,746 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:20:21,472 DEBUG Query successful
2024-10-25 13:20:21,474 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:20:24,809 DEBUG Query successful
2024-10-25 13:20:25,274 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:20:33,756 DEBUG Query successful
2024-10-25 13:20:33,834 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:20:48,812 DEBUG Query successful
2024-10-25 13:20:48,837 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:21:16,696 DEBUG Query successful
2024-10-25 13:21:16,698 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:21:20,473 DEBUG Query successful
2024-10-25 13:21:20,990 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:21:31,483 DEBUG Query successful
2024-10-25 13:21:31,565 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:21:48,435 DEBUG Query successful
2024-10-25 13:21:48,461 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:22:16,887 DEBUG Query successful
2024-10-25 13:22:16,889 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:22:20,762 DEBUG Query successful
2024-10-25 13:22:21,386 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:22:32,155 DEBUG Query successful
2024-10-25 13:22:32,233 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:22:48,400 DEBUG Query successful
2024-10-25 13:22:48,420 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:23:17,324 DEBUG Query successful
2024-10-25 13:23:17,326 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:23:20,661 DEBUG Query successful
2024-10-25 13:23:21,222 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:23:29,605 DEBUG Query successful
2024-10-25 13:23:29,683 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:23:47,816 DEBUG Query successful
2024-10-25 13:23:47,840 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:24:20,707 DEBUG Query successful
2024-10-25 13:24:20,709 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:24:23,445 DEBUG Query successful
2024-10-25 13:24:23,997 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:24:31,960 DEBUG Query successful
2024-10-25 13:24:32,045 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:24:51,632 DEBUG Query successful
2024-10-25 13:24:51,661 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:25:26,921 DEBUG Query successful
2024-10-25 13:25:26,923 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:25:30,175 DEBUG Query successful
2024-10-25 13:25:30,684 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:25:38,690 DEBUG Query successful
2024-10-25 13:25:38,769 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:25:58,791 DEBUG Query successful
2024-10-25 13:25:58,820 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:26:35,888 DEBUG Query successful
2024-10-25 13:26:35,890 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:26:39,157 DEBUG Query successful
2024-10-25 13:26:39,724 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:26:49,932 DEBUG Query successful
2024-10-25 13:26:50,010 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:27:11,911 DEBUG Query successful
2024-10-25 13:27:11,938 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:27:49,698 DEBUG Query successful
2024-10-25 13:27:49,699 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:27:52,385 DEBUG Query successful
2024-10-25 13:27:53,020 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:28:02,723 DEBUG Query successful
2024-10-25 13:28:02,804 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:28:21,599 DEBUG Query successful
2024-10-25 13:28:21,627 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:29:01,010 DEBUG Query successful
2024-10-25 13:29:01,012 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:29:04,341 DEBUG Query successful
2024-10-25 13:29:04,786 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:29:13,349 DEBUG Query successful
2024-10-25 13:29:13,428 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:29:34,481 DEBUG Query successful
2024-10-25 13:29:34,510 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:30:18,869 DEBUG Query successful
2024-10-25 13:30:18,870 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:30:22,264 DEBUG Query successful
2024-10-25 13:30:22,748 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:30:29,984 DEBUG Query successful
2024-10-25 13:30:30,068 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:30:51,583 DEBUG Query successful
2024-10-25 13:30:51,611 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:31:34,981 DEBUG Query successful
2024-10-25 13:31:34,982 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:31:38,316 DEBUG Query successful
2024-10-25 13:31:38,815 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:31:47,733 DEBUG Query successful
2024-10-25 13:31:47,821 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:32:11,332 DEBUG Query successful
2024-10-25 13:32:11,362 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:32:56,423 DEBUG Query successful
2024-10-25 13:32:56,425 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:32:59,803 DEBUG Query successful
2024-10-25 13:33:00,345 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:33:09,656 DEBUG Query successful
2024-10-25 13:33:09,734 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:33:27,097 DEBUG Query successful
2024-10-25 13:33:27,123 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:34:22,397 DEBUG Query successful
2024-10-25 13:34:22,398 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:34:26,174 DEBUG Query successful
2024-10-25 13:34:26,700 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:34:34,783 DEBUG Query successful
2024-10-25 13:34:34,863 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:34:53,366 DEBUG Query successful
2024-10-25 13:34:53,397 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:35:50,190 DEBUG Query successful
2024-10-25 13:35:50,191 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:35:53,768 DEBUG Query successful
2024-10-25 13:35:54,246 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:36:02,097 DEBUG Query successful
2024-10-25 13:36:02,174 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:36:25,110 DEBUG Query successful
2024-10-25 13:36:25,138 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:37:21,439 DEBUG Query successful
2024-10-25 13:37:21,442 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:37:24,815 DEBUG Query successful
2024-10-25 13:37:25,441 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:37:32,619 DEBUG Query successful
2024-10-25 13:37:32,702 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:37:52,409 DEBUG Query successful
2024-10-25 13:37:52,438 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:38:46,212 DEBUG Query successful
2024-10-25 13:38:46,215 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:38:49,811 DEBUG Query successful
2024-10-25 13:38:50,328 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:38:57,095 DEBUG Query successful
2024-10-25 13:38:57,172 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:39:16,971 DEBUG Query successful
2024-10-25 13:39:16,996 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:40:14,807 DEBUG Query successful
2024-10-25 13:40:14,808 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:40:19,735 DEBUG Query successful
2024-10-25 13:40:20,284 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:40:27,927 DEBUG Query successful
2024-10-25 13:40:28,009 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:40:48,501 DEBUG Query successful
2024-10-25 13:40:48,529 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:43:42,182 DEBUG Query successful
2024-10-25 13:43:42,184 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:43:46,071 DEBUG Query successful
2024-10-25 13:43:46,608 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:43:53,281 DEBUG Query successful
2024-10-25 13:43:53,360 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:44:14,906 DEBUG Query successful
2024-10-25 13:44:14,934 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:45:13,070 DEBUG Query successful
2024-10-25 13:45:13,071 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:45:17,041 DEBUG Query successful
2024-10-25 13:45:17,547 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:45:25,723 DEBUG Query successful
2024-10-25 13:45:25,805 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:45:48,575 DEBUG Query successful
2024-10-25 13:45:48,605 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:46:45,897 DEBUG Query successful
2024-10-25 13:46:45,898 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:46:49,694 DEBUG Query successful
2024-10-25 13:46:50,288 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:46:58,676 DEBUG Query successful
2024-10-25 13:46:58,754 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:47:17,033 DEBUG Query successful
2024-10-25 13:47:17,064 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:50:21,324 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 13:50:21,357 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:50:26,239 DEBUG Query successful
2024-10-25 13:50:26,713 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:50:33,841 DEBUG Query successful
2024-10-25 13:50:33,926 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:50:54,304 DEBUG Query successful
2024-10-25 13:50:54,330 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:53:57,756 DEBUG Query successful
2024-10-25 13:53:57,759 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:54:03,009 DEBUG Query successful
2024-10-25 13:54:03,467 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:54:11,662 DEBUG Query successful
2024-10-25 13:54:11,742 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:54:32,276 DEBUG Query successful
2024-10-25 13:54:32,301 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:57:36,668 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 13:57:36,719 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:57:41,340 DEBUG Query successful
2024-10-25 13:57:41,926 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:57:50,653 DEBUG Query successful
2024-10-25 13:57:50,730 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:58:11,317 DEBUG Query successful
2024-10-25 13:58:11,344 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 14:01:15,763 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 14:01:15,804 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 14:01:21,218 DEBUG Query successful
2024-10-25 16:51:38,752 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 16:51:38,779 DEBUG OpenAI client created
2024-10-25 16:51:38,780 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 16:51:38,780 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 16:51:38,802 DEBUG OpenAI client created
2024-10-25 16:51:38,802 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 16:51:56,809 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:52:06,598 DEBUG Query successful
2024-10-25 16:52:06,683 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:52:28,018 DEBUG Query successful
2024-10-25 16:52:28,047 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:52:37,481 DEBUG Query successful
2024-10-25 16:52:37,483 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:52:44,793 DEBUG Query successful
2024-10-25 16:54:15,575 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 16:54:15,599 DEBUG OpenAI client created
2024-10-25 16:54:15,599 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 16:54:15,599 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 16:54:15,620 DEBUG OpenAI client created
2024-10-25 16:54:15,620 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 16:54:37,482 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:54:47,247 DEBUG Query successful
2024-10-25 16:54:47,323 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:54:58,247 DEBUG Query successful
2024-10-25 16:54:58,274 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:55:06,911 DEBUG Query successful
2024-10-25 16:55:06,913 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:55:10,692 DEBUG Query successful
2024-10-25 17:02:37,974 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 17:02:37,998 DEBUG OpenAI client created
2024-10-25 17:02:37,999 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 17:02:37,999 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 17:02:38,023 DEBUG OpenAI client created
2024-10-25 17:02:38,024 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 17:02:55,463 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:03:05,335 DEBUG Query successful
2024-10-25 17:03:05,412 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:03:20,142 DEBUG Query successful
2024-10-25 17:03:20,168 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:03:29,251 DEBUG Query successful
2024-10-25 17:03:29,253 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:03:32,023 DEBUG Query successful
2024-10-25 17:03:32,834 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:03:40,880 DEBUG Query successful
2024-10-25 17:03:40,957 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:03:53,820 DEBUG Query successful
2024-10-25 17:03:53,846 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:04:04,109 DEBUG Query successful
2024-10-25 17:04:04,112 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:04:06,882 DEBUG Query successful
2024-10-25 17:04:07,343 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:04:14,544 DEBUG Query successful
2024-10-25 17:04:14,622 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:04:28,350 DEBUG Query successful
2024-10-25 17:04:28,375 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:04:42,220 DEBUG Query successful
2024-10-25 17:04:42,221 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:04:45,863 DEBUG Query successful
2024-10-25 17:04:46,460 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:04:57,240 DEBUG Query successful
2024-10-25 17:04:57,319 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:05:11,475 DEBUG Query successful
2024-10-25 17:05:11,505 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:05:27,090 DEBUG Query successful
2024-10-25 17:05:27,091 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:05:30,710 DEBUG Query successful
2024-10-25 17:05:31,260 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:05:38,339 DEBUG Query successful
2024-10-25 17:05:38,415 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:05:53,177 DEBUG Query successful
2024-10-25 17:05:53,202 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:06:12,498 DEBUG Query successful
2024-10-25 17:06:12,501 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:06:18,281 DEBUG Query successful
2024-10-25 17:06:18,824 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:06:26,153 DEBUG Query successful
2024-10-25 17:06:26,227 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:06:42,561 DEBUG Query successful
2024-10-25 17:06:42,591 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:07:02,612 DEBUG Query successful
2024-10-25 17:07:02,613 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:07:07,668 DEBUG Query successful
2024-10-25 17:07:08,182 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:07:18,767 DEBUG Query successful
2024-10-25 17:07:18,841 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:07:38,601 DEBUG Query successful
2024-10-25 17:07:38,628 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:08:00,592 DEBUG Query successful
2024-10-25 17:08:00,595 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:08:05,240 DEBUG Query successful
2024-10-25 17:08:05,809 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:08:17,635 DEBUG Query successful
2024-10-25 17:08:17,712 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:08:34,770 DEBUG Query successful
2024-10-25 17:08:34,800 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:09:00,078 DEBUG Query successful
2024-10-25 17:09:00,079 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:09:04,157 DEBUG Query successful
2024-10-25 17:09:04,725 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:09:13,294 DEBUG Query successful
2024-10-25 17:09:13,370 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:09:29,698 DEBUG Query successful
2024-10-25 17:09:29,725 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:09:55,440 DEBUG Query successful
2024-10-25 17:09:55,442 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:09:59,779 DEBUG Query successful
2024-10-25 17:10:00,267 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:10:08,636 DEBUG Query successful
2024-10-25 17:10:08,713 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:10:26,928 DEBUG Query successful
2024-10-25 17:10:26,954 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:10:57,715 DEBUG Query successful
2024-10-25 17:10:57,716 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:11:01,462 DEBUG Query successful
2024-10-25 17:11:01,942 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:11:10,018 DEBUG Query successful
2024-10-25 17:11:10,093 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:11:27,443 DEBUG Query successful
2024-10-25 17:11:27,467 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:12:00,251 DEBUG Query successful
2024-10-25 17:12:00,252 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:12:06,114 DEBUG Query successful
2024-10-25 17:12:06,626 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:12:14,844 DEBUG Query successful
2024-10-25 17:12:14,919 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:12:33,518 DEBUG Query successful
2024-10-25 17:12:33,542 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:13:05,975 DEBUG Query successful
2024-10-25 17:13:05,977 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:13:11,202 DEBUG Query successful
2024-10-25 17:13:11,856 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:13:18,700 DEBUG Query successful
2024-10-25 17:13:18,777 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:13:38,594 DEBUG Query successful
2024-10-25 17:13:38,618 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:14:18,503 DEBUG Query successful
2024-10-25 17:14:18,504 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:14:22,893 DEBUG Query successful
2024-10-25 17:14:23,424 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:14:32,290 DEBUG Query successful
2024-10-25 17:14:32,369 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:14:52,656 DEBUG Query successful
2024-10-25 17:14:52,681 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:15:34,609 DEBUG Query successful
2024-10-25 17:15:34,610 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:15:38,975 DEBUG Query successful
2024-10-25 17:15:39,434 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:15:47,827 DEBUG Query successful
2024-10-25 17:15:47,904 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:16:13,743 DEBUG Query successful
2024-10-25 17:16:13,776 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:17:06,241 DEBUG Query successful
2024-10-25 17:17:06,243 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:17:10,591 DEBUG Query successful
2024-10-25 17:17:11,445 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:17:19,664 DEBUG Query successful
2024-10-25 17:17:19,744 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:17:46,033 DEBUG Query successful
2024-10-25 17:17:46,069 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:18:36,151 DEBUG Query successful
2024-10-25 17:18:36,153 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:18:41,812 DEBUG Query successful
2024-10-25 17:18:42,350 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:18:50,266 DEBUG Query successful
2024-10-25 17:18:50,348 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:19:14,554 DEBUG Query successful
2024-10-25 17:19:14,585 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:20:05,596 DEBUG Query successful
2024-10-25 17:20:05,598 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:20:10,798 DEBUG Query successful
2024-10-25 17:20:11,359 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:20:19,901 DEBUG Query successful
2024-10-25 17:20:19,983 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:20:46,779 DEBUG Query successful
2024-10-25 17:20:46,810 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:21:45,585 DEBUG Query successful
2024-10-25 17:21:45,586 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:21:50,609 DEBUG Query successful
2024-10-25 17:21:51,177 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:22:01,431 DEBUG Query successful
2024-10-25 17:22:01,508 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:22:29,417 DEBUG Query successful
2024-10-25 17:22:29,449 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:26:29,437 DEBUG Query successful
2024-10-25 17:26:29,440 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:26:35,097 DEBUG Query successful
2024-10-25 17:26:35,576 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:26:45,418 DEBUG Query successful
2024-10-25 17:26:45,494 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:27:12,356 DEBUG Query successful
2024-10-25 17:27:12,379 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:28:29,986 DEBUG Query successful
2024-10-25 17:28:29,988 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:28:35,264 DEBUG Query successful
2024-10-25 17:28:35,710 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:28:41,709 DEBUG Query successful
2024-10-25 17:28:41,785 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:29:11,211 DEBUG Query successful
2024-10-25 17:29:11,238 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:30:41,150 DEBUG Query successful
2024-10-25 17:30:41,152 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:30:47,997 DEBUG Query successful
2024-10-25 17:30:48,610 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:30:57,788 DEBUG Query successful
2024-10-25 17:30:57,861 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:31:26,887 DEBUG Query successful
2024-10-25 17:31:26,913 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:34:48,834 DEBUG Query successful
2024-10-25 17:34:48,839 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:34:54,955 DEBUG Query successful
2024-10-25 17:34:55,439 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:35:01,591 DEBUG Query successful
2024-10-25 17:35:01,665 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:35:33,367 DEBUG Query successful
2024-10-25 17:35:33,390 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:41:37,166 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 17:41:37,199 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:41:42,995 DEBUG Query successful
2024-10-25 17:41:43,461 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:41:51,714 DEBUG Query successful
2024-10-25 17:41:51,787 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:42:21,020 DEBUG Query successful
2024-10-25 17:42:21,042 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:49:55,076 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 17:49:55,104 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:50:01,246 DEBUG Query successful
2024-10-25 17:50:01,757 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:50:10,029 DEBUG Query successful
2024-10-25 17:50:10,103 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:50:42,484 DEBUG Query successful
2024-10-25 17:50:42,511 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:53:40,813 DEBUG Query successful
2024-10-25 17:53:40,814 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:53:52,311 DEBUG Query successful
2024-10-25 17:53:56,323 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:54:08,425 DEBUG Query successful
2024-10-25 17:54:08,504 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:54:42,589 DEBUG Query successful
2024-10-25 17:54:42,611 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:58:07,398 DEBUG Query successful
2024-10-25 17:58:07,400 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:58:14,472 DEBUG Query successful
2024-10-25 17:58:15,010 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:58:28,433 DEBUG Query successful
2024-10-25 17:58:28,510 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:59:05,841 DEBUG Query successful
2024-10-25 17:59:05,866 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:10:40,025 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 18:10:40,043 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:10:51,963 DEBUG Query successful
2024-10-25 18:10:52,441 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:11:08,354 DEBUG Query successful
2024-10-25 18:11:08,436 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:11:48,188 DEBUG Query successful
2024-10-25 18:11:48,217 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:26:50,491 DEBUG Query successful
2024-10-25 18:26:50,497 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:27:02,074 DEBUG Query successful
2024-10-25 18:27:02,685 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:27:20,676 DEBUG Query successful
2024-10-25 18:27:20,753 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:27:57,343 DEBUG Query successful
2024-10-25 18:27:57,372 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:45:31,712 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 18:45:31,744 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:45:46,213 DEBUG Query successful
2024-10-25 18:45:46,856 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:46:06,502 DEBUG Query successful
2024-10-25 18:46:06,584 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:46:47,339 DEBUG Query successful
2024-10-25 18:46:47,372 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 19:06:51,176 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 19:06:51,216 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 19:07:03,715 DEBUG Query successful
2024-10-25 19:07:04,246 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 19:07:27,356 DEBUG Query successful
2024-10-25 19:07:27,432 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 19:08:13,299 DEBUG Query successful
2024-10-25 19:08:13,326 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 19:31:17,566 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 19:31:17,599 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 19:31:33,858 DEBUG Query successful
2024-10-25 19:31:34,469 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 19:32:00,374 DEBUG Query successful
2024-10-25 19:32:00,450 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:52:47,201 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-26 22:52:47,226 DEBUG OpenAI client created
2024-10-26 22:52:47,226 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-26 22:52:47,226 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-26 22:52:47,245 DEBUG OpenAI client created
2024-10-26 22:52:47,245 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-26 22:53:03,451 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:53:14,525 DEBUG Query successful
2024-10-26 22:53:14,592 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:53:29,001 DEBUG Query successful
2024-10-26 22:53:29,020 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:53:36,679 DEBUG Query successful
2024-10-26 22:53:36,681 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:53:39,492 DEBUG Query successful
2024-10-26 22:53:39,946 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:53:48,357 DEBUG Query successful
2024-10-26 22:53:48,421 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:53:59,104 DEBUG Query successful
2024-10-26 22:53:59,122 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:54:18,944 DEBUG Query successful
2024-10-26 22:54:18,945 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:54:22,073 DEBUG Query successful
2024-10-26 22:54:22,448 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:54:36,575 DEBUG Query successful
2024-10-26 22:54:36,637 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:54:48,509 DEBUG Query successful
2024-10-26 22:54:48,529 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:55:03,303 DEBUG Query successful
2024-10-26 22:55:03,304 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:55:06,385 DEBUG Query successful
2024-10-26 22:55:06,882 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:55:18,616 DEBUG Query successful
2024-10-26 22:55:18,679 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:55:30,330 DEBUG Query successful
2024-10-26 22:55:30,350 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:55:45,975 DEBUG Query successful
2024-10-26 22:55:45,975 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:55:49,029 DEBUG Query successful
2024-10-26 22:55:49,462 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:55:55,468 DEBUG Query successful
2024-10-26 22:55:55,529 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:56:08,721 DEBUG Query successful
2024-10-26 22:56:08,740 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:56:26,957 DEBUG Query successful
2024-10-26 22:56:26,958 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:56:30,476 DEBUG Query successful
2024-10-26 22:56:30,878 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:56:38,463 DEBUG Query successful
2024-10-26 22:56:38,526 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:56:52,865 DEBUG Query successful
2024-10-26 22:56:52,895 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:57:12,077 DEBUG Query successful
2024-10-26 22:57:12,078 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:57:15,549 DEBUG Query successful
2024-10-26 22:57:15,937 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:57:26,416 DEBUG Query successful
2024-10-26 22:57:26,478 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:57:41,106 DEBUG Query successful
2024-10-26 22:57:41,126 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:58:01,388 DEBUG Query successful
2024-10-26 22:58:01,388 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:58:05,011 DEBUG Query successful
2024-10-26 22:58:05,440 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:58:12,203 DEBUG Query successful
2024-10-26 22:58:12,265 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:58:27,752 DEBUG Query successful
2024-10-26 22:58:27,771 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:58:57,106 DEBUG Query successful
2024-10-26 22:58:57,106 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:59:01,192 DEBUG Query successful
2024-10-26 22:59:01,601 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:59:09,917 DEBUG Query successful
2024-10-26 22:59:09,979 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:59:24,184 DEBUG Query successful
2024-10-26 22:59:24,203 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:59:51,307 DEBUG Query successful
2024-10-26 22:59:51,307 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:59:55,666 DEBUG Query successful
2024-10-26 22:59:56,053 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:00:02,812 DEBUG Query successful
2024-10-26 23:00:02,874 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:00:18,790 DEBUG Query successful
2024-10-26 23:00:18,810 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:00:52,730 DEBUG Query successful
2024-10-26 23:00:52,731 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:00:56,558 DEBUG Query successful
2024-10-26 23:00:56,920 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:01:04,415 DEBUG Query successful
2024-10-26 23:01:04,478 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:01:19,950 DEBUG Query successful
2024-10-26 23:01:19,969 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:01:57,160 DEBUG Query successful
2024-10-26 23:01:57,161 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:02:02,083 DEBUG Query successful
2024-10-26 23:02:02,471 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:02:12,450 DEBUG Query successful
2024-10-26 23:02:12,514 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:02:30,896 DEBUG Query successful
2024-10-26 23:02:30,914 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:03:05,471 DEBUG Query successful
2024-10-26 23:03:05,471 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:03:09,410 DEBUG Query successful
2024-10-26 23:03:09,825 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:03:16,850 DEBUG Query successful
2024-10-26 23:03:16,912 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:03:35,645 DEBUG Query successful
2024-10-26 23:03:35,663 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:04:11,227 DEBUG Query successful
2024-10-26 23:04:11,228 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:04:15,824 DEBUG Query successful
2024-10-26 23:04:16,278 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:04:23,395 DEBUG Query successful
2024-10-26 23:04:23,456 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:04:42,817 DEBUG Query successful
2024-10-26 23:04:42,836 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:05:22,585 DEBUG Query successful
2024-10-26 23:05:22,586 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:05:26,683 DEBUG Query successful
2024-10-26 23:05:27,064 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:05:33,191 DEBUG Query successful
2024-10-26 23:05:33,253 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:05:55,951 DEBUG Query successful
2024-10-26 23:05:55,971 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:06:40,720 DEBUG Query successful
2024-10-26 23:06:40,720 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:06:45,422 DEBUG Query successful
2024-10-26 23:06:45,848 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:06:51,988 DEBUG Query successful
2024-10-26 23:06:52,052 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:07:18,537 DEBUG Query successful
2024-10-26 23:07:18,558 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:08:03,752 DEBUG Query successful
2024-10-26 23:08:03,752 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:08:08,656 DEBUG Query successful
2024-10-26 23:08:09,061 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:08:16,810 DEBUG Query successful
2024-10-26 23:08:16,872 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:08:42,680 DEBUG Query successful
2024-10-26 23:08:42,699 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:09:32,018 DEBUG Query successful
2024-10-26 23:09:32,019 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:09:36,958 DEBUG Query successful
2024-10-26 23:09:37,333 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:09:44,366 DEBUG Query successful
2024-10-26 23:09:44,425 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:10:13,206 DEBUG Query successful
2024-10-26 23:10:13,226 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:11:07,262 DEBUG Query successful
2024-10-26 23:11:07,263 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:11:12,642 DEBUG Query successful
2024-10-26 23:11:13,000 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:11:19,623 DEBUG Query successful
2024-10-26 23:11:19,684 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:11:48,247 DEBUG Query successful
2024-10-26 23:11:48,266 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:12:40,483 DEBUG Query successful
2024-10-26 23:12:40,483 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:12:45,452 DEBUG Query successful
2024-10-26 23:12:45,809 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:12:52,260 DEBUG Query successful
2024-10-26 23:12:52,321 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:13:20,690 DEBUG Query successful
2024-10-26 23:13:20,709 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:15:17,508 DEBUG Query successful
2024-10-26 23:15:17,509 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:15:24,423 DEBUG Query successful
2024-10-26 23:15:24,781 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:15:34,547 DEBUG Query successful
2024-10-26 23:15:34,609 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:16:07,955 DEBUG Query successful
2024-10-26 23:16:07,978 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:17:04,593 DEBUG Query successful
2024-10-26 23:17:04,593 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:17:09,798 DEBUG Query successful
2024-10-26 23:17:10,185 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:17:16,908 DEBUG Query successful
2024-10-26 23:17:16,969 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:17:48,872 DEBUG Query successful
2024-10-26 23:17:48,894 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:19:46,656 DEBUG Query successful
2024-10-26 23:19:46,657 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:19:51,627 DEBUG Query successful
2024-10-26 23:19:52,025 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:20:05,647 DEBUG Query successful
2024-10-26 23:20:05,709 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:20:39,397 DEBUG Query successful
2024-10-26 23:20:39,413 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:22:38,626 DEBUG Query successful
2024-10-26 23:22:38,627 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:22:44,071 DEBUG Query successful
2024-10-26 23:22:44,444 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:22:58,641 DEBUG Query successful
2024-10-26 23:22:58,702 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:23:32,885 DEBUG Query successful
2024-10-26 23:23:32,904 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:24:32,866 DEBUG Query successful
2024-10-26 23:24:32,866 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:24:40,024 DEBUG Query successful
2024-10-26 23:24:40,398 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:24:47,155 DEBUG Query successful
2024-10-26 23:24:47,219 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:25:21,917 DEBUG Query successful
2024-10-26 23:25:21,939 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:28:24,390 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 72, in map_httpcore_exceptions
    yield
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 236, in handle_request
    resp = self._pool.handle_request(req)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 990, in _request
    response = self._client.send(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 926, in send
    response = self._send_handling_auth(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 954, in _send_handling_auth
    response = self._send_handling_redirects(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 991, in _send_handling_redirects
    response = self._send_single_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 1027, in _send_single_request
    response = transport.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 235, in handle_request
    with map_httpcore_exceptions():
  File "F:\ide\anaconda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 89, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\python_file\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 815, in create
    return self._post(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1277, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 954, in request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1024, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-26 23:28:24,417 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:28:31,521 DEBUG Query successful
2024-10-26 23:28:34,833 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:28:44,202 DEBUG Query successful
2024-10-26 23:28:44,271 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:34:13,086 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-26 23:34:13,107 DEBUG OpenAI client created
2024-10-26 23:34:13,107 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-26 23:34:13,107 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-26 23:34:13,128 DEBUG OpenAI client created
2024-10-26 23:34:13,128 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-26 23:34:28,053 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:34:35,059 DEBUG Query successful
2024-10-26 23:34:35,124 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:34:56,357 DEBUG Query successful
2024-10-26 23:34:56,377 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:35:06,604 DEBUG Query successful
2024-10-26 23:35:06,605 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:35:10,422 DEBUG Query successful
2024-10-26 23:35:10,874 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:35:19,666 DEBUG Query successful
2024-10-26 23:35:19,737 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:35:31,411 DEBUG Query successful
2024-10-26 23:35:31,436 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:35:43,199 DEBUG Query successful
2024-10-26 23:35:43,199 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:35:46,005 DEBUG Query successful
2024-10-26 23:35:46,426 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:35:52,861 DEBUG Query successful
2024-10-26 23:35:52,931 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:36:04,587 DEBUG Query successful
2024-10-26 23:36:04,614 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:36:17,112 DEBUG Query successful
2024-10-26 23:36:17,113 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:36:19,983 DEBUG Query successful
2024-10-26 23:36:20,428 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:36:27,149 DEBUG Query successful
2024-10-26 23:36:27,213 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:36:40,535 DEBUG Query successful
2024-10-26 23:36:40,555 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:36:56,508 DEBUG Query successful
2024-10-26 23:36:56,509 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:36:59,875 DEBUG Query successful
2024-10-26 23:37:00,336 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:37:09,154 DEBUG Query successful
2024-10-26 23:37:09,216 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:37:22,308 DEBUG Query successful
2024-10-26 23:37:22,327 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:37:39,437 DEBUG Query successful
2024-10-26 23:37:39,438 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:37:44,768 DEBUG Query successful
2024-10-26 23:37:45,219 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:37:53,463 DEBUG Query successful
2024-10-26 23:37:53,528 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:38:12,231 DEBUG Query successful
2024-10-26 23:38:12,248 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:38:34,369 DEBUG Query successful
2024-10-26 23:38:34,369 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:38:38,905 DEBUG Query successful
2024-10-26 23:38:39,306 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:38:49,360 DEBUG Query successful
2024-10-26 23:38:49,424 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:39:07,487 DEBUG Query successful
2024-10-26 23:39:07,508 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:39:29,968 DEBUG Query successful
2024-10-26 23:39:29,969 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:39:34,126 DEBUG Query successful
2024-10-26 23:39:34,547 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:39:44,558 DEBUG Query successful
2024-10-26 23:39:44,621 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:40:03,999 DEBUG Query successful
2024-10-26 23:40:04,016 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:40:26,389 DEBUG Query successful
2024-10-26 23:40:26,389 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:40:31,992 DEBUG Query successful
2024-10-26 23:40:32,361 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:40:41,414 DEBUG Query successful
2024-10-26 23:40:41,480 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:40:56,382 DEBUG Query successful
2024-10-26 23:40:56,404 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:41:22,550 DEBUG Query successful
2024-10-26 23:41:22,551 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:41:26,072 DEBUG Query successful
2024-10-26 23:41:26,434 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:41:39,348 DEBUG Query successful
2024-10-26 23:41:39,411 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:41:55,286 DEBUG Query successful
2024-10-26 23:41:55,308 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:42:23,064 DEBUG Query successful
2024-10-26 23:42:23,065 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:42:27,398 DEBUG Query successful
2024-10-26 23:42:27,805 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:42:36,299 DEBUG Query successful
2024-10-26 23:42:36,370 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:42:55,634 DEBUG Query successful
2024-10-26 23:42:55,655 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:43:25,938 DEBUG Query successful
2024-10-26 23:43:25,939 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:43:29,708 DEBUG Query successful
2024-10-26 23:43:30,072 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:43:36,980 DEBUG Query successful
2024-10-26 23:43:37,050 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:43:54,929 DEBUG Query successful
2024-10-26 23:43:54,947 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:44:29,755 DEBUG Query successful
2024-10-26 23:44:29,756 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:44:34,309 DEBUG Query successful
2024-10-26 23:44:34,678 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:44:41,112 DEBUG Query successful
2024-10-26 23:44:41,177 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:44:57,409 DEBUG Query successful
2024-10-26 23:44:57,428 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:45:33,288 DEBUG Query successful
2024-10-26 23:45:33,290 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:45:37,356 DEBUG Query successful
2024-10-26 23:45:37,735 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:45:46,260 DEBUG Query successful
2024-10-26 23:45:46,326 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:46:04,064 DEBUG Query successful
2024-10-26 23:46:04,083 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:46:39,069 DEBUG Query successful
2024-10-26 23:46:39,070 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:46:42,855 DEBUG Query successful
2024-10-26 23:46:43,215 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:46:49,739 DEBUG Query successful
2024-10-26 23:46:49,802 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:47:12,781 DEBUG Query successful
2024-10-26 23:47:12,799 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:47:52,993 DEBUG Query successful
2024-10-26 23:47:52,994 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:47:57,295 DEBUG Query successful
2024-10-26 23:47:57,670 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:48:12,696 DEBUG Query successful
2024-10-26 23:48:12,760 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:48:40,762 DEBUG Query successful
2024-10-26 23:48:40,781 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:49:22,444 DEBUG Query successful
2024-10-26 23:49:22,445 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:49:26,302 DEBUG Query successful
2024-10-26 23:49:26,669 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:49:34,967 DEBUG Query successful
2024-10-26 23:49:35,033 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:49:54,664 DEBUG Query successful
2024-10-26 23:49:54,683 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:50:43,109 DEBUG Query successful
2024-10-26 23:50:43,110 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:50:47,406 DEBUG Query successful
2024-10-26 23:50:47,872 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:50:54,324 DEBUG Query successful
2024-10-26 23:50:54,393 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:51:14,128 DEBUG Query successful
2024-10-26 23:51:14,147 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:52:02,294 DEBUG Query successful
2024-10-26 23:52:02,295 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:52:06,617 DEBUG Query successful
2024-10-26 23:52:06,989 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:52:13,777 DEBUG Query successful
2024-10-26 23:52:13,843 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:52:36,991 DEBUG Query successful
2024-10-26 23:52:37,011 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:53:26,083 DEBUG Query successful
2024-10-26 23:53:26,084 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:53:30,270 DEBUG Query successful
2024-10-26 23:53:30,636 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:53:37,087 DEBUG Query successful
2024-10-26 23:53:37,154 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:53:59,635 DEBUG Query successful
2024-10-26 23:53:59,658 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:54:49,755 DEBUG Query successful
2024-10-26 23:54:49,756 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:54:53,913 DEBUG Query successful
2024-10-26 23:54:54,285 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:55:01,345 DEBUG Query successful
2024-10-26 23:55:01,411 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:55:23,333 DEBUG Query successful
2024-10-26 23:55:23,351 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:56:20,123 DEBUG Query successful
2024-10-26 23:56:20,125 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:56:24,842 DEBUG Query successful
2024-10-26 23:56:25,218 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:56:32,978 DEBUG Query successful
2024-10-26 23:56:33,045 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:56:49,871 DEBUG Query successful
2024-10-26 23:56:49,892 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:57:50,072 DEBUG Query successful
2024-10-26 23:57:50,072 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:57:54,003 DEBUG Query successful
2024-10-26 23:57:54,403 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:58:01,838 DEBUG Query successful
2024-10-26 23:58:01,917 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:58:19,296 DEBUG Query successful
2024-10-26 23:58:19,313 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:01:21,753 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 72, in map_httpcore_exceptions
    yield
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 236, in handle_request
    resp = self._pool.handle_request(req)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 990, in _request
    response = self._client.send(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 926, in send
    response = self._send_handling_auth(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 954, in _send_handling_auth
    response = self._send_handling_redirects(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 991, in _send_handling_redirects
    response = self._send_single_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 1027, in _send_single_request
    response = transport.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 235, in handle_request
    with map_httpcore_exceptions():
  File "F:\ide\anaconda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 89, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\python_file\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 815, in create
    return self._post(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1277, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 954, in request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1024, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-27 00:01:21,771 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:01:26,846 DEBUG Query successful
2024-10-27 00:01:27,250 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:01:34,442 DEBUG Query successful
2024-10-27 00:01:34,508 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:01:51,289 DEBUG Query successful
2024-10-27 00:01:51,309 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:02:48,252 DEBUG Query successful
2024-10-27 00:02:48,252 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:02:52,279 DEBUG Query successful
2024-10-27 00:02:52,643 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:02:59,025 DEBUG Query successful
2024-10-27 00:02:59,091 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:03:16,724 DEBUG Query successful
2024-10-27 00:03:16,748 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:05:16,981 DEBUG Query successful
2024-10-27 00:05:16,982 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:05:32,121 DEBUG Query successful
2024-10-27 00:05:32,494 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:05:43,814 DEBUG Query successful
2024-10-27 00:05:43,881 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:06:02,821 DEBUG Query successful
2024-10-27 00:06:02,847 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:09:05,435 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 72, in map_httpcore_exceptions
    yield
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 236, in handle_request
    resp = self._pool.handle_request(req)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 990, in _request
    response = self._client.send(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 926, in send
    response = self._send_handling_auth(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 954, in _send_handling_auth
    response = self._send_handling_redirects(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 991, in _send_handling_redirects
    response = self._send_single_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 1027, in _send_single_request
    response = transport.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 235, in handle_request
    with map_httpcore_exceptions():
  File "F:\ide\anaconda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 89, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\python_file\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 815, in create
    return self._post(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1277, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 954, in request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1024, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-27 00:09:05,460 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:09:10,156 DEBUG Query successful
2024-10-27 00:09:10,541 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:09:22,308 DEBUG Query successful
2024-10-27 00:09:22,374 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:09:40,858 DEBUG Query successful
2024-10-27 00:09:40,876 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:12:43,348 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 72, in map_httpcore_exceptions
    yield
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 236, in handle_request
    resp = self._pool.handle_request(req)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 990, in _request
    response = self._client.send(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 926, in send
    response = self._send_handling_auth(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 954, in _send_handling_auth
    response = self._send_handling_redirects(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 991, in _send_handling_redirects
    response = self._send_single_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 1027, in _send_single_request
    response = transport.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 235, in handle_request
    with map_httpcore_exceptions():
  File "F:\ide\anaconda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 89, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\python_file\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 815, in create
    return self._post(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1277, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 954, in request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1024, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-27 00:12:43,366 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:12:48,290 DEBUG Query successful
2024-10-27 00:12:48,632 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:12:56,392 DEBUG Query successful
2024-10-27 00:12:56,454 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:13:38,176 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:38,215 DEBUG OpenAI client created
2024-10-27 13:13:38,215 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:38,215 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:38,233 DEBUG OpenAI client created
2024-10-27 13:13:38,233 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:40,263 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:40,284 DEBUG OpenAI client created
2024-10-27 13:13:40,284 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:40,284 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:40,303 DEBUG OpenAI client created
2024-10-27 13:13:40,303 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:42,571 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:42,593 DEBUG OpenAI client created
2024-10-27 13:13:42,593 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:42,593 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:42,613 DEBUG OpenAI client created
2024-10-27 13:13:42,613 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:44,683 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:44,705 DEBUG OpenAI client created
2024-10-27 13:13:44,705 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:44,705 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:44,725 DEBUG OpenAI client created
2024-10-27 13:13:44,725 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:55,127 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:13:56,010 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:00,472 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:01,274 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:02,974 DEBUG Query successful
2024-10-27 13:14:03,043 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:03,512 DEBUG Query successful
2024-10-27 13:14:03,581 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:09,183 DEBUG Query successful
2024-10-27 13:14:09,250 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:09,929 DEBUG Query successful
2024-10-27 13:14:09,954 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:11,300 DEBUG Query successful
2024-10-27 13:14:11,320 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:15,477 DEBUG Query successful
2024-10-27 13:14:15,546 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:17,954 DEBUG Query successful
2024-10-27 13:14:17,974 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:18,217 DEBUG Query successful
2024-10-27 13:14:18,219 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:19,299 DEBUG Query successful
2024-10-27 13:14:19,300 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:20,583 DEBUG Query successful
2024-10-27 13:14:21,035 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:21,374 DEBUG Query successful
2024-10-27 13:14:21,438 DEBUG Query successful
2024-10-27 13:14:21,460 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:21,837 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:27,452 DEBUG Query successful
2024-10-27 13:14:27,453 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:28,156 DEBUG Query successful
2024-10-27 13:14:28,228 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:29,805 DEBUG Query successful
2024-10-27 13:14:29,999 DEBUG Query successful
2024-10-27 13:14:30,067 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:30,282 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:31,121 DEBUG Query successful
2024-10-27 13:14:31,122 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:33,270 DEBUG Query successful
2024-10-27 13:14:33,741 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:37,569 DEBUG Query successful
2024-10-27 13:14:37,589 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:37,949 DEBUG Query successful
2024-10-27 13:14:37,990 DEBUG Query successful
2024-10-27 13:14:38,013 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:38,015 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:40,160 DEBUG Query successful
2024-10-27 13:14:40,225 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:46,487 DEBUG Query successful
2024-10-27 13:14:46,510 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:48,123 DEBUG Query successful
2024-10-27 13:14:48,151 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:48,652 DEBUG Query successful
2024-10-27 13:14:48,653 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:51,123 DEBUG Query successful
2024-10-27 13:14:51,544 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:55,743 DEBUG Query successful
2024-10-27 13:14:55,744 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:58,139 DEBUG Query successful
2024-10-27 13:14:58,536 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:58,637 DEBUG Query successful
2024-10-27 13:14:58,638 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:59,188 DEBUG Query successful
2024-10-27 13:14:59,256 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:08,817 DEBUG Query successful
2024-10-27 13:15:09,017 DEBUG Query successful
2024-10-27 13:15:09,017 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:09,235 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:11,439 DEBUG Query successful
2024-10-27 13:15:11,835 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:11,936 DEBUG Query successful
2024-10-27 13:15:12,007 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:12,090 DEBUG Query successful
2024-10-27 13:15:12,111 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:14,935 DEBUG Query successful
2024-10-27 13:15:14,999 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:18,724 DEBUG Query successful
2024-10-27 13:15:18,791 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:20,596 DEBUG Query successful
2024-10-27 13:15:20,620 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:23,448 DEBUG Query successful
2024-10-27 13:15:23,466 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:24,055 DEBUG Query successful
2024-10-27 13:15:24,056 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:26,836 DEBUG Query successful
2024-10-27 13:15:27,225 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:28,103 DEBUG Query successful
2024-10-27 13:15:28,125 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:32,921 DEBUG Query successful
2024-10-27 13:15:32,922 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:35,356 DEBUG Query successful
2024-10-27 13:15:35,785 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:35,819 DEBUG Query successful
2024-10-27 13:15:35,890 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:36,860 DEBUG Query successful
2024-10-27 13:15:36,861 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:39,262 DEBUG Query successful
2024-10-27 13:15:39,657 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:41,344 DEBUG Query successful
2024-10-27 13:15:41,345 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:42,657 DEBUG Query successful
2024-10-27 13:15:42,729 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:44,147 DEBUG Query successful
2024-10-27 13:15:44,592 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:46,775 DEBUG Query successful
2024-10-27 13:15:46,797 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:48,811 DEBUG Query successful
2024-10-27 13:15:48,876 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:51,724 DEBUG Query successful
2024-10-27 13:15:51,791 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:52,592 DEBUG Query successful
2024-10-27 13:15:52,613 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:58,440 DEBUG Query successful
2024-10-27 13:15:58,462 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:02,868 DEBUG Query successful
2024-10-27 13:16:02,870 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:05,747 DEBUG Query successful
2024-10-27 13:16:06,176 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:07,254 DEBUG Query successful
2024-10-27 13:16:07,254 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:09,914 DEBUG Query successful
2024-10-27 13:16:10,366 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:10,879 DEBUG Query successful
2024-10-27 13:16:10,901 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:13,241 DEBUG Query successful
2024-10-27 13:16:13,311 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:13,664 DEBUG Query successful
2024-10-27 13:16:13,665 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:16,242 DEBUG Query successful
2024-10-27 13:16:16,694 DEBUG Query successful
2024-10-27 13:16:16,725 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:16,762 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:22,994 DEBUG Query successful
2024-10-27 13:16:23,060 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:27,406 DEBUG Query successful
2024-10-27 13:16:27,434 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:27,878 DEBUG Query successful
2024-10-27 13:16:27,880 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:30,075 DEBUG Query successful
2024-10-27 13:16:30,095 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:30,766 DEBUG Query successful
2024-10-27 13:16:31,276 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:34,627 DEBUG Query successful
2024-10-27 13:16:34,651 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:38,025 DEBUG Query successful
2024-10-27 13:16:38,089 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:46,200 DEBUG Query successful
2024-10-27 13:16:46,202 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:47,098 DEBUG Query successful
2024-10-27 13:16:47,099 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:49,263 DEBUG Query successful
2024-10-27 13:16:49,616 DEBUG Query successful
2024-10-27 13:16:49,815 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:50,150 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:50,542 DEBUG Query successful
2024-10-27 13:16:50,561 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:51,064 DEBUG Query successful
2024-10-27 13:16:51,065 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:54,235 DEBUG Query successful
2024-10-27 13:16:54,743 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:58,021 DEBUG Query successful
2024-10-27 13:16:58,086 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:58,196 DEBUG Query successful
2024-10-27 13:16:58,265 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:00,750 DEBUG Query successful
2024-10-27 13:17:00,815 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:04,978 DEBUG Query successful
2024-10-27 13:17:04,980 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:08,389 DEBUG Query successful
2024-10-27 13:17:08,937 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:10,255 DEBUG Query successful
2024-10-27 13:17:10,273 DEBUG Query successful
2024-10-27 13:17:10,283 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:10,293 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:13,566 DEBUG Query successful
2024-10-27 13:17:13,589 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:19,492 DEBUG Query successful
2024-10-27 13:17:19,557 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:32,372 DEBUG Query successful
2024-10-27 13:17:32,373 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:32,755 DEBUG Query successful
2024-10-27 13:17:32,757 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:33,312 DEBUG Query successful
2024-10-27 13:17:33,339 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:35,217 DEBUG Query successful
2024-10-27 13:17:35,627 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:35,636 DEBUG Query successful
2024-10-27 13:17:36,052 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:36,641 DEBUG Query successful
2024-10-27 13:17:36,642 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:39,710 DEBUG Query successful
2024-10-27 13:17:40,139 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:42,668 DEBUG Query successful
2024-10-27 13:17:42,737 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:42,872 DEBUG Query successful
2024-10-27 13:17:42,940 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:45,877 DEBUG Query successful
2024-10-27 13:17:45,948 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:49,043 DEBUG Query successful
2024-10-27 13:17:49,045 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:56,264 DEBUG Query successful
2024-10-27 13:17:56,287 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:56,465 DEBUG Query successful
2024-10-27 13:17:56,488 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:59,262 DEBUG Query successful
2024-10-27 13:17:59,284 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:00,918 DEBUG Query successful
2024-10-27 13:18:01,325 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:10,478 DEBUG Query successful
2024-10-27 13:18:10,541 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:20,929 DEBUG Query successful
2024-10-27 13:18:20,930 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:24,224 DEBUG Query successful
2024-10-27 13:18:24,245 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:24,362 DEBUG Query successful
2024-10-27 13:18:24,835 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:25,190 DEBUG Query successful
2024-10-27 13:18:25,190 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:26,000 DEBUG Query successful
2024-10-27 13:18:26,002 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:28,707 DEBUG Query successful
2024-10-27 13:18:29,201 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:29,385 DEBUG Query successful
2024-10-27 13:18:29,812 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:31,546 DEBUG Query successful
2024-10-27 13:18:31,614 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:36,324 DEBUG Query successful
2024-10-27 13:18:36,391 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:37,385 DEBUG Query successful
2024-10-27 13:18:37,451 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:45,796 DEBUG Query successful
2024-10-27 13:18:45,797 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:46,083 DEBUG Query successful
2024-10-27 13:18:46,105 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:49,145 DEBUG Query successful
2024-10-27 13:18:49,596 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:50,569 DEBUG Query successful
2024-10-27 13:18:50,592 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:51,912 DEBUG Query successful
2024-10-27 13:18:51,938 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:56,071 DEBUG Query successful
2024-10-27 13:18:56,137 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:12,906 DEBUG Query successful
2024-10-27 13:19:12,925 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:13,093 DEBUG Query successful
2024-10-27 13:19:13,093 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:14,922 DEBUG Query successful
2024-10-27 13:19:14,922 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:16,714 DEBUG Query successful
2024-10-27 13:19:17,183 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:18,435 DEBUG Query successful
2024-10-27 13:19:18,818 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:19,767 DEBUG Query successful
2024-10-27 13:19:19,767 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:22,698 DEBUG Query successful
2024-10-27 13:19:23,127 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:25,361 DEBUG Query successful
2024-10-27 13:19:25,422 DEBUG Query successful
2024-10-27 13:19:25,428 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:25,491 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:29,586 DEBUG Query successful
2024-10-27 13:19:29,653 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:40,727 DEBUG Query successful
2024-10-27 13:19:40,745 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:43,801 DEBUG Query successful
2024-10-27 13:19:43,802 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:45,867 DEBUG Query successful
2024-10-27 13:19:45,891 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:47,510 DEBUG Query successful
2024-10-27 13:19:47,869 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:47,992 DEBUG Query successful
2024-10-27 13:19:48,017 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:04,325 DEBUG Query successful
2024-10-27 13:20:04,326 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:07,823 DEBUG Query successful
2024-10-27 13:20:08,225 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:09,644 DEBUG Query successful
2024-10-27 13:20:09,707 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:14,120 DEBUG Query successful
2024-10-27 13:20:14,121 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:14,906 DEBUG Query successful
2024-10-27 13:20:14,972 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:17,381 DEBUG Query successful
2024-10-27 13:20:17,813 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:19,198 DEBUG Query successful
2024-10-27 13:20:19,200 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:22,755 DEBUG Query successful
2024-10-27 13:20:23,163 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:23,638 DEBUG Query successful
2024-10-27 13:20:23,705 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:27,592 DEBUG Query successful
2024-10-27 13:20:27,613 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:30,245 DEBUG Query successful
2024-10-27 13:20:30,315 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:40,086 DEBUG Query successful
2024-10-27 13:20:40,106 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:41,053 DEBUG Query successful
2024-10-27 13:20:41,073 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:48,364 DEBUG Query successful
2024-10-27 13:20:48,384 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:51,726 DEBUG Query successful
2024-10-27 13:20:51,727 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:56,632 DEBUG Query successful
2024-10-27 13:20:57,027 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:03,386 DEBUG Query successful
2024-10-27 13:21:03,450 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:10,044 DEBUG Query successful
2024-10-27 13:21:10,045 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:13,867 DEBUG Query successful
2024-10-27 13:21:14,265 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:14,676 DEBUG Query successful
2024-10-27 13:21:14,677 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:18,170 DEBUG Query successful
2024-10-27 13:21:18,604 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:20,033 DEBUG Query successful
2024-10-27 13:21:20,034 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:20,886 DEBUG Query successful
2024-10-27 13:21:20,955 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:22,576 DEBUG Query successful
2024-10-27 13:21:22,599 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:23,728 DEBUG Query successful
2024-10-27 13:21:24,149 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:24,521 DEBUG Query successful
2024-10-27 13:21:24,586 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:31,309 DEBUG Query successful
2024-10-27 13:21:31,373 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:37,702 DEBUG Query successful
2024-10-27 13:21:37,723 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:40,829 DEBUG Query successful
2024-10-27 13:21:40,849 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:46,722 DEBUG Query successful
2024-10-27 13:21:46,742 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:53,725 DEBUG Query successful
2024-10-27 13:21:53,727 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:58,104 DEBUG Query successful
2024-10-27 13:21:58,573 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:05,787 DEBUG Query successful
2024-10-27 13:22:05,852 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:09,380 DEBUG Query successful
2024-10-27 13:22:09,381 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:12,941 DEBUG Query successful
2024-10-27 13:22:13,360 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:15,400 DEBUG Query successful
2024-10-27 13:22:15,400 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:19,284 DEBUG Query successful
2024-10-27 13:22:19,378 DEBUG Query successful
2024-10-27 13:22:19,379 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:19,701 DEBUG Query successful
2024-10-27 13:22:19,709 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:19,766 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:22,447 DEBUG Query successful
2024-10-27 13:22:22,827 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:23,593 DEBUG Query successful
2024-10-27 13:22:23,613 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:26,295 DEBUG Query successful
2024-10-27 13:22:26,367 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:37,039 DEBUG Query successful
2024-10-27 13:22:37,061 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:42,672 DEBUG Query successful
2024-10-27 13:22:42,743 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:43,241 DEBUG Query successful
2024-10-27 13:22:43,267 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:58,607 DEBUG Query successful
2024-10-27 13:22:58,644 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:10,644 DEBUG Query successful
2024-10-27 13:23:10,645 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:14,108 DEBUG Query successful
2024-10-27 13:23:14,571 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:16,796 DEBUG Query successful
2024-10-27 13:23:16,797 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:18,596 DEBUG Query successful
2024-10-27 13:23:18,597 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:20,268 DEBUG Query successful
2024-10-27 13:23:20,347 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:20,467 DEBUG Query successful
2024-10-27 13:23:20,907 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:22,783 DEBUG Query successful
2024-10-27 13:23:23,244 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:27,487 DEBUG Query successful
2024-10-27 13:23:27,555 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:28,853 DEBUG Query successful
2024-10-27 13:23:28,853 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:29,305 DEBUG Query successful
2024-10-27 13:23:29,381 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:31,883 DEBUG Query successful
2024-10-27 13:23:32,336 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:37,694 DEBUG Query successful
2024-10-27 13:23:37,720 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:38,886 DEBUG Query successful
2024-10-27 13:23:38,954 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:47,316 DEBUG Query successful
2024-10-27 13:23:47,339 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:47,499 DEBUG Query successful
2024-10-27 13:23:47,521 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:57,348 DEBUG Query successful
2024-10-27 13:23:57,367 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:24:09,049 DEBUG Query successful
2024-10-27 13:24:09,050 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:50:57,113 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:50:57,133 DEBUG OpenAI client created
2024-10-27 13:50:57,133 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:50:57,133 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:50:57,153 DEBUG OpenAI client created
2024-10-27 13:50:57,153 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:51:11,577 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:51:19,124 DEBUG Query successful
2024-10-27 13:51:19,188 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:51:26,547 DEBUG Query successful
2024-10-27 13:51:26,567 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:51:34,612 DEBUG Query successful
2024-10-27 13:51:34,613 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:51:37,303 DEBUG Query successful
2024-10-27 13:51:37,867 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:51:45,209 DEBUG Query successful
2024-10-27 13:51:45,271 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:51:53,273 DEBUG Query successful
2024-10-27 13:51:53,294 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:52:03,313 DEBUG Query successful
2024-10-27 13:52:03,314 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:52:14,083 DEBUG Query successful
2024-10-27 13:52:14,450 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:52:21,019 DEBUG Query successful
2024-10-27 13:52:21,083 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:52:30,725 DEBUG Query successful
2024-10-27 13:52:30,745 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:52:44,134 DEBUG Query successful
2024-10-27 13:52:44,135 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:52:58,084 DEBUG Query successful
2024-10-27 13:52:58,534 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:53:05,450 DEBUG Query successful
2024-10-27 13:53:05,515 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:53:16,843 DEBUG Query successful
2024-10-27 13:53:16,866 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:53:33,442 DEBUG Query successful
2024-10-27 13:53:33,444 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:53:36,060 DEBUG Query successful
2024-10-27 13:53:36,489 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:53:44,752 DEBUG Query successful
2024-10-27 13:53:44,817 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:53:57,949 DEBUG Query successful
2024-10-27 13:53:57,967 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:54:16,844 DEBUG Query successful
2024-10-27 13:54:16,845 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:54:20,162 DEBUG Query successful
2024-10-27 13:54:20,523 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:54:28,847 DEBUG Query successful
2024-10-27 13:54:28,908 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:54:42,635 DEBUG Query successful
2024-10-27 13:54:42,654 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:55:01,018 DEBUG Query successful
2024-10-27 13:55:01,019 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:55:03,903 DEBUG Query successful
2024-10-27 13:55:04,291 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:55:10,645 DEBUG Query successful
2024-10-27 13:55:10,706 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:55:25,708 DEBUG Query successful
2024-10-27 13:55:25,729 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:55:45,470 DEBUG Query successful
2024-10-27 13:55:45,471 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:55:49,435 DEBUG Query successful
2024-10-27 13:55:49,774 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:55:56,362 DEBUG Query successful
2024-10-27 13:55:56,423 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:56:12,310 DEBUG Query successful
2024-10-27 13:56:12,329 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:56:44,280 DEBUG Query successful
2024-10-27 13:56:44,281 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:56:48,956 DEBUG Query successful
2024-10-27 13:56:49,296 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:57:02,800 DEBUG Query successful
2024-10-27 13:57:02,862 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:57:19,546 DEBUG Query successful
2024-10-27 13:57:19,566 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:57:44,447 DEBUG Query successful
2024-10-27 13:57:44,449 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:57:47,606 DEBUG Query successful
2024-10-27 13:57:47,931 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:57:54,766 DEBUG Query successful
2024-10-27 13:57:54,827 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:58:13,591 DEBUG Query successful
2024-10-27 13:58:13,611 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:58:47,843 DEBUG Query successful
2024-10-27 13:58:47,845 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:58:51,974 DEBUG Query successful
2024-10-27 13:58:52,313 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:59:03,496 DEBUG Query successful
2024-10-27 13:59:03,558 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:59:23,029 DEBUG Query successful
2024-10-27 13:59:23,048 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:59:55,827 DEBUG Query successful
2024-10-27 13:59:55,828 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:59:59,363 DEBUG Query successful
2024-10-27 13:59:59,716 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:00:06,166 DEBUG Query successful
2024-10-27 14:00:06,228 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:00:25,451 DEBUG Query successful
2024-10-27 14:00:25,471 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:00:59,662 DEBUG Query successful
2024-10-27 14:00:59,662 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:01:03,621 DEBUG Query successful
2024-10-27 14:01:03,971 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:01:10,419 DEBUG Query successful
2024-10-27 14:01:10,479 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:01:30,378 DEBUG Query successful
2024-10-27 14:01:30,398 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:02:08,849 DEBUG Query successful
2024-10-27 14:02:08,851 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:02:12,409 DEBUG Query successful
2024-10-27 14:02:12,753 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:02:29,755 DEBUG Query successful
2024-10-27 14:02:29,816 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:02:51,076 DEBUG Query successful
2024-10-27 14:02:51,094 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:03:26,336 DEBUG Query successful
2024-10-27 14:03:26,337 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:03:29,761 DEBUG Query successful
2024-10-27 14:03:30,111 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:03:38,610 DEBUG Query successful
2024-10-27 14:03:38,673 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-11-09 18:59:00,268 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 18:59:01,023 DEBUG Model set to: gpt-4o-mini
2024-11-09 18:59:12,794 DEBUG Query successful
2024-11-09 18:59:12,879 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 18:59:12,902 DEBUG Model set to: gpt-4o-mini
2024-11-09 18:59:25,515 DEBUG Query successful
2024-11-09 19:00:21,294 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:00:21,747 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:00:38,584 DEBUG Query successful
2024-11-09 19:00:38,670 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:00:38,693 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:00:57,278 DEBUG Query successful
2024-11-09 19:00:57,281 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:00:57,301 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:01:15,180 DEBUG Query successful
2024-11-09 19:01:15,184 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:01:15,207 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:01:20,868 DEBUG Query successful
2024-11-09 19:01:20,870 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:01:20,895 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:01:25,868 DEBUG Query successful
2024-11-09 19:01:25,869 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:01:25,896 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:01:30,435 DEBUG Query successful
2024-11-09 19:01:30,986 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:01:31,028 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:01:48,964 DEBUG Query successful
2024-11-09 19:01:49,049 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:01:49,072 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:02:01,766 DEBUG Query successful
2024-11-09 19:02:01,769 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:02:01,792 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:04:00,763 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:04:01,227 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:04:21,796 DEBUG Query successful
2024-11-09 19:04:21,876 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:04:21,901 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:04:32,876 DEBUG Query successful
2024-11-09 19:04:32,882 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:04:32,903 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:04:48,158 DEBUG Query successful
2024-11-09 19:04:48,162 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:04:48,187 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:04:58,616 DEBUG Query successful
2024-11-09 19:04:58,617 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:04:58,640 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:05:05,218 DEBUG Query successful
2024-11-09 19:05:05,220 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:05:05,241 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:05:20,242 DEBUG Query successful
2024-11-09 19:05:20,747 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:05:20,786 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:05:37,200 DEBUG Query successful
2024-11-09 19:05:37,277 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:05:37,302 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:05:50,498 DEBUG Query successful
2024-11-09 19:05:50,500 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:05:50,522 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:06:03,295 DEBUG Query successful
2024-11-09 19:06:03,298 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:06:03,325 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:06:07,468 DEBUG Query successful
2024-11-09 19:06:07,470 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:06:07,498 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:06:11,993 DEBUG Query successful
2024-11-09 19:06:11,994 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:06:12,017 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:06:16,477 DEBUG Query successful
2024-11-09 19:06:16,955 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:06:16,985 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:06:27,814 DEBUG Query successful
2024-11-09 19:06:27,890 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:06:27,912 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:06:38,648 DEBUG Query successful
2024-11-09 19:06:38,653 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:06:38,675 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:06:48,315 DEBUG Query successful
2024-11-09 19:06:48,322 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:06:48,343 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:06:53,336 DEBUG Query successful
2024-11-09 19:06:53,337 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:06:53,358 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:06:57,701 DEBUG Query successful
2024-11-09 19:06:57,702 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:06:57,737 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:07:03,297 DEBUG Query successful
2024-11-09 19:07:03,735 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:07:03,757 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:07:12,825 DEBUG Query successful
2024-11-09 19:07:12,904 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:07:12,927 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:07:23,343 DEBUG Query successful
2024-11-09 19:07:23,347 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:07:23,368 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:07:33,218 DEBUG Query successful
2024-11-09 19:07:33,222 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:07:33,245 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:07:43,930 DEBUG Query successful
2024-11-09 19:07:43,936 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:07:43,962 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:07:51,203 DEBUG Query successful
2024-11-09 19:07:51,204 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:07:51,226 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:07:57,601 DEBUG Query successful
2024-11-09 19:07:58,087 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:07:58,121 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:08:07,025 DEBUG Query successful
2024-11-09 19:08:07,107 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:08:07,128 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:08:17,602 DEBUG Query successful
2024-11-09 19:08:17,604 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:08:17,624 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:08:26,961 DEBUG Query successful
2024-11-09 19:08:26,964 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:08:26,989 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:08:34,321 DEBUG Query successful
2024-11-09 19:08:34,322 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:08:34,342 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:08:39,857 DEBUG Query successful
2024-11-09 19:08:39,859 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:08:39,885 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:08:46,651 DEBUG Query successful
2024-11-09 19:08:47,092 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:08:47,115 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:08:56,121 DEBUG Query successful
2024-11-09 19:08:56,197 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:08:56,219 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:09:08,660 DEBUG Query successful
2024-11-09 19:09:08,661 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:09:08,683 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:09:18,796 DEBUG Query successful
2024-11-09 19:09:18,800 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:09:18,822 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:09:24,010 DEBUG Query successful
2024-11-09 19:09:24,011 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:09:24,031 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:09:30,233 DEBUG Query successful
2024-11-09 19:09:30,235 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:09:30,256 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:09:34,470 DEBUG Query successful
2024-11-09 19:09:34,853 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:09:34,877 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:09:43,690 DEBUG Query successful
2024-11-09 19:09:43,761 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:09:43,781 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:09:54,573 DEBUG Query successful
2024-11-09 19:09:54,578 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:09:54,600 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:10:04,302 DEBUG Query successful
2024-11-09 19:10:04,308 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:10:04,327 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:10:08,946 DEBUG Query successful
2024-11-09 19:10:08,947 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:10:08,967 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:10:14,231 DEBUG Query successful
2024-11-09 19:10:14,232 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:10:14,252 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:10:18,566 DEBUG Query successful
2024-11-09 19:10:18,967 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:10:18,987 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:10:28,128 DEBUG Query successful
2024-11-09 19:10:28,205 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:10:28,256 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:10:39,127 DEBUG Query successful
2024-11-09 19:10:39,132 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:10:39,153 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:10:47,582 DEBUG Query successful
2024-11-09 19:10:47,585 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:10:47,608 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:10:54,220 DEBUG Query successful
2024-11-09 19:10:54,225 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:10:54,250 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:11:02,882 DEBUG Query successful
2024-11-09 19:11:02,883 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:11:02,909 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:11:08,451 DEBUG Query successful
2024-11-09 19:11:08,854 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:11:08,875 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:11:18,211 DEBUG Query successful
2024-11-09 19:11:18,289 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:11:18,313 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:11:28,751 DEBUG Query successful
2024-11-09 19:11:28,755 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:11:28,780 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:11:38,165 DEBUG Query successful
2024-11-09 19:11:38,166 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:11:38,187 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:11:43,576 DEBUG Query successful
2024-11-09 19:11:43,580 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:11:43,601 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:11:48,781 DEBUG Query successful
2024-11-09 19:11:48,786 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:11:48,805 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:11:53,830 DEBUG Query successful
2024-11-09 19:11:54,213 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:11:54,234 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:12:03,520 DEBUG Query successful
2024-11-09 19:12:03,607 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:12:03,633 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:12:14,826 DEBUG Query successful
2024-11-09 19:12:14,827 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:12:14,850 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:12:22,827 DEBUG Query successful
2024-11-09 19:12:22,830 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:12:22,851 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:25:00,639 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:25:01,108 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:25:10,958 DEBUG Query successful
2024-11-09 19:25:11,036 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:25:11,056 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:25:21,233 DEBUG Query successful
2024-11-09 19:25:21,238 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:25:21,260 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:25:30,889 DEBUG Query successful
2024-11-09 19:25:30,893 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:25:30,914 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:25:36,420 DEBUG Query successful
2024-11-09 19:25:36,980 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:25:37,008 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:25:46,949 DEBUG Query successful
2024-11-09 19:25:47,027 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:25:47,050 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:25:59,697 DEBUG Query successful
2024-11-09 19:25:59,702 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:25:59,723 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:26:08,395 DEBUG Query successful
2024-11-09 19:26:08,397 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:26:08,419 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:26:13,578 DEBUG Query successful
2024-11-09 19:26:14,168 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:26:14,199 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:26:23,145 DEBUG Query successful
2024-11-09 19:26:23,227 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:26:23,251 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:26:34,123 DEBUG Query successful
2024-11-09 19:26:34,125 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:26:34,146 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:26:44,455 DEBUG Query successful
2024-11-09 19:26:44,458 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:26:44,478 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:26:51,210 DEBUG Query successful
2024-11-09 19:26:51,623 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:26:51,645 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:27:00,611 DEBUG Query successful
2024-11-09 19:27:00,685 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:27:00,705 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:27:11,589 DEBUG Query successful
2024-11-09 19:27:11,594 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:27:11,614 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:27:21,050 DEBUG Query successful
2024-11-09 19:27:21,056 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:27:21,077 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:27:26,583 DEBUG Query successful
2024-11-09 19:27:26,978 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:27:27,031 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:27:36,339 DEBUG Query successful
2024-11-09 19:27:36,416 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:27:36,436 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:27:47,572 DEBUG Query successful
2024-11-09 19:27:47,577 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:27:47,599 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:27:56,868 DEBUG Query successful
2024-11-09 19:27:56,871 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:27:56,890 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:28:02,026 DEBUG Query successful
2024-11-09 19:28:02,461 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:28:02,489 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:28:10,642 DEBUG Query successful
2024-11-09 19:28:10,728 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:28:10,748 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:28:22,736 DEBUG Query successful
2024-11-09 19:28:22,740 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:28:22,760 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:28:31,497 DEBUG Query successful
2024-11-09 19:28:31,499 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:28:31,522 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:28:35,810 DEBUG Query successful
2024-11-09 19:28:36,221 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:28:36,245 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:28:44,527 DEBUG Query successful
2024-11-09 19:28:44,602 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:28:44,630 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:28:58,853 DEBUG Query successful
2024-11-09 19:28:58,859 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:28:58,880 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:29:08,025 DEBUG Query successful
2024-11-09 19:29:08,028 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:29:08,048 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:29:13,118 DEBUG Query successful
2024-11-09 19:29:13,749 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:29:13,805 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:29:22,444 DEBUG Query successful
2024-11-09 19:29:22,525 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:29:22,546 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:29:33,756 DEBUG Query successful
2024-11-09 19:29:33,762 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:29:33,783 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:29:45,731 DEBUG Query successful
2024-11-09 19:29:45,733 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:29:45,754 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:29:50,587 DEBUG Query successful
2024-11-09 19:29:51,056 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:29:51,083 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:30:00,856 DEBUG Query successful
2024-11-09 19:30:00,931 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:30:00,953 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:30:10,741 DEBUG Query successful
2024-11-09 19:30:10,743 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:30:10,764 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:30:20,592 DEBUG Query successful
2024-11-09 19:30:20,596 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:30:20,616 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:30:25,921 DEBUG Query successful
2024-11-09 19:30:26,337 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:30:26,358 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:30:34,968 DEBUG Query successful
2024-11-09 19:30:35,044 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:30:35,068 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:30:45,716 DEBUG Query successful
2024-11-09 19:30:45,717 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:30:45,740 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:30:54,425 DEBUG Query successful
2024-11-09 19:30:54,427 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:30:54,448 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:30:58,057 DEBUG Query successful
2024-11-09 19:30:58,465 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:30:58,487 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:31:07,785 DEBUG Query successful
2024-11-09 19:31:07,863 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:31:07,885 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:31:18,989 DEBUG Query successful
2024-11-09 19:31:18,993 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:31:19,020 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:31:28,392 DEBUG Query successful
2024-11-09 19:31:28,394 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:31:28,416 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:31:31,392 DEBUG Query successful
2024-11-09 19:31:31,796 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:31:31,816 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:31:41,324 DEBUG Query successful
2024-11-09 19:31:41,415 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:31:41,441 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:31:50,033 DEBUG Query successful
2024-11-09 19:31:50,039 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:31:50,058 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:31:59,296 DEBUG Query successful
2024-11-09 19:31:59,299 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:31:59,321 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:32:02,898 DEBUG Query successful
2024-11-09 19:32:03,320 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:32:03,341 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:32:11,133 DEBUG Query successful
2024-11-09 19:32:11,211 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:32:11,232 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:32:19,161 DEBUG Query successful
2024-11-09 19:32:19,166 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:32:19,188 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:32:28,889 DEBUG Query successful
2024-11-09 19:32:28,896 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:32:28,919 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:32:33,293 DEBUG Query successful
2024-11-09 19:32:33,730 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:32:33,752 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:32:42,542 DEBUG Query successful
2024-11-09 19:32:42,624 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:32:42,645 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:32:51,293 DEBUG Query successful
2024-11-09 19:32:51,295 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:32:51,316 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:32:59,688 DEBUG Query successful
2024-11-09 19:32:59,691 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:32:59,715 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:33:02,700 DEBUG Query successful
2024-11-09 19:33:03,092 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:33:03,116 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:33:13,016 DEBUG Query successful
2024-11-09 19:33:13,091 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:33:13,112 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:33:24,168 DEBUG Query successful
2024-11-09 19:33:24,173 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:33:24,193 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:33:32,890 DEBUG Query successful
2024-11-09 19:33:32,893 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:33:32,912 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:33:36,056 DEBUG Query successful
2024-11-09 19:33:36,448 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:33:36,468 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:33:46,668 DEBUG Query successful
2024-11-09 19:33:46,750 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:33:46,771 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:33:58,817 DEBUG Query successful
2024-11-09 19:33:58,822 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:33:58,841 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:34:08,422 DEBUG Query successful
2024-11-09 19:34:08,424 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:34:08,445 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:34:13,065 DEBUG Query successful
2024-11-09 19:34:13,480 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:34:13,501 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:34:22,386 DEBUG Query successful
2024-11-09 19:34:22,459 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:34:22,481 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:34:32,781 DEBUG Query successful
2024-11-09 19:34:32,786 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:34:32,806 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:34:41,028 DEBUG Query successful
2024-11-09 19:34:41,034 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:34:41,056 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:34:43,779 DEBUG Query successful
2024-11-09 19:34:44,177 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:34:44,197 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:34:53,061 DEBUG Query successful
2024-11-09 19:34:53,137 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:34:53,161 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:35:05,067 DEBUG Query successful
2024-11-09 19:35:05,073 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:35:05,094 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:35:15,455 DEBUG Query successful
2024-11-09 19:35:15,457 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:35:15,481 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:35:18,324 DEBUG Query successful
2024-11-09 19:35:18,700 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:35:18,721 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:35:28,977 DEBUG Query successful
2024-11-09 19:35:29,063 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:35:29,087 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:35:40,578 DEBUG Query successful
2024-11-09 19:35:40,584 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:35:40,606 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:35:50,283 DEBUG Query successful
2024-11-09 19:35:50,284 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:35:50,305 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:35:53,456 DEBUG Query successful
2024-11-09 19:35:53,837 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:35:53,860 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:36:03,192 DEBUG Query successful
2024-11-09 19:36:03,268 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:36:03,316 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:36:12,640 DEBUG Query successful
2024-11-09 19:36:12,645 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:36:12,665 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:36:20,997 DEBUG Query successful
2024-11-09 19:36:20,997 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:36:21,019 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:36:24,505 DEBUG Query successful
2024-11-09 19:36:24,893 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:36:24,914 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:36:37,301 DEBUG Query successful
2024-11-09 19:36:37,374 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:36:37,395 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:36:52,932 DEBUG Query successful
2024-11-09 19:36:52,937 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:36:52,960 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:37:01,706 DEBUG Query successful
2024-11-09 19:37:01,711 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:37:01,735 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:37:05,594 DEBUG Query successful
2024-11-09 19:37:05,994 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:37:06,017 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:37:14,094 DEBUG Query successful
2024-11-09 19:37:14,169 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:37:14,190 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:37:24,076 DEBUG Query successful
2024-11-09 19:37:24,081 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:37:24,100 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:37:35,320 DEBUG Query successful
2024-11-09 19:37:35,321 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:37:35,342 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:37:39,090 DEBUG Query successful
2024-11-09 19:37:39,470 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:37:39,494 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:37:49,289 DEBUG Query successful
2024-11-09 19:37:49,366 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:37:49,387 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:38:00,971 DEBUG Query successful
2024-11-09 19:38:00,976 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:38:00,996 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:38:11,336 DEBUG Query successful
2024-11-09 19:38:11,337 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:38:11,357 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:38:16,040 DEBUG Query successful
2024-11-09 19:38:16,457 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:38:16,479 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:38:24,985 DEBUG Query successful
2024-11-09 19:38:25,057 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:38:25,076 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:38:33,426 DEBUG Query successful
2024-11-09 19:38:33,427 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:38:33,449 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:38:42,259 DEBUG Query successful
2024-11-09 19:38:42,261 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:38:42,282 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:38:45,060 DEBUG Query successful
2024-11-09 19:38:45,427 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:38:45,450 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:38:56,603 DEBUG Query successful
2024-11-09 19:38:56,675 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:38:56,696 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:39:04,003 DEBUG Query successful
2024-11-09 19:39:04,007 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:39:04,027 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:39:12,263 DEBUG Query successful
2024-11-09 19:39:12,264 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:39:12,283 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:39:16,951 DEBUG Query successful
2024-11-09 19:39:17,351 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:39:17,372 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:39:25,555 DEBUG Query successful
2024-11-09 19:39:25,630 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:39:25,652 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:39:34,596 DEBUG Query successful
2024-11-09 19:39:34,598 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:39:34,620 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:39:48,145 DEBUG Query successful
2024-11-09 19:39:48,147 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:39:48,169 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:39:50,824 DEBUG Query successful
2024-11-09 19:39:51,203 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:39:51,225 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:39:59,973 DEBUG Query successful
2024-11-09 19:40:00,047 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:40:00,067 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:40:09,081 DEBUG Query successful
2024-11-09 19:40:09,086 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:40:09,107 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:40:19,815 DEBUG Query successful
2024-11-09 19:40:19,821 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:40:19,848 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:40:23,212 DEBUG Query successful
2024-11-09 19:40:23,609 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:40:23,632 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:40:31,236 DEBUG Query successful
2024-11-09 19:40:31,310 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:40:31,330 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:40:39,297 DEBUG Query successful
2024-11-09 19:40:39,302 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:40:39,324 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:40:50,251 DEBUG Query successful
2024-11-09 19:40:50,252 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:40:50,274 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:40:53,342 DEBUG Query successful
2024-11-09 19:40:53,749 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:40:53,769 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:41:01,617 DEBUG Query successful
2024-11-09 19:41:01,691 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:41:01,712 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:41:08,335 DEBUG Query successful
2024-11-09 19:41:08,337 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:41:08,357 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:41:17,062 DEBUG Query successful
2024-11-09 19:41:17,063 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:41:17,104 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:41:20,285 DEBUG Query successful
2024-11-09 19:41:20,666 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:41:20,689 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:41:27,908 DEBUG Query successful
2024-11-09 19:41:27,977 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:41:27,997 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:41:37,693 DEBUG Query successful
2024-11-09 19:41:37,694 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:41:37,716 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:41:49,614 DEBUG Query successful
2024-11-09 19:41:49,616 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:41:49,638 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:41:53,972 DEBUG Query successful
2024-11-09 19:41:54,381 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:41:54,401 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:42:01,790 DEBUG Query successful
2024-11-09 19:42:01,857 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:42:01,877 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:42:09,364 DEBUG Query successful
2024-11-09 19:42:09,366 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:42:09,387 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:42:19,059 DEBUG Query successful
2024-11-09 19:42:19,067 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:42:19,087 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:42:22,220 DEBUG Query successful
2024-11-09 19:42:22,580 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:42:22,603 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:42:30,844 DEBUG Query successful
2024-11-09 19:42:30,915 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:42:30,937 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:42:37,500 DEBUG Query successful
2024-11-09 19:42:37,505 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:42:37,527 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:42:49,071 DEBUG Query successful
2024-11-09 19:42:49,074 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:42:49,094 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:42:54,187 DEBUG Query successful
2024-11-09 19:42:54,602 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:42:54,623 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:43:02,738 DEBUG Query successful
2024-11-09 19:43:02,820 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:43:02,840 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:43:10,761 DEBUG Query successful
2024-11-09 19:43:10,762 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:43:10,784 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:43:20,542 DEBUG Query successful
2024-11-09 19:43:20,545 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:43:20,567 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:43:24,321 DEBUG Query successful
2024-11-09 19:50:25,992 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:50:26,543 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:50:35,607 DEBUG Query successful
2024-11-09 19:50:35,686 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:50:35,710 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:50:45,066 DEBUG Query successful
2024-11-09 19:50:45,069 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:50:45,092 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:50:58,057 DEBUG Query successful
2024-11-09 19:50:58,060 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:50:58,089 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:51:02,978 DEBUG Query successful
2024-11-09 19:51:02,980 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:51:03,003 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:51:06,029 DEBUG Query successful
2024-11-09 19:51:06,032 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:51:06,055 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:51:09,546 DEBUG Query successful
2024-11-09 19:51:10,054 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:51:10,082 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:51:20,089 DEBUG Query successful
2024-11-09 19:51:20,167 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:51:20,188 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:51:36,872 DEBUG Query successful
2024-11-09 19:51:36,873 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:51:36,895 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:51:50,103 DEBUG Query successful
2024-11-09 19:51:50,106 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:51:50,138 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:51:57,982 DEBUG Query successful
2024-11-09 19:54:52,610 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:54:53,151 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:55:10,495 DEBUG Query successful
2024-11-09 19:55:10,578 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:55:10,601 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:55:23,263 DEBUG Query successful
2024-11-09 19:55:23,270 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:55:23,295 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:55:37,227 DEBUG Query successful
2024-11-09 19:55:37,229 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:55:37,250 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:55:40,556 DEBUG Query successful
2024-11-09 19:55:40,561 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 19:55:40,584 DEBUG Model set to: gpt-4o-mini
2024-11-09 19:55:46,260 DEBUG Query successful
2024-11-09 20:07:50,314 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:07:50,999 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:08:02,569 DEBUG Query successful
2024-11-09 20:08:02,647 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:08:02,669 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:08:14,168 DEBUG Query successful
2024-11-09 20:08:14,170 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:08:14,191 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:08:25,520 DEBUG Query successful
2024-11-09 20:08:25,522 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:08:25,543 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:08:28,265 DEBUG Query successful
2024-11-09 20:08:28,797 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:08:28,828 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:08:37,955 DEBUG Query successful
2024-11-09 20:08:38,036 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:08:38,070 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:08:52,105 DEBUG Query successful
2024-11-09 20:08:52,110 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:08:52,129 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:09:01,284 DEBUG Query successful
2024-11-09 20:09:01,286 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:09:01,310 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:09:05,452 DEBUG Query successful
2024-11-09 20:09:06,018 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:09:06,044 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:09:14,987 DEBUG Query successful
2024-11-09 20:09:15,057 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:09:15,081 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:09:27,809 DEBUG Query successful
2024-11-09 20:09:27,814 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:09:27,833 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:09:37,651 DEBUG Query successful
2024-11-09 20:09:37,656 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:09:37,679 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:09:40,610 DEBUG Query successful
2024-11-09 20:09:41,090 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:09:41,117 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:09:50,580 DEBUG Query successful
2024-11-09 20:09:50,659 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:09:50,682 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:10:02,997 DEBUG Query successful
2024-11-09 20:10:03,001 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:10:03,022 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:10:11,599 DEBUG Query successful
2024-11-09 20:10:11,602 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:10:11,624 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:10:15,267 DEBUG Query successful
2024-11-09 20:10:15,738 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:10:15,764 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:10:25,599 DEBUG Query successful
2024-11-09 20:10:25,715 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:10:25,745 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:10:36,838 DEBUG Query successful
2024-11-09 20:10:36,840 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:10:36,866 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:10:46,510 DEBUG Query successful
2024-11-09 20:10:46,516 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:10:46,539 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:10:49,358 DEBUG Query successful
2024-11-09 20:10:49,952 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:10:49,977 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:10:58,321 DEBUG Query successful
2024-11-09 20:10:58,398 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:10:58,420 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:11:10,249 DEBUG Query successful
2024-11-09 20:11:10,254 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:11:10,277 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:11:19,427 DEBUG Query successful
2024-11-09 20:11:19,434 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:11:19,455 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:11:22,908 DEBUG Query successful
2024-11-09 20:11:23,548 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:11:23,572 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:11:32,294 DEBUG Query successful
2024-11-09 20:11:32,378 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:11:32,403 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:11:43,454 DEBUG Query successful
2024-11-09 20:11:43,456 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:11:43,481 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:11:54,111 DEBUG Query successful
2024-11-09 20:11:54,116 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:11:54,204 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:11:56,993 DEBUG Query successful
2024-11-09 20:11:57,524 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:11:57,547 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:12:06,609 DEBUG Query successful
2024-11-09 20:12:06,689 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:12:06,712 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:12:18,649 DEBUG Query successful
2024-11-09 20:12:18,654 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:12:18,678 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:12:29,191 DEBUG Query successful
2024-11-09 20:12:29,194 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:12:29,219 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:12:31,945 DEBUG Query successful
2024-11-09 20:12:32,488 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:12:32,513 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:12:41,219 DEBUG Query successful
2024-11-09 20:12:41,317 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:12:41,475 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:12:52,373 DEBUG Query successful
2024-11-09 20:12:52,375 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:12:52,397 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:13:01,420 DEBUG Query successful
2024-11-09 20:13:01,422 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:13:01,447 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:13:04,500 DEBUG Query successful
2024-11-09 20:13:04,987 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:13:05,010 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:13:14,260 DEBUG Query successful
2024-11-09 20:13:14,338 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:13:14,362 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:13:22,949 DEBUG Query successful
2024-11-09 20:13:22,952 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:13:22,976 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:13:31,972 DEBUG Query successful
2024-11-09 20:13:31,979 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:13:32,002 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:13:35,013 DEBUG Query successful
2024-11-09 20:13:35,549 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:13:35,608 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:13:45,114 DEBUG Query successful
2024-11-09 20:13:45,191 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:13:45,213 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:13:54,877 DEBUG Query successful
2024-11-09 20:13:54,879 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:13:54,899 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:14:03,520 DEBUG Query successful
2024-11-09 20:14:03,543 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:14:03,567 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:14:06,389 DEBUG Query successful
2024-11-09 20:14:06,830 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:14:06,855 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:14:16,687 DEBUG Query successful
2024-11-09 20:14:16,762 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:14:16,783 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:14:25,170 DEBUG Query successful
2024-11-09 20:14:25,171 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:14:25,193 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:14:32,434 DEBUG Query successful
2024-11-09 20:14:32,440 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:14:32,462 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:14:34,607 DEBUG Query successful
2024-11-09 20:14:35,114 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:14:35,142 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:14:43,836 DEBUG Query successful
2024-11-09 20:14:43,915 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:14:43,939 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:14:52,226 DEBUG Query successful
2024-11-09 20:14:52,231 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:14:52,254 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:15:00,876 DEBUG Query successful
2024-11-09 20:15:00,882 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:15:00,905 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:15:02,861 DEBUG Query successful
2024-11-09 20:15:03,328 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:15:03,354 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:15:13,228 DEBUG Query successful
2024-11-09 20:15:13,309 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:15:13,335 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:15:22,340 DEBUG Query successful
2024-11-09 20:15:22,344 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:15:22,367 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:15:30,992 DEBUG Query successful
2024-11-09 20:15:30,999 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:15:31,023 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:15:33,043 DEBUG Query successful
2024-11-09 20:15:33,703 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:15:33,724 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:30:14,654 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:30:15,130 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:30:25,688 DEBUG Query successful
2024-11-09 20:30:25,771 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:30:25,794 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:30:39,823 DEBUG Query successful
2024-11-09 20:30:39,828 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:30:39,850 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:30:49,603 DEBUG Query successful
2024-11-09 20:30:49,610 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:30:49,631 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:30:56,540 DEBUG Query successful
2024-11-09 20:30:57,118 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:30:57,143 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:31:07,035 DEBUG Query successful
2024-11-09 20:31:07,120 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:31:07,154 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:31:20,342 DEBUG Query successful
2024-11-09 20:31:20,346 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:31:20,371 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:31:32,409 DEBUG Query successful
2024-11-09 20:31:32,413 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:31:32,438 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:31:40,574 DEBUG Query successful
2024-11-09 20:31:41,074 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:31:41,101 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:31:50,697 DEBUG Query successful
2024-11-09 20:31:50,779 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:31:50,805 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:32:04,286 DEBUG Query successful
2024-11-09 20:32:04,290 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:32:04,312 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:32:18,233 DEBUG Query successful
2024-11-09 20:32:18,236 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:32:18,258 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:32:22,855 DEBUG Query successful
2024-11-09 20:32:23,573 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:32:23,599 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:32:33,176 DEBUG Query successful
2024-11-09 20:32:33,260 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:32:33,285 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:33:05,906 DEBUG Query successful
2024-11-09 20:33:05,911 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:33:05,934 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:33:15,073 DEBUG Query successful
2024-11-09 20:33:15,075 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:33:15,097 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:33:20,587 DEBUG Query successful
2024-11-09 20:33:21,136 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:33:21,163 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:33:31,115 DEBUG Query successful
2024-11-09 20:33:31,193 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:33:31,215 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:33:42,853 DEBUG Query successful
2024-11-09 20:33:42,857 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:33:42,878 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:33:50,877 DEBUG Query successful
2024-11-09 20:33:50,879 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:33:50,901 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:33:57,009 DEBUG Query successful
2024-11-09 20:33:57,633 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:33:57,664 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:34:07,466 DEBUG Query successful
2024-11-09 20:34:07,545 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:34:07,567 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:34:20,039 DEBUG Query successful
2024-11-09 20:34:20,045 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:34:20,065 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:34:28,906 DEBUG Query successful
2024-11-09 20:34:28,908 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:34:28,930 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:34:38,536 DEBUG Query successful
2024-11-09 20:34:39,037 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:34:39,067 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:34:50,049 DEBUG Query successful
2024-11-09 20:34:50,130 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:34:50,153 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:35:02,581 DEBUG Query successful
2024-11-09 20:35:02,582 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:35:02,604 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:35:10,862 DEBUG Query successful
2024-11-09 20:35:10,864 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:35:10,885 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:35:16,874 DEBUG Query successful
2024-11-09 20:35:17,405 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:35:17,431 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:35:27,231 DEBUG Query successful
2024-11-09 20:35:27,308 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:35:27,331 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:35:39,054 DEBUG Query successful
2024-11-09 20:35:39,059 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:35:39,081 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:35:47,506 DEBUG Query successful
2024-11-09 20:35:47,508 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:35:47,530 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:35:55,537 DEBUG Query successful
2024-11-09 20:35:56,038 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:35:56,061 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:36:06,306 DEBUG Query successful
2024-11-09 20:36:06,388 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:36:06,409 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:36:19,020 DEBUG Query successful
2024-11-09 20:36:19,025 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:36:19,048 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:36:28,773 DEBUG Query successful
2024-11-09 20:36:28,779 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:36:28,801 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:36:35,305 DEBUG Query successful
2024-11-09 20:36:35,838 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:36:35,860 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:36:45,339 DEBUG Query successful
2024-11-09 20:36:45,416 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:36:45,438 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:36:58,207 DEBUG Query successful
2024-11-09 20:36:58,212 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:36:58,234 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:37:06,273 DEBUG Query successful
2024-11-09 20:37:06,277 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:37:06,297 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:37:11,211 DEBUG Query successful
2024-11-09 20:37:11,687 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:37:11,708 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:37:21,522 DEBUG Query successful
2024-11-09 20:37:21,596 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:37:21,617 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:37:36,141 DEBUG Query successful
2024-11-09 20:37:36,147 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:37:36,168 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:37:48,086 DEBUG Query successful
2024-11-09 20:37:48,090 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:37:48,111 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:37:54,683 DEBUG Query successful
2024-11-09 20:37:55,129 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:37:55,151 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:38:04,537 DEBUG Query successful
2024-11-09 20:38:04,614 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:38:04,636 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:38:16,653 DEBUG Query successful
2024-11-09 20:38:16,658 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:38:16,678 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:38:29,465 DEBUG Query successful
2024-11-09 20:38:29,469 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:38:29,488 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:38:33,214 DEBUG Query successful
2024-11-09 20:38:33,764 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:38:33,789 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:38:42,133 DEBUG Query successful
2024-11-09 20:38:42,208 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:38:42,229 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:38:54,863 DEBUG Query successful
2024-11-09 20:38:54,868 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:38:54,890 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:39:02,665 DEBUG Query successful
2024-11-09 20:39:02,673 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:39:02,702 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:39:05,677 DEBUG Query successful
2024-11-09 20:39:06,124 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:39:06,146 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:39:14,666 DEBUG Query successful
2024-11-09 20:39:14,743 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:39:14,764 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:39:22,861 DEBUG Query successful
2024-11-09 20:39:22,862 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:39:22,883 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:39:30,264 DEBUG Query successful
2024-11-09 20:39:30,265 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:39:30,288 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:39:38,811 DEBUG Query successful
2024-11-09 20:39:39,256 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:39:39,281 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:39:49,087 DEBUG Query successful
2024-11-09 20:39:49,168 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:39:49,189 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:39:58,042 DEBUG Query successful
2024-11-09 20:39:58,048 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:39:58,069 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:40:06,976 DEBUG Query successful
2024-11-09 20:40:06,978 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:40:07,001 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:40:10,762 DEBUG Query successful
2024-11-09 20:40:11,386 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:40:11,410 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:40:22,755 DEBUG Query successful
2024-11-09 20:40:22,832 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:40:22,853 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:40:33,871 DEBUG Query successful
2024-11-09 20:40:33,872 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:40:33,895 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:40:42,448 DEBUG Query successful
2024-11-09 20:40:42,451 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:40:42,473 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:40:45,969 DEBUG Query successful
2024-11-09 20:40:46,399 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:40:46,422 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:40:55,362 DEBUG Query successful
2024-11-09 20:40:55,440 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:40:55,461 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:41:04,984 DEBUG Query successful
2024-11-09 20:41:04,985 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:41:05,008 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:41:14,060 DEBUG Query successful
2024-11-09 20:41:14,063 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:41:14,087 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:41:19,080 DEBUG Query successful
2024-11-09 20:41:19,601 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:41:19,625 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:41:28,822 DEBUG Query successful
2024-11-09 20:41:28,902 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:41:28,922 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:41:38,051 DEBUG Query successful
2024-11-09 20:41:38,056 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:41:38,076 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:41:47,613 DEBUG Query successful
2024-11-09 20:41:47,616 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:41:47,637 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:41:50,897 DEBUG Query successful
2024-11-09 20:41:51,332 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:41:51,354 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:42:00,361 DEBUG Query successful
2024-11-09 20:42:00,444 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:42:00,466 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:42:11,387 DEBUG Query successful
2024-11-09 20:42:11,393 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:42:11,415 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:42:19,465 DEBUG Query successful
2024-11-09 20:42:19,468 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:42:19,489 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:42:25,235 DEBUG Query successful
2024-11-09 20:42:25,822 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:42:25,849 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:42:35,348 DEBUG Query successful
2024-11-09 20:42:35,431 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:42:35,479 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:42:45,409 DEBUG Query successful
2024-11-09 20:42:45,409 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:42:45,431 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:42:52,962 DEBUG Query successful
2024-11-09 20:42:52,964 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:42:52,986 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:42:58,697 DEBUG Query successful
2024-11-09 20:42:59,138 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:42:59,161 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:43:09,436 DEBUG Query successful
2024-11-09 20:43:09,517 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:43:09,539 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:43:19,185 DEBUG Query successful
2024-11-09 20:43:19,186 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:43:19,207 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:43:26,763 DEBUG Query successful
2024-11-09 20:43:26,771 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:43:26,794 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:43:29,619 DEBUG Query successful
2024-11-09 20:43:30,038 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:43:30,061 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:43:39,382 DEBUG Query successful
2024-11-09 20:43:39,458 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:43:39,479 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:43:49,457 DEBUG Query successful
2024-11-09 20:43:49,461 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:43:49,481 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:43:58,027 DEBUG Query successful
2024-11-09 20:43:58,029 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:43:58,054 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:44:00,832 DEBUG Query successful
2024-11-09 20:44:01,427 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:44:01,459 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:44:08,594 DEBUG Query successful
2024-11-09 20:44:08,674 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:44:08,696 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:44:17,295 DEBUG Query successful
2024-11-09 20:44:17,296 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:44:17,318 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:44:27,221 DEBUG Query successful
2024-11-09 20:44:27,224 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:44:27,245 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:44:29,309 DEBUG Query successful
2024-11-09 20:44:29,809 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:44:29,837 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:44:37,877 DEBUG Query successful
2024-11-09 20:44:37,959 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:44:37,981 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:44:47,857 DEBUG Query successful
2024-11-09 20:44:47,858 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:44:47,880 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:44:55,787 DEBUG Query successful
2024-11-09 20:44:55,790 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:44:55,814 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:45:03,085 DEBUG Query successful
2024-11-09 20:45:03,539 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:45:03,561 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:45:10,605 DEBUG Query successful
2024-11-09 20:45:10,682 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:45:10,704 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:45:20,611 DEBUG Query successful
2024-11-09 20:45:20,612 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:45:20,634 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:45:28,618 DEBUG Query successful
2024-11-09 20:45:28,622 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:45:28,647 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:45:35,018 DEBUG Query successful
2024-11-09 20:45:35,436 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:45:35,466 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:45:43,535 DEBUG Query successful
2024-11-09 20:45:43,613 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:45:43,638 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:45:53,204 DEBUG Query successful
2024-11-09 20:45:53,208 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:45:53,232 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:45:59,928 DEBUG Query successful
2024-11-09 20:45:59,931 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:45:59,952 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:46:02,407 DEBUG Query successful
2024-11-09 20:46:02,962 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:46:02,985 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:46:10,325 DEBUG Query successful
2024-11-09 20:46:10,403 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:46:10,425 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:46:19,082 DEBUG Query successful
2024-11-09 20:46:19,083 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:46:19,104 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:46:29,770 DEBUG Query successful
2024-11-09 20:46:29,772 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 20:46:29,793 DEBUG Model set to: gpt-4o-mini
2024-11-09 20:46:32,204 DEBUG Query successful
2024-11-09 22:06:24,223 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:24,373 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:25,346 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:26,016 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:26,664 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:27,264 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:31,736 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:33,073 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:37,124 DEBUG Query successful
2024-11-09 22:06:37,227 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:37,256 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:37,819 DEBUG Query successful
2024-11-09 22:06:37,966 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:38,037 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:39,742 DEBUG Query successful
2024-11-09 22:06:39,852 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:39,894 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:41,057 DEBUG Query successful
2024-11-09 22:06:41,058 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:41,088 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:41,235 DEBUG Query successful
2024-11-09 22:06:41,237 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:41,275 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:42,221 DEBUG Query successful
2024-11-09 22:06:42,356 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:42,392 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:43,206 DEBUG Query successful
2024-11-09 22:06:43,215 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:43,285 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:46,318 DEBUG Query successful
2024-11-09 22:06:46,320 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:46,365 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:49,529 DEBUG Query successful
2024-11-09 22:06:49,533 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:49,565 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:50,078 DEBUG Query successful
2024-11-09 22:06:50,082 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:50,110 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:52,617 DEBUG Query successful
2024-11-09 22:06:52,623 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:52,645 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:54,041 DEBUG Query successful
2024-11-09 22:06:54,044 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:54,065 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:55,041 DEBUG Query successful
2024-11-09 22:06:55,522 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:55,550 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:55,747 DEBUG Query successful
2024-11-09 22:06:56,435 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:56,469 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:56,589 DEBUG Query successful
2024-11-09 22:06:57,069 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:57,093 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:06:57,850 DEBUG Query successful
2024-11-09 22:06:58,527 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:06:58,554 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:04,668 DEBUG Query successful
2024-11-09 22:07:04,763 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:04,840 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:05,309 DEBUG Query successful
2024-11-09 22:07:05,395 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:05,426 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:05,548 DEBUG Query successful
2024-11-09 22:07:05,645 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:05,690 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:06,982 DEBUG Query successful
2024-11-09 22:07:07,079 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:07,116 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:09,251 DEBUG Query successful
2024-11-09 22:07:09,252 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:09,272 DEBUG Query successful
2024-11-09 22:07:09,273 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:09,279 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:09,297 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:10,228 DEBUG Query successful
2024-11-09 22:07:10,232 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:10,253 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:11,158 DEBUG Query successful
2024-11-09 22:07:11,160 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:11,186 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:17,207 DEBUG Query successful
2024-11-09 22:07:17,215 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:17,244 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:18,248 DEBUG Query successful
2024-11-09 22:07:18,250 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:18,277 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:18,793 DEBUG Query successful
2024-11-09 22:07:18,797 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:18,826 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:19,239 DEBUG Query successful
2024-11-09 22:07:19,244 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:19,284 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:21,040 DEBUG Query successful
2024-11-09 22:07:21,479 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:21,513 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:22,497 DEBUG Query successful
2024-11-09 22:07:22,916 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:22,938 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:24,474 DEBUG Query successful
2024-11-09 22:07:24,856 DEBUG Query successful
2024-11-09 22:07:24,948 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:24,974 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:25,361 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:25,383 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:31,000 DEBUG Query successful
2024-11-09 22:07:31,088 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:31,113 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:31,183 DEBUG Query successful
2024-11-09 22:07:31,279 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:31,312 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:34,805 DEBUG Query successful
2024-11-09 22:07:34,898 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:34,922 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:34,978 DEBUG Query successful
2024-11-09 22:07:35,071 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:35,076 DEBUG Query successful
2024-11-09 22:07:35,078 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:35,103 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:35,109 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:35,335 DEBUG Query successful
2024-11-09 22:07:35,338 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:35,371 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:38,225 DEBUG Query successful
2024-11-09 22:07:38,226 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:38,259 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:38,377 DEBUG Query successful
2024-11-09 22:07:38,378 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:38,400 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:42,406 DEBUG Query successful
2024-11-09 22:07:42,412 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:42,447 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:43,197 DEBUG Query successful
2024-11-09 22:07:43,200 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:43,228 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:46,711 DEBUG Query successful
2024-11-09 22:07:46,715 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:46,737 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:46,950 DEBUG Query successful
2024-11-09 22:07:46,956 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:46,983 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:46,984 DEBUG Query successful
2024-11-09 22:07:47,503 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:47,533 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:47,946 DEBUG Query successful
2024-11-09 22:07:48,443 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:48,468 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:50,625 DEBUG Query successful
2024-11-09 22:07:50,847 DEBUG Query successful
2024-11-09 22:07:51,237 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:51,276 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:51,434 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:51,460 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:56,112 DEBUG Query successful
2024-11-09 22:07:56,198 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:56,221 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:57,557 DEBUG Query successful
2024-11-09 22:07:57,649 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:57,677 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:07:58,955 DEBUG Query successful
2024-11-09 22:07:59,045 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:07:59,084 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:00,066 DEBUG Query successful
2024-11-09 22:08:00,153 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:00,176 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:02,609 DEBUG Query successful
2024-11-09 22:08:02,610 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:02,632 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:04,886 DEBUG Query successful
2024-11-09 22:08:04,888 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:04,911 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:08,876 DEBUG Query successful
2024-11-09 22:08:08,877 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:08,899 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:09,534 DEBUG Query successful
2024-11-09 22:08:09,536 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:09,561 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:11,575 DEBUG Query successful
2024-11-09 22:08:11,578 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:11,605 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:12,116 DEBUG Query successful
2024-11-09 22:08:12,118 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:12,149 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:15,660 DEBUG Query successful
2024-11-09 22:08:15,985 DEBUG Query successful
2024-11-09 22:08:16,116 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:16,146 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:16,723 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:16,749 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:17,754 DEBUG Query successful
2024-11-09 22:08:17,758 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:17,814 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:19,104 DEBUG Query successful
2024-11-09 22:08:19,108 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:19,130 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:21,839 DEBUG Query successful
2024-11-09 22:08:22,305 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:22,339 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:22,891 DEBUG Query successful
2024-11-09 22:08:23,479 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:23,515 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:26,412 DEBUG Query successful
2024-11-09 22:08:26,498 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:26,522 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:27,576 DEBUG Query successful
2024-11-09 22:08:27,659 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:27,690 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:30,279 DEBUG Query successful
2024-11-09 22:08:30,281 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:30,302 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:30,867 DEBUG Query successful
2024-11-09 22:08:30,868 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:30,894 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:31,529 DEBUG Query successful
2024-11-09 22:08:31,623 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:31,653 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:31,887 DEBUG Query successful
2024-11-09 22:08:31,978 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:32,002 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:35,162 DEBUG Query successful
2024-11-09 22:08:35,163 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:35,185 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:37,554 DEBUG Query successful
2024-11-09 22:08:37,556 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:37,585 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:38,219 DEBUG Query successful
2024-11-09 22:08:38,222 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:38,255 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:38,314 DEBUG Query successful
2024-11-09 22:08:38,316 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:38,349 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:42,503 DEBUG Query successful
2024-11-09 22:08:43,034 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:43,060 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:43,812 DEBUG Query successful
2024-11-09 22:08:44,226 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:44,253 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:45,986 DEBUG Query successful
2024-11-09 22:08:45,990 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:46,005 DEBUG Query successful
2024-11-09 22:08:46,008 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:46,016 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:46,034 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:48,846 DEBUG Query successful
2024-11-09 22:08:49,485 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:49,512 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:50,053 DEBUG Query successful
2024-11-09 22:08:50,496 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:50,534 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:51,950 DEBUG Query successful
2024-11-09 22:08:52,041 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:52,072 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:54,420 DEBUG Query successful
2024-11-09 22:08:54,510 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:54,539 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:56,415 DEBUG Query successful
2024-11-09 22:08:56,417 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:56,440 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:58,638 DEBUG Query successful
2024-11-09 22:08:58,725 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:58,748 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:59,476 DEBUG Query successful
2024-11-09 22:08:59,539 DEBUG Query successful
2024-11-09 22:08:59,542 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:59,564 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:08:59,575 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:08:59,587 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:03,559 DEBUG Query successful
2024-11-09 22:09:03,560 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:03,582 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:04,087 DEBUG Query successful
2024-11-09 22:09:04,089 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:04,118 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:04,482 DEBUG Query successful
2024-11-09 22:09:04,483 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:04,510 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:07,533 DEBUG Query successful
2024-11-09 22:09:07,993 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:08,022 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:08,118 DEBUG Query successful
2024-11-09 22:09:08,122 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:08,150 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:11,436 DEBUG Query successful
2024-11-09 22:09:11,439 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:11,465 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:12,408 DEBUG Query successful
2024-11-09 22:09:12,902 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:12,925 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:14,386 DEBUG Query successful
2024-11-09 22:09:14,829 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:14,854 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:15,239 DEBUG Query successful
2024-11-09 22:09:15,242 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:15,266 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:17,037 DEBUG Query successful
2024-11-09 22:09:17,135 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:17,160 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:17,865 DEBUG Query successful
2024-11-09 22:09:18,357 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:18,381 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:20,071 DEBUG Query successful
2024-11-09 22:09:20,159 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:20,187 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:22,424 DEBUG Query successful
2024-11-09 22:09:22,426 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:22,458 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:23,513 DEBUG Query successful
2024-11-09 22:09:23,600 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:23,628 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:24,202 DEBUG Query successful
2024-11-09 22:09:24,206 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:24,234 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:27,180 DEBUG Query successful
2024-11-09 22:09:27,269 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:27,293 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:28,022 DEBUG Query successful
2024-11-09 22:09:28,024 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:28,048 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:30,926 DEBUG Query successful
2024-11-09 22:09:30,930 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:30,964 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:31,858 DEBUG Query successful
2024-11-09 22:09:31,861 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:31,893 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:32,658 DEBUG Query successful
2024-11-09 22:09:32,664 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:32,687 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:34,719 DEBUG Query successful
2024-11-09 22:09:35,088 DEBUG Query successful
2024-11-09 22:09:35,118 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:35,150 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:35,259 DEBUG Query successful
2024-11-09 22:09:35,270 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:35,298 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:35,553 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:35,578 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:38,209 DEBUG Query successful
2024-11-09 22:09:38,663 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:38,688 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:41,047 DEBUG Query successful
2024-11-09 22:09:41,052 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:41,081 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:43,233 DEBUG Query successful
2024-11-09 22:09:43,322 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:43,345 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:43,702 DEBUG Query successful
2024-11-09 22:09:43,794 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:43,816 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:44,188 DEBUG Query successful
2024-11-09 22:09:44,651 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:44,678 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:46,975 DEBUG Query successful
2024-11-09 22:09:46,977 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:47,013 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:47,159 DEBUG Query successful
2024-11-09 22:09:47,259 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:47,291 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:48,127 DEBUG Query successful
2024-11-09 22:09:48,129 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:48,165 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:50,444 DEBUG Query successful
2024-11-09 22:09:50,448 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:50,477 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:53,548 DEBUG Query successful
2024-11-09 22:09:53,637 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:53,666 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:56,291 DEBUG Query successful
2024-11-09 22:09:56,295 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:56,316 DEBUG Query successful
2024-11-09 22:09:56,319 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:56,325 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:56,341 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:57,924 DEBUG Query successful
2024-11-09 22:09:57,925 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:57,949 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:58,329 DEBUG Query successful
2024-11-09 22:09:58,332 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:58,356 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:58,533 DEBUG Query successful
2024-11-09 22:09:58,965 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:58,988 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:09:59,407 DEBUG Query successful
2024-11-09 22:09:59,824 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:09:59,854 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:00,267 DEBUG Query successful
2024-11-09 22:10:00,711 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:00,738 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:06,657 DEBUG Query successful
2024-11-09 22:10:06,760 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:06,787 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:07,636 DEBUG Query successful
2024-11-09 22:10:07,725 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:07,752 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:08,451 DEBUG Query successful
2024-11-09 22:10:08,543 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:08,570 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:10,395 DEBUG Query successful
2024-11-09 22:10:10,398 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:10,419 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:11,779 DEBUG Query successful
2024-11-09 22:10:11,779 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:11,807 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:12,297 DEBUG Query successful
2024-11-09 22:10:12,299 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:12,321 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:13,038 DEBUG Query successful
2024-11-09 22:10:13,040 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:13,064 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:13,456 DEBUG Query successful
2024-11-09 22:10:13,931 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:13,954 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:19,543 DEBUG Query successful
2024-11-09 22:10:19,544 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:19,565 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:20,202 DEBUG Query successful
2024-11-09 22:10:20,205 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:20,229 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:21,453 DEBUG Query successful
2024-11-09 22:10:21,456 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:21,486 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:21,739 DEBUG Query successful
2024-11-09 22:10:22,113 DEBUG Query successful
2024-11-09 22:10:22,216 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:22,246 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:22,572 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:22,596 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:23,596 DEBUG Query successful
2024-11-09 22:10:23,698 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:23,725 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:25,314 DEBUG Query successful
2024-11-09 22:10:29,534 DEBUG Query successful
2024-11-09 22:10:29,619 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:29,643 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:30,075 DEBUG Query successful
2024-11-09 22:10:30,078 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:30,099 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:30,172 DEBUG Query successful
2024-11-09 22:10:30,254 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:30,279 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:32,702 DEBUG Query successful
2024-11-09 22:10:32,705 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:32,728 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:33,603 DEBUG Query successful
2024-11-09 22:10:33,606 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:33,629 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:37,742 DEBUG Query successful
2024-11-09 22:10:37,744 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:37,766 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:39,665 DEBUG Query successful
2024-11-09 22:10:39,668 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:39,689 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:40,846 DEBUG Query successful
2024-11-09 22:10:40,850 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:40,872 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:41,002 DEBUG Query successful
2024-11-09 22:10:41,382 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:41,403 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:41,747 DEBUG Query successful
2024-11-09 22:10:42,190 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:42,224 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:42,548 DEBUG Query successful
2024-11-09 22:10:48,725 DEBUG Query successful
2024-11-09 22:10:48,808 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:48,833 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:49,161 DEBUG Query successful
2024-11-09 22:10:49,256 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:49,282 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:53,311 DEBUG Query successful
2024-11-09 22:10:53,314 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:53,340 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:53,564 DEBUG Query successful
2024-11-09 22:10:53,569 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:53,592 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:10:59,709 DEBUG Query successful
2024-11-09 22:10:59,711 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:10:59,735 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:11:01,697 DEBUG Query successful
2024-11-09 22:11:02,093 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:11:02,124 DEBUG Query successful
2024-11-09 22:11:02,127 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:11:02,159 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:11:02,168 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:11:04,242 DEBUG Query successful
2024-11-09 22:11:04,576 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:11:04,601 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:11:09,211 DEBUG Query successful
2024-11-09 22:11:09,297 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:11:09,321 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:11:12,132 DEBUG Query successful
2024-11-09 22:11:12,212 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:11:12,236 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:11:13,466 DEBUG Query successful
2024-11-09 22:11:13,470 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:11:13,493 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:11:15,717 DEBUG Query successful
2024-11-09 22:11:15,718 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:11:15,741 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:11:21,904 DEBUG Query successful
2024-11-09 22:11:21,907 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:11:21,929 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:11:23,757 DEBUG Query successful
2024-11-09 22:11:23,954 DEBUG Query successful
2024-11-09 22:11:23,958 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:11:23,991 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:11:27,768 DEBUG Query successful
2024-11-09 22:11:28,085 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:11:28,109 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:11:35,849 DEBUG Query successful
2024-11-09 22:11:35,928 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:11:35,949 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:11:40,594 DEBUG Query successful
2024-11-09 22:11:40,596 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:11:40,619 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:11:47,286 DEBUG Query successful
2024-11-09 22:11:47,291 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 22:11:47,312 DEBUG Model set to: gpt-4o-mini
2024-11-09 22:11:48,973 DEBUG Query successful
2024-11-10 13:23:39,915 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:23:41,454 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:23:51,637 DEBUG Query successful
2024-11-10 13:23:51,727 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:23:51,753 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:23:59,716 DEBUG Query successful
2024-11-10 13:23:59,721 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:23:59,743 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:24:06,963 DEBUG Query successful
2024-11-10 13:24:06,966 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:24:06,988 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:24:10,425 DEBUG Query successful
2024-11-10 13:24:10,978 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:24:11,002 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:24:19,108 DEBUG Query successful
2024-11-10 13:24:19,187 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:24:19,217 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:24:26,738 DEBUG Query successful
2024-11-10 13:24:26,747 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:24:26,769 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:24:34,943 DEBUG Query successful
2024-11-10 13:24:34,946 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:24:34,972 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:24:38,805 DEBUG Query successful
2024-11-10 13:24:39,412 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:24:39,439 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:24:47,354 DEBUG Query successful
2024-11-10 13:24:47,433 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:24:47,456 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:24:54,919 DEBUG Query successful
2024-11-10 13:24:54,920 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:24:54,944 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:25:02,152 DEBUG Query successful
2024-11-10 13:25:02,155 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:25:02,176 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:25:05,767 DEBUG Query successful
2024-11-10 13:25:06,231 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:25:06,264 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:25:14,728 DEBUG Query successful
2024-11-10 13:25:14,861 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:25:14,884 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:25:22,659 DEBUG Query successful
2024-11-10 13:25:22,664 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:25:22,688 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:25:30,263 DEBUG Query successful
2024-11-10 13:25:30,267 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:25:30,288 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:25:33,833 DEBUG Query successful
2024-11-10 13:25:34,297 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:25:34,319 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:25:43,791 DEBUG Query successful
2024-11-10 13:25:43,878 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:25:43,899 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:25:51,244 DEBUG Query successful
2024-11-10 13:25:51,245 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:25:51,268 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:25:58,742 DEBUG Query successful
2024-11-10 13:25:58,744 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:25:58,772 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:26:03,254 DEBUG Query successful
2024-11-10 13:26:03,716 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:26:03,744 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:26:11,894 DEBUG Query successful
2024-11-10 13:26:11,974 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:26:11,995 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:26:19,462 DEBUG Query successful
2024-11-10 13:26:19,466 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:26:19,488 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:26:27,324 DEBUG Query successful
2024-11-10 13:26:27,325 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:26:27,351 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:26:31,349 DEBUG Query successful
2024-11-10 13:26:31,795 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:26:31,820 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:26:39,917 DEBUG Query successful
2024-11-10 13:26:39,992 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:26:40,014 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:26:49,303 DEBUG Query successful
2024-11-10 13:26:49,307 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:26:49,332 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:26:57,224 DEBUG Query successful
2024-11-10 13:26:57,226 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:26:57,247 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:27:00,783 DEBUG Query successful
2024-11-10 13:27:01,259 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:27:01,283 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:27:09,791 DEBUG Query successful
2024-11-10 13:27:09,875 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:27:09,897 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:27:19,912 DEBUG Query successful
2024-11-10 13:27:19,913 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:27:19,934 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:27:27,825 DEBUG Query successful
2024-11-10 13:27:27,828 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:27:27,847 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:27:31,760 DEBUG Query successful
2024-11-10 13:27:32,167 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:27:32,190 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:27:40,245 DEBUG Query successful
2024-11-10 13:27:40,357 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:27:40,412 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:27:47,063 DEBUG Query successful
2024-11-10 13:27:47,065 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:27:47,087 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:27:54,464 DEBUG Query successful
2024-11-10 13:27:54,468 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:27:54,493 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:27:58,470 DEBUG Query successful
2024-11-10 13:27:58,879 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:27:58,900 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:28:07,142 DEBUG Query successful
2024-11-10 13:28:07,225 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:28:07,247 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:28:14,152 DEBUG Query successful
2024-11-10 13:28:14,154 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:28:14,181 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:28:22,526 DEBUG Query successful
2024-11-10 13:28:22,531 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:28:22,561 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:28:25,007 DEBUG Query successful
2024-11-10 13:28:25,421 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:28:25,441 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:28:34,873 DEBUG Query successful
2024-11-10 13:28:34,948 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:28:34,969 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:28:44,603 DEBUG Query successful
2024-11-10 13:28:44,608 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:28:44,630 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:28:52,434 DEBUG Query successful
2024-11-10 13:28:52,437 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:28:52,458 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:28:55,513 DEBUG Query successful
2024-11-10 13:28:55,925 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:28:55,947 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:29:03,875 DEBUG Query successful
2024-11-10 13:29:03,950 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:29:03,970 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:29:11,824 DEBUG Query successful
2024-11-10 13:29:11,828 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:29:11,854 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:29:19,442 DEBUG Query successful
2024-11-10 13:29:19,444 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:29:19,464 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:29:22,127 DEBUG Query successful
2024-11-10 13:29:22,533 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:29:22,554 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:29:34,257 DEBUG Query successful
2024-11-10 13:29:34,327 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:29:34,351 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:29:41,477 DEBUG Query successful
2024-11-10 13:29:41,478 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:29:41,499 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:29:49,301 DEBUG Query successful
2024-11-10 13:29:49,304 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:29:49,325 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:29:51,950 DEBUG Query successful
2024-11-10 13:29:52,367 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:29:52,390 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:30:00,375 DEBUG Query successful
2024-11-10 13:30:00,449 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:30:00,471 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:30:08,602 DEBUG Query successful
2024-11-10 13:30:08,603 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:30:08,622 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:30:16,032 DEBUG Query successful
2024-11-10 13:30:16,036 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:30:16,056 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:30:20,331 DEBUG Query successful
2024-11-10 13:30:20,715 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:30:20,736 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:30:29,252 DEBUG Query successful
2024-11-10 13:30:29,325 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:30:29,348 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:30:37,383 DEBUG Query successful
2024-11-10 13:30:37,386 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:30:37,408 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:30:45,064 DEBUG Query successful
2024-11-10 13:30:45,066 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:30:45,086 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:30:47,232 DEBUG Query successful
2024-11-10 13:30:47,617 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:30:47,641 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:30:55,634 DEBUG Query successful
2024-11-10 13:30:55,707 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:30:55,727 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:31:02,389 DEBUG Query successful
2024-11-10 13:31:02,394 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:31:02,416 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:31:10,608 DEBUG Query successful
2024-11-10 13:31:10,612 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:31:10,639 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:31:13,345 DEBUG Query successful
2024-11-10 13:31:13,743 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:31:13,764 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:31:21,188 DEBUG Query successful
2024-11-10 13:31:21,264 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:31:21,286 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:31:26,935 DEBUG Query successful
2024-11-10 13:31:26,939 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:31:26,958 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:31:35,242 DEBUG Query successful
2024-11-10 13:31:35,244 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:31:35,264 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:31:36,615 DEBUG Query successful
2024-11-10 13:31:36,990 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:31:37,009 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:31:44,192 DEBUG Query successful
2024-11-10 13:31:44,266 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:31:44,288 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:31:52,689 DEBUG Query successful
2024-11-10 13:31:52,694 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:31:52,714 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:32:00,363 DEBUG Query successful
2024-11-10 13:32:00,364 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:32:00,387 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:32:01,884 DEBUG Query successful
2024-11-10 13:32:02,258 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:32:02,282 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:32:10,346 DEBUG Query successful
2024-11-10 13:32:10,419 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:32:10,439 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:32:17,122 DEBUG Query successful
2024-11-10 13:32:17,127 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:32:17,151 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:32:24,455 DEBUG Query successful
2024-11-10 13:32:24,456 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:32:24,478 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:32:26,782 DEBUG Query successful
2024-11-10 13:32:27,164 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:32:27,186 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:32:34,061 DEBUG Query successful
2024-11-10 13:32:34,134 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:32:34,156 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:32:43,583 DEBUG Query successful
2024-11-10 13:32:43,588 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:32:43,608 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:33:19,627 DEBUG Query successful
2024-11-10 13:33:19,629 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:33:19,653 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:33:21,150 DEBUG Query successful
2024-11-10 13:33:21,536 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:33:21,559 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:33:28,994 DEBUG Query successful
2024-11-10 13:33:29,070 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:33:29,091 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:33:36,012 DEBUG Query successful
2024-11-10 13:33:36,017 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:33:36,039 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:33:43,107 DEBUG Query successful
2024-11-10 13:33:43,109 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:33:43,135 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:33:44,474 DEBUG Query successful
2024-11-10 13:33:44,936 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:33:44,967 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:33:53,318 DEBUG Query successful
2024-11-10 13:33:53,397 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:33:53,420 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:34:02,065 DEBUG Query successful
2024-11-10 13:34:02,066 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:34:02,095 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:34:08,437 DEBUG Query successful
2024-11-10 13:34:08,441 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:34:08,466 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:34:09,774 DEBUG Query successful
2024-11-10 13:34:10,217 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:34:10,241 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:34:18,853 DEBUG Query successful
2024-11-10 13:34:18,929 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:34:18,951 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:34:27,515 DEBUG Query successful
2024-11-10 13:34:27,519 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:34:27,539 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:34:34,769 DEBUG Query successful
2024-11-10 13:34:34,772 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:34:34,793 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:34:36,156 DEBUG Query successful
2024-11-10 13:34:36,827 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:34:36,847 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:34:47,964 DEBUG Query successful
2024-11-10 13:34:48,043 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:34:48,067 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:34:54,903 DEBUG Query successful
2024-11-10 13:34:54,904 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:34:54,932 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:35:01,590 DEBUG Query successful
2024-11-10 13:35:01,592 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:35:01,614 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:35:03,441 DEBUG Query successful
2024-11-10 13:35:03,956 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:35:03,978 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:35:14,983 DEBUG Query successful
2024-11-10 13:35:15,083 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:35:15,114 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:35:23,181 DEBUG Query successful
2024-11-10 13:35:23,186 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:35:23,209 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:35:30,476 DEBUG Query successful
2024-11-10 13:35:30,480 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:35:30,508 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:35:32,762 DEBUG Query successful
2024-11-10 13:35:33,152 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:35:33,173 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:35:41,194 DEBUG Query successful
2024-11-10 13:35:41,274 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:35:41,296 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:35:51,345 DEBUG Query successful
2024-11-10 13:35:51,350 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:35:51,374 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:35:58,638 DEBUG Query successful
2024-11-10 13:35:58,640 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:35:58,661 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:36:00,745 DEBUG Query successful
2024-11-10 13:36:01,139 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:36:01,164 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:36:10,966 DEBUG Query successful
2024-11-10 13:36:11,054 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:36:11,075 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:36:17,439 DEBUG Query successful
2024-11-10 13:36:17,440 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:36:17,470 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:36:24,267 DEBUG Query successful
2024-11-10 13:36:24,270 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:36:24,292 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:36:25,647 DEBUG Query successful
2024-11-10 13:36:26,035 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:36:26,064 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:36:33,922 DEBUG Query successful
2024-11-10 13:36:34,012 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:36:34,033 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:36:41,754 DEBUG Query successful
2024-11-10 13:36:41,758 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:36:41,780 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:36:49,427 DEBUG Query successful
2024-11-10 13:36:49,429 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:36:49,452 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:36:50,928 DEBUG Query successful
2024-11-10 13:36:51,312 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:36:51,337 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:37:00,870 DEBUG Query successful
2024-11-10 13:37:00,951 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:37:00,978 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:37:09,746 DEBUG Query successful
2024-11-10 13:37:09,748 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:37:09,771 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:37:17,479 DEBUG Query successful
2024-11-10 13:37:17,482 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:37:17,505 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:37:23,983 DEBUG Query successful
2024-11-10 13:37:24,430 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:37:24,452 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:37:32,104 DEBUG Query successful
2024-11-10 13:37:32,188 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:37:32,210 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:37:38,215 DEBUG Query successful
2024-11-10 13:37:38,216 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:37:38,237 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:37:45,732 DEBUG Query successful
2024-11-10 13:37:45,734 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:37:45,758 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:37:49,092 DEBUG Query successful
2024-11-10 13:37:49,500 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:37:49,524 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:37:56,635 DEBUG Query successful
2024-11-10 13:37:56,712 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:37:56,733 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:38:02,262 DEBUG Query successful
2024-11-10 13:38:02,262 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:38:02,285 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:38:08,634 DEBUG Query successful
2024-11-10 13:38:08,635 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:38:08,658 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:38:10,324 DEBUG Query successful
2024-11-10 13:38:10,856 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:38:10,882 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:38:17,934 DEBUG Query successful
2024-11-10 13:38:18,007 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:38:18,029 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:38:23,591 DEBUG Query successful
2024-11-10 13:38:23,592 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:38:23,614 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:38:30,510 DEBUG Query successful
2024-11-10 13:38:30,514 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:38:30,535 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:38:31,995 DEBUG Query successful
2024-11-10 13:38:32,460 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:38:32,482 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:38:40,787 DEBUG Query successful
2024-11-10 13:38:40,862 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:38:40,886 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:38:46,422 DEBUG Query successful
2024-11-10 13:38:46,423 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:38:46,448 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:38:53,620 DEBUG Query successful
2024-11-10 13:38:53,622 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:38:53,648 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:38:55,131 DEBUG Query successful
2024-11-10 13:38:55,535 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:38:55,563 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:39:02,830 DEBUG Query successful
2024-11-10 13:39:02,908 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:39:02,932 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:39:08,311 DEBUG Query successful
2024-11-10 13:39:08,312 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:39:08,334 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:39:15,708 DEBUG Query successful
2024-11-10 13:39:15,711 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:39:15,732 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:39:17,213 DEBUG Query successful
2024-11-10 13:39:17,763 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:39:17,794 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:39:24,649 DEBUG Query successful
2024-11-10 13:39:24,730 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:39:24,751 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:39:31,100 DEBUG Query successful
2024-11-10 13:39:31,102 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:39:31,130 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:39:37,918 DEBUG Query successful
2024-11-10 13:39:37,922 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:39:37,952 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:39:39,420 DEBUG Query successful
2024-11-10 13:39:39,995 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:39:40,026 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:39:48,668 DEBUG Query successful
2024-11-10 13:39:48,760 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:39:48,792 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:39:54,815 DEBUG Query successful
2024-11-10 13:39:54,817 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:39:54,840 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:40:02,251 DEBUG Query successful
2024-11-10 13:40:02,253 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:40:02,277 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:40:03,806 DEBUG Query successful
2024-11-10 13:40:04,257 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:40:04,280 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:40:11,615 DEBUG Query successful
2024-11-10 13:40:11,702 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:40:11,727 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:40:17,027 DEBUG Query successful
2024-11-10 13:40:17,028 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:40:17,049 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:40:23,945 DEBUG Query successful
2024-11-10 13:40:23,949 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:40:23,972 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:40:25,600 DEBUG Query successful
2024-11-10 13:40:26,032 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:40:26,054 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:40:33,573 DEBUG Query successful
2024-11-10 13:40:33,666 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:40:33,687 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:40:39,022 DEBUG Query successful
2024-11-10 13:40:39,023 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:40:39,053 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:40:46,493 DEBUG Query successful
2024-11-10 13:40:46,495 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:40:46,560 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:40:48,079 DEBUG Query successful
2024-11-10 13:40:48,500 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:40:48,525 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:40:55,296 DEBUG Query successful
2024-11-10 13:40:55,384 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:40:55,426 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:41:00,148 DEBUG Query successful
2024-11-10 13:41:00,149 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:41:00,208 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:41:07,966 DEBUG Query successful
2024-11-10 13:41:07,969 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-10 13:41:08,001 DEBUG Model set to: gpt-4o-mini
2024-11-10 13:41:09,381 DEBUG Query successful
2024-11-11 16:03:27,818 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:03:28,053 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:03:41,141 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:03:44,744 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:04:30,017 DEBUG Query successful
2024-11-11 16:04:30,104 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:04:30,131 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:04:34,919 DEBUG Query successful
2024-11-11 16:04:35,002 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:04:35,030 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:04:36,462 DEBUG Query successful
2024-11-11 16:04:36,463 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:04:36,487 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:04:39,850 DEBUG Query successful
2024-11-11 16:04:39,850 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:04:39,875 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:05:04,578 DEBUG Query successful
2024-11-11 16:05:04,580 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:05:04,605 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:05:08,331 DEBUG Query successful
2024-11-11 16:05:08,331 DEBUG Query successful
2024-11-11 16:05:08,343 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:05:08,367 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:05:08,694 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:05:08,720 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:05:17,193 DEBUG Query successful
2024-11-11 16:05:17,508 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:05:17,532 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:06:04,793 DEBUG Query successful
2024-11-11 16:06:04,874 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:06:04,899 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:06:10,054 DEBUG Query successful
2024-11-11 16:06:10,055 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:06:10,089 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:06:10,723 DEBUG Query successful
2024-11-11 16:06:10,810 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:06:10,835 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:06:17,428 DEBUG Query successful
2024-11-11 16:06:17,429 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:06:17,451 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:07:07,309 DEBUG Query successful
2024-11-11 16:07:07,310 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:07:07,334 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:07:14,680 DEBUG Query successful
2024-11-11 16:07:15,036 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:07:15,061 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:07:17,473 DEBUG Query successful
2024-11-11 16:07:17,476 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:07:17,499 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:07:26,809 DEBUG Query successful
2024-11-11 16:07:27,134 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:07:27,160 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:07:44,586 DEBUG Query successful
2024-11-11 16:07:44,676 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:07:44,702 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:07:56,830 DEBUG Query successful
2024-11-11 16:07:56,831 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:07:56,855 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:08:01,432 DEBUG Query successful
2024-11-11 16:08:01,521 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:08:01,544 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:08:16,778 DEBUG Query successful
2024-11-11 16:08:16,779 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:08:16,802 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:08:30,120 DEBUG Query successful
2024-11-11 16:08:30,121 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:08:30,144 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:08:40,164 DEBUG Query successful
2024-11-11 16:08:40,520 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:08:40,544 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:08:50,186 DEBUG Query successful
2024-11-11 16:08:50,188 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:08:50,216 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:08:57,372 DEBUG Query successful
2024-11-11 16:08:57,395 DEBUG Query successful
2024-11-11 16:08:57,461 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:08:57,485 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:08:57,787 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:08:57,811 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:09:04,849 DEBUG Query successful
2024-11-11 16:09:04,850 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:09:04,876 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:09:10,679 DEBUG Query successful
2024-11-11 16:09:10,770 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:09:10,803 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:09:21,460 DEBUG Query successful
2024-11-11 16:09:21,460 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:09:21,488 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:09:21,860 DEBUG Query successful
2024-11-11 16:09:21,861 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:09:21,885 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:09:28,197 DEBUG Query successful
2024-11-11 16:09:28,495 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:09:28,521 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:09:59,353 DEBUG Query successful
2024-11-11 16:09:59,354 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:09:59,377 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:10:09,304 DEBUG Query successful
2024-11-11 16:10:09,707 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:10:09,735 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:10:17,675 DEBUG Query successful
2024-11-11 16:10:17,755 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:10:17,782 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:10:34,627 DEBUG Query successful
2024-11-11 16:10:34,628 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:10:34,652 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:10:39,026 DEBUG Query successful
2024-11-11 16:10:39,105 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:10:39,136 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:10:58,063 DEBUG Query successful
2024-11-11 16:10:58,063 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:10:58,087 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:11:06,186 DEBUG Query successful
2024-11-11 16:11:06,188 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:11:06,211 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:11:23,620 DEBUG Query successful
2024-11-11 16:11:23,947 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:11:23,972 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:11:30,359 DEBUG Query successful
2024-11-11 16:11:30,360 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:11:30,384 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:11:39,590 DEBUG Query successful
2024-11-11 16:11:39,940 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:11:39,964 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:11:47,548 DEBUG Query successful
2024-11-11 16:11:47,627 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:11:47,651 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:12:11,079 DEBUG Query successful
2024-11-11 16:12:11,080 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:12:11,103 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:12:18,785 DEBUG Query successful
2024-11-11 16:12:18,862 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:12:18,886 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:12:42,749 DEBUG Query successful
2024-11-11 16:12:42,750 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:12:42,773 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:12:46,857 DEBUG Query successful
2024-11-11 16:12:46,859 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:12:46,884 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:13:02,308 DEBUG Query successful
2024-11-11 16:13:02,667 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:13:02,701 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:13:13,668 DEBUG Query successful
2024-11-11 16:13:13,670 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:13:13,695 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:13:32,333 DEBUG Query successful
2024-11-11 16:13:32,751 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:13:32,791 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:13:45,411 DEBUG Query successful
2024-11-11 16:13:45,488 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:13:45,512 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:14:02,030 DEBUG Query successful
2024-11-11 16:14:02,044 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:14:02,069 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:14:06,898 DEBUG Query successful
2024-11-11 16:14:06,976 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:14:07,001 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:14:23,490 DEBUG Query successful
2024-11-11 16:14:23,503 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:14:23,535 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:14:29,215 DEBUG Query successful
2024-11-11 16:14:29,216 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:14:29,240 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:14:45,161 DEBUG Query successful
2024-11-11 16:14:45,518 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:14:45,555 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:14:54,262 DEBUG Query successful
2024-11-11 16:14:54,263 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:14:54,286 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:15:02,064 DEBUG Query successful
2024-11-11 16:15:02,378 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:15:02,403 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:15:04,931 DEBUG Query successful
2024-11-11 16:15:05,012 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:15:05,037 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:15:15,720 DEBUG Query successful
2024-11-11 16:15:15,722 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:15:15,745 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:15:24,745 DEBUG Query successful
2024-11-11 16:15:24,822 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:15:24,846 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:15:31,185 DEBUG Query successful
2024-11-11 16:15:31,186 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:15:31,211 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:15:31,477 DEBUG Query successful
2024-11-11 16:15:31,477 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:15:31,500 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:15:35,169 DEBUG Query successful
2024-11-11 16:15:35,518 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:15:35,541 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:16:11,996 DEBUG Query successful
2024-11-11 16:16:11,998 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:16:12,022 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:16:28,364 DEBUG Query successful
2024-11-11 16:16:28,753 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:16:28,777 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:16:32,570 DEBUG Query successful
2024-11-11 16:16:32,649 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:16:32,673 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:17:00,094 DEBUG Query successful
2024-11-11 16:17:00,096 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:17:00,119 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:17:10,877 DEBUG Query successful
2024-11-11 16:17:10,955 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:17:10,979 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:17:35,435 DEBUG Query successful
2024-11-11 16:17:35,436 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:17:35,460 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:17:49,996 DEBUG Query successful
2024-11-11 16:17:49,998 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:17:50,021 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:18:05,400 DEBUG Query successful
2024-11-11 16:18:05,718 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:18:05,752 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:18:24,533 DEBUG Query successful
2024-11-11 16:18:24,534 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:18:24,557 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:18:39,114 DEBUG Query successful
2024-11-11 16:18:39,422 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:18:39,445 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:18:59,487 DEBUG Query successful
2024-11-11 16:18:59,566 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:18:59,589 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:19:27,886 DEBUG Query successful
2024-11-11 16:19:27,887 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:19:27,909 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:19:33,043 DEBUG Query successful
2024-11-11 16:19:33,120 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:19:33,143 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:19:54,941 DEBUG Query successful
2024-11-11 16:19:54,941 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:19:54,965 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:20:11,294 DEBUG Query successful
2024-11-11 16:20:11,295 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:20:11,319 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:20:24,531 DEBUG Query successful
2024-11-11 16:20:33,939 DEBUG Query successful
2024-11-11 16:20:33,941 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:20:33,963 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:20:36,152 DEBUG Query successful
2024-11-11 16:20:36,471 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:20:36,494 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:21:10,861 DEBUG Query successful
2024-11-11 16:21:10,938 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:21:10,961 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:21:17,365 DEBUG Query successful
2024-11-11 16:21:17,366 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:21:17,390 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:21:40,736 DEBUG Query successful
2024-11-11 16:21:40,737 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:21:40,760 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:21:44,609 DEBUG Query successful
2024-11-11 16:21:44,957 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:21:44,980 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:22:08,637 DEBUG Query successful
2024-11-11 16:22:08,715 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:22:08,738 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:22:15,777 DEBUG Query successful
2024-11-11 16:22:15,778 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:22:15,802 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:22:31,843 DEBUG Query successful
2024-11-11 16:22:31,844 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 16:22:31,866 DEBUG Model set to: gpt-4o-mini
2024-11-11 16:22:33,825 DEBUG Query successful
2024-11-11 18:25:47,233 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:25:48,191 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:25:50,320 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:25:50,523 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:25:51,844 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:25:51,940 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:25:52,120 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:25:53,137 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:00,386 DEBUG Query successful
2024-11-11 18:26:00,529 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:00,554 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:05,092 DEBUG Query successful
2024-11-11 18:26:05,175 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:05,202 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:06,321 DEBUG Query successful
2024-11-11 18:26:06,429 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:06,462 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:08,209 DEBUG Query successful
2024-11-11 18:26:08,289 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:08,314 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:11,963 DEBUG Query successful
2024-11-11 18:26:11,964 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:11,989 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:17,333 DEBUG Query successful
2024-11-11 18:26:17,334 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:17,360 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:19,249 DEBUG Query successful
2024-11-11 18:26:19,261 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:19,286 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:19,802 DEBUG Query successful
2024-11-11 18:26:19,816 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:19,852 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:23,034 DEBUG Query successful
2024-11-11 18:26:23,046 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:23,073 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:27,898 DEBUG Query successful
2024-11-11 18:26:27,912 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:27,939 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:29,281 DEBUG Query successful
2024-11-11 18:26:29,693 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:29,726 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:31,641 DEBUG Query successful
2024-11-11 18:26:31,655 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:31,679 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:33,932 DEBUG Query successful
2024-11-11 18:26:34,388 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:34,414 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:34,716 DEBUG Query successful
2024-11-11 18:26:34,729 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:34,756 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:38,196 DEBUG Query successful
2024-11-11 18:26:38,600 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:38,627 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:43,421 DEBUG Query successful
2024-11-11 18:26:43,876 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:43,903 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:45,444 DEBUG Query successful
2024-11-11 18:26:45,562 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:45,623 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:49,425 DEBUG Query successful
2024-11-11 18:26:49,513 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:49,539 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:53,327 DEBUG Query successful
2024-11-11 18:26:53,413 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:53,437 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:55,835 DEBUG Query successful
2024-11-11 18:26:55,917 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:55,943 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:26:59,419 DEBUG Query successful
2024-11-11 18:26:59,432 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:26:59,458 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:00,997 DEBUG Query successful
2024-11-11 18:27:00,999 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:01,031 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:06,764 DEBUG Query successful
2024-11-11 18:27:06,777 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:06,812 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:07,731 DEBUG Query successful
2024-11-11 18:27:07,733 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:07,757 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:11,016 DEBUG Query successful
2024-11-11 18:27:11,017 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:11,042 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:13,233 DEBUG Query successful
2024-11-11 18:27:13,234 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:13,257 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:17,231 DEBUG Query successful
2024-11-11 18:27:17,233 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:17,263 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:19,309 DEBUG Query successful
2024-11-11 18:27:19,310 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:19,334 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:19,854 DEBUG Query successful
2024-11-11 18:27:20,271 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:20,298 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:20,896 DEBUG Query successful
2024-11-11 18:27:21,351 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:21,387 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:22,579 DEBUG Query successful
2024-11-11 18:27:23,047 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:23,071 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:25,352 DEBUG Query successful
2024-11-11 18:27:25,775 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:25,809 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:34,141 DEBUG Query successful
2024-11-11 18:27:34,230 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:34,255 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:38,786 DEBUG Query successful
2024-11-11 18:27:38,883 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:38,908 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:41,319 DEBUG Query successful
2024-11-11 18:27:41,410 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:41,437 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:45,420 DEBUG Query successful
2024-11-11 18:27:45,504 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:45,541 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:57,584 DEBUG Query successful
2024-11-11 18:27:57,597 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:57,621 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:27:59,023 DEBUG Query successful
2024-11-11 18:27:59,035 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:27:59,062 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:03,756 DEBUG Query successful
2024-11-11 18:28:03,756 DEBUG Query successful
2024-11-11 18:28:03,757 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:28:03,758 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:28:03,784 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:03,792 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:17,538 DEBUG Query successful
2024-11-11 18:28:17,540 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:28:17,565 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:23,532 DEBUG Query successful
2024-11-11 18:28:23,975 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:28:24,009 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:24,397 DEBUG Query successful
2024-11-11 18:28:24,399 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:28:24,423 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:24,470 DEBUG Query successful
2024-11-11 18:28:24,472 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:28:24,497 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:24,574 DEBUG Query successful
2024-11-11 18:28:24,575 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:28:24,600 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:32,836 DEBUG Query successful
2024-11-11 18:28:33,267 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:28:33,303 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:33,318 DEBUG Query successful
2024-11-11 18:28:33,494 DEBUG Query successful
2024-11-11 18:28:33,810 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:28:33,842 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:34,006 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:28:34,037 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:36,840 DEBUG Query successful
2024-11-11 18:28:36,930 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:28:36,956 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:47,665 DEBUG Query successful
2024-11-11 18:28:47,757 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:28:47,782 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:52,773 DEBUG Query successful
2024-11-11 18:28:52,773 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:28:52,802 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:58,932 DEBUG Query successful
2024-11-11 18:28:59,025 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:28:59,056 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:28:59,953 DEBUG Query successful
2024-11-11 18:29:00,043 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:00,068 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:03,097 DEBUG Query successful
2024-11-11 18:29:03,097 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:03,122 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:03,731 DEBUG Query successful
2024-11-11 18:29:03,733 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:03,757 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:09,042 DEBUG Query successful
2024-11-11 18:29:09,475 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:09,501 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:11,485 DEBUG Query successful
2024-11-11 18:29:11,496 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:11,521 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:12,806 DEBUG Query successful
2024-11-11 18:29:12,819 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:12,843 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:15,188 DEBUG Query successful
2024-11-11 18:29:15,190 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:15,216 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:20,348 DEBUG Query successful
2024-11-11 18:29:20,428 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:20,452 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:22,251 DEBUG Query successful
2024-11-11 18:29:22,253 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:22,277 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:25,060 DEBUG Query successful
2024-11-11 18:29:25,412 DEBUG Query successful
2024-11-11 18:29:25,413 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:25,437 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:25,451 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:25,476 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:29,096 DEBUG Query successful
2024-11-11 18:29:29,483 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:29,509 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:34,358 DEBUG Query successful
2024-11-11 18:29:34,682 DEBUG Query successful
2024-11-11 18:29:34,695 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:34,750 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:34,758 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:34,784 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:39,762 DEBUG Query successful
2024-11-11 18:29:39,847 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:39,886 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:45,469 DEBUG Query successful
2024-11-11 18:29:45,550 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:45,578 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:46,475 DEBUG Query successful
2024-11-11 18:29:46,476 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:46,501 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:48,677 DEBUG Query successful
2024-11-11 18:29:48,759 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:48,788 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:51,119 DEBUG Query successful
2024-11-11 18:29:51,538 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:51,567 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:29:55,483 DEBUG Query successful
2024-11-11 18:29:55,484 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:29:55,540 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:00,017 DEBUG Query successful
2024-11-11 18:30:00,030 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:00,084 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:00,480 DEBUG Query successful
2024-11-11 18:30:00,493 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:00,549 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:03,953 DEBUG Query successful
2024-11-11 18:30:04,032 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:04,058 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:12,831 DEBUG Query successful
2024-11-11 18:30:12,833 DEBUG Query successful
2024-11-11 18:30:12,833 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:12,834 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:12,858 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:12,859 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:16,159 DEBUG Query successful
2024-11-11 18:30:16,162 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:16,188 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:23,235 DEBUG Query successful
2024-11-11 18:30:23,386 DEBUG Query successful
2024-11-11 18:30:23,387 DEBUG Query successful
2024-11-11 18:30:23,387 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:23,422 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:23,688 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:23,717 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:23,832 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:23,858 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:30,516 DEBUG Query successful
2024-11-11 18:30:30,566 DEBUG Query successful
2024-11-11 18:30:30,568 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:30,604 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:31,016 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:31,044 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:41,058 DEBUG Query successful
2024-11-11 18:30:41,149 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:41,177 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:44,593 DEBUG Query successful
2024-11-11 18:30:45,041 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:45,071 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:46,431 DEBUG Query successful
2024-11-11 18:30:46,516 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:46,541 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:50,632 DEBUG Query successful
2024-11-11 18:30:50,714 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:50,739 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:55,333 DEBUG Query successful
2024-11-11 18:30:55,345 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:55,370 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:30:58,263 DEBUG Query successful
2024-11-11 18:30:58,342 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:30:58,367 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:00,275 DEBUG Query successful
2024-11-11 18:31:00,287 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:00,313 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:07,145 DEBUG Query successful
2024-11-11 18:31:07,146 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:07,171 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:10,642 DEBUG Query successful
2024-11-11 18:31:10,655 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:10,681 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:12,765 DEBUG Query successful
2024-11-11 18:31:13,176 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:13,205 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:15,703 DEBUG Query successful
2024-11-11 18:31:15,704 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:15,739 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:20,273 DEBUG Query successful
2024-11-11 18:31:20,286 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:20,311 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:22,842 DEBUG Query successful
2024-11-11 18:31:23,198 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:23,226 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:24,250 DEBUG Query successful
2024-11-11 18:31:24,252 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:24,277 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:24,361 DEBUG Query successful
2024-11-11 18:31:24,443 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:24,469 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:29,212 DEBUG Query successful
2024-11-11 18:31:29,595 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:29,621 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:34,620 DEBUG Query successful
2024-11-11 18:31:34,622 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:34,656 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:34,944 DEBUG Query successful
2024-11-11 18:31:35,026 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:35,055 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:38,694 DEBUG Query successful
2024-11-11 18:31:38,707 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:38,731 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:40,525 DEBUG Query successful
2024-11-11 18:31:40,900 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:40,927 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:44,616 DEBUG Query successful
2024-11-11 18:31:44,697 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:44,722 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:54,180 DEBUG Query successful
2024-11-11 18:31:54,180 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:54,206 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:54,953 DEBUG Query successful
2024-11-11 18:31:54,954 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:54,978 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:57,844 DEBUG Query successful
2024-11-11 18:31:57,932 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:57,958 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:31:59,735 DEBUG Query successful
2024-11-11 18:31:59,746 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:31:59,771 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:00,719 DEBUG Query successful
2024-11-11 18:32:01,209 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:01,239 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:07,021 DEBUG Query successful
2024-11-11 18:32:07,023 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:07,050 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:08,333 DEBUG Query successful
2024-11-11 18:32:08,345 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:08,377 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:12,629 DEBUG Query successful
2024-11-11 18:32:12,631 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:12,654 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:13,167 DEBUG Query successful
2024-11-11 18:32:13,532 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:13,559 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:18,154 DEBUG Query successful
2024-11-11 18:32:18,234 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:18,259 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:19,660 DEBUG Query successful
2024-11-11 18:32:20,022 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:20,057 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:21,289 DEBUG Query successful
2024-11-11 18:32:21,290 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:21,323 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:27,733 DEBUG Query successful
2024-11-11 18:32:27,829 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:27,854 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:28,210 DEBUG Query successful
2024-11-11 18:32:28,688 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:28,713 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:30,937 DEBUG Query successful
2024-11-11 18:32:31,035 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:31,065 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:37,116 DEBUG Query successful
2024-11-11 18:32:37,128 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:37,153 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:38,728 DEBUG Query successful
2024-11-11 18:32:38,819 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:38,845 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:42,307 DEBUG Query successful
2024-11-11 18:32:42,319 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:42,345 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:44,548 DEBUG Query successful
2024-11-11 18:32:44,550 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:44,577 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:50,732 DEBUG Query successful
2024-11-11 18:32:50,733 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:50,758 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:56,483 DEBUG Query successful
2024-11-11 18:32:56,484 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:56,511 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:32:56,696 DEBUG Query successful
2024-11-11 18:32:57,135 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:32:57,161 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:01,973 DEBUG Query successful
2024-11-11 18:33:01,974 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:01,999 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:05,771 DEBUG Query successful
2024-11-11 18:33:05,784 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:05,819 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:07,700 DEBUG Query successful
2024-11-11 18:33:08,141 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:08,168 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:14,479 DEBUG Query successful
2024-11-11 18:33:14,865 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:14,891 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:14,926 DEBUG Query successful
2024-11-11 18:33:15,009 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:15,036 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:19,521 DEBUG Query successful
2024-11-11 18:33:19,523 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:19,547 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:20,635 DEBUG Query successful
2024-11-11 18:33:20,724 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:20,748 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:25,022 DEBUG Query successful
2024-11-11 18:33:25,104 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:25,131 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:25,834 DEBUG Query successful
2024-11-11 18:33:26,265 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:26,290 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:33,809 DEBUG Query successful
2024-11-11 18:33:33,810 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:33,846 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:37,408 DEBUG Query successful
2024-11-11 18:33:37,408 DEBUG Query successful
2024-11-11 18:33:37,409 DEBUG Query successful
2024-11-11 18:33:37,421 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:37,426 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:37,454 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:37,455 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:37,503 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:37,543 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:47,820 DEBUG Query successful
2024-11-11 18:33:47,821 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:47,847 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:48,280 DEBUG Query successful
2024-11-11 18:33:48,292 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:48,317 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:52,840 DEBUG Query successful
2024-11-11 18:33:53,203 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:53,229 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:53,235 DEBUG Query successful
2024-11-11 18:33:53,236 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:53,243 DEBUG Query successful
2024-11-11 18:33:53,244 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:53,265 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:53,277 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:58,226 DEBUG Query successful
2024-11-11 18:33:58,622 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:58,647 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:33:59,179 DEBUG Query successful
2024-11-11 18:33:59,646 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:33:59,680 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:00,276 DEBUG Query successful
2024-11-11 18:34:00,278 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:00,305 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:05,566 DEBUG Query successful
2024-11-11 18:34:05,599 DEBUG Query successful
2024-11-11 18:34:05,687 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:05,714 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:05,923 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:05,948 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:15,537 DEBUG Query successful
2024-11-11 18:34:15,643 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:15,670 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:16,344 DEBUG Query successful
2024-11-11 18:34:16,429 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:16,457 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:16,819 DEBUG Query successful
2024-11-11 18:34:16,831 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:16,832 DEBUG Query successful
2024-11-11 18:34:16,856 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:16,916 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:16,941 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:26,330 DEBUG Query successful
2024-11-11 18:34:26,342 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:26,366 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:26,919 DEBUG Query successful
2024-11-11 18:34:26,932 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:26,972 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:27,032 DEBUG Query successful
2024-11-11 18:34:27,033 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:27,071 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:29,583 DEBUG Query successful
2024-11-11 18:34:29,585 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:29,610 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:34,612 DEBUG Query successful
2024-11-11 18:34:34,963 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:34,992 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:40,431 DEBUG Query successful
2024-11-11 18:34:40,432 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:40,459 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:43,526 DEBUG Query successful
2024-11-11 18:34:43,527 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:43,552 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:44,300 DEBUG Query successful
2024-11-11 18:34:44,302 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:44,327 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:44,381 DEBUG Query successful
2024-11-11 18:34:44,480 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:44,506 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:45,587 DEBUG Query successful
2024-11-11 18:34:46,095 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:46,124 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:47,303 DEBUG Query successful
2024-11-11 18:34:47,666 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:47,694 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:48,332 DEBUG Query successful
2024-11-11 18:34:48,719 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:48,747 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:55,902 DEBUG Query successful
2024-11-11 18:34:55,914 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:55,940 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:34:56,745 DEBUG Query successful
2024-11-11 18:34:56,828 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:34:56,854 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:00,420 DEBUG Query successful
2024-11-11 18:35:00,500 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:00,528 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:02,416 DEBUG Query successful
2024-11-11 18:35:02,497 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:02,532 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:05,248 DEBUG Query successful
2024-11-11 18:35:05,261 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:05,286 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:07,437 DEBUG Query successful
2024-11-11 18:35:07,439 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:07,464 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:11,374 DEBUG Query successful
2024-11-11 18:35:11,386 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:11,410 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:12,108 DEBUG Query successful
2024-11-11 18:35:12,121 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:12,147 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:12,501 DEBUG Query successful
2024-11-11 18:35:12,924 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:12,955 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:17,756 DEBUG Query successful
2024-11-11 18:35:17,757 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:17,782 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:23,690 DEBUG Query successful
2024-11-11 18:35:23,692 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:23,717 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:24,360 DEBUG Query successful
2024-11-11 18:35:24,740 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:24,767 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:27,121 DEBUG Query successful
2024-11-11 18:35:27,504 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:27,529 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:27,980 DEBUG Query successful
2024-11-11 18:35:28,072 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:28,096 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:29,978 DEBUG Query successful
2024-11-11 18:35:29,979 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:30,004 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:33,863 DEBUG Query successful
2024-11-11 18:35:34,294 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:34,322 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:38,431 DEBUG Query successful
2024-11-11 18:35:38,522 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:38,548 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:41,015 DEBUG Query successful
2024-11-11 18:35:41,099 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:41,129 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:44,524 DEBUG Query successful
2024-11-11 18:35:44,525 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:44,550 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:45,771 DEBUG Query successful
2024-11-11 18:35:45,857 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:45,882 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:50,189 DEBUG Query successful
2024-11-11 18:35:50,201 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:50,228 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:53,452 DEBUG Query successful
2024-11-11 18:35:53,464 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:53,492 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:55,731 DEBUG Query successful
2024-11-11 18:35:55,732 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:55,757 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:35:58,849 DEBUG Query successful
2024-11-11 18:35:58,851 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:35:58,875 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:01,050 DEBUG Query successful
2024-11-11 18:36:01,055 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:01,110 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:03,254 DEBUG Query successful
2024-11-11 18:36:03,655 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:03,685 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:04,576 DEBUG Query successful
2024-11-11 18:36:04,964 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:04,993 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:08,350 DEBUG Query successful
2024-11-11 18:36:08,351 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:08,375 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:08,392 DEBUG Query successful
2024-11-11 18:36:08,394 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:08,418 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:14,277 DEBUG Query successful
2024-11-11 18:36:14,642 DEBUG Query successful
2024-11-11 18:36:14,721 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:14,754 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:15,045 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:15,071 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:15,204 DEBUG Query successful
2024-11-11 18:36:15,320 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:15,349 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:17,178 DEBUG Query successful
2024-11-11 18:36:17,260 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:17,286 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:24,152 DEBUG Query successful
2024-11-11 18:36:24,165 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:24,190 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:25,726 DEBUG Query successful
2024-11-11 18:36:25,817 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:25,847 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:27,059 DEBUG Query successful
2024-11-11 18:36:27,071 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:27,120 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:27,827 DEBUG Query successful
2024-11-11 18:36:27,950 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:27,976 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:33,561 DEBUG Query successful
2024-11-11 18:36:33,563 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:33,587 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:37,578 DEBUG Query successful
2024-11-11 18:36:37,579 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:37,605 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:37,955 DEBUG Query successful
2024-11-11 18:36:38,331 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:38,358 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:38,667 DEBUG Query successful
2024-11-11 18:36:38,679 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:38,705 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:38,982 DEBUG Query successful
2024-11-11 18:36:38,983 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:39,008 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:42,154 DEBUG Query successful
2024-11-11 18:36:42,610 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:42,636 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:49,946 DEBUG Query successful
2024-11-11 18:36:50,026 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:50,052 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:53,361 DEBUG Query successful
2024-11-11 18:36:53,363 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:53,386 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:54,408 DEBUG Query successful
2024-11-11 18:36:54,410 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:54,434 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:56,660 DEBUG Query successful
2024-11-11 18:36:57,014 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:57,041 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:36:58,629 DEBUG Query successful
2024-11-11 18:36:58,713 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:36:58,743 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:00,574 DEBUG Query successful
2024-11-11 18:37:01,036 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:01,121 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:02,807 DEBUG Query successful
2024-11-11 18:37:02,820 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:02,844 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:08,116 DEBUG Query successful
2024-11-11 18:37:08,194 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:08,220 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:12,884 DEBUG Query successful
2024-11-11 18:37:12,966 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:13,001 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:13,438 DEBUG Query successful
2024-11-11 18:37:13,451 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:13,477 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:13,724 DEBUG Query successful
2024-11-11 18:37:13,726 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:13,752 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:18,165 DEBUG Query successful
2024-11-11 18:37:18,551 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:18,581 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:22,471 DEBUG Query successful
2024-11-11 18:37:22,483 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:22,509 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:23,520 DEBUG Query successful
2024-11-11 18:37:23,522 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:23,548 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:27,530 DEBUG Query successful
2024-11-11 18:37:27,541 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:27,565 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:28,965 DEBUG Query successful
2024-11-11 18:37:29,348 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:29,375 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:29,622 DEBUG Query successful
2024-11-11 18:37:29,706 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:29,742 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:35,111 DEBUG Query successful
2024-11-11 18:37:35,113 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:35,138 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:39,732 DEBUG Query successful
2024-11-11 18:37:39,733 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:39,758 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:40,211 DEBUG Query successful
2024-11-11 18:37:40,552 DEBUG Query successful
2024-11-11 18:37:40,603 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:40,627 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:40,647 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:40,676 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:44,238 DEBUG Query successful
2024-11-11 18:37:44,251 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:44,276 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:44,300 DEBUG Query successful
2024-11-11 18:37:44,754 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:44,784 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:52,237 DEBUG Query successful
2024-11-11 18:37:52,316 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:52,341 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:55,394 DEBUG Query successful
2024-11-11 18:37:55,396 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:55,420 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:57,565 DEBUG Query successful
2024-11-11 18:37:57,578 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:57,607 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:58,694 DEBUG Query successful
2024-11-11 18:37:59,083 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:59,116 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:37:59,160 DEBUG Query successful
2024-11-11 18:37:59,249 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:37:59,275 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:07,629 DEBUG Query successful
2024-11-11 18:38:07,630 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:07,659 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:07,971 DEBUG Query successful
2024-11-11 18:38:07,984 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:08,008 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:11,742 DEBUG Query successful
2024-11-11 18:38:11,833 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:11,859 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:12,339 DEBUG Query successful
2024-11-11 18:38:12,675 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:12,699 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:13,644 DEBUG Query successful
2024-11-11 18:38:13,656 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:13,682 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:18,533 DEBUG Query successful
2024-11-11 18:38:18,535 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:18,559 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:23,043 DEBUG Query successful
2024-11-11 18:38:23,362 DEBUG Query successful
2024-11-11 18:38:23,425 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:23,449 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:23,455 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:23,487 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:25,666 DEBUG Query successful
2024-11-11 18:38:25,667 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:25,693 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:29,965 DEBUG Query successful
2024-11-11 18:38:29,967 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:29,995 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:33,643 DEBUG Query successful
2024-11-11 18:38:34,009 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:34,036 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:34,342 DEBUG Query successful
2024-11-11 18:38:34,355 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:34,384 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:34,967 DEBUG Query successful
2024-11-11 18:38:35,066 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:35,104 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:37,937 DEBUG Query successful
2024-11-11 18:38:37,938 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:37,962 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:41,483 DEBUG Query successful
2024-11-11 18:38:41,849 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:41,875 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:45,730 DEBUG Query successful
2024-11-11 18:38:45,832 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:45,864 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:47,400 DEBUG Query successful
2024-11-11 18:38:47,413 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:47,437 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:48,170 DEBUG Query successful
2024-11-11 18:38:48,172 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:48,199 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:52,194 DEBUG Query successful
2024-11-11 18:38:52,596 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:52,601 DEBUG Query successful
2024-11-11 18:38:52,625 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:52,697 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:52,735 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:56,423 DEBUG Query successful
2024-11-11 18:38:56,435 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:56,461 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:38:58,383 DEBUG Query successful
2024-11-11 18:38:58,385 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:38:58,417 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:01,685 DEBUG Query successful
2024-11-11 18:39:01,698 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:01,731 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:04,096 DEBUG Query successful
2024-11-11 18:39:04,437 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:04,462 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:06,289 DEBUG Query successful
2024-11-11 18:39:06,370 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:06,397 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:08,936 DEBUG Query successful
2024-11-11 18:39:08,938 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:08,962 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:12,519 DEBUG Query successful
2024-11-11 18:39:12,871 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:12,899 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:14,789 DEBUG Query successful
2024-11-11 18:39:14,790 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:14,822 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:17,176 DEBUG Query successful
2024-11-11 18:39:17,258 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:17,284 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:20,557 DEBUG Query successful
2024-11-11 18:39:20,930 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:20,954 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:22,198 DEBUG Query successful
2024-11-11 18:39:22,210 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:22,237 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:26,464 DEBUG Query successful
2024-11-11 18:39:26,557 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:26,583 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:34,969 DEBUG Query successful
2024-11-11 18:39:35,050 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:35,075 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:36,833 DEBUG Query successful
2024-11-11 18:39:36,835 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:36,860 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:39,378 DEBUG Query successful
2024-11-11 18:39:39,390 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:39,415 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:40,092 DEBUG Query successful
2024-11-11 18:39:40,458 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:40,485 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:46,581 DEBUG Query successful
2024-11-11 18:39:46,593 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:46,624 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:46,869 DEBUG Query successful
2024-11-11 18:39:46,881 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:46,908 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:50,959 DEBUG Query successful
2024-11-11 18:39:50,961 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:50,985 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:51,183 DEBUG Query successful
2024-11-11 18:39:51,264 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:51,290 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:57,196 DEBUG Query successful
2024-11-11 18:39:57,197 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:57,221 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:39:59,267 DEBUG Query successful
2024-11-11 18:39:59,693 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:39:59,722 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:00,932 DEBUG Query successful
2024-11-11 18:40:01,407 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:01,441 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:02,876 DEBUG Query successful
2024-11-11 18:40:02,878 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:02,903 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:06,511 DEBUG Query successful
2024-11-11 18:40:06,859 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:06,885 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:07,072 DEBUG Query successful
2024-11-11 18:40:07,084 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:07,122 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:11,558 DEBUG Query successful
2024-11-11 18:40:11,639 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:11,665 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:12,116 DEBUG Query successful
2024-11-11 18:40:12,195 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:12,221 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:17,637 DEBUG Query successful
2024-11-11 18:40:17,719 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:17,746 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:21,489 DEBUG Query successful
2024-11-11 18:40:21,490 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:21,516 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:24,520 DEBUG Query successful
2024-11-11 18:40:24,533 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:24,558 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:25,138 DEBUG Query successful
2024-11-11 18:40:25,530 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:25,571 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:25,900 DEBUG Query successful
2024-11-11 18:40:25,911 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:25,937 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:28,405 DEBUG Query successful
2024-11-11 18:40:28,417 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:28,442 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:35,409 DEBUG Query successful
2024-11-11 18:40:35,411 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:35,438 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:40,335 DEBUG Query successful
2024-11-11 18:40:40,414 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:40,439 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:42,014 DEBUG Query successful
2024-11-11 18:40:42,209 DEBUG Query successful
2024-11-11 18:40:42,211 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:42,239 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:42,432 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:42,459 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:44,944 DEBUG Query successful
2024-11-11 18:40:44,946 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:44,970 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:46,645 DEBUG Query successful
2024-11-11 18:40:46,987 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:47,011 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:48,064 DEBUG Query successful
2024-11-11 18:40:48,065 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:48,095 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:48,554 DEBUG Query successful
2024-11-11 18:40:48,898 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:48,926 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:55,644 DEBUG Query successful
2024-11-11 18:40:55,741 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:55,765 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:40:57,223 DEBUG Query successful
2024-11-11 18:40:57,306 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:40:57,333 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:03,989 DEBUG Query successful
2024-11-11 18:41:03,990 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:04,015 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:04,990 DEBUG Query successful
2024-11-11 18:41:05,075 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:05,105 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:06,601 DEBUG Query successful
2024-11-11 18:41:06,941 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:06,967 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:12,309 DEBUG Query successful
2024-11-11 18:41:12,322 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:12,352 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:13,056 DEBUG Query successful
2024-11-11 18:41:13,057 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:13,082 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:16,585 DEBUG Query successful
2024-11-11 18:41:16,587 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:16,613 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:18,523 DEBUG Query successful
2024-11-11 18:41:18,604 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:18,628 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:21,896 DEBUG Query successful
2024-11-11 18:41:21,897 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:21,921 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:25,435 DEBUG Query successful
2024-11-11 18:41:25,790 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:25,819 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:27,267 DEBUG Query successful
2024-11-11 18:41:27,268 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:27,292 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:29,180 DEBUG Query successful
2024-11-11 18:41:29,182 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:29,206 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:29,886 DEBUG Query successful
2024-11-11 18:41:30,247 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:30,274 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:30,474 DEBUG Query successful
2024-11-11 18:41:30,476 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:30,499 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:31,662 DEBUG Query successful
2024-11-11 18:41:32,097 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:32,126 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:37,074 DEBUG Query successful
2024-11-11 18:41:37,158 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:37,185 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:41,049 DEBUG Query successful
2024-11-11 18:41:41,140 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:41,166 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:45,437 DEBUG Query successful
2024-11-11 18:41:45,439 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:45,464 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:47,462 DEBUG Query successful
2024-11-11 18:41:47,540 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:47,567 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:50,234 DEBUG Query successful
2024-11-11 18:41:50,334 DEBUG Query successful
2024-11-11 18:41:50,335 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:50,360 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:50,617 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:50,651 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:51,696 DEBUG Query successful
2024-11-11 18:41:51,708 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:51,734 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:41:57,752 DEBUG Query successful
2024-11-11 18:41:57,764 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:41:57,790 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:03,845 DEBUG Query successful
2024-11-11 18:42:03,847 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:03,874 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:05,520 DEBUG Query successful
2024-11-11 18:42:05,522 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:05,558 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:07,310 DEBUG Query successful
2024-11-11 18:42:07,391 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:07,417 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:08,916 DEBUG Query successful
2024-11-11 18:42:09,244 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:09,276 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:10,300 DEBUG Query successful
2024-11-11 18:42:10,302 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:10,341 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:10,491 DEBUG Query successful
2024-11-11 18:42:10,869 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:10,894 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:12,628 DEBUG Query successful
2024-11-11 18:42:12,961 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:12,985 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:17,884 DEBUG Query successful
2024-11-11 18:42:17,885 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:17,910 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:19,874 DEBUG Query successful
2024-11-11 18:42:19,959 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:19,986 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:22,211 DEBUG Query successful
2024-11-11 18:42:22,290 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:22,316 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:25,584 DEBUG Query successful
2024-11-11 18:42:25,669 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:25,694 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:28,502 DEBUG Query successful
2024-11-11 18:42:28,504 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:28,530 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:31,388 DEBUG Query successful
2024-11-11 18:42:31,768 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:31,798 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:33,471 DEBUG Query successful
2024-11-11 18:42:33,472 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:33,507 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:33,567 DEBUG Query successful
2024-11-11 18:42:33,569 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:33,595 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:34,569 DEBUG Query successful
2024-11-11 18:42:34,583 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:34,616 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:42,093 DEBUG Query successful
2024-11-11 18:42:42,176 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:42,203 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:46,583 DEBUG Query successful
2024-11-11 18:42:46,585 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:46,610 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:47,242 DEBUG Query successful
2024-11-11 18:42:47,244 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:47,268 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:50,149 DEBUG Query successful
2024-11-11 18:42:50,151 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:50,178 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:50,754 DEBUG Query successful
2024-11-11 18:42:51,148 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:51,179 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:51,491 DEBUG Query successful
2024-11-11 18:42:51,889 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:51,915 DEBUG Query successful
2024-11-11 18:42:51,915 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:51,921 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:51,949 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:42:53,089 DEBUG Query successful
2024-11-11 18:42:53,498 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:42:53,526 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:03,171 DEBUG Query successful
2024-11-11 18:43:03,249 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:03,274 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:08,531 DEBUG Query successful
2024-11-11 18:43:08,532 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:08,541 DEBUG Query successful
2024-11-11 18:43:08,557 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:08,625 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:08,651 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:08,667 DEBUG Query successful
2024-11-11 18:43:08,747 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:08,774 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:12,198 DEBUG Query successful
2024-11-11 18:43:12,564 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:12,591 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:14,598 DEBUG Query successful
2024-11-11 18:43:14,610 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:14,636 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:20,906 DEBUG Query successful
2024-11-11 18:43:20,919 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:20,945 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:21,679 DEBUG Query successful
2024-11-11 18:43:21,691 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:21,716 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:22,204 DEBUG Query successful
2024-11-11 18:43:22,286 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:22,313 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:24,624 DEBUG Query successful
2024-11-11 18:43:24,626 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:24,651 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:29,031 DEBUG Query successful
2024-11-11 18:43:29,397 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:29,422 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:30,304 DEBUG Query successful
2024-11-11 18:43:30,305 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:30,331 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:31,663 DEBUG Query successful
2024-11-11 18:43:31,665 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:31,691 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:32,749 DEBUG Query successful
2024-11-11 18:43:32,749 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:32,778 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:37,954 DEBUG Query successful
2024-11-11 18:43:38,280 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:38,305 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:39,337 DEBUG Query successful
2024-11-11 18:43:39,426 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:39,453 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:40,826 DEBUG Query successful
2024-11-11 18:43:41,187 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:41,221 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:42,250 DEBUG Query successful
2024-11-11 18:43:42,252 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:42,278 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:44,456 DEBUG Query successful
2024-11-11 18:43:44,840 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:44,878 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:49,375 DEBUG Query successful
2024-11-11 18:43:49,375 DEBUG Query successful
2024-11-11 18:43:49,387 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:49,413 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:49,454 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:49,482 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:53,221 DEBUG Query successful
2024-11-11 18:43:53,301 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:53,326 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:54,263 DEBUG Query successful
2024-11-11 18:43:54,341 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:54,366 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:59,528 DEBUG Query successful
2024-11-11 18:43:59,541 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:59,565 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:43:59,679 DEBUG Query successful
2024-11-11 18:43:59,680 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:43:59,704 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:04,324 DEBUG Query successful
2024-11-11 18:44:04,661 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:04,686 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:04,974 DEBUG Query successful
2024-11-11 18:44:04,975 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:04,999 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:06,025 DEBUG Query successful
2024-11-11 18:44:06,026 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:06,056 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:11,679 DEBUG Query successful
2024-11-11 18:44:11,680 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:11,707 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:14,758 DEBUG Query successful
2024-11-11 18:44:15,106 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:15,133 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:15,404 DEBUG Query successful
2024-11-11 18:44:15,484 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:15,510 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:20,843 DEBUG Query successful
2024-11-11 18:44:20,846 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:20,872 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:20,891 DEBUG Query successful
2024-11-11 18:44:20,893 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:20,920 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:25,177 DEBUG Query successful
2024-11-11 18:44:25,516 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:25,548 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:25,665 DEBUG Query successful
2024-11-11 18:44:26,020 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:26,048 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:30,527 DEBUG Query successful
2024-11-11 18:44:30,527 DEBUG Query successful
2024-11-11 18:44:30,539 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:30,566 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:30,607 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:30,634 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:39,083 DEBUG Query successful
2024-11-11 18:44:39,161 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:39,186 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:41,214 DEBUG Query successful
2024-11-11 18:44:41,227 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:41,251 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:41,919 DEBUG Query successful
2024-11-11 18:44:42,010 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:42,040 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:44,943 DEBUG Query successful
2024-11-11 18:44:44,945 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:44,969 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:46,862 DEBUG Query successful
2024-11-11 18:44:46,863 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:46,889 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:50,894 DEBUG Query successful
2024-11-11 18:44:51,221 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:51,252 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:52,009 DEBUG Query successful
2024-11-11 18:44:52,021 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:52,053 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:53,083 DEBUG Query successful
2024-11-11 18:44:53,084 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:53,118 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:55,844 DEBUG Query successful
2024-11-11 18:44:55,846 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:55,872 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:44:59,417 DEBUG Query successful
2024-11-11 18:44:59,764 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:44:59,790 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:00,458 DEBUG Query successful
2024-11-11 18:45:00,792 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:00,818 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:06,338 DEBUG Query successful
2024-11-11 18:45:06,405 DEBUG Query successful
2024-11-11 18:45:06,407 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:06,420 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:06,436 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:06,454 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:12,261 DEBUG Query successful
2024-11-11 18:45:12,586 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:12,612 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:13,524 DEBUG Query successful
2024-11-11 18:45:13,606 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:13,632 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:16,923 DEBUG Query successful
2024-11-11 18:45:17,021 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:17,051 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:19,747 DEBUG Query successful
2024-11-11 18:45:19,748 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:19,773 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:24,273 DEBUG Query successful
2024-11-11 18:45:24,273 DEBUG Query successful
2024-11-11 18:45:24,274 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:24,301 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:24,355 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:24,389 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:25,179 DEBUG Query successful
2024-11-11 18:45:25,180 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:25,207 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:33,662 DEBUG Query successful
2024-11-11 18:45:33,663 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:33,689 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:35,608 DEBUG Query successful
2024-11-11 18:45:35,621 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:35,646 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:53,926 DEBUG Query successful
2024-11-11 18:45:53,927 DEBUG Query successful
2024-11-11 18:45:53,927 DEBUG Query successful
2024-11-11 18:45:53,929 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:53,929 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:53,954 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:53,955 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:45:54,259 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:45:54,285 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:05,487 DEBUG Query successful
2024-11-11 18:46:05,487 DEBUG Query successful
2024-11-11 18:46:05,860 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:05,882 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:05,888 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:05,908 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:18,147 DEBUG Query successful
2024-11-11 18:46:18,148 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:18,172 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:24,475 DEBUG Query successful
2024-11-11 18:46:24,557 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:24,584 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:26,501 DEBUG Query successful
2024-11-11 18:46:26,827 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:26,852 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:30,691 DEBUG Query successful
2024-11-11 18:46:30,784 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:30,808 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:31,888 DEBUG Query successful
2024-11-11 18:46:31,971 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:31,999 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:37,517 DEBUG Query successful
2024-11-11 18:46:37,517 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:37,542 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:40,958 DEBUG Query successful
2024-11-11 18:46:41,041 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:41,066 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:43,955 DEBUG Query successful
2024-11-11 18:46:43,967 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:43,992 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:44,436 DEBUG Query successful
2024-11-11 18:46:44,438 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:44,463 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:50,113 DEBUG Query successful
2024-11-11 18:46:50,115 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:50,140 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:55,896 DEBUG Query successful
2024-11-11 18:46:55,897 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:55,922 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:56,375 DEBUG Query successful
2024-11-11 18:46:56,691 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:56,721 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:58,000 DEBUG Query successful
2024-11-11 18:46:58,133 DEBUG Query successful
2024-11-11 18:46:58,134 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:58,162 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:46:58,357 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:46:58,387 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:01,699 DEBUG Query successful
2024-11-11 18:47:01,700 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:01,727 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:06,682 DEBUG Query successful
2024-11-11 18:47:07,064 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:07,090 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:09,037 DEBUG Query successful
2024-11-11 18:47:09,121 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:09,146 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:15,065 DEBUG Query successful
2024-11-11 18:47:15,067 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:15,091 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:17,336 DEBUG Query successful
2024-11-11 18:47:17,682 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:17,705 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:18,323 DEBUG Query successful
2024-11-11 18:47:18,404 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:18,428 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:19,344 DEBUG Query successful
2024-11-11 18:47:19,426 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:19,452 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:19,591 DEBUG Query successful
2024-11-11 18:47:19,593 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:19,618 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:28,401 DEBUG Query successful
2024-11-11 18:47:28,402 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:28,426 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:29,088 DEBUG Query successful
2024-11-11 18:47:29,177 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:29,205 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:33,159 DEBUG Query successful
2024-11-11 18:47:33,161 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:33,187 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:35,348 DEBUG Query successful
2024-11-11 18:47:35,350 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:35,377 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:38,268 DEBUG Query successful
2024-11-11 18:47:38,269 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:38,292 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:38,399 DEBUG Query successful
2024-11-11 18:47:38,400 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:38,426 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:40,217 DEBUG Query successful
2024-11-11 18:47:40,244 DEBUG Query successful
2024-11-11 18:47:40,606 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:40,624 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:40,644 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:40,660 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:49,448 DEBUG Query successful
2024-11-11 18:47:49,450 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:49,485 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:49,555 DEBUG Query successful
2024-11-11 18:47:49,565 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:49,590 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:51,692 DEBUG Query successful
2024-11-11 18:47:51,985 DEBUG Query successful
2024-11-11 18:47:52,021 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:52,054 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:52,356 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:52,384 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:56,081 DEBUG Query successful
2024-11-11 18:47:56,081 DEBUG Query successful
2024-11-11 18:47:56,164 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:56,177 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:47:56,200 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:47:56,203 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:05,582 DEBUG Query successful
2024-11-11 18:48:05,662 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:05,686 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:07,630 DEBUG Query successful
2024-11-11 18:48:07,708 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:07,735 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:07,820 DEBUG Query successful
2024-11-11 18:48:07,821 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:07,845 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:10,582 DEBUG Query successful
2024-11-11 18:48:10,583 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:10,609 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:14,634 DEBUG Query successful
2024-11-11 18:48:14,646 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:14,681 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:15,767 DEBUG Query successful
2024-11-11 18:48:15,768 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:15,805 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:19,107 DEBUG Query successful
2024-11-11 18:48:19,108 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:19,133 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:21,493 DEBUG Query successful
2024-11-11 18:48:21,808 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:21,835 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:23,745 DEBUG Query successful
2024-11-11 18:48:23,747 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:23,771 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:26,572 DEBUG Query successful
2024-11-11 18:48:26,573 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:26,600 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:28,317 DEBUG Query successful
2024-11-11 18:48:28,319 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:28,343 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:28,656 DEBUG Query successful
2024-11-11 18:48:28,978 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:29,004 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:30,020 DEBUG Query successful
2024-11-11 18:48:30,360 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:30,387 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:30,555 DEBUG Query successful
2024-11-11 18:48:30,909 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:30,934 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:33,230 DEBUG Query successful
2024-11-11 18:48:33,314 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:33,339 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:41,568 DEBUG Query successful
2024-11-11 18:48:41,649 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:41,673 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:42,456 DEBUG Query successful
2024-11-11 18:48:42,457 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:42,481 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:47,105 DEBUG Query successful
2024-11-11 18:48:47,157 DEBUG Query successful
2024-11-11 18:48:47,201 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:47,230 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:47,242 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:47,273 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:51,194 DEBUG Query successful
2024-11-11 18:48:51,206 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:51,231 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:55,759 DEBUG Query successful
2024-11-11 18:48:55,759 DEBUG Query successful
2024-11-11 18:48:55,760 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:55,761 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:55,786 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:55,790 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:48:57,745 DEBUG Query successful
2024-11-11 18:48:58,085 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:48:58,109 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:05,340 DEBUG Query successful
2024-11-11 18:49:05,340 DEBUG Query successful
2024-11-11 18:49:05,342 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:05,342 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:05,367 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:05,373 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:07,516 DEBUG Query successful
2024-11-11 18:49:07,830 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:07,855 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:09,324 DEBUG Query successful
2024-11-11 18:49:09,325 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:09,350 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:12,847 DEBUG Query successful
2024-11-11 18:49:13,002 DEBUG Query successful
2024-11-11 18:49:13,088 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:13,123 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:13,166 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:13,192 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:17,504 DEBUG Query successful
2024-11-11 18:49:17,506 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:17,530 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:19,919 DEBUG Query successful
2024-11-11 18:49:19,920 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:19,945 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:20,904 DEBUG Query successful
2024-11-11 18:49:21,233 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:21,260 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:21,508 DEBUG Query successful
2024-11-11 18:49:21,588 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:21,615 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:23,968 DEBUG Query successful
2024-11-11 18:49:24,062 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:24,095 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:30,259 DEBUG Query successful
2024-11-11 18:49:30,260 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:30,288 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:32,833 DEBUG Query successful
2024-11-11 18:49:32,928 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:32,954 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:34,108 DEBUG Query successful
2024-11-11 18:49:34,459 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:34,483 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:35,731 DEBUG Query successful
2024-11-11 18:49:35,732 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:35,766 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:41,700 DEBUG Query successful
2024-11-11 18:49:41,701 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:41,725 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:45,323 DEBUG Query successful
2024-11-11 18:49:45,403 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:45,434 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:45,982 DEBUG Query successful
2024-11-11 18:49:45,995 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:46,023 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:47,861 DEBUG Query successful
2024-11-11 18:49:47,863 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:47,888 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:52,101 DEBUG Query successful
2024-11-11 18:49:52,155 DEBUG Query successful
2024-11-11 18:49:52,158 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:52,184 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:52,451 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:52,478 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:53,342 DEBUG Query successful
2024-11-11 18:49:53,343 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:53,367 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:49:56,305 DEBUG Query successful
2024-11-11 18:49:58,823 DEBUG Query successful
2024-11-11 18:49:58,825 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:49:58,848 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:00,661 DEBUG Query successful
2024-11-11 18:50:00,979 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:01,003 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:04,938 DEBUG Query successful
2024-11-11 18:50:04,941 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:04,971 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:05,803 DEBUG Query successful
2024-11-11 18:50:05,882 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:05,909 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:07,700 DEBUG Query successful
2024-11-11 18:50:08,018 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:08,042 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:13,253 DEBUG Query successful
2024-11-11 18:50:13,254 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:13,279 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:13,366 DEBUG Query successful
2024-11-11 18:50:13,444 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:13,471 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:18,412 DEBUG Query successful
2024-11-11 18:50:18,489 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:18,513 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:22,568 DEBUG Query successful
2024-11-11 18:50:22,580 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:22,604 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:26,315 DEBUG Query successful
2024-11-11 18:50:26,328 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:26,352 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:27,556 DEBUG Query successful
2024-11-11 18:50:27,558 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:27,583 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:30,117 DEBUG Query successful
2024-11-11 18:50:30,451 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:30,477 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:36,405 DEBUG Query successful
2024-11-11 18:50:36,407 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:36,430 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:36,442 DEBUG Query successful
2024-11-11 18:50:36,443 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:36,467 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:38,266 DEBUG Query successful
2024-11-11 18:50:38,583 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:38,612 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:38,776 DEBUG Query successful
2024-11-11 18:50:39,120 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:39,151 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:44,292 DEBUG Query successful
2024-11-11 18:50:44,371 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:44,396 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:50,501 DEBUG Query successful
2024-11-11 18:50:50,502 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:50,525 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:51,061 DEBUG Query successful
2024-11-11 18:50:51,138 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:51,163 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:50:51,591 DEBUG Query successful
2024-11-11 18:50:51,673 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:50:51,698 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:00,118 DEBUG Query successful
2024-11-11 18:51:00,131 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:00,155 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:01,612 DEBUG Query successful
2024-11-11 18:51:01,614 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:01,641 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:04,393 DEBUG Query successful
2024-11-11 18:51:04,406 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:04,431 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:04,752 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:04,777 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:09,500 DEBUG Query successful
2024-11-11 18:51:09,513 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:09,537 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:13,873 DEBUG Query successful
2024-11-11 18:51:17,740 DEBUG Query successful
2024-11-11 18:51:17,826 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:17,850 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:17,865 DEBUG Query successful
2024-11-11 18:51:17,878 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:17,901 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:20,277 DEBUG Query successful
2024-11-11 18:51:20,576 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:20,604 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:28,568 DEBUG Query successful
2024-11-11 18:51:28,569 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:28,592 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:32,965 DEBUG Query successful
2024-11-11 18:51:33,043 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:33,067 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:39,535 DEBUG Query successful
2024-11-11 18:51:39,537 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:39,559 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:41,544 DEBUG Query successful
2024-11-11 18:51:41,888 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:41,912 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:43,763 DEBUG Query successful
2024-11-11 18:51:43,774 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:43,798 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:54,489 DEBUG Query successful
2024-11-11 18:51:54,576 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:54,600 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:51:57,985 DEBUG Query successful
2024-11-11 18:51:57,987 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:51:58,010 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:52:01,675 DEBUG Query successful
2024-11-11 18:52:02,106 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:52:02,129 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:52:04,040 DEBUG Query successful
2024-11-11 18:52:04,053 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:52:04,076 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:52:14,834 DEBUG Query successful
2024-11-11 18:52:14,837 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:52:14,860 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:52:16,781 DEBUG Query successful
2024-11-11 18:52:17,199 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:52:17,223 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:52:20,412 DEBUG Query successful
2024-11-11 18:52:20,498 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:52:20,522 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:52:25,205 DEBUG Query successful
2024-11-11 18:52:25,282 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:52:25,317 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:52:33,129 DEBUG Query successful
2024-11-11 18:52:33,130 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:52:33,154 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:52:34,020 DEBUG Query successful
2024-11-11 18:52:34,022 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:52:34,046 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:52:42,488 DEBUG Query successful
2024-11-11 18:52:42,489 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:52:42,513 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:52:43,638 DEBUG Query successful
2024-11-11 18:52:43,640 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:52:43,663 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:52:44,552 DEBUG Query successful
2024-11-11 18:52:44,893 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:52:44,916 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:52:45,885 DEBUG Query successful
2024-11-11 18:52:46,207 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:52:46,230 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:52:53,290 DEBUG Query successful
2024-11-11 18:52:53,367 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:52:53,391 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:52:59,438 DEBUG Query successful
2024-11-11 18:52:59,526 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:52:59,551 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:01,533 DEBUG Query successful
2024-11-11 18:53:01,535 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:01,558 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:08,669 DEBUG Query successful
2024-11-11 18:53:08,671 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:08,708 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:10,522 DEBUG Query successful
2024-11-11 18:53:10,523 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:10,547 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:14,387 DEBUG Query successful
2024-11-11 18:53:14,731 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:14,755 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:20,037 DEBUG Query successful
2024-11-11 18:53:20,039 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:20,062 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:22,241 DEBUG Query successful
2024-11-11 18:53:22,584 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:22,608 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:22,950 DEBUG Query successful
2024-11-11 18:53:23,037 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:23,062 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:29,636 DEBUG Query successful
2024-11-11 18:53:29,637 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:29,660 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:34,960 DEBUG Query successful
2024-11-11 18:53:35,034 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:35,058 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:39,276 DEBUG Query successful
2024-11-11 18:53:39,289 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:39,312 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:43,458 DEBUG Query successful
2024-11-11 18:53:43,790 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:43,817 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:43,978 DEBUG Query successful
2024-11-11 18:53:43,989 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:44,022 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:51,590 DEBUG Query successful
2024-11-11 18:53:51,676 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:51,699 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:56,289 DEBUG Query successful
2024-11-11 18:53:56,290 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:56,322 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:58,787 DEBUG Query successful
2024-11-11 18:53:59,007 DEBUG Query successful
2024-11-11 18:53:59,008 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:59,046 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:53:59,128 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:53:59,151 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:54:09,362 DEBUG Query successful
2024-11-11 18:54:09,374 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:54:09,398 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:54:11,505 DEBUG Query successful
2024-11-11 18:54:11,844 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:54:11,868 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:54:14,749 DEBUG Query successful
2024-11-11 18:54:14,824 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:54:14,848 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:54:21,153 DEBUG Query successful
2024-11-11 18:54:21,240 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:54:21,264 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:54:22,624 DEBUG Query successful
2024-11-11 18:54:22,636 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:54:22,659 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:54:28,428 DEBUG Query successful
2024-11-11 18:54:28,430 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:54:28,454 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:54:34,143 DEBUG Query successful
2024-11-11 18:54:34,145 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:54:34,168 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:54:36,219 DEBUG Query successful
2024-11-11 18:54:36,572 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:54:36,596 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:54:37,952 DEBUG Query successful
2024-11-11 18:54:37,953 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:54:37,976 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:54:40,268 DEBUG Query successful
2024-11-11 18:54:40,653 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:54:40,676 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:54:47,176 DEBUG Query successful
2024-11-11 18:54:47,267 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:54:47,290 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:54:48,146 DEBUG Query successful
2024-11-11 18:54:48,223 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:54:48,250 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:54:54,285 DEBUG Query successful
2024-11-11 18:54:54,299 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:54:54,323 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:54:55,474 DEBUG Query successful
2024-11-11 18:54:55,475 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:54:55,499 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:55:03,594 DEBUG Query successful
2024-11-11 18:55:03,595 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:55:03,620 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:55:06,051 DEBUG Query successful
2024-11-11 18:55:06,052 DEBUG Query successful
2024-11-11 18:55:06,053 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:55:06,084 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:55:07,968 DEBUG Query successful
2024-11-11 18:55:08,263 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:55:08,288 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:55:20,267 DEBUG Query successful
2024-11-11 18:55:20,353 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:55:20,376 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:55:28,863 DEBUG Query successful
2024-11-11 18:55:28,864 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:55:28,888 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:55:38,348 DEBUG Query successful
2024-11-11 18:55:38,360 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:55:38,383 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:55:40,397 DEBUG Query successful
2024-11-11 18:55:40,723 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:55:40,749 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:55:51,571 DEBUG Query successful
2024-11-11 18:55:51,658 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:55:51,681 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:56:00,455 DEBUG Query successful
2024-11-11 18:56:00,457 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:56:00,480 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:56:10,352 DEBUG Query successful
2024-11-11 18:56:10,354 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:56:10,378 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:56:12,159 DEBUG Query successful
2024-11-11 18:56:12,468 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:56:12,493 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:56:25,345 DEBUG Query successful
2024-11-11 18:56:25,421 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:56:25,444 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:56:34,498 DEBUG Query successful
2024-11-11 18:56:34,500 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:56:34,533 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:56:45,791 DEBUG Query successful
2024-11-11 18:56:45,792 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:56:45,815 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:56:47,897 DEBUG Query successful
2024-11-11 18:56:48,194 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:56:48,216 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:56:58,536 DEBUG Query successful
2024-11-11 18:56:58,613 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:56:58,644 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:57:05,037 DEBUG Query successful
2024-11-11 18:57:05,038 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:57:05,064 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:57:16,336 DEBUG Query successful
2024-11-11 18:57:16,338 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 18:57:16,361 DEBUG Model set to: gpt-4o-mini
2024-11-11 18:57:18,354 DEBUG Query successful
2024-11-11 22:11:05,791 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:11:06,387 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:11:06,601 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:11:07,151 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:11:07,663 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:11:07,777 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:11:11,065 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:11:11,725 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:11:22,357 DEBUG Query successful
2024-11-11 22:11:22,440 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:11:22,469 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:11:33,338 DEBUG Query successful
2024-11-11 22:11:33,428 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:11:33,455 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:11:43,895 DEBUG Query successful
2024-11-11 22:11:43,996 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:11:44,029 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:12:01,037 DEBUG Query successful
2024-11-11 22:12:01,037 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:12:01,069 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:12:02,509 DEBUG Query successful
2024-11-11 22:12:02,598 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:12:02,624 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:12:02,876 DEBUG Query successful
2024-11-11 22:12:02,877 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:12:02,901 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:12:03,094 DEBUG Query successful
2024-11-11 22:12:03,095 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:12:03,121 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:12:14,569 DEBUG Query successful
2024-11-11 22:12:14,581 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:12:14,612 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:12:15,841 DEBUG Query successful
2024-11-11 22:12:15,842 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:12:15,871 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:12:33,967 DEBUG Query successful
2024-11-11 22:12:34,324 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:12:34,353 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:12:34,954 DEBUG Query successful
2024-11-11 22:12:34,967 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:12:34,994 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:12:36,278 DEBUG Query successful
2024-11-11 22:12:36,292 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:12:36,321 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:12:41,746 DEBUG Query successful
2024-11-11 22:12:42,118 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:12:42,143 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:12:44,349 DEBUG Query successful
2024-11-11 22:12:44,669 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:12:44,696 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:12:45,597 DEBUG Query successful
2024-11-11 22:12:45,600 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:12:45,625 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:12:56,611 DEBUG Query successful
2024-11-11 22:12:56,694 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:12:56,719 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:12:58,189 DEBUG Query successful
2024-11-11 22:12:58,600 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:12:58,629 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:05,975 DEBUG Query successful
2024-11-11 22:13:06,054 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:06,079 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:10,931 DEBUG Query successful
2024-11-11 22:13:10,932 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:10,958 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:14,299 DEBUG Query successful
2024-11-11 22:13:14,381 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:14,405 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:15,659 DEBUG Query successful
2024-11-11 22:13:15,660 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:15,688 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:21,766 DEBUG Query successful
2024-11-11 22:13:21,846 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:21,871 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:24,028 DEBUG Query successful
2024-11-11 22:13:24,030 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:24,055 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:25,179 DEBUG Query successful
2024-11-11 22:13:25,180 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:25,205 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:28,916 DEBUG Query successful
2024-11-11 22:13:29,260 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:29,298 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:31,030 DEBUG Query successful
2024-11-11 22:13:31,031 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:31,056 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:31,513 DEBUG Query successful
2024-11-11 22:13:31,515 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:31,538 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:43,199 DEBUG Query successful
2024-11-11 22:13:43,564 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:43,591 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:47,979 DEBUG Query successful
2024-11-11 22:13:47,980 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:48,004 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:53,783 DEBUG Query successful
2024-11-11 22:13:53,785 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:53,810 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:55,695 DEBUG Query successful
2024-11-11 22:13:56,032 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:56,057 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:56,689 DEBUG Query successful
2024-11-11 22:13:56,774 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:56,801 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:57,691 DEBUG Query successful
2024-11-11 22:13:58,033 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:13:58,060 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:13:59,938 DEBUG Query successful
2024-11-11 22:14:00,029 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:00,054 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:03,513 DEBUG Query successful
2024-11-11 22:14:03,513 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:03,545 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:08,665 DEBUG Query successful
2024-11-11 22:14:08,744 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:08,769 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:11,999 DEBUG Query successful
2024-11-11 22:14:12,000 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:12,025 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:12,655 DEBUG Query successful
2024-11-11 22:14:12,739 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:12,763 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:19,757 DEBUG Query successful
2024-11-11 22:14:19,760 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:19,785 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:21,317 DEBUG Query successful
2024-11-11 22:14:21,318 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:21,346 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:21,823 DEBUG Query successful
2024-11-11 22:14:21,825 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:21,855 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:22,451 DEBUG Query successful
2024-11-11 22:14:22,452 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:22,476 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:24,010 DEBUG Query successful
2024-11-11 22:14:24,349 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:24,377 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:25,841 DEBUG Query successful
2024-11-11 22:14:26,315 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:26,355 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:34,277 DEBUG Query successful
2024-11-11 22:14:34,278 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:34,303 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:38,501 DEBUG Query successful
2024-11-11 22:14:38,502 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:38,528 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:44,680 DEBUG Query successful
2024-11-11 22:14:44,789 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:44,814 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:47,111 DEBUG Query successful
2024-11-11 22:14:47,182 DEBUG Query successful
2024-11-11 22:14:47,556 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:47,587 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:47,648 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:47,678 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:51,610 DEBUG Query successful
2024-11-11 22:14:51,700 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:51,727 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:14:51,728 DEBUG Query successful
2024-11-11 22:14:51,729 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:14:51,755 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:00,596 DEBUG Query successful
2024-11-11 22:15:00,682 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:00,710 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:01,991 DEBUG Query successful
2024-11-11 22:15:01,992 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:02,019 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:04,675 DEBUG Query successful
2024-11-11 22:15:04,767 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:04,793 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:07,795 DEBUG Query successful
2024-11-11 22:15:07,795 DEBUG Query successful
2024-11-11 22:15:07,796 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:07,796 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:07,822 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:07,822 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:12,651 DEBUG Query successful
2024-11-11 22:15:12,969 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:12,997 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:14,316 DEBUG Query successful
2024-11-11 22:15:14,318 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:14,346 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:16,722 DEBUG Query successful
2024-11-11 22:15:16,724 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:16,749 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:19,838 DEBUG Query successful
2024-11-11 22:15:19,840 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:19,866 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:21,750 DEBUG Query successful
2024-11-11 22:15:22,094 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:22,120 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:26,450 DEBUG Query successful
2024-11-11 22:15:26,768 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:26,792 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:27,532 DEBUG Query successful
2024-11-11 22:15:27,623 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:27,647 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:28,037 DEBUG Query successful
2024-11-11 22:15:28,038 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:28,063 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:33,669 DEBUG Query successful
2024-11-11 22:15:33,670 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:33,695 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:34,763 DEBUG Query successful
2024-11-11 22:15:34,853 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:34,892 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:35,589 DEBUG Query successful
2024-11-11 22:15:35,958 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:35,983 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:38,448 DEBUG Query successful
2024-11-11 22:15:38,530 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:38,553 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:41,682 DEBUG Query successful
2024-11-11 22:15:41,683 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:41,717 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:45,952 DEBUG Query successful
2024-11-11 22:15:45,954 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:45,981 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:46,079 DEBUG Query successful
2024-11-11 22:15:46,080 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:46,104 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:52,575 DEBUG Query successful
2024-11-11 22:15:52,660 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:52,684 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:57,268 DEBUG Query successful
2024-11-11 22:15:57,675 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:57,701 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:15:59,697 DEBUG Query successful
2024-11-11 22:15:59,698 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:15:59,723 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:06,968 DEBUG Query successful
2024-11-11 22:16:06,969 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:06,993 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:07,061 DEBUG Query successful
2024-11-11 22:16:07,063 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:07,089 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:08,886 DEBUG Query successful
2024-11-11 22:16:09,251 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:09,277 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:10,478 DEBUG Query successful
2024-11-11 22:16:10,794 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:10,819 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:19,161 DEBUG Query successful
2024-11-11 22:16:19,243 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:19,273 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:21,199 DEBUG Query successful
2024-11-11 22:16:21,201 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:21,228 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:27,010 DEBUG Query successful
2024-11-11 22:16:27,431 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:27,457 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:29,816 DEBUG Query successful
2024-11-11 22:16:29,896 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:29,920 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:31,242 DEBUG Query successful
2024-11-11 22:16:31,244 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:31,270 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:31,661 DEBUG Query successful
2024-11-11 22:16:31,741 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:31,765 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:40,193 DEBUG Query successful
2024-11-11 22:16:40,286 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:40,323 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:42,058 DEBUG Query successful
2024-11-11 22:16:42,059 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:42,083 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:42,146 DEBUG Query successful
2024-11-11 22:16:42,147 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:42,171 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:42,890 DEBUG Query successful
2024-11-11 22:16:42,891 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:42,916 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:46,935 DEBUG Query successful
2024-11-11 22:16:47,384 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:47,411 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:49,676 DEBUG Query successful
2024-11-11 22:16:49,678 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:49,702 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:16:56,889 DEBUG Query successful
2024-11-11 22:16:56,890 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:16:56,914 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:02,008 DEBUG Query successful
2024-11-11 22:17:02,009 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:02,033 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:10,099 DEBUG Query successful
2024-11-11 22:17:10,488 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:10,515 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:10,666 DEBUG Query successful
2024-11-11 22:17:11,065 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:11,094 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:12,200 DEBUG Query successful
2024-11-11 22:17:12,201 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:12,226 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:15,345 DEBUG Query successful
2024-11-11 22:17:15,425 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:15,450 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:20,744 DEBUG Query successful
2024-11-11 22:17:21,085 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:21,111 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:26,920 DEBUG Query successful
2024-11-11 22:17:26,932 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:26,971 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:27,273 DEBUG Query successful
2024-11-11 22:17:27,359 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:27,384 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:35,851 DEBUG Query successful
2024-11-11 22:17:35,932 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:35,956 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:37,532 DEBUG Query successful
2024-11-11 22:17:37,611 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:37,635 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:40,470 DEBUG Query successful
2024-11-11 22:17:40,473 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:40,507 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:44,918 DEBUG Query successful
2024-11-11 22:17:44,919 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:44,944 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:45,946 DEBUG Query successful
2024-11-11 22:17:45,958 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:45,984 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:46,310 DEBUG Query successful
2024-11-11 22:17:46,323 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:46,350 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:52,170 DEBUG Query successful
2024-11-11 22:17:52,535 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:52,567 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:53,327 DEBUG Query successful
2024-11-11 22:17:53,330 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:53,356 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:17:58,139 DEBUG Query successful
2024-11-11 22:17:58,498 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:17:58,529 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:03,805 DEBUG Query successful
2024-11-11 22:18:03,807 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:03,832 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:12,092 DEBUG Query successful
2024-11-11 22:18:12,469 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:12,495 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:16,775 DEBUG Query successful
2024-11-11 22:18:16,778 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:16,808 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:17,119 DEBUG Query successful
2024-11-11 22:18:17,206 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:17,239 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:21,628 DEBUG Query successful
2024-11-11 22:18:21,708 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:21,745 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:23,223 DEBUG Query successful
2024-11-11 22:18:23,619 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:23,645 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:26,323 DEBUG Query successful
2024-11-11 22:18:26,402 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:26,428 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:26,657 DEBUG Query successful
2024-11-11 22:18:26,658 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:26,684 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:28,412 DEBUG Query successful
2024-11-11 22:18:28,413 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:28,438 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:38,407 DEBUG Query successful
2024-11-11 22:18:38,408 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:38,434 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:38,917 DEBUG Query successful
2024-11-11 22:18:39,000 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:39,030 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:48,050 DEBUG Query successful
2024-11-11 22:18:48,052 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:48,076 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:48,909 DEBUG Query successful
2024-11-11 22:18:48,910 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:48,935 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:49,642 DEBUG Query successful
2024-11-11 22:18:49,644 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:49,675 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:50,432 DEBUG Query successful
2024-11-11 22:18:50,830 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:50,860 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:51,076 DEBUG Query successful
2024-11-11 22:18:51,078 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:51,103 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:52,420 DEBUG Query successful
2024-11-11 22:18:52,736 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:52,765 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:18:59,457 DEBUG Query successful
2024-11-11 22:18:59,769 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:18:59,796 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:01,922 DEBUG Query successful
2024-11-11 22:19:01,924 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:01,952 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:12,559 DEBUG Query successful
2024-11-11 22:19:12,647 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:12,671 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:16,163 DEBUG Query successful
2024-11-11 22:19:16,243 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:16,268 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:21,156 DEBUG Query successful
2024-11-11 22:19:21,534 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:21,560 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:24,036 DEBUG Query successful
2024-11-11 22:19:24,037 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:24,062 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:26,199 DEBUG Query successful
2024-11-11 22:19:26,282 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:26,307 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:27,022 DEBUG Query successful
2024-11-11 22:19:27,023 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:27,048 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:36,342 DEBUG Query successful
2024-11-11 22:19:36,422 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:36,447 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:38,163 DEBUG Query successful
2024-11-11 22:19:38,164 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:38,192 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:46,791 DEBUG Query successful
2024-11-11 22:19:46,793 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:46,817 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:46,925 DEBUG Query successful
2024-11-11 22:19:46,927 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:46,951 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:49,573 DEBUG Query successful
2024-11-11 22:19:49,575 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:49,602 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:51,969 DEBUG Query successful
2024-11-11 22:19:52,288 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:52,314 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:53,493 DEBUG Query successful
2024-11-11 22:19:53,494 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:53,520 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:19:56,338 DEBUG Query successful
2024-11-11 22:19:58,908 DEBUG Query successful
2024-11-11 22:19:59,260 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:19:59,283 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:00,175 DEBUG Query successful
2024-11-11 22:20:00,176 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:00,199 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:02,429 DEBUG Query successful
2024-11-11 22:20:02,723 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:02,748 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:04,459 DEBUG Query successful
2024-11-11 22:20:04,536 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:04,561 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:09,541 DEBUG Query successful
2024-11-11 22:20:09,622 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:09,646 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:16,936 DEBUG Query successful
2024-11-11 22:20:16,937 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:16,961 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:18,155 DEBUG Query successful
2024-11-11 22:20:18,235 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:18,262 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:19,223 DEBUG Query successful
2024-11-11 22:20:19,224 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:19,249 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:31,695 DEBUG Query successful
2024-11-11 22:20:31,696 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:31,720 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:32,118 DEBUG Query successful
2024-11-11 22:20:32,120 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:32,144 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:34,156 DEBUG Query successful
2024-11-11 22:20:34,158 DEBUG Query successful
2024-11-11 22:20:34,158 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:34,182 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:34,513 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:34,539 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:37,483 DEBUG Query successful
2024-11-11 22:20:37,765 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:37,792 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:41,234 DEBUG Query successful
2024-11-11 22:20:41,235 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:41,267 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:45,448 DEBUG Query successful
2024-11-11 22:20:45,531 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:45,556 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:49,329 DEBUG Query successful
2024-11-11 22:20:52,075 DEBUG Query successful
2024-11-11 22:20:52,076 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:52,100 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:52,142 DEBUG Query successful
2024-11-11 22:20:52,222 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:52,246 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:20:58,753 DEBUG Query successful
2024-11-11 22:20:58,754 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:20:58,777 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:21:03,818 DEBUG Query successful
2024-11-11 22:21:03,819 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:21:03,843 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:21:07,782 DEBUG Query successful
2024-11-11 22:21:11,296 DEBUG Query successful
2024-11-11 22:21:11,297 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-11 22:21:11,329 DEBUG Model set to: gpt-4o-mini
2024-11-11 22:21:13,163 DEBUG Query successful
2024-11-12 12:11:45,890 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:11:47,114 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:11:48,030 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:11:49,360 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:11:50,042 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:11:50,743 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:11:52,772 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:11:53,424 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:13:10,207 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:13:10,538 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:13:10,538 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:13:10,538 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:13:10,567 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:13:10,572 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:13:10,572 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:13:10,573 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:14:34,382 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:14:34,383 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:14:34,383 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:14:34,383 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:14:34,392 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:14:34,392 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:14:34,392 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:14:34,393 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:14:34,430 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:14:34,433 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:14:34,435 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:14:34,435 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:15:58,570 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:15:58,570 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:15:58,582 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:15:58,582 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:15:58,583 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:15:58,584 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:15:58,609 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:15:58,610 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:15:58,619 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:15:58,620 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:17:22,720 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:17:22,720 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:17:22,720 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:17:22,727 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:17:22,721 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:17:22,727 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:17:22,728 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:17:22,729 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:17:22,758 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:17:22,758 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:17:22,762 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:17:22,770 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:18:46,896 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:18:46,895 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:18:46,897 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:18:46,896 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:18:46,905 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:18:46,905 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:18:46,907 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:18:46,907 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:18:46,940 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:18:46,940 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:18:46,943 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:18:46,951 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:20:11,064 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:20:11,064 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:20:11,064 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:20:11,064 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:20:11,651 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:20:11,654 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:20:11,657 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:20:11,662 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:20:11,704 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:20:11,712 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:20:11,714 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:20:11,718 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:21:35,231 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:21:35,231 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:21:35,231 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:21:35,231 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:21:35,344 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:21:35,348 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:21:35,359 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:21:35,360 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:21:35,373 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:21:35,375 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:21:35,389 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:21:35,390 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:22:59,409 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:22:59,409 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:22:59,410 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:22:59,421 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:22:59,421 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:22:59,422 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:22:59,448 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:22:59,449 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:22:59,449 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:22:59,452 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:24:23,593 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:24:23,592 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:24:23,592 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:24:23,593 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:24:23,604 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:24:23,604 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:24:23,605 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:24:23,606 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:24:23,632 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:24:23,634 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:24:23,637 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:24:23,643 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:25:47,782 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:25:47,783 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:25:47,794 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:25:47,794 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:25:47,794 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:25:47,795 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:25:47,820 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:25:47,823 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:25:47,825 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:25:47,826 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:27:11,962 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:27:11,962 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:27:11,962 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:27:11,962 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:27:11,969 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:27:11,969 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:27:11,969 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:27:11,970 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:27:11,999 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:27:12,001 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:27:12,001 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:27:12,004 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:28:36,151 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:28:36,151 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:28:36,151 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:28:36,151 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:28:36,696 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:28:36,709 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:28:36,716 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:28:36,738 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:28:36,738 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:28:36,749 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:28:36,752 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:28:36,770 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:30:00,298 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:30:00,298 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:30:00,298 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:30:00,298 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:30:00,413 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:30:00,423 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:30:00,424 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:30:00,426 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:30:00,449 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:30:00,451 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:30:00,455 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:30:00,458 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:31:24,441 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:31:24,441 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:31:24,441 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:31:24,441 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:31:24,449 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:31:24,450 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:31:24,450 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:31:24,450 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:31:24,478 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:31:24,480 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:31:24,481 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:31:24,483 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:32:48,628 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:32:48,628 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:32:48,628 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:32:48,628 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:32:48,639 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:32:48,640 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:32:48,640 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:32:48,640 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:32:48,666 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:32:48,667 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:32:48,668 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:32:48,669 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:34:12,771 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:34:12,771 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:34:12,771 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:34:12,771 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:34:12,790 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:34:12,790 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:34:12,790 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:34:12,818 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:34:12,824 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:34:12,825 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:34:12,825 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:35:36,933 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:35:36,933 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:35:36,933 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:35:36,933 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:35:36,945 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:35:36,945 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:35:36,946 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:35:36,947 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:35:36,977 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:35:36,977 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:35:36,981 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:35:36,987 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:37:01,120 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:37:01,120 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:37:01,121 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:37:01,121 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:37:01,683 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:37:01,693 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:37:01,694 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:37:01,716 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:37:01,717 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:37:01,724 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:37:01,728 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:38:25,298 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:38:25,298 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:38:25,298 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:38:25,298 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:38:25,412 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:38:25,419 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:38:25,419 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:38:25,421 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:38:25,448 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:38:25,449 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:38:25,458 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:38:25,467 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:39:49,499 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:39:49,499 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:39:49,499 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:39:49,499 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:39:49,510 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:39:49,510 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:39:49,510 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:39:49,539 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:39:49,541 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:39:49,543 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:39:49,550 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:41:13,653 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:41:13,653 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:41:13,653 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:41:13,653 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:41:13,663 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:41:13,664 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:41:13,664 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:41:13,665 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:41:13,693 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:41:13,694 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:41:13,697 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:41:13,705 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:42:37,806 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:42:37,806 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:42:37,806 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:42:37,815 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:42:37,816 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:42:37,816 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:42:37,817 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:42:37,843 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:42:37,846 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:42:37,850 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:42:37,850 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:44:02,442 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:44:02,442 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:44:02,442 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:44:02,442 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:44:02,451 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:44:02,451 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:44:02,452 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:44:02,483 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:44:02,488 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:44:02,490 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:44:02,491 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:45:26,610 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:45:26,610 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:45:26,610 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:45:26,611 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:45:27,152 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:45:27,167 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:45:27,176 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:45:27,183 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:45:27,185 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:45:27,199 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:45:27,212 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:45:27,221 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:46:50,762 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:46:50,762 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:46:50,762 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:46:50,892 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:46:50,892 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:46:50,899 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:46:50,904 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:46:50,922 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:46:50,925 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:46:50,935 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:46:50,937 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:48:14,924 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:48:14,924 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:48:14,923 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:48:14,933 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:48:14,934 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:48:14,934 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:48:14,934 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:48:14,965 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:48:14,968 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:48:14,968 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:48:14,971 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:49:39,072 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:49:39,072 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:49:39,072 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:49:39,072 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:49:39,085 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:49:39,085 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:49:39,085 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:49:39,086 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:49:39,112 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:49:39,118 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:49:39,120 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:49:39,122 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:51:03,259 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:51:03,259 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:51:03,259 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:51:03,259 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:51:03,269 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:51:03,269 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:51:03,269 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:51:03,270 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:51:03,297 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:51:03,302 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:51:03,311 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:51:03,312 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:52:27,379 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:52:27,379 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:52:27,379 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:52:27,379 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:52:27,400 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:52:27,400 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:52:27,400 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:52:27,400 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:52:27,428 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:52:27,432 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:52:27,436 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:52:27,438 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:53:51,571 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:53:51,571 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:53:51,571 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:53:51,571 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:53:52,173 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:53:52,202 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:53:52,225 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:53:52,228 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:53:52,257 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:53:52,269 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:53:52,270 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:55:15,748 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:55:15,748 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:55:15,748 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:55:15,748 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:55:15,863 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:55:15,865 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:55:15,868 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:55:15,869 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:55:15,895 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:55:15,899 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:55:15,899 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:55:15,899 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:56:39,937 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:56:39,936 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:56:39,937 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:56:39,937 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:56:39,946 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:56:39,946 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:56:39,946 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:56:39,947 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:56:39,972 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:56:39,974 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:56:39,979 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:56:39,980 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:58:04,088 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:58:04,089 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:58:04,089 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 62, in start_tls
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 57, in start_tls
    sock = ssl_context.wrap_socket(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1070, in _create
    self.do_handshake()
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    stream = stream.start_tls(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 54, in start_tls
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1005, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1015, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-12 12:58:04,100 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:58:04,101 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:58:04,101 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:58:04,102 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 12:58:04,127 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:58:04,130 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:58:04,131 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:58:04,140 DEBUG Model set to: gpt-4o-mini
2024-11-12 12:58:50,686 DEBUG Query successful
2024-11-12 12:58:51,334 DEBUG Query successful
2024-11-12 12:58:53,450 DEBUG Query successful
2024-11-12 12:58:53,670 DEBUG Query successful
2024-11-12 17:46:27,103 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:46:27,181 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:46:28,413 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:46:28,417 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:46:30,014 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:46:30,796 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:47:30,648 DEBUG Query successful
2024-11-12 17:47:30,737 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:47:30,764 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:47:38,391 DEBUG Query successful
2024-11-12 17:47:38,392 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:47:38,418 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:48:09,906 DEBUG Query successful
2024-11-12 17:48:09,986 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:48:10,009 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:48:16,936 DEBUG Query successful
2024-11-12 17:48:17,026 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:48:17,073 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:48:19,220 DEBUG Query successful
2024-11-12 17:48:19,222 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:48:19,246 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:48:20,425 DEBUG Query successful
2024-11-12 17:48:20,438 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:48:20,464 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:48:22,408 DEBUG Query successful
2024-11-12 17:48:22,409 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:48:22,433 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:48:23,793 DEBUG Query successful
2024-11-12 17:48:24,235 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:48:24,271 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:04,993 DEBUG Query successful
2024-11-12 17:49:04,995 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:05,020 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:09,312 DEBUG Query successful
2024-11-12 17:49:09,721 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:09,760 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:18,167 DEBUG Query successful
2024-11-12 17:49:18,180 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:18,206 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:18,249 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:18,274 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:20,700 DEBUG Query successful
2024-11-12 17:49:21,147 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:21,172 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:23,881 DEBUG Query successful
2024-11-12 17:49:23,882 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:23,907 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:26,394 DEBUG Query successful
2024-11-12 17:49:26,482 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:26,506 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:33,531 DEBUG Query successful
2024-11-12 17:49:33,532 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:33,556 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:41,133 DEBUG Query successful
2024-11-12 17:49:41,134 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:41,163 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:42,103 DEBUG Query successful
2024-11-12 17:49:42,183 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:42,209 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:43,138 DEBUG Query successful
2024-11-12 17:49:43,498 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:43,523 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:47,046 DEBUG Query successful
2024-11-12 17:49:47,047 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:47,070 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:50,756 DEBUG Query successful
2024-11-12 17:49:50,758 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:50,790 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:56,348 DEBUG Query successful
2024-11-12 17:49:56,743 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:56,777 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:49:58,454 DEBUG Query successful
2024-11-12 17:49:58,544 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:49:58,569 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:50:04,380 DEBUG Query successful
2024-11-12 17:50:04,381 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:50:04,406 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:50:06,354 DEBUG Query successful
2024-11-12 17:50:06,355 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:50:06,380 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:50:07,902 DEBUG Query successful
2024-11-12 17:50:08,265 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:50:08,295 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:50:13,647 DEBUG Query successful
2024-11-12 17:50:13,741 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:50:13,774 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:50:21,543 DEBUG Query successful
2024-11-12 17:50:21,544 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:50:21,569 DEBUG Model set to: gpt-4o-mini
2024-11-12 17:50:25,309 DEBUG Query successful
2024-11-12 17:50:25,311 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 17:50:25,339 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:03:13,753 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:03:14,116 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:03:14,445 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:03:15,517 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:03:15,884 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:03:15,922 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:03:18,379 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:03:19,016 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:03:44,737 DEBUG Query successful
2024-11-12 21:03:44,786 DEBUG Query successful
2024-11-12 21:03:44,818 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:03:44,847 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:03:44,875 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:03:44,899 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:03:47,840 DEBUG Query successful
2024-11-12 21:03:47,927 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:03:47,952 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:03:49,003 DEBUG Query successful
2024-11-12 21:03:49,083 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:03:49,108 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:03:51,969 DEBUG Query successful
2024-11-12 21:03:51,970 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:03:51,998 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:03:52,480 DEBUG Query successful
2024-11-12 21:03:52,480 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:03:52,506 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:03:54,316 DEBUG Query successful
2024-11-12 21:03:54,317 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:03:54,350 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:03:54,958 DEBUG Query successful
2024-11-12 21:03:54,958 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:03:54,988 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:05:53,098 DEBUG Query successful
2024-11-12 21:05:53,106 DEBUG Query successful
2024-11-12 21:05:53,111 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:05:53,111 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:05:53,139 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:05:53,142 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:05:53,238 DEBUG Query successful
2024-11-12 21:05:53,240 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:05:53,269 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:05:53,479 DEBUG Query successful
2024-11-12 21:05:53,492 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:05:53,519 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:05:59,829 DEBUG Query successful
2024-11-12 21:05:59,966 DEBUG Query successful
2024-11-12 21:05:59,966 DEBUG Query successful
2024-11-12 21:10:03,914 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:10:04,102 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:10:04,565 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:10:05,264 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:10:05,467 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:10:05,577 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:10:09,440 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:10:10,058 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:11:01,700 DEBUG Query successful
2024-11-12 21:11:01,781 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:11:01,807 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:11:06,807 DEBUG Query successful
2024-11-12 21:11:06,890 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:11:06,918 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:11:07,057 DEBUG Query successful
2024-11-12 21:11:07,138 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:11:07,163 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:11:07,754 DEBUG Query successful
2024-11-12 21:11:07,841 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:11:07,867 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:11:09,108 DEBUG Query successful
2024-11-12 21:11:09,109 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:11:09,135 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:11:12,232 DEBUG Query successful
2024-11-12 21:11:12,244 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:11:12,270 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:11:14,072 DEBUG Query successful
2024-11-12 21:11:14,074 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:11:14,100 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:11:15,496 DEBUG Query successful
2024-11-12 21:11:15,497 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:11:15,523 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:12:07,892 DEBUG Query successful
2024-11-12 21:12:07,906 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:12:07,936 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:12:16,582 DEBUG Query successful
2024-11-12 21:12:16,993 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:12:17,025 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:12:22,789 DEBUG Query successful
2024-11-12 21:12:22,802 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:12:22,830 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:12:27,915 DEBUG Query successful
2024-11-12 21:12:28,330 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:12:28,359 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:12:42,098 DEBUG Query successful
2024-11-12 21:12:42,100 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:12:42,125 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:12:42,908 DEBUG Query successful
2024-11-12 21:12:42,921 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:12:42,951 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:12:45,282 DEBUG Query successful
2024-11-12 21:12:45,620 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:12:45,647 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:12:47,181 DEBUG Query successful
2024-11-12 21:12:47,277 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:12:47,296 DEBUG Query successful
2024-11-12 21:12:47,320 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:12:48,022 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:12:48,075 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:12:50,196 DEBUG Query successful
2024-11-12 21:12:50,298 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:12:50,333 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:12:54,513 DEBUG Query successful
2024-11-12 21:12:54,526 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:12:54,554 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:12:58,307 DEBUG Query successful
2024-11-12 21:12:58,308 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:12:58,334 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:12:59,858 DEBUG Query successful
2024-11-12 21:12:59,947 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:12:59,978 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:13:10,432 DEBUG Query successful
2024-11-12 21:13:10,525 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:13:10,551 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:13:12,138 DEBUG Query successful
2024-11-12 21:13:12,139 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:13:12,165 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:13:26,885 DEBUG Query successful
2024-11-12 21:13:26,886 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:13:26,911 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:01,923 DEBUG Query successful
2024-11-12 21:15:01,924 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:01,948 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:03,429 DEBUG Query successful
2024-11-12 21:15:03,433 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:03,461 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:03,699 DEBUG Query successful
2024-11-12 21:15:03,701 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:03,725 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:05,007 DEBUG Query successful
2024-11-12 21:15:05,391 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:05,419 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:07,266 DEBUG Query successful
2024-11-12 21:15:07,655 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:07,680 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:08,649 DEBUG Query successful
2024-11-12 21:15:09,008 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:09,043 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:22,369 DEBUG Query successful
2024-11-12 21:15:22,370 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:22,398 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:29,464 DEBUG Query successful
2024-11-12 21:15:29,835 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:29,862 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:40,569 DEBUG Query successful
2024-11-12 21:15:40,661 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:40,690 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:45,034 DEBUG Query successful
2024-11-12 21:15:45,141 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:45,164 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:47,268 DEBUG Query successful
2024-11-12 21:15:47,361 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:47,386 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:52,100 DEBUG Query successful
2024-11-12 21:15:52,101 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:52,128 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:54,135 DEBUG Query successful
2024-11-12 21:15:54,136 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:54,161 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:56,477 DEBUG Query successful
2024-11-12 21:15:56,478 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:56,503 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:15:56,639 DEBUG Query successful
2024-11-12 21:15:56,751 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:15:56,777 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:16:03,798 DEBUG Query successful
2024-11-12 21:16:03,800 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:16:03,826 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:16:08,056 DEBUG Query successful
2024-11-12 21:16:08,057 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:16:08,082 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:16:13,982 DEBUG Query successful
2024-11-12 21:16:13,983 DEBUG Query successful
2024-11-12 21:16:13,984 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:16:14,010 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:16:14,345 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:16:14,371 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:16:16,206 DEBUG Query successful
2024-11-12 21:16:16,207 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:16:16,231 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:16:18,532 DEBUG Query successful
2024-11-12 21:16:18,890 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:16:18,915 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:16:21,886 DEBUG Query successful
2024-11-12 21:16:22,253 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:16:22,277 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:16:22,982 DEBUG Query successful
2024-11-12 21:16:22,984 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:16:23,020 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:16:34,119 DEBUG Query successful
2024-11-12 21:16:34,487 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:16:34,529 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:16:42,514 DEBUG Query successful
2024-11-12 21:16:42,595 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:16:42,620 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:17:04,628 DEBUG Query successful
2024-11-12 21:17:04,629 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:17:04,654 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:19:53,257 DEBUG Query successful
2024-11-12 21:19:53,346 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:19:53,372 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:20:08,869 DEBUG Query successful
2024-11-12 21:20:08,870 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:20:08,895 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:20:12,806 DEBUG Query successful
2024-11-12 21:20:12,807 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:20:12,832 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:20:22,801 DEBUG Query successful
2024-11-12 21:20:23,158 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:20:23,182 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:20:44,661 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 28, in read
    return self._sock.recv(max_bytes)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1258, in recv
    return self.read(buflen)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1131, in read
    return self._sslobj.read(len)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 316, in handle_request
    return self._connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 112, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 91, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 155, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 191, in _receive_event
    data = self._network_stream.read(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 26, in read
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 28, in read
    return self._sock.recv(max_bytes)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1258, in recv
    return self.read(buflen)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1131, in read
    return self._sslobj.read(len)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 316, in handle_request
    return self._connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 112, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 91, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 155, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 191, in _receive_event
    data = self._network_stream.read(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 26, in read
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 28, in read
    return self._sock.recv(max_bytes)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1258, in recv
    return self.read(buflen)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1131, in read
    return self._sslobj.read(len)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 316, in handle_request
    return self._connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 112, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 91, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 155, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 191, in _receive_event
    data = self._network_stream.read(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 26, in read
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 28, in read
    return self._sock.recv(max_bytes)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1258, in recv
    return self.read(buflen)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1131, in read
    return self._sslobj.read(len)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 316, in handle_request
    return self._connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 112, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 91, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 155, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 191, in _receive_event
    data = self._network_stream.read(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 26, in read
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 990, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 990, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 990, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1000, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-11-12 21:20:44,935 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:20:44,961 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:20:56,949 DEBUG Query successful
2024-11-12 21:20:56,949 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:20:56,990 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:20:58,884 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 28, in read
    return self._sock.recv(max_bytes)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1258, in recv
    return self.read(buflen)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1131, in read
    return self._sslobj.read(len)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 316, in handle_request
    return self._connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 112, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 91, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 155, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 191, in _receive_event
    data = self._network_stream.read(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 26, in read
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 28, in read
    return self._sock.recv(max_bytes)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1258, in recv
    return self.read(buflen)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1131, in read
    return self._sslobj.read(len)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 316, in handle_request
    return self._connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 112, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 91, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 155, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 191, in _receive_event
    data = self._network_stream.read(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 26, in read
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 28, in read
    return self._sock.recv(max_bytes)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1258, in recv
    return self.read(buflen)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1131, in read
    return self._sslobj.read(len)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 316, in handle_request
    return self._connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 112, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 91, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 155, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 191, in _receive_event
    data = self._network_stream.read(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 26, in read
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 28, in read
    return self._sock.recv(max_bytes)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1258, in recv
    return self.read(buflen)
  File "D:\ide\conda\envs\vlm_sc2\lib\ssl.py", line 1131, in read
    return self._sslobj.read(len)
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 316, in handle_request
    return self._connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 112, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 91, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 155, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 191, in _receive_event
    data = self._network_stream.read(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 26, in read
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 981, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 388, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\agents\chat_agent.py", line 676, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\utils\commons.py", line 271, in wrapper
    return func(self, *args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\camel\models\openai_model.py", line 112, in run
    response = self._client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 704, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1268, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 945, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 990, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 990, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 990, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1083, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1000, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-11-12 21:20:58,996 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:20:59,022 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:21:11,974 DEBUG Query successful
2024-11-12 21:21:11,975 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:21:12,001 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:21:14,665 DEBUG Query successful
2024-11-12 21:21:14,667 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:21:14,692 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:21:23,495 DEBUG Query successful
2024-11-12 21:21:23,576 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:21:23,601 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:21:28,045 DEBUG Query successful
2024-11-12 21:21:28,377 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:21:28,402 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:21:40,134 DEBUG Query successful
2024-11-12 21:21:40,135 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:21:40,160 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:22:28,251 DEBUG Query successful
2024-11-12 21:22:28,253 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:22:28,279 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:22:31,359 DEBUG Query successful
2024-11-12 21:22:31,441 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:22:31,466 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:22:31,903 DEBUG Query successful
2024-11-12 21:22:32,255 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:22:32,284 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:22:33,964 DEBUG Query successful
2024-11-12 21:22:33,965 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:22:33,990 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:22:36,271 DEBUG Query successful
2024-11-12 21:22:36,283 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:22:36,307 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:22:37,594 DEBUG Query successful
2024-11-12 21:22:37,595 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:22:37,618 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:22:41,519 DEBUG Query successful
2024-11-12 21:22:41,856 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:22:41,885 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:22:43,045 DEBUG Query successful
2024-11-12 21:22:43,381 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:22:43,406 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:22:49,511 DEBUG Query successful
2024-11-12 21:22:49,591 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:22:49,616 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:22:50,346 DEBUG Query successful
2024-11-12 21:22:50,348 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:22:50,371 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:22:53,160 DEBUG Query successful
2024-11-12 21:22:53,513 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:22:53,540 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:22:54,993 DEBUG Query successful
2024-11-12 21:22:55,077 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:22:55,102 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:22:56,274 DEBUG Query successful
2024-11-12 21:22:56,275 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:22:56,301 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:22:59,011 DEBUG Query successful
2024-11-12 21:22:59,092 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:22:59,117 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:23:02,802 DEBUG Query successful
2024-11-12 21:23:02,804 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:23:02,830 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:23:06,847 DEBUG Query successful
2024-11-12 21:23:06,933 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:23:06,964 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:23:13,624 DEBUG Query successful
2024-11-12 21:23:13,625 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:23:13,649 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:23:18,882 DEBUG Query successful
2024-11-12 21:23:18,883 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:23:18,909 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:23:18,913 DEBUG Query successful
2024-11-12 21:23:18,916 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:23:18,943 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:23:22,656 DEBUG Query successful
2024-11-12 21:23:22,658 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:23:22,683 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:23:23,907 DEBUG Query successful
2024-11-12 21:23:24,295 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:23:24,326 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:23:27,043 DEBUG Query successful
2024-11-12 21:23:27,045 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:23:27,069 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:23:28,731 DEBUG Query successful
2024-11-12 21:23:29,083 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:23:29,107 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:23:32,530 DEBUG Query successful
2024-11-12 21:23:32,838 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:23:32,863 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:23:33,918 DEBUG Query successful
2024-11-12 21:23:33,919 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:23:33,951 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:23:40,982 DEBUG Query successful
2024-11-12 21:23:41,395 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:23:41,426 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:23:47,491 DEBUG Query successful
2024-11-12 21:23:47,592 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:23:47,616 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:23:55,700 DEBUG Query successful
2024-11-12 21:23:55,785 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:23:55,812 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:00,689 DEBUG Query successful
2024-11-12 21:24:00,701 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:00,727 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:07,501 DEBUG Query successful
2024-11-12 21:24:07,502 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:07,527 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:11,803 DEBUG Query successful
2024-11-12 21:24:11,904 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:11,931 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:12,936 DEBUG Query successful
2024-11-12 21:24:13,025 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:13,051 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:17,386 DEBUG Query successful
2024-11-12 21:24:17,387 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:17,416 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:20,697 DEBUG Query successful
2024-11-12 21:24:21,089 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:21,115 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:23,269 DEBUG Query successful
2024-11-12 21:24:23,270 DEBUG Query successful
2024-11-12 21:24:23,271 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:23,271 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:23,301 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:23,314 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:24,653 DEBUG Query successful
2024-11-12 21:24:24,655 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:24,680 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:29,092 DEBUG Query successful
2024-11-12 21:24:29,515 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:29,540 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:39,134 DEBUG Query successful
2024-11-12 21:24:39,214 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:39,240 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:41,398 DEBUG Query successful
2024-11-12 21:24:41,400 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:41,425 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:42,870 DEBUG Query successful
2024-11-12 21:24:42,871 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:42,896 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:44,003 DEBUG Query successful
2024-11-12 21:24:44,380 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:44,413 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:47,362 DEBUG Query successful
2024-11-12 21:24:47,777 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:47,804 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:49,768 DEBUG Query successful
2024-11-12 21:24:49,769 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:49,803 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:51,396 DEBUG Query successful
2024-11-12 21:24:51,488 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:51,514 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:58,093 DEBUG Query successful
2024-11-12 21:24:58,105 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:24:58,132 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:24:59,919 DEBUG Query successful
2024-11-12 21:25:00,010 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:00,038 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:04,873 DEBUG Query successful
2024-11-12 21:25:04,875 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:04,901 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:08,390 DEBUG Query successful
2024-11-12 21:25:08,392 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:08,415 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:08,512 DEBUG Query successful
2024-11-12 21:25:08,893 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:08,919 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:12,299 DEBUG Query successful
2024-11-12 21:25:12,377 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:12,403 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:12,875 DEBUG Query successful
2024-11-12 21:25:12,876 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:12,902 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:17,700 DEBUG Query successful
2024-11-12 21:25:18,109 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:18,134 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:20,429 DEBUG Query successful
2024-11-12 21:25:20,430 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:20,456 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:27,103 DEBUG Query successful
2024-11-12 21:25:27,104 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:27,133 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:31,284 DEBUG Query successful
2024-11-12 21:25:31,284 DEBUG Query successful
2024-11-12 21:25:31,369 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:31,395 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:31,660 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:31,686 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:39,891 DEBUG Query successful
2024-11-12 21:25:39,904 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:39,929 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:40,881 DEBUG Query successful
2024-11-12 21:25:40,883 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:40,907 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:43,342 DEBUG Query successful
2024-11-12 21:25:43,697 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:43,722 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:44,459 DEBUG Query successful
2024-11-12 21:25:44,551 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:44,576 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:50,148 DEBUG Query successful
2024-11-12 21:25:50,237 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:50,265 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:52,016 DEBUG Query successful
2024-11-12 21:25:52,018 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:52,044 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:52,480 DEBUG Query successful
2024-11-12 21:25:52,481 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:25:52,505 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:25:55,075 DEBUG Query successful
2024-11-12 21:26:00,503 DEBUG Query successful
2024-11-12 21:26:00,582 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:26:00,606 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:26:02,908 DEBUG Query successful
2024-11-12 21:26:02,908 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:26:02,933 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:26:06,872 DEBUG Query successful
2024-11-12 21:26:06,874 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:26:06,906 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:26:10,580 DEBUG Query successful
2024-11-12 21:26:10,919 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:26:10,943 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:26:16,059 DEBUG Query successful
2024-11-12 21:26:16,060 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:26:16,083 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:26:20,273 DEBUG Query successful
2024-11-12 21:26:20,613 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:26:20,637 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:26:25,537 DEBUG Query successful
2024-11-12 21:26:25,548 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:26:25,580 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:26:37,318 DEBUG Query successful
2024-11-12 21:26:37,398 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:26:37,422 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:26:40,141 DEBUG Query successful
2024-11-12 21:26:40,233 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:26:40,259 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:26:42,075 DEBUG Query successful
2024-11-12 21:26:42,076 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:26:42,101 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:26:45,177 DEBUG Query successful
2024-11-12 21:26:45,179 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:26:45,202 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:26:46,109 DEBUG Query successful
2024-11-12 21:26:46,499 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:26:46,523 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:26:49,882 DEBUG Query successful
2024-11-12 21:26:49,883 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:26:49,912 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:27:00,521 DEBUG Query successful
2024-11-12 21:27:00,523 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:27:00,547 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:27:02,818 DEBUG Query successful
2024-11-12 21:27:03,128 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:27:03,153 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:27:05,759 DEBUG Query successful
2024-11-12 21:27:05,796 DEBUG Query successful
2024-11-12 21:27:05,797 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:27:05,823 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:27:05,850 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:27:05,874 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:27:09,390 DEBUG Query successful
2024-11-12 21:27:14,269 DEBUG Query successful
2024-11-12 21:27:14,270 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:27:14,305 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:27:17,220 DEBUG Query successful
2024-11-12 21:27:17,299 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:27:17,323 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:27:25,236 DEBUG Query successful
2024-11-12 21:27:25,237 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:27:25,260 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:27:25,950 DEBUG Query successful
2024-11-12 21:27:25,951 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:27:25,976 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:27:28,687 DEBUG Query successful
2024-11-12 21:27:28,988 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:27:29,017 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:27:38,967 DEBUG Query successful
2024-11-12 21:27:38,968 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:27:38,992 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:27:41,304 DEBUG Query successful
2024-11-12 21:27:45,327 DEBUG Query successful
2024-11-12 21:27:45,404 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:27:45,427 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:27:53,153 DEBUG Query successful
2024-11-12 21:27:53,153 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:27:53,176 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:28:05,309 DEBUG Query successful
2024-11-12 21:28:05,311 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-12 21:28:05,333 DEBUG Model set to: gpt-4o-mini
2024-11-12 21:28:07,498 DEBUG Query successful
2024-11-15 14:48:03,928 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:04,286 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:05,331 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:05,467 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:06,400 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:07,743 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:08,199 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:09,071 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:20,088 DEBUG Query successful
2024-11-15 14:48:20,089 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:20,115 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:23,021 DEBUG Query successful
2024-11-15 14:48:23,022 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:23,048 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:23,146 DEBUG Query successful
2024-11-15 14:48:23,148 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:23,175 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:25,181 DEBUG Query successful
2024-11-15 14:48:25,183 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:25,207 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:31,828 DEBUG Query successful
2024-11-15 14:48:31,925 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:31,957 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:36,053 DEBUG Query successful
2024-11-15 14:48:36,138 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:36,164 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:36,249 DEBUG Query successful
2024-11-15 14:48:36,358 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:36,389 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:38,110 DEBUG Query successful
2024-11-15 14:48:38,200 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:38,226 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:40,498 DEBUG Query successful
2024-11-15 14:48:40,499 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:40,526 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:41,100 DEBUG Query successful
2024-11-15 14:48:41,101 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:41,126 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:41,225 DEBUG Query successful
2024-11-15 14:48:41,237 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:41,262 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:43,070 DEBUG Query successful
2024-11-15 14:48:43,071 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:43,097 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:51,517 DEBUG Query successful
2024-11-15 14:48:51,530 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:51,564 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:52,438 DEBUG Query successful
2024-11-15 14:48:52,452 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:52,487 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:55,120 DEBUG Query successful
2024-11-15 14:48:55,139 DEBUG Query successful
2024-11-15 14:48:55,580 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:55,584 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:55,613 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:55,613 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:48:57,775 DEBUG Query successful
2024-11-15 14:48:57,787 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:48:57,812 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:00,811 DEBUG Query successful
2024-11-15 14:49:00,825 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:00,851 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:05,636 DEBUG Query successful
2024-11-15 14:49:05,998 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:06,024 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:09,085 DEBUG Query successful
2024-11-15 14:49:09,484 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:09,511 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:15,962 DEBUG Query successful
2024-11-15 14:49:16,047 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:16,072 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:16,657 DEBUG Query successful
2024-11-15 14:49:16,741 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:16,766 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:21,296 DEBUG Query successful
2024-11-15 14:49:21,378 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:21,403 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:24,226 DEBUG Query successful
2024-11-15 14:49:24,227 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:24,254 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:25,815 DEBUG Query successful
2024-11-15 14:49:25,816 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:25,841 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:27,877 DEBUG Query successful
2024-11-15 14:49:27,905 DEBUG Query successful
2024-11-15 14:49:27,906 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:27,930 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:27,974 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:28,004 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:35,579 DEBUG Query successful
2024-11-15 14:49:35,580 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:35,608 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:40,086 DEBUG Query successful
2024-11-15 14:49:40,098 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:40,124 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:40,202 DEBUG Query successful
2024-11-15 14:49:40,203 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:40,230 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:41,038 DEBUG Query successful
2024-11-15 14:49:41,432 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:41,464 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:42,114 DEBUG Query successful
2024-11-15 14:49:42,466 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:42,496 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:46,004 DEBUG Query successful
2024-11-15 14:49:46,006 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:46,039 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:52,121 DEBUG Query successful
2024-11-15 14:49:52,122 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:52,147 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:49:54,998 DEBUG Query successful
2024-11-15 14:49:55,357 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:49:55,382 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:02,109 DEBUG Query successful
2024-11-15 14:50:02,526 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:02,566 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:09,642 DEBUG Query successful
2024-11-15 14:50:09,729 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:09,761 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:11,256 DEBUG Query successful
2024-11-15 14:50:11,361 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:11,390 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:16,998 DEBUG Query successful
2024-11-15 14:50:17,105 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:17,131 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:19,516 DEBUG Query successful
2024-11-15 14:50:19,518 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:19,547 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:20,524 DEBUG Query successful
2024-11-15 14:50:20,525 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:20,555 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:23,577 DEBUG Query successful
2024-11-15 14:50:23,672 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:23,698 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:23,781 DEBUG Query successful
2024-11-15 14:50:23,782 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:23,807 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:35,351 DEBUG Query successful
2024-11-15 14:50:35,364 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:35,392 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:38,814 DEBUG Query successful
2024-11-15 14:50:38,828 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:38,854 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:41,339 DEBUG Query successful
2024-11-15 14:50:41,684 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:41,710 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:41,905 DEBUG Query successful
2024-11-15 14:50:41,919 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:41,946 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:47,946 DEBUG Query successful
2024-11-15 14:50:47,948 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:47,972 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:48,333 DEBUG Query successful
2024-11-15 14:50:48,696 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:48,721 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:52,542 DEBUG Query successful
2024-11-15 14:50:52,544 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:52,567 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:53,283 DEBUG Query successful
2024-11-15 14:50:53,682 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:53,724 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:59,322 DEBUG Query successful
2024-11-15 14:50:59,417 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:50:59,443 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:50:59,747 DEBUG Query successful
2024-11-15 14:51:00,152 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:00,177 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:03,733 DEBUG Query successful
2024-11-15 14:51:03,824 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:03,849 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:12,706 DEBUG Query successful
2024-11-15 14:51:12,707 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:12,733 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:12,776 DEBUG Query successful
2024-11-15 14:51:12,777 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:12,825 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:13,060 DEBUG Query successful
2024-11-15 14:51:13,178 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:13,213 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:16,338 DEBUG Query successful
2024-11-15 14:51:16,422 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:16,451 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:17,788 DEBUG Query successful
2024-11-15 14:51:17,790 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:17,814 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:26,214 DEBUG Query successful
2024-11-15 14:51:26,215 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:26,241 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:27,105 DEBUG Query successful
2024-11-15 14:51:27,106 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:27,137 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:29,458 DEBUG Query successful
2024-11-15 14:51:29,825 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:29,884 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:30,659 DEBUG Query successful
2024-11-15 14:51:30,660 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:30,685 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:34,814 DEBUG Query successful
2024-11-15 14:51:34,816 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:34,852 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:34,856 DEBUG Query successful
2024-11-15 14:51:35,235 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:35,297 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:37,095 DEBUG Query successful
2024-11-15 14:51:37,440 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:37,499 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:38,769 DEBUG Query successful
2024-11-15 14:51:38,771 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:38,797 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:40,893 DEBUG Query successful
2024-11-15 14:51:40,981 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:41,007 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:42,823 DEBUG Query successful
2024-11-15 14:51:43,187 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:43,248 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:45,718 DEBUG Query successful
2024-11-15 14:51:45,799 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:45,824 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:48,521 DEBUG Query successful
2024-11-15 14:51:48,614 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:48,641 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:48,963 DEBUG Query successful
2024-11-15 14:51:48,965 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:48,991 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:51:52,984 DEBUG Query successful
2024-11-15 14:51:52,996 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:51:53,027 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:56:56,222 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:56:57,303 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:56:57,694 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:56:58,472 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:56:59,618 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:57:00,424 DEBUG Model set to: gpt-4o-mini
2024-11-15 14:57:01,981 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 14:57:02,658 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:10:18,970 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:10:19,640 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:10:19,657 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:10:19,878 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:10:20,328 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:10:21,054 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:10:21,127 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:10:21,400 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:10:33,283 DEBUG Query successful
2024-11-15 15:10:33,284 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:10:33,309 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:10:41,419 DEBUG Query successful
2024-11-15 15:10:41,420 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:10:41,458 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:10:41,852 DEBUG Query successful
2024-11-15 15:10:41,854 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:10:41,883 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:10:42,619 DEBUG Query successful
2024-11-15 15:10:42,621 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:10:42,656 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:10:48,818 DEBUG Query successful
2024-11-15 15:10:48,900 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:10:48,927 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:10:59,064 DEBUG Query successful
2024-11-15 15:10:59,155 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:10:59,182 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:06,749 DEBUG Query successful
2024-11-15 15:11:06,750 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:06,777 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:11,218 DEBUG Query successful
2024-11-15 15:11:11,219 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:11,230 DEBUG Query successful
2024-11-15 15:11:11,230 DEBUG Query successful
2024-11-15 15:11:11,251 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:11,314 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:11,316 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:11,343 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:11,345 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:16,792 DEBUG Query successful
2024-11-15 15:11:16,793 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:16,818 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:17,056 DEBUG Query successful
2024-11-15 15:11:17,069 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:17,097 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:17,611 DEBUG Query successful
2024-11-15 15:11:17,612 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:17,638 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:20,846 DEBUG Query successful
2024-11-15 15:11:20,987 DEBUG Query successful
2024-11-15 15:11:21,001 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:21,027 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:21,224 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:21,254 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:24,981 DEBUG Query successful
2024-11-15 15:11:25,524 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:25,574 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:29,620 DEBUG Query successful
2024-11-15 15:11:29,623 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:29,651 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:33,848 DEBUG Query successful
2024-11-15 15:11:33,860 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:33,886 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:35,592 DEBUG Query successful
2024-11-15 15:11:35,672 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:35,698 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:36,535 DEBUG Query successful
2024-11-15 15:11:36,620 DEBUG Query successful
2024-11-15 15:11:36,919 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:36,948 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:37,005 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:37,034 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:41,630 DEBUG Query successful
2024-11-15 15:11:41,726 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:41,753 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:43,858 DEBUG Query successful
2024-11-15 15:11:43,870 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:43,900 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:48,388 DEBUG Query successful
2024-11-15 15:11:48,469 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:48,494 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:50,538 DEBUG Query successful
2024-11-15 15:11:50,541 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:50,571 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:50,983 DEBUG Query successful
2024-11-15 15:11:51,078 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:51,103 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:54,589 DEBUG Query successful
2024-11-15 15:11:54,590 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:54,615 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:11:54,862 DEBUG Query successful
2024-11-15 15:11:54,864 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:11:54,895 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:00,175 DEBUG Query successful
2024-11-15 15:12:00,175 DEBUG Query successful
2024-11-15 15:12:00,176 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:00,201 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:00,538 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:00,566 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:01,148 DEBUG Query successful
2024-11-15 15:12:01,149 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:01,176 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:05,414 DEBUG Query successful
2024-11-15 15:12:05,770 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:05,804 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:05,905 DEBUG Query successful
2024-11-15 15:12:05,906 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:05,933 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:12,653 DEBUG Query successful
2024-11-15 15:12:12,653 DEBUG Query successful
2024-11-15 15:12:12,654 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:12,680 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:13,031 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:13,060 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:15,041 DEBUG Query successful
2024-11-15 15:12:15,382 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:15,409 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:17,582 DEBUG Query successful
2024-11-15 15:12:17,663 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:17,688 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:19,546 DEBUG Query successful
2024-11-15 15:12:19,642 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:19,683 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:26,193 DEBUG Query successful
2024-11-15 15:12:26,292 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:26,319 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:28,765 DEBUG Query successful
2024-11-15 15:12:28,765 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:28,790 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:29,912 DEBUG Query successful
2024-11-15 15:12:29,914 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:29,941 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:33,733 DEBUG Query successful
2024-11-15 15:12:33,816 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:33,843 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:34,636 DEBUG Query successful
2024-11-15 15:12:34,637 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:34,663 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:44,725 DEBUG Query successful
2024-11-15 15:12:44,738 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:44,772 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:44,823 DEBUG Query successful
2024-11-15 15:12:44,824 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:44,848 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:44,877 DEBUG Query successful
2024-11-15 15:12:44,878 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:44,905 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:47,027 DEBUG Query successful
2024-11-15 15:12:47,360 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:47,385 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:51,891 DEBUG Query successful
2024-11-15 15:12:51,891 DEBUG Query successful
2024-11-15 15:12:51,906 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:51,946 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:52,231 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:52,255 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:12:56,072 DEBUG Query successful
2024-11-15 15:12:56,391 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:12:56,416 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:13:00,861 DEBUG Query successful
2024-11-15 15:13:00,863 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:13:00,887 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:25:38,198 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:25:38,631 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:25:39,422 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:25:39,818 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:25:40,509 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:25:41,401 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:25:43,129 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:25:43,803 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:26:05,101 DEBUG Query successful
2024-11-15 15:26:05,102 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:26:05,127 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:26:35,605 DEBUG Query successful
2024-11-15 15:26:35,606 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:26:35,631 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:26:36,520 DEBUG Query successful
2024-11-15 15:26:36,523 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:26:36,550 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:26:39,372 DEBUG Query successful
2024-11-15 15:26:39,374 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:26:39,399 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:26:54,748 DEBUG Query successful
2024-11-15 15:26:54,829 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:26:54,857 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:27:08,781 DEBUG Query successful
2024-11-15 15:27:08,782 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:27:08,807 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:27:14,597 DEBUG Query successful
2024-11-15 15:27:14,649 DEBUG Query successful
2024-11-15 15:27:14,690 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:27:14,717 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:27:14,749 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:27:14,774 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:27:15,962 DEBUG Query successful
2024-11-15 15:27:16,043 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:27:16,069 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:27:23,739 DEBUG Query successful
2024-11-15 15:27:23,740 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:27:23,766 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:27:24,143 DEBUG Query successful
2024-11-15 15:27:24,157 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:27:24,191 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:27:24,320 DEBUG Query successful
2024-11-15 15:27:24,322 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:27:24,355 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:27:25,238 DEBUG Query successful
2024-11-15 15:27:25,239 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:27:25,264 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:27:26,557 DEBUG Query successful
2024-11-15 15:27:27,141 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:27:27,180 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:27:41,363 DEBUG Query successful
2024-11-15 15:27:41,376 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:27:41,402 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:28:04,363 DEBUG Query successful
2024-11-15 15:28:04,774 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:28:04,828 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:28:11,728 DEBUG Query successful
2024-11-15 15:28:11,812 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:28:11,851 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:28:12,840 DEBUG Query successful
2024-11-15 15:28:12,842 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:28:12,868 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:28:13,436 DEBUG Query successful
2024-11-15 15:28:13,450 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:28:13,482 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:28:20,190 DEBUG Query successful
2024-11-15 15:28:20,285 DEBUG Query successful
2024-11-15 15:28:20,582 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:28:20,610 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:28:20,655 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:28:20,682 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:28:23,408 DEBUG Query successful
2024-11-15 15:28:23,421 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:28:23,447 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:28:25,528 DEBUG Query successful
2024-11-15 15:28:25,615 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:28:25,643 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:28:39,006 DEBUG Query successful
2024-11-15 15:28:39,108 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:28:39,131 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:28:47,567 DEBUG Query successful
2024-11-15 15:28:47,664 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:28:47,690 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:28:53,403 DEBUG Query successful
2024-11-15 15:28:53,404 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:28:53,434 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:28:57,295 DEBUG Query successful
2024-11-15 15:28:57,296 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:28:57,321 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:28:57,428 DEBUG Query successful
2024-11-15 15:28:57,429 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:28:57,455 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:28:58,627 DEBUG Query successful
2024-11-15 15:28:58,629 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:28:58,656 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:29:05,692 DEBUG Query successful
2024-11-15 15:29:06,029 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:29:06,056 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:29:10,916 DEBUG Query successful
2024-11-15 15:29:10,917 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:29:10,944 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:29:20,479 DEBUG Query successful
2024-11-15 15:29:20,479 DEBUG Query successful
2024-11-15 15:29:20,480 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:29:20,507 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:29:20,807 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:29:20,832 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:29:21,589 DEBUG Query successful
2024-11-15 15:29:21,601 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:29:21,627 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:29:23,589 DEBUG Query successful
2024-11-15 15:29:23,768 DEBUG Query successful
2024-11-15 15:29:23,913 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:29:23,944 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:29:24,132 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:29:24,158 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:29:30,555 DEBUG Query successful
2024-11-15 15:29:30,634 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:29:30,659 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:29:38,775 DEBUG Query successful
2024-11-15 15:29:38,857 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:29:38,882 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:29:59,141 DEBUG Query successful
2024-11-15 15:29:59,142 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:29:59,168 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:30:00,407 DEBUG Query successful
2024-11-15 15:30:00,408 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:30:00,439 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:30:06,362 DEBUG Query successful
2024-11-15 15:30:06,363 DEBUG Query successful
2024-11-15 15:30:06,460 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:30:06,460 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:30:06,486 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:30:06,487 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:30:23,257 DEBUG Query successful
2024-11-15 15:30:23,259 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:30:23,283 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:30:23,394 DEBUG Query successful
2024-11-15 15:30:23,394 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:30:23,421 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:30:23,946 DEBUG Query successful
2024-11-15 15:30:23,948 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:30:23,984 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:30:34,236 DEBUG Query successful
2024-11-15 15:30:34,656 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:30:34,681 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:30:37,324 DEBUG Query successful
2024-11-15 15:30:37,325 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:30:37,353 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:31:07,319 DEBUG Query successful
2024-11-15 15:31:07,320 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:31:07,348 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:31:07,440 DEBUG Query successful
2024-11-15 15:31:07,745 DEBUG Query successful
2024-11-15 15:31:07,746 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:31:07,776 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:31:07,819 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:31:07,845 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:31:15,179 DEBUG Query successful
2024-11-15 15:31:15,547 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:31:15,574 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:31:17,322 DEBUG Query successful
2024-11-15 15:31:17,690 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:31:17,714 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:31:24,216 DEBUG Query successful
2024-11-15 15:31:24,306 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:31:24,335 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:31:35,548 DEBUG Query successful
2024-11-15 15:31:35,558 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:31:35,585 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:31:36,470 DEBUG Query successful
2024-11-15 15:31:36,566 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:31:36,592 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:32:01,557 DEBUG Query successful
2024-11-15 15:32:01,558 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:32:01,587 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:32:02,779 DEBUG Query successful
2024-11-15 15:32:02,874 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:32:02,903 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:32:07,149 DEBUG Query successful
2024-11-15 15:32:07,229 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:32:07,255 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:32:20,213 DEBUG Query successful
2024-11-15 15:32:20,214 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:32:20,244 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:32:22,772 DEBUG Query successful
2024-11-15 15:32:22,773 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:32:22,786 DEBUG Query successful
2024-11-15 15:32:22,798 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:32:22,798 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:32:22,824 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:32:32,182 DEBUG Query successful
2024-11-15 15:32:32,525 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:32:32,596 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:32:35,472 DEBUG Query successful
2024-11-15 15:32:35,473 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:32:35,504 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:33:09,245 DEBUG Query successful
2024-11-15 15:33:09,247 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:33:09,270 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:33:09,622 DEBUG Query successful
2024-11-15 15:33:09,623 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:33:09,648 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:33:27,214 DEBUG Query successful
2024-11-15 15:33:27,385 DEBUG Query successful
2024-11-15 15:33:27,514 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:33:27,603 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:33:27,745 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:33:27,806 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:33:27,929 DEBUG Query successful
2024-11-15 15:33:28,285 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:33:28,346 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:33:32,905 DEBUG Query successful
2024-11-15 15:33:33,003 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:33:33,029 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:33:54,518 DEBUG Query successful
2024-11-15 15:33:54,519 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:33:54,545 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:33:58,155 DEBUG Query successful
2024-11-15 15:33:58,236 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:33:58,262 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:34:37,041 DEBUG Query successful
2024-11-15 15:34:37,123 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:34:37,126 DEBUG Query successful
2024-11-15 15:34:37,155 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:34:37,232 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:34:37,256 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:35:02,902 DEBUG Query successful
2024-11-15 15:35:02,903 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:35:02,927 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:35:03,239 DEBUG Query successful
2024-11-15 15:35:03,240 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:35:03,264 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:35:03,710 DEBUG Query successful
2024-11-15 15:35:03,711 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:35:03,744 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:35:03,758 DEBUG Query successful
2024-11-15 15:35:03,760 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:35:03,788 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:35:06,606 DEBUG Query successful
2024-11-15 15:35:06,966 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:35:06,992 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:35:28,144 DEBUG Query successful
2024-11-15 15:35:28,146 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:35:28,172 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:36:43,676 DEBUG Query successful
2024-11-15 15:36:44,064 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:36:44,088 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:37:31,033 DEBUG Query successful
2024-11-15 15:37:31,035 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:37:31,062 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:37:31,712 DEBUG Query successful
2024-11-15 15:37:31,794 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:37:31,822 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:37:32,281 DEBUG Query successful
2024-11-15 15:37:32,282 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:37:32,307 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:37:33,407 DEBUG Query successful
2024-11-15 15:37:33,768 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:37:33,794 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:37:35,090 DEBUG Query successful
2024-11-15 15:37:35,476 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:37:35,502 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:37:36,791 DEBUG Query successful
2024-11-15 15:37:36,872 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:37:36,897 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:37:37,349 DEBUG Query successful
2024-11-15 15:37:37,361 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:37:37,385 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:37:54,195 DEBUG Query successful
2024-11-15 15:37:54,196 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:37:54,222 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:37:56,729 DEBUG Query successful
2024-11-15 15:37:56,810 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:37:56,836 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:38:42,972 DEBUG Query successful
2024-11-15 15:38:42,973 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:38:42,997 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:38:45,232 DEBUG Query successful
2024-11-15 15:38:45,324 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:38:45,349 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:39:00,543 DEBUG Query successful
2024-11-15 15:39:00,544 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:39:00,568 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:39:16,288 DEBUG Query successful
2024-11-15 15:39:16,683 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:39:16,709 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:39:19,778 DEBUG Query successful
2024-11-15 15:39:19,779 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:39:19,806 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:39:21,318 DEBUG Query successful
2024-11-15 15:39:21,319 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:39:21,347 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:39:24,669 DEBUG Query successful
2024-11-15 15:39:25,122 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:39:25,153 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:39:28,058 DEBUG Query successful
2024-11-15 15:39:28,070 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:39:28,103 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:39:47,490 DEBUG Query successful
2024-11-15 15:39:47,574 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:39:47,599 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:39:51,821 DEBUG Query successful
2024-11-15 15:39:52,203 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:39:52,232 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:39:53,477 DEBUG Query successful
2024-11-15 15:39:53,478 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:39:53,505 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:39:56,981 DEBUG Query successful
2024-11-15 15:39:57,290 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:39:57,318 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:02,487 DEBUG Query successful
2024-11-15 15:40:02,579 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:02,605 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:05,045 DEBUG Query successful
2024-11-15 15:40:05,057 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:05,081 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:09,833 DEBUG Query successful
2024-11-15 15:40:09,916 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:09,942 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:12,831 DEBUG Query successful
2024-11-15 15:40:12,843 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:12,868 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:17,639 DEBUG Query successful
2024-11-15 15:40:17,651 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:17,679 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:17,716 DEBUG Query successful
2024-11-15 15:40:17,718 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:17,727 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:17,743 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:17,751 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:22,817 DEBUG Query successful
2024-11-15 15:40:23,146 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:23,172 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:25,703 DEBUG Query successful
2024-11-15 15:40:25,715 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:25,742 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:33,165 DEBUG Query successful
2024-11-15 15:40:33,166 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:33,190 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:39,536 DEBUG Query successful
2024-11-15 15:40:39,954 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:39,981 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:40,026 DEBUG Query successful
2024-11-15 15:40:40,118 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:40,146 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:52,020 DEBUG Query successful
2024-11-15 15:40:52,020 DEBUG Query successful
2024-11-15 15:40:52,033 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:52,033 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:52,058 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:52,059 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:56,126 DEBUG Query successful
2024-11-15 15:40:56,127 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:56,158 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:40:56,335 DEBUG Query successful
2024-11-15 15:40:56,702 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:40:56,726 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:41:03,030 DEBUG Query successful
2024-11-15 15:41:03,384 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:41:03,408 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:41:04,091 DEBUG Query successful
2024-11-15 15:41:04,185 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:41:04,212 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:41:07,907 DEBUG Query successful
2024-11-15 15:41:07,909 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:41:07,935 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:41:17,206 DEBUG Query successful
2024-11-15 15:41:17,554 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:41:17,586 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:41:21,687 DEBUG Query successful
2024-11-15 15:41:21,688 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:41:21,714 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:41:21,730 DEBUG Query successful
2024-11-15 15:41:21,828 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:41:21,860 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:41:32,448 DEBUG Query successful
2024-11-15 15:41:32,449 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:41:32,491 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:41:32,581 DEBUG Query successful
2024-11-15 15:41:32,666 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:41:32,691 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:41:46,549 DEBUG Query successful
2024-11-15 15:41:46,551 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:41:46,577 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:41:50,281 DEBUG Query successful
2024-11-15 15:41:50,283 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:41:50,308 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:41:51,113 DEBUG Query successful
2024-11-15 15:41:51,205 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:41:51,231 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:41:54,942 DEBUG Query successful
2024-11-15 15:41:54,945 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:41:54,971 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:41:57,453 DEBUG Query successful
2024-11-15 15:41:57,509 DEBUG Query successful
2024-11-15 15:41:57,825 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:41:57,861 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:41:57,903 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:41:57,933 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:42:01,224 DEBUG Query successful
2024-11-15 15:42:01,236 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:42:01,263 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:42:04,135 DEBUG Query successful
2024-11-15 15:42:04,147 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:42:04,172 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:42:11,170 DEBUG Query successful
2024-11-15 15:42:11,493 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:42:11,520 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:42:13,611 DEBUG Query successful
2024-11-15 15:42:13,705 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:42:13,729 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:42:16,446 DEBUG Query successful
2024-11-15 15:42:16,460 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:42:16,487 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:42:16,538 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:42:16,564 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:42:25,742 DEBUG Query successful
2024-11-15 15:42:30,259 DEBUG Query successful
2024-11-15 15:42:30,260 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:42:30,283 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:42:30,470 DEBUG Query successful
2024-11-15 15:42:30,470 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:42:30,498 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:42:32,937 DEBUG Query successful
2024-11-15 15:42:33,015 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:42:33,040 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:42:43,162 DEBUG Query successful
2024-11-15 15:42:43,164 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:42:43,190 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:42:45,280 DEBUG Query successful
2024-11-15 15:42:46,717 DEBUG Query successful
2024-11-15 15:42:46,718 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:42:46,750 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:42:51,786 DEBUG Query successful
2024-11-15 15:42:51,787 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:42:51,811 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:42:59,903 DEBUG Query successful
2024-11-15 15:43:04,436 DEBUG Query successful
2024-11-15 15:43:04,438 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:43:04,461 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:43:06,455 DEBUG Query successful
2024-11-15 15:43:06,794 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:43:06,819 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:43:20,678 DEBUG Query successful
2024-11-15 15:43:20,754 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:43:20,778 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:43:26,389 DEBUG Query successful
2024-11-15 15:43:26,390 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:43:26,414 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:43:42,134 DEBUG Query successful
2024-11-15 15:43:42,135 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 15:43:42,159 DEBUG Model set to: gpt-4o-mini
2024-11-15 15:43:44,103 DEBUG Query successful
2024-11-15 16:29:28,708 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:29:28,858 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:29:28,863 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:29:30,633 DEBUG Model set to: gpt-4o
2024-11-15 16:29:30,634 DEBUG Model set to: gpt-4o
2024-11-15 16:29:30,662 DEBUG Model set to: gpt-4o
2024-11-15 16:29:32,181 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:29:33,013 DEBUG Model set to: gpt-4o
2024-11-15 16:29:44,657 DEBUG Query successful
2024-11-15 16:29:44,658 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:29:44,691 DEBUG Model set to: gpt-4o
2024-11-15 16:29:52,344 DEBUG Query successful
2024-11-15 16:29:52,345 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:29:52,373 DEBUG Model set to: gpt-4o
2024-11-15 16:29:56,438 DEBUG Query successful
2024-11-15 16:29:56,441 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:29:56,470 DEBUG Model set to: gpt-4o
2024-11-15 16:30:01,453 DEBUG Query successful
2024-11-15 16:30:01,455 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:30:01,481 DEBUG Model set to: gpt-4o
2024-11-15 16:30:13,178 DEBUG Query successful
2024-11-15 16:30:13,262 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:30:13,290 DEBUG Model set to: gpt-4o
2024-11-15 16:30:15,193 DEBUG Query successful
2024-11-15 16:30:15,288 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:30:15,316 DEBUG Model set to: gpt-4o
2024-11-15 16:30:17,280 DEBUG Query successful
2024-11-15 16:30:17,373 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:30:17,400 DEBUG Model set to: gpt-4o
2024-11-15 16:30:23,824 DEBUG Query successful
2024-11-15 16:30:23,825 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:30:23,851 DEBUG Model set to: gpt-4o
2024-11-15 16:30:24,334 DEBUG Query successful
2024-11-15 16:30:24,336 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:30:24,362 DEBUG Model set to: gpt-4o
2024-11-15 16:30:24,446 DEBUG Query successful
2024-11-15 16:30:24,540 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:30:24,572 DEBUG Model set to: gpt-4o
2024-11-15 16:30:27,143 DEBUG Query successful
2024-11-15 16:30:27,157 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:30:27,182 DEBUG Model set to: gpt-4o
2024-11-15 16:30:30,231 DEBUG Query successful
2024-11-15 16:30:30,233 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:30:30,257 DEBUG Model set to: gpt-4o
2024-11-15 16:30:39,175 DEBUG Query successful
2024-11-15 16:30:39,188 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:30:39,213 DEBUG Model set to: gpt-4o
2024-11-15 16:31:03,328 DEBUG Query successful
2024-11-15 16:31:03,683 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:31:03,709 DEBUG Model set to: gpt-4o
2024-11-15 16:31:06,019 DEBUG Query successful
2024-11-15 16:31:06,022 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:31:06,047 DEBUG Model set to: gpt-4o
2024-11-15 16:31:06,216 DEBUG Query successful
2024-11-15 16:31:06,218 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:31:06,245 DEBUG Model set to: gpt-4o
2024-11-15 16:31:07,190 DEBUG Query successful
2024-11-15 16:31:07,192 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:31:07,219 DEBUG Model set to: gpt-4o
2024-11-15 16:31:19,212 DEBUG Query successful
2024-11-15 16:31:19,265 DEBUG Query successful
2024-11-15 16:31:19,311 DEBUG Query successful
2024-11-15 16:31:19,627 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:31:19,667 DEBUG Model set to: gpt-4o
2024-11-15 16:31:19,725 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:31:19,734 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:31:19,754 DEBUG Model set to: gpt-4o
2024-11-15 16:31:19,764 DEBUG Model set to: gpt-4o
2024-11-15 16:31:26,132 DEBUG Query successful
2024-11-15 16:31:26,216 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:31:26,242 DEBUG Model set to: gpt-4o
2024-11-15 16:31:33,619 DEBUG Query successful
2024-11-15 16:31:33,701 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:31:33,726 DEBUG Model set to: gpt-4o
2024-11-15 16:31:44,452 DEBUG Query successful
2024-11-15 16:31:44,455 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:31:44,481 DEBUG Model set to: gpt-4o
2024-11-15 16:31:46,448 DEBUG Query successful
2024-11-15 16:31:46,449 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:31:46,474 DEBUG Model set to: gpt-4o
2024-11-15 16:31:48,655 DEBUG Query successful
2024-11-15 16:31:48,712 DEBUG Query successful
2024-11-15 16:31:48,737 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:31:48,765 DEBUG Model set to: gpt-4o
2024-11-15 16:31:48,811 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:31:48,836 DEBUG Model set to: gpt-4o
2024-11-15 16:32:01,883 DEBUG Query successful
2024-11-15 16:32:01,885 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:32:01,910 DEBUG Model set to: gpt-4o
2024-11-15 16:32:02,129 DEBUG Query successful
2024-11-15 16:32:02,129 DEBUG Query successful
2024-11-15 16:32:02,130 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:32:02,141 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:32:02,155 DEBUG Model set to: gpt-4o
2024-11-15 16:32:02,167 DEBUG Model set to: gpt-4o
2024-11-15 16:32:14,308 DEBUG Query successful
2024-11-15 16:32:14,638 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:32:14,667 DEBUG Model set to: gpt-4o
2024-11-15 16:32:16,708 DEBUG Query successful
2024-11-15 16:32:16,709 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:32:16,737 DEBUG Model set to: gpt-4o
2024-11-15 16:32:31,219 DEBUG Query successful
2024-11-15 16:32:31,221 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:32:31,254 DEBUG Model set to: gpt-4o
2024-11-15 16:32:35,751 DEBUG Query successful
2024-11-15 16:32:35,753 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:32:35,779 DEBUG Model set to: gpt-4o
2024-11-15 16:32:39,489 DEBUG Query successful
2024-11-15 16:32:39,937 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:32:39,968 DEBUG Model set to: gpt-4o
2024-11-15 16:32:40,362 DEBUG Query successful
2024-11-15 16:32:40,737 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:32:40,767 DEBUG Model set to: gpt-4o
2024-11-15 16:32:42,559 DEBUG Query successful
2024-11-15 16:32:42,928 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:32:42,955 DEBUG Model set to: gpt-4o
2024-11-15 16:32:43,987 DEBUG Query successful
2024-11-15 16:32:44,070 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:32:44,096 DEBUG Model set to: gpt-4o
2024-11-15 16:32:53,613 DEBUG Query successful
2024-11-15 16:32:53,614 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:32:53,638 DEBUG Model set to: gpt-4o
2024-11-15 16:32:55,379 DEBUG Query successful
2024-11-15 16:32:55,474 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:32:55,500 DEBUG Model set to: gpt-4o
2024-11-15 16:33:07,918 DEBUG Query successful
2024-11-15 16:33:07,918 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:07,945 DEBUG Model set to: gpt-4o
2024-11-15 16:33:09,428 DEBUG Query successful
2024-11-15 16:33:09,510 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:09,539 DEBUG Model set to: gpt-4o
2024-11-15 16:33:12,261 DEBUG Query successful
2024-11-15 16:33:12,361 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:12,388 DEBUG Model set to: gpt-4o
2024-11-15 16:33:13,881 DEBUG Query successful
2024-11-15 16:33:13,883 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:13,911 DEBUG Model set to: gpt-4o
2024-11-15 16:33:16,357 DEBUG Query successful
2024-11-15 16:33:16,731 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:16,759 DEBUG Model set to: gpt-4o
2024-11-15 16:33:17,184 DEBUG Query successful
2024-11-15 16:33:17,185 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:17,210 DEBUG Model set to: gpt-4o
2024-11-15 16:33:17,281 DEBUG Query successful
2024-11-15 16:33:17,282 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:17,306 DEBUG Model set to: gpt-4o
2024-11-15 16:33:18,894 DEBUG Query successful
2024-11-15 16:33:18,896 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:18,919 DEBUG Model set to: gpt-4o
2024-11-15 16:33:26,403 DEBUG Query successful
2024-11-15 16:33:26,748 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:26,774 DEBUG Model set to: gpt-4o
2024-11-15 16:33:30,431 DEBUG Query successful
2024-11-15 16:33:30,519 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:30,544 DEBUG Model set to: gpt-4o
2024-11-15 16:33:34,535 DEBUG Query successful
2024-11-15 16:33:34,536 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:34,561 DEBUG Model set to: gpt-4o
2024-11-15 16:33:35,319 DEBUG Query successful
2024-11-15 16:33:35,321 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:35,344 DEBUG Model set to: gpt-4o
2024-11-15 16:33:36,559 DEBUG Query successful
2024-11-15 16:33:36,651 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:36,676 DEBUG Model set to: gpt-4o
2024-11-15 16:33:41,182 DEBUG Query successful
2024-11-15 16:33:41,184 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:41,211 DEBUG Model set to: gpt-4o
2024-11-15 16:33:42,062 DEBUG Query successful
2024-11-15 16:33:42,807 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:42,835 DEBUG Model set to: gpt-4o
2024-11-15 16:33:50,754 DEBUG Query successful
2024-11-15 16:33:51,094 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:51,119 DEBUG Model set to: gpt-4o
2024-11-15 16:33:52,379 DEBUG Query successful
2024-11-15 16:33:52,380 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:52,406 DEBUG Model set to: gpt-4o
2024-11-15 16:33:55,645 DEBUG Query successful
2024-11-15 16:33:55,646 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:33:55,671 DEBUG Model set to: gpt-4o
2024-11-15 16:34:01,000 DEBUG Query successful
2024-11-15 16:34:01,103 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:01,136 DEBUG Model set to: gpt-4o
2024-11-15 16:34:04,636 DEBUG Query successful
2024-11-15 16:34:04,955 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:05,026 DEBUG Model set to: gpt-4o
2024-11-15 16:34:07,507 DEBUG Query successful
2024-11-15 16:34:07,509 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:07,519 DEBUG Query successful
2024-11-15 16:34:07,535 DEBUG Model set to: gpt-4o
2024-11-15 16:34:07,610 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:07,641 DEBUG Model set to: gpt-4o
2024-11-15 16:34:07,921 DEBUG Query successful
2024-11-15 16:34:07,933 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:07,957 DEBUG Model set to: gpt-4o
2024-11-15 16:34:10,737 DEBUG Query successful
2024-11-15 16:34:11,029 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:11,096 DEBUG Model set to: gpt-4o
2024-11-15 16:34:12,253 DEBUG Query successful
2024-11-15 16:34:12,253 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:12,279 DEBUG Model set to: gpt-4o
2024-11-15 16:34:15,916 DEBUG Query successful
2024-11-15 16:34:15,918 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:15,943 DEBUG Model set to: gpt-4o
2024-11-15 16:34:16,526 DEBUG Query successful
2024-11-15 16:34:16,607 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:16,641 DEBUG Model set to: gpt-4o
2024-11-15 16:34:24,976 DEBUG Query successful
2024-11-15 16:34:25,327 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:25,387 DEBUG Model set to: gpt-4o
2024-11-15 16:34:27,451 DEBUG Query successful
2024-11-15 16:34:27,533 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:27,552 DEBUG Query successful
2024-11-15 16:34:27,553 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:27,563 DEBUG Model set to: gpt-4o
2024-11-15 16:34:27,580 DEBUG Model set to: gpt-4o
2024-11-15 16:34:34,565 DEBUG Query successful
2024-11-15 16:34:34,577 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:34,601 DEBUG Model set to: gpt-4o
2024-11-15 16:34:35,133 DEBUG Query successful
2024-11-15 16:34:35,134 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:35,159 DEBUG Model set to: gpt-4o
2024-11-15 16:34:49,109 DEBUG Query successful
2024-11-15 16:34:49,506 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:49,573 DEBUG Model set to: gpt-4o
2024-11-15 16:34:52,494 DEBUG Query successful
2024-11-15 16:34:52,496 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:52,521 DEBUG Model set to: gpt-4o
2024-11-15 16:34:52,572 DEBUG Query successful
2024-11-15 16:34:52,662 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:34:52,687 DEBUG Model set to: gpt-4o
2024-11-15 16:35:02,527 DEBUG Query successful
2024-11-15 16:35:02,881 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:35:02,906 DEBUG Model set to: gpt-4o
2024-11-15 16:35:04,616 DEBUG Query successful
2024-11-15 16:35:04,616 DEBUG Query successful
2024-11-15 16:35:04,617 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:35:04,629 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:35:04,645 DEBUG Model set to: gpt-4o
2024-11-15 16:35:04,656 DEBUG Model set to: gpt-4o
2024-11-15 16:37:23,158 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:37:23,481 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:37:24,442 DEBUG Model set to: gpt-4o
2024-11-15 16:37:24,520 DEBUG Model set to: gpt-4o
2024-11-15 16:37:25,905 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:37:27,281 DEBUG Model set to: gpt-4o
2024-11-15 16:37:27,469 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:37:28,200 DEBUG Model set to: gpt-4o
2024-11-15 16:37:34,728 DEBUG Query successful
2024-11-15 16:37:34,730 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:37:34,756 DEBUG Model set to: gpt-4o
2024-11-15 16:37:36,583 DEBUG Query successful
2024-11-15 16:37:36,585 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:37:36,611 DEBUG Model set to: gpt-4o
2024-11-15 16:37:38,196 DEBUG Query successful
2024-11-15 16:37:38,198 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:37:38,224 DEBUG Model set to: gpt-4o
2024-11-15 16:37:42,905 DEBUG Query successful
2024-11-15 16:37:42,907 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:37:42,933 DEBUG Model set to: gpt-4o
2024-11-15 16:37:50,677 DEBUG Query successful
2024-11-15 16:37:50,757 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:37:50,783 DEBUG Model set to: gpt-4o
2024-11-15 16:38:03,558 DEBUG Query successful
2024-11-15 16:38:03,559 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:38:03,585 DEBUG Model set to: gpt-4o
2024-11-15 16:38:04,577 DEBUG Query successful
2024-11-15 16:38:04,671 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:38:04,697 DEBUG Model set to: gpt-4o
2024-11-15 16:38:10,850 DEBUG Query successful
2024-11-15 16:38:10,851 DEBUG Query successful
2024-11-15 16:38:10,851 DEBUG Query successful
2024-11-15 16:38:10,853 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:38:10,886 DEBUG Model set to: gpt-4o
2024-11-15 16:38:10,939 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:38:10,946 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:38:10,972 DEBUG Model set to: gpt-4o
2024-11-15 16:38:10,981 DEBUG Model set to: gpt-4o
2024-11-15 16:38:18,919 DEBUG Query successful
2024-11-15 16:38:18,920 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:38:18,946 DEBUG Model set to: gpt-4o
2024-11-15 16:38:19,480 DEBUG Query successful
2024-11-15 16:38:19,482 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:38:19,518 DEBUG Model set to: gpt-4o
2024-11-15 16:38:20,574 DEBUG Query successful
2024-11-15 16:38:20,584 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:38:20,609 DEBUG Model set to: gpt-4o
2024-11-15 16:38:27,871 DEBUG Query successful
2024-11-15 16:38:28,254 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:38:28,280 DEBUG Model set to: gpt-4o
2024-11-15 16:38:30,517 DEBUG Query successful
2024-11-15 16:38:30,531 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:38:30,557 DEBUG Model set to: gpt-4o
2024-11-15 16:38:46,523 DEBUG Query successful
2024-11-15 16:38:46,870 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:38:46,897 DEBUG Model set to: gpt-4o
2024-11-15 16:38:50,715 DEBUG Query successful
2024-11-15 16:38:50,716 DEBUG Query successful
2024-11-15 16:38:50,718 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:38:50,718 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:38:50,744 DEBUG Model set to: gpt-4o
2024-11-15 16:38:50,744 DEBUG Model set to: gpt-4o
2024-11-15 16:38:57,423 DEBUG Query successful
2024-11-15 16:38:57,504 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:38:57,530 DEBUG Model set to: gpt-4o
2024-11-15 16:39:03,756 DEBUG Query successful
2024-11-15 16:39:03,756 DEBUG Query successful
2024-11-15 16:39:04,238 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:39:04,249 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:39:04,268 DEBUG Model set to: gpt-4o
2024-11-15 16:39:04,282 DEBUG Model set to: gpt-4o
2024-11-15 16:39:06,467 DEBUG Query successful
2024-11-15 16:39:06,468 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:39:06,494 DEBUG Model set to: gpt-4o
2024-11-15 16:39:08,719 DEBUG Query successful
2024-11-15 16:39:08,800 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:39:08,829 DEBUG Model set to: gpt-4o
2024-11-15 16:39:33,275 DEBUG Query successful
2024-11-15 16:39:33,359 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:39:33,383 DEBUG Model set to: gpt-4o
2024-11-15 16:39:33,813 DEBUG Query successful
2024-11-15 16:39:33,893 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:39:33,920 DEBUG Model set to: gpt-4o
2024-11-15 16:39:34,117 DEBUG Query successful
2024-11-15 16:39:34,118 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:39:34,144 DEBUG Model set to: gpt-4o
2024-11-15 16:39:36,954 DEBUG Query successful
2024-11-15 16:39:36,956 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:39:36,983 DEBUG Model set to: gpt-4o
2024-11-15 16:39:37,204 DEBUG Query successful
2024-11-15 16:39:37,205 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:39:37,235 DEBUG Model set to: gpt-4o
2024-11-15 16:39:38,372 DEBUG Query successful
2024-11-15 16:39:38,374 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:39:38,399 DEBUG Model set to: gpt-4o
2024-11-15 16:39:49,664 DEBUG Query successful
2024-11-15 16:39:50,063 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:39:50,090 DEBUG Model set to: gpt-4o
2024-11-15 16:39:51,375 DEBUG Query successful
2024-11-15 16:39:51,377 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:39:51,403 DEBUG Model set to: gpt-4o
2024-11-15 16:40:17,972 DEBUG Query successful
2024-11-15 16:40:18,346 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:40:18,371 DEBUG Model set to: gpt-4o
2024-11-15 16:40:21,011 DEBUG Query successful
2024-11-15 16:40:21,023 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:40:21,049 DEBUG Model set to: gpt-4o
2024-11-15 16:40:23,693 DEBUG Query successful
2024-11-15 16:40:23,696 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:40:23,725 DEBUG Model set to: gpt-4o
2024-11-15 16:40:37,644 DEBUG Query successful
2024-11-15 16:40:37,668 DEBUG Query successful
2024-11-15 16:40:38,038 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:40:38,071 DEBUG Model set to: gpt-4o
2024-11-15 16:40:38,100 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:40:38,127 DEBUG Model set to: gpt-4o
2024-11-15 16:40:41,473 DEBUG Query successful
2024-11-15 16:40:41,562 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:40:41,588 DEBUG Model set to: gpt-4o
2024-11-15 16:40:56,441 DEBUG Query successful
2024-11-15 16:40:56,442 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:40:56,467 DEBUG Model set to: gpt-4o
2024-11-15 16:40:59,783 DEBUG Query successful
2024-11-15 16:40:59,874 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:40:59,902 DEBUG Model set to: gpt-4o
2024-11-15 16:41:25,796 DEBUG Query successful
2024-11-15 16:41:25,890 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:41:25,914 DEBUG Model set to: gpt-4o
2024-11-15 16:41:26,797 DEBUG Query successful
2024-11-15 16:41:26,888 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:41:26,914 DEBUG Model set to: gpt-4o
2024-11-15 16:41:38,632 DEBUG Query successful
2024-11-15 16:41:38,633 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:41:38,659 DEBUG Model set to: gpt-4o
2024-11-15 16:41:39,075 DEBUG Query successful
2024-11-15 16:41:39,076 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:41:39,106 DEBUG Model set to: gpt-4o
2024-11-15 16:41:40,185 DEBUG Query successful
2024-11-15 16:41:40,187 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:41:40,211 DEBUG Model set to: gpt-4o
2024-11-15 16:41:41,432 DEBUG Query successful
2024-11-15 16:41:41,433 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:41:41,457 DEBUG Model set to: gpt-4o
2024-11-15 16:41:52,616 DEBUG Query successful
2024-11-15 16:41:52,949 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:41:52,973 DEBUG Model set to: gpt-4o
2024-11-15 16:41:55,033 DEBUG Query successful
2024-11-15 16:41:55,034 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:41:55,059 DEBUG Model set to: gpt-4o
2024-11-15 16:42:10,353 DEBUG Query successful
2024-11-15 16:42:10,355 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:10,379 DEBUG Model set to: gpt-4o
2024-11-15 16:42:14,969 DEBUG Query successful
2024-11-15 16:42:15,290 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:15,314 DEBUG Model set to: gpt-4o
2024-11-15 16:42:15,534 DEBUG Query successful
2024-11-15 16:42:15,547 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:15,572 DEBUG Model set to: gpt-4o
2024-11-15 16:42:18,183 DEBUG Query successful
2024-11-15 16:42:18,385 DEBUG Query successful
2024-11-15 16:42:18,516 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:18,544 DEBUG Model set to: gpt-4o
2024-11-15 16:42:18,676 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:18,702 DEBUG Model set to: gpt-4o
2024-11-15 16:42:21,960 DEBUG Query successful
2024-11-15 16:42:22,038 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:22,065 DEBUG Model set to: gpt-4o
2024-11-15 16:42:28,166 DEBUG Query successful
2024-11-15 16:42:28,167 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:28,191 DEBUG Model set to: gpt-4o
2024-11-15 16:42:35,034 DEBUG Query successful
2024-11-15 16:42:35,127 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:35,151 DEBUG Model set to: gpt-4o
2024-11-15 16:42:45,231 DEBUG Query successful
2024-11-15 16:42:45,232 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:45,257 DEBUG Model set to: gpt-4o
2024-11-15 16:42:46,647 DEBUG Query successful
2024-11-15 16:42:46,713 DEBUG Query successful
2024-11-15 16:42:46,740 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:46,770 DEBUG Model set to: gpt-4o
2024-11-15 16:42:46,802 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:46,828 DEBUG Model set to: gpt-4o
2024-11-15 16:42:53,268 DEBUG Query successful
2024-11-15 16:42:53,269 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:53,296 DEBUG Model set to: gpt-4o
2024-11-15 16:42:53,809 DEBUG Query successful
2024-11-15 16:42:53,821 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:53,844 DEBUG Model set to: gpt-4o
2024-11-15 16:42:53,890 DEBUG Query successful
2024-11-15 16:42:53,892 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:53,917 DEBUG Model set to: gpt-4o
2024-11-15 16:42:58,687 DEBUG Query successful
2024-11-15 16:42:59,039 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:42:59,098 DEBUG Model set to: gpt-4o
2024-11-15 16:43:01,715 DEBUG Query successful
2024-11-15 16:43:01,717 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:43:01,743 DEBUG Model set to: gpt-4o
2024-11-15 16:43:22,764 DEBUG Query successful
2024-11-15 16:43:23,085 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:43:23,143 DEBUG Model set to: gpt-4o
2024-11-15 16:43:25,074 DEBUG Query successful
2024-11-15 16:43:25,076 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:43:25,108 DEBUG Model set to: gpt-4o
2024-11-15 16:43:27,162 DEBUG Query successful
2024-11-15 16:43:27,163 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:43:27,187 DEBUG Model set to: gpt-4o
2024-11-15 16:43:29,952 DEBUG Query successful
2024-11-15 16:43:30,046 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:43:30,068 DEBUG Model set to: gpt-4o
2024-11-15 16:43:34,085 DEBUG Query successful
2024-11-15 16:43:34,421 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:43:34,424 DEBUG Query successful
2024-11-15 16:43:34,490 DEBUG Model set to: gpt-4o
2024-11-15 16:43:34,737 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:43:34,798 DEBUG Model set to: gpt-4o
2024-11-15 16:43:36,405 DEBUG Query successful
2024-11-15 16:43:36,407 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:43:36,431 DEBUG Model set to: gpt-4o
2024-11-15 16:43:45,220 DEBUG Query successful
2024-11-15 16:43:45,314 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:43:45,338 DEBUG Model set to: gpt-4o
2024-11-15 16:43:51,330 DEBUG Query successful
2024-11-15 16:43:51,410 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:43:51,435 DEBUG Model set to: gpt-4o
2024-11-15 16:44:05,524 DEBUG Query successful
2024-11-15 16:44:05,525 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:44:05,549 DEBUG Model set to: gpt-4o
2024-11-15 16:44:07,265 DEBUG Query successful
2024-11-15 16:44:07,266 DEBUG Query successful
2024-11-15 16:44:07,268 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:44:07,293 DEBUG Model set to: gpt-4o
2024-11-15 16:44:07,355 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:44:07,379 DEBUG Model set to: gpt-4o
2024-11-15 16:44:07,539 DEBUG Query successful
2024-11-15 16:44:07,540 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:44:07,563 DEBUG Model set to: gpt-4o
2024-11-15 16:44:26,174 DEBUG Query successful
2024-11-15 16:44:26,184 DEBUG Query successful
2024-11-15 16:44:26,195 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:44:26,199 DEBUG Query successful
2024-11-15 16:44:26,201 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:44:26,223 DEBUG Model set to: gpt-4o
2024-11-15 16:44:26,229 DEBUG Model set to: gpt-4o
2024-11-15 16:44:26,514 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:44:26,539 DEBUG Model set to: gpt-4o
2024-11-15 16:44:36,890 DEBUG Query successful
2024-11-15 16:44:36,891 DEBUG Query successful
2024-11-15 16:44:36,892 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:44:36,918 DEBUG Model set to: gpt-4o
2024-11-15 16:44:37,267 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:44:37,293 DEBUG Model set to: gpt-4o
2024-11-15 16:44:50,962 DEBUG Query successful
2024-11-15 16:44:51,279 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:44:51,304 DEBUG Model set to: gpt-4o
2024-11-15 16:44:54,408 DEBUG Query successful
2024-11-15 16:44:54,410 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:44:54,435 DEBUG Model set to: gpt-4o
2024-11-15 16:44:56,014 DEBUG Query successful
2024-11-15 16:44:56,096 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:44:56,120 DEBUG Model set to: gpt-4o
2024-11-15 16:44:57,116 DEBUG Query successful
2024-11-15 16:44:57,393 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:44:57,419 DEBUG Model set to: gpt-4o
2024-11-15 16:45:01,840 DEBUG Query successful
2024-11-15 16:45:01,929 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:01,952 DEBUG Model set to: gpt-4o
2024-11-15 16:45:03,214 DEBUG Query successful
2024-11-15 16:45:03,227 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:03,252 DEBUG Model set to: gpt-4o
2024-11-15 16:45:05,759 DEBUG Query successful
2024-11-15 16:45:05,838 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:05,862 DEBUG Model set to: gpt-4o
2024-11-15 16:45:07,205 DEBUG Query successful
2024-11-15 16:45:07,205 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:07,230 DEBUG Model set to: gpt-4o
2024-11-15 16:45:09,165 DEBUG Query successful
2024-11-15 16:45:09,244 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:09,268 DEBUG Model set to: gpt-4o
2024-11-15 16:45:11,737 DEBUG Query successful
2024-11-15 16:45:11,737 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:11,761 DEBUG Model set to: gpt-4o
2024-11-15 16:45:16,168 DEBUG Query successful
2024-11-15 16:45:16,169 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:16,195 DEBUG Model set to: gpt-4o
2024-11-15 16:45:17,759 DEBUG Query successful
2024-11-15 16:45:17,761 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:17,784 DEBUG Model set to: gpt-4o
2024-11-15 16:45:20,401 DEBUG Query successful
2024-11-15 16:45:20,402 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:20,426 DEBUG Model set to: gpt-4o
2024-11-15 16:45:21,962 DEBUG Query successful
2024-11-15 16:45:22,290 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:22,316 DEBUG Model set to: gpt-4o
2024-11-15 16:45:22,890 DEBUG Query successful
2024-11-15 16:45:23,235 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:23,262 DEBUG Model set to: gpt-4o
2024-11-15 16:45:25,336 DEBUG Query successful
2024-11-15 16:45:25,337 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:25,363 DEBUG Model set to: gpt-4o
2024-11-15 16:45:33,946 DEBUG Query successful
2024-11-15 16:45:34,305 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:34,330 DEBUG Model set to: gpt-4o
2024-11-15 16:45:35,195 DEBUG Query successful
2024-11-15 16:45:35,196 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:35,221 DEBUG Model set to: gpt-4o
2024-11-15 16:45:52,930 DEBUG Query successful
2024-11-15 16:45:53,216 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:45:53,242 DEBUG Model set to: gpt-4o
2024-11-15 16:46:00,235 DEBUG Query successful
2024-11-15 16:46:00,316 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:00,340 DEBUG Model set to: gpt-4o
2024-11-15 16:46:01,726 DEBUG Query successful
2024-11-15 16:46:01,817 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:01,841 DEBUG Model set to: gpt-4o
2024-11-15 16:46:04,304 DEBUG Query successful
2024-11-15 16:46:04,386 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:04,412 DEBUG Model set to: gpt-4o
2024-11-15 16:46:06,508 DEBUG Query successful
2024-11-15 16:46:06,520 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:06,546 DEBUG Model set to: gpt-4o
2024-11-15 16:46:06,753 DEBUG Query successful
2024-11-15 16:46:06,764 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:06,788 DEBUG Model set to: gpt-4o
2024-11-15 16:46:10,707 DEBUG Query successful
2024-11-15 16:46:10,719 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:10,743 DEBUG Model set to: gpt-4o
2024-11-15 16:46:13,083 DEBUG Query successful
2024-11-15 16:46:13,166 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:13,190 DEBUG Model set to: gpt-4o
2024-11-15 16:46:25,341 DEBUG Query successful
2024-11-15 16:46:25,354 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:25,378 DEBUG Model set to: gpt-4o
2024-11-15 16:46:27,200 DEBUG Query successful
2024-11-15 16:46:27,201 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:27,226 DEBUG Model set to: gpt-4o
2024-11-15 16:46:39,201 DEBUG Query successful
2024-11-15 16:46:39,205 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:39,235 DEBUG Model set to: gpt-4o
2024-11-15 16:46:40,340 DEBUG Query successful
2024-11-15 16:46:40,600 DEBUG Query successful
2024-11-15 16:46:40,613 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:40,640 DEBUG Model set to: gpt-4o
2024-11-15 16:46:40,717 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:40,741 DEBUG Model set to: gpt-4o
2024-11-15 16:46:41,194 DEBUG Query successful
2024-11-15 16:46:41,609 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:41,640 DEBUG Model set to: gpt-4o
2024-11-15 16:46:43,125 DEBUG Query successful
2024-11-15 16:46:43,447 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:43,473 DEBUG Model set to: gpt-4o
2024-11-15 16:46:45,640 DEBUG Query successful
2024-11-15 16:46:45,641 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:45,667 DEBUG Model set to: gpt-4o
2024-11-15 16:46:50,947 DEBUG Query successful
2024-11-15 16:46:51,040 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:51,071 DEBUG Model set to: gpt-4o
2024-11-15 16:46:51,957 DEBUG Query successful
2024-11-15 16:46:52,320 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:52,344 DEBUG Model set to: gpt-4o
2024-11-15 16:46:55,848 DEBUG Query successful
2024-11-15 16:46:55,849 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:55,873 DEBUG Model set to: gpt-4o
2024-11-15 16:46:57,145 DEBUG Query successful
2024-11-15 16:46:57,178 DEBUG Query successful
2024-11-15 16:46:57,235 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:57,270 DEBUG Model set to: gpt-4o
2024-11-15 16:46:57,271 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:46:57,295 DEBUG Model set to: gpt-4o
2024-11-15 16:47:03,139 DEBUG Query successful
2024-11-15 16:47:03,140 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:47:03,171 DEBUG Model set to: gpt-4o
2024-11-15 16:47:06,331 DEBUG Query successful
2024-11-15 16:47:06,415 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:47:06,436 DEBUG Query successful
2024-11-15 16:47:06,438 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:47:06,444 DEBUG Model set to: gpt-4o
2024-11-15 16:47:06,472 DEBUG Model set to: gpt-4o
2024-11-15 16:47:15,975 DEBUG Query successful
2024-11-15 16:47:15,976 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:47:16,003 DEBUG Model set to: gpt-4o
2024-11-15 16:47:17,878 DEBUG Query successful
2024-11-15 16:47:17,879 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:47:17,905 DEBUG Model set to: gpt-4o
2024-11-15 16:47:47,284 DEBUG Query successful
2024-11-15 16:47:47,678 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:47:47,705 DEBUG Model set to: gpt-4o
2024-11-15 16:47:48,795 DEBUG Query successful
2024-11-15 16:47:48,798 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:47:48,823 DEBUG Model set to: gpt-4o
2024-11-15 16:47:49,834 DEBUG Query successful
2024-11-15 16:47:49,836 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:47:49,865 DEBUG Model set to: gpt-4o
2024-11-15 16:48:00,944 DEBUG Query successful
2024-11-15 16:48:01,264 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:01,290 DEBUG Model set to: gpt-4o
2024-11-15 16:48:02,455 DEBUG Query successful
2024-11-15 16:48:02,468 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:02,493 DEBUG Model set to: gpt-4o
2024-11-15 16:48:04,993 DEBUG Query successful
2024-11-15 16:48:05,306 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:05,331 DEBUG Model set to: gpt-4o
2024-11-15 16:48:07,986 DEBUG Query successful
2024-11-15 16:48:08,372 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:08,396 DEBUG Model set to: gpt-4o
2024-11-15 16:48:14,214 DEBUG Query successful
2024-11-15 16:48:14,295 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:14,324 DEBUG Model set to: gpt-4o
2024-11-15 16:48:24,972 DEBUG Query successful
2024-11-15 16:48:24,974 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:25,012 DEBUG Model set to: gpt-4o
2024-11-15 16:48:27,838 DEBUG Query successful
2024-11-15 16:48:27,938 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:27,982 DEBUG Model set to: gpt-4o
2024-11-15 16:48:39,675 DEBUG Query successful
2024-11-15 16:48:39,754 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:39,784 DEBUG Model set to: gpt-4o
2024-11-15 16:48:40,317 DEBUG Query successful
2024-11-15 16:48:40,398 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:40,424 DEBUG Model set to: gpt-4o
2024-11-15 16:48:44,113 DEBUG Query successful
2024-11-15 16:48:44,115 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:44,140 DEBUG Model set to: gpt-4o
2024-11-15 16:48:44,596 DEBUG Query successful
2024-11-15 16:48:44,598 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:44,626 DEBUG Model set to: gpt-4o
2024-11-15 16:48:45,329 DEBUG Query successful
2024-11-15 16:48:45,332 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:45,382 DEBUG Model set to: gpt-4o
2024-11-15 16:48:46,488 DEBUG Query successful
2024-11-15 16:48:46,489 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:46,521 DEBUG Model set to: gpt-4o
2024-11-15 16:48:47,008 DEBUG Query successful
2024-11-15 16:48:47,395 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:47,419 DEBUG Model set to: gpt-4o
2024-11-15 16:48:57,517 DEBUG Query successful
2024-11-15 16:48:57,519 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:48:57,545 DEBUG Model set to: gpt-4o
2024-11-15 16:49:10,806 DEBUG Query successful
2024-11-15 16:49:11,186 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:49:11,211 DEBUG Model set to: gpt-4o
2024-11-15 16:49:14,948 DEBUG Query successful
2024-11-15 16:49:14,949 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:49:14,975 DEBUG Model set to: gpt-4o
2024-11-15 16:49:16,492 DEBUG Query successful
2024-11-15 16:49:16,493 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:49:16,519 DEBUG Model set to: gpt-4o
2024-11-15 16:49:18,839 DEBUG Query successful
2024-11-15 16:49:18,921 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:49:18,946 DEBUG Model set to: gpt-4o
2024-11-15 16:49:20,176 DEBUG Query successful
2024-11-15 16:49:21,784 DEBUG Query successful
2024-11-15 16:49:23,159 DEBUG Query successful
2024-11-15 16:49:23,159 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:49:23,184 DEBUG Model set to: gpt-4o
2024-11-15 16:49:24,842 DEBUG Query successful
2024-11-15 16:49:24,932 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:49:24,956 DEBUG Model set to: gpt-4o
2024-11-15 16:49:33,847 DEBUG Query successful
2024-11-15 16:49:33,848 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:49:33,872 DEBUG Model set to: gpt-4o
2024-11-15 16:49:35,424 DEBUG Query successful
2024-11-15 16:49:35,426 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:49:35,451 DEBUG Model set to: gpt-4o
2024-11-15 16:49:44,987 DEBUG Query successful
2024-11-15 16:49:45,624 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:49:45,660 DEBUG Model set to: gpt-4o
2024-11-15 16:49:47,673 DEBUG Query successful
2024-11-15 16:49:47,675 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:49:47,699 DEBUG Model set to: gpt-4o
2024-11-15 16:50:02,086 DEBUG Query successful
2024-11-15 16:50:02,458 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:50:02,482 DEBUG Model set to: gpt-4o
2024-11-15 16:50:08,492 DEBUG Query successful
2024-11-15 16:50:08,570 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:50:08,595 DEBUG Model set to: gpt-4o
2024-11-15 16:50:13,099 DEBUG Query successful
2024-11-15 16:50:13,178 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:50:13,201 DEBUG Model set to: gpt-4o
2024-11-15 16:50:14,227 DEBUG Query successful
2024-11-15 16:50:14,241 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:50:14,265 DEBUG Model set to: gpt-4o
2024-11-15 16:50:23,312 DEBUG Query successful
2024-11-15 16:50:23,314 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:50:23,338 DEBUG Model set to: gpt-4o
2024-11-15 16:50:31,128 DEBUG Query successful
2024-11-15 16:50:31,129 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:50:31,155 DEBUG Model set to: gpt-4o
2024-11-15 16:50:33,080 DEBUG Query successful
2024-11-15 16:50:33,093 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:50:33,117 DEBUG Model set to: gpt-4o
2024-11-15 16:50:35,452 DEBUG Query successful
2024-11-15 16:50:35,743 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:50:35,771 DEBUG Model set to: gpt-4o
2024-11-15 16:50:36,728 DEBUG Query successful
2024-11-15 16:50:50,725 DEBUG Query successful
2024-11-15 16:50:50,801 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:50:50,824 DEBUG Model set to: gpt-4o
2024-11-15 16:50:58,492 DEBUG Query successful
2024-11-15 16:50:58,493 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:50:58,517 DEBUG Model set to: gpt-4o
2024-11-15 16:51:07,598 DEBUG Query successful
2024-11-15 16:51:07,598 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 16:51:07,622 DEBUG Model set to: gpt-4o
2024-11-15 16:51:09,340 DEBUG Query successful
2024-11-15 17:05:00,977 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:05:02,121 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:05:02,180 DEBUG Model set to: gpt-4o
2024-11-15 17:05:03,313 DEBUG Model set to: gpt-4o
2024-11-15 17:05:03,378 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:05:04,326 DEBUG Model set to: gpt-4o
2024-11-15 17:05:05,893 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:05:06,627 DEBUG Model set to: gpt-4o
2024-11-15 17:05:17,140 DEBUG Query successful
2024-11-15 17:05:17,143 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:05:17,170 DEBUG Model set to: gpt-4o
2024-11-15 17:05:41,335 DEBUG Query successful
2024-11-15 17:05:41,337 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:05:41,366 DEBUG Model set to: gpt-4o
2024-11-15 17:05:42,160 DEBUG Query successful
2024-11-15 17:05:42,162 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:05:42,186 DEBUG Model set to: gpt-4o
2024-11-15 17:05:42,450 DEBUG Query successful
2024-11-15 17:05:42,452 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:05:42,486 DEBUG Model set to: gpt-4o
2024-11-15 17:05:46,021 DEBUG Query successful
2024-11-15 17:05:46,104 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:05:46,130 DEBUG Model set to: gpt-4o
2024-11-15 17:05:51,580 DEBUG Query successful
2024-11-15 17:05:51,582 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:05:51,607 DEBUG Model set to: gpt-4o
2024-11-15 17:05:56,732 DEBUG Query successful
2024-11-15 17:05:56,814 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:05:56,840 DEBUG Model set to: gpt-4o
2024-11-15 17:06:04,613 DEBUG Query successful
2024-11-15 17:06:04,694 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:04,721 DEBUG Model set to: gpt-4o
2024-11-15 17:06:05,441 DEBUG Query successful
2024-11-15 17:06:05,583 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:05,613 DEBUG Model set to: gpt-4o
2024-11-15 17:06:08,651 DEBUG Query successful
2024-11-15 17:06:08,664 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:08,689 DEBUG Model set to: gpt-4o
2024-11-15 17:06:10,758 DEBUG Query successful
2024-11-15 17:06:10,759 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:10,784 DEBUG Model set to: gpt-4o
2024-11-15 17:06:11,231 DEBUG Query successful
2024-11-15 17:06:11,232 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:11,258 DEBUG Model set to: gpt-4o
2024-11-15 17:06:12,568 DEBUG Query successful
2024-11-15 17:06:12,945 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:12,971 DEBUG Model set to: gpt-4o
2024-11-15 17:06:13,635 DEBUG Query successful
2024-11-15 17:06:13,636 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:13,666 DEBUG Model set to: gpt-4o
2024-11-15 17:06:20,572 DEBUG Query successful
2024-11-15 17:06:20,576 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:20,610 DEBUG Model set to: gpt-4o
2024-11-15 17:06:31,450 DEBUG Query successful
2024-11-15 17:06:31,464 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:31,491 DEBUG Model set to: gpt-4o
2024-11-15 17:06:33,244 DEBUG Query successful
2024-11-15 17:06:33,327 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:33,353 DEBUG Model set to: gpt-4o
2024-11-15 17:06:35,348 DEBUG Query successful
2024-11-15 17:06:35,689 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:35,717 DEBUG Model set to: gpt-4o
2024-11-15 17:06:35,807 DEBUG Query successful
2024-11-15 17:06:36,157 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:36,195 DEBUG Model set to: gpt-4o
2024-11-15 17:06:37,499 DEBUG Query successful
2024-11-15 17:06:37,513 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:37,540 DEBUG Model set to: gpt-4o
2024-11-15 17:06:41,115 DEBUG Query successful
2024-11-15 17:06:41,116 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:41,140 DEBUG Model set to: gpt-4o
2024-11-15 17:06:47,698 DEBUG Query successful
2024-11-15 17:06:48,135 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:48,165 DEBUG Model set to: gpt-4o
2024-11-15 17:06:51,367 DEBUG Query successful
2024-11-15 17:06:51,449 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:06:51,478 DEBUG Model set to: gpt-4o
2024-11-15 17:07:12,135 DEBUG Query successful
2024-11-15 17:07:12,216 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:07:12,242 DEBUG Model set to: gpt-4o
2024-11-15 17:07:12,957 DEBUG Query successful
2024-11-15 17:07:12,957 DEBUG Query successful
2024-11-15 17:07:12,959 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:07:12,959 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:07:12,983 DEBUG Model set to: gpt-4o
2024-11-15 17:07:12,998 DEBUG Model set to: gpt-4o
2024-11-15 17:07:21,268 DEBUG Query successful
2024-11-15 17:07:21,620 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:07:21,651 DEBUG Model set to: gpt-4o
2024-11-15 17:07:23,158 DEBUG Query successful
2024-11-15 17:07:23,159 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:07:23,183 DEBUG Model set to: gpt-4o
2024-11-15 17:07:24,527 DEBUG Query successful
2024-11-15 17:07:24,612 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:07:24,647 DEBUG Model set to: gpt-4o
2024-11-15 17:07:29,332 DEBUG Query successful
2024-11-15 17:07:29,334 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:07:29,360 DEBUG Model set to: gpt-4o
2024-11-15 17:07:30,131 DEBUG Query successful
2024-11-15 17:07:30,132 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:07:30,158 DEBUG Model set to: gpt-4o
2024-11-15 17:07:54,201 DEBUG Query successful
2024-11-15 17:07:54,580 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:07:54,606 DEBUG Model set to: gpt-4o
2024-11-15 17:07:57,817 DEBUG Query successful
2024-11-15 17:07:57,819 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:07:57,844 DEBUG Model set to: gpt-4o
2024-11-15 17:07:58,146 DEBUG Query successful
2024-11-15 17:07:58,235 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:07:58,259 DEBUG Model set to: gpt-4o
2024-11-15 17:08:06,818 DEBUG Query successful
2024-11-15 17:08:06,819 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:06,845 DEBUG Model set to: gpt-4o
2024-11-15 17:08:14,284 DEBUG Query successful
2024-11-15 17:08:14,285 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:14,310 DEBUG Model set to: gpt-4o
2024-11-15 17:08:15,841 DEBUG Query successful
2024-11-15 17:08:16,233 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:16,262 DEBUG Model set to: gpt-4o
2024-11-15 17:08:16,965 DEBUG Query successful
2024-11-15 17:08:17,323 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:17,350 DEBUG Model set to: gpt-4o
2024-11-15 17:08:21,780 DEBUG Query successful
2024-11-15 17:08:21,878 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:21,912 DEBUG Model set to: gpt-4o
2024-11-15 17:08:33,110 DEBUG Query successful
2024-11-15 17:08:33,111 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:33,135 DEBUG Model set to: gpt-4o
2024-11-15 17:08:34,653 DEBUG Query successful
2024-11-15 17:08:34,654 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:34,679 DEBUG Model set to: gpt-4o
2024-11-15 17:08:38,695 DEBUG Query successful
2024-11-15 17:08:39,092 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:39,118 DEBUG Model set to: gpt-4o
2024-11-15 17:08:43,308 DEBUG Query successful
2024-11-15 17:08:43,311 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:43,339 DEBUG Model set to: gpt-4o
2024-11-15 17:08:44,000 DEBUG Query successful
2024-11-15 17:08:44,092 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:44,121 DEBUG Model set to: gpt-4o
2024-11-15 17:08:44,196 DEBUG Query successful
2024-11-15 17:08:44,286 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:44,315 DEBUG Model set to: gpt-4o
2024-11-15 17:08:46,957 DEBUG Query successful
2024-11-15 17:08:47,356 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:47,383 DEBUG Model set to: gpt-4o
2024-11-15 17:08:53,388 DEBUG Query successful
2024-11-15 17:08:53,388 DEBUG Query successful
2024-11-15 17:08:53,388 DEBUG Query successful
2024-11-15 17:08:53,389 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:53,389 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:53,419 DEBUG Model set to: gpt-4o
2024-11-15 17:08:53,420 DEBUG Model set to: gpt-4o
2024-11-15 17:08:53,486 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:08:53,512 DEBUG Model set to: gpt-4o
2024-11-15 17:09:03,038 DEBUG Query successful
2024-11-15 17:09:03,040 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:03,063 DEBUG Model set to: gpt-4o
2024-11-15 17:09:05,170 DEBUG Query successful
2024-11-15 17:09:05,259 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:05,285 DEBUG Model set to: gpt-4o
2024-11-15 17:09:20,334 DEBUG Query successful
2024-11-15 17:09:20,336 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:20,361 DEBUG Model set to: gpt-4o
2024-11-15 17:09:23,126 DEBUG Query successful
2024-11-15 17:09:23,127 DEBUG Query successful
2024-11-15 17:09:23,128 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:23,153 DEBUG Model set to: gpt-4o
2024-11-15 17:09:23,202 DEBUG Query successful
2024-11-15 17:09:23,204 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:23,249 DEBUG Model set to: gpt-4o
2024-11-15 17:09:23,471 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:23,496 DEBUG Model set to: gpt-4o
2024-11-15 17:09:24,510 DEBUG Query successful
2024-11-15 17:09:24,512 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:24,537 DEBUG Model set to: gpt-4o
2024-11-15 17:09:26,178 DEBUG Query successful
2024-11-15 17:09:26,502 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:26,527 DEBUG Model set to: gpt-4o
2024-11-15 17:09:29,343 DEBUG Query successful
2024-11-15 17:09:29,704 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:29,730 DEBUG Model set to: gpt-4o
2024-11-15 17:09:33,649 DEBUG Query successful
2024-11-15 17:09:33,651 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:33,677 DEBUG Model set to: gpt-4o
2024-11-15 17:09:39,929 DEBUG Query successful
2024-11-15 17:09:40,020 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:40,043 DEBUG Model set to: gpt-4o
2024-11-15 17:09:40,167 DEBUG Query successful
2024-11-15 17:09:40,260 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:40,284 DEBUG Model set to: gpt-4o
2024-11-15 17:09:40,580 DEBUG Query successful
2024-11-15 17:09:40,896 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:40,922 DEBUG Model set to: gpt-4o
2024-11-15 17:09:45,541 DEBUG Query successful
2024-11-15 17:09:45,545 DEBUG Query successful
2024-11-15 17:09:45,546 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:45,578 DEBUG Model set to: gpt-4o
2024-11-15 17:09:45,626 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:45,654 DEBUG Model set to: gpt-4o
2024-11-15 17:09:46,182 DEBUG Query successful
2024-11-15 17:09:46,182 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:46,207 DEBUG Model set to: gpt-4o
2024-11-15 17:09:54,110 DEBUG Query successful
2024-11-15 17:09:54,191 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:54,216 DEBUG Model set to: gpt-4o
2024-11-15 17:09:55,479 DEBUG Query successful
2024-11-15 17:09:55,480 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:55,505 DEBUG Model set to: gpt-4o
2024-11-15 17:09:59,078 DEBUG Query successful
2024-11-15 17:09:59,079 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:59,106 DEBUG Model set to: gpt-4o
2024-11-15 17:09:59,482 DEBUG Query successful
2024-11-15 17:09:59,484 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:09:59,509 DEBUG Model set to: gpt-4o
2024-11-15 17:10:00,201 DEBUG Query successful
2024-11-15 17:10:00,203 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:00,230 DEBUG Model set to: gpt-4o
2024-11-15 17:10:03,001 DEBUG Query successful
2024-11-15 17:10:03,340 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:03,366 DEBUG Model set to: gpt-4o
2024-11-15 17:10:03,906 DEBUG Query successful
2024-11-15 17:10:04,266 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:04,292 DEBUG Model set to: gpt-4o
2024-11-15 17:10:05,210 DEBUG Query successful
2024-11-15 17:10:05,212 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:05,259 DEBUG Model set to: gpt-4o
2024-11-15 17:10:08,885 DEBUG Query successful
2024-11-15 17:10:08,887 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:08,915 DEBUG Model set to: gpt-4o
2024-11-15 17:10:12,533 DEBUG Query successful
2024-11-15 17:10:12,912 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:12,937 DEBUG Model set to: gpt-4o
2024-11-15 17:10:15,120 DEBUG Query successful
2024-11-15 17:10:15,204 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:15,230 DEBUG Model set to: gpt-4o
2024-11-15 17:10:16,748 DEBUG Query successful
2024-11-15 17:10:17,072 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:17,097 DEBUG Model set to: gpt-4o
2024-11-15 17:10:20,697 DEBUG Query successful
2024-11-15 17:10:20,796 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:20,822 DEBUG Model set to: gpt-4o
2024-11-15 17:10:25,969 DEBUG Query successful
2024-11-15 17:10:25,970 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:25,995 DEBUG Model set to: gpt-4o
2024-11-15 17:10:27,274 DEBUG Query successful
2024-11-15 17:10:27,275 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:27,300 DEBUG Model set to: gpt-4o
2024-11-15 17:10:27,619 DEBUG Query successful
2024-11-15 17:10:27,701 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:27,728 DEBUG Model set to: gpt-4o
2024-11-15 17:10:28,732 DEBUG Query successful
2024-11-15 17:10:28,812 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:28,838 DEBUG Model set to: gpt-4o
2024-11-15 17:10:32,515 DEBUG Query successful
2024-11-15 17:10:32,516 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:32,541 DEBUG Model set to: gpt-4o
2024-11-15 17:10:35,079 DEBUG Query successful
2024-11-15 17:10:35,463 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:35,487 DEBUG Model set to: gpt-4o
2024-11-15 17:10:35,850 DEBUG Query successful
2024-11-15 17:10:35,852 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:35,881 DEBUG Model set to: gpt-4o
2024-11-15 17:10:37,688 DEBUG Query successful
2024-11-15 17:10:37,689 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:37,715 DEBUG Model set to: gpt-4o
2024-11-15 17:10:40,383 DEBUG Query successful
2024-11-15 17:10:40,383 DEBUG Query successful
2024-11-15 17:10:40,384 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:40,408 DEBUG Model set to: gpt-4o
2024-11-15 17:10:40,745 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:40,769 DEBUG Model set to: gpt-4o
2024-11-15 17:10:50,237 DEBUG Query successful
2024-11-15 17:10:50,239 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:50,262 DEBUG Model set to: gpt-4o
2024-11-15 17:10:50,316 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:50,343 DEBUG Model set to: gpt-4o
2024-11-15 17:10:52,985 DEBUG Query successful
2024-11-15 17:10:53,327 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:10:53,356 DEBUG Model set to: gpt-4o
2024-11-15 17:11:00,075 DEBUG Query successful
2024-11-15 17:11:00,076 DEBUG Query successful
2024-11-15 17:11:00,077 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:00,078 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:00,115 DEBUG Model set to: gpt-4o
2024-11-15 17:11:00,119 DEBUG Model set to: gpt-4o
2024-11-15 17:11:00,184 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:00,216 DEBUG Model set to: gpt-4o
2024-11-15 17:11:07,036 DEBUG Query successful
2024-11-15 17:11:07,036 DEBUG Query successful
2024-11-15 17:11:07,037 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:07,063 DEBUG Model set to: gpt-4o
2024-11-15 17:11:07,507 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:07,537 DEBUG Model set to: gpt-4o
2024-11-15 17:11:09,130 DEBUG Query successful
2024-11-15 17:11:09,220 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:09,244 DEBUG Model set to: gpt-4o
2024-11-15 17:11:13,272 DEBUG Query successful
2024-11-15 17:11:13,285 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:13,307 DEBUG Model set to: gpt-4o
2024-11-15 17:11:18,391 DEBUG Query successful
2024-11-15 17:11:18,686 DEBUG Query successful
2024-11-15 17:11:18,687 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:18,706 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:18,711 DEBUG Model set to: gpt-4o
2024-11-15 17:11:18,730 DEBUG Model set to: gpt-4o
2024-11-15 17:11:19,683 DEBUG Query successful
2024-11-15 17:11:19,695 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:19,719 DEBUG Model set to: gpt-4o
2024-11-15 17:11:26,219 DEBUG Query successful
2024-11-15 17:11:26,219 DEBUG Query successful
2024-11-15 17:11:26,313 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:26,343 DEBUG Model set to: gpt-4o
2024-11-15 17:11:26,637 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:26,662 DEBUG Model set to: gpt-4o
2024-11-15 17:11:37,117 DEBUG Query successful
2024-11-15 17:11:37,117 DEBUG Query successful
2024-11-15 17:11:37,117 DEBUG Query successful
2024-11-15 17:11:37,119 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:37,130 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:37,149 DEBUG Model set to: gpt-4o
2024-11-15 17:11:37,157 DEBUG Model set to: gpt-4o
2024-11-15 17:11:37,211 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:37,240 DEBUG Model set to: gpt-4o
2024-11-15 17:11:42,625 DEBUG Query successful
2024-11-15 17:11:42,626 DEBUG Query successful
2024-11-15 17:11:42,639 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:42,672 DEBUG Model set to: gpt-4o
2024-11-15 17:11:43,128 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:43,154 DEBUG Model set to: gpt-4o
2024-11-15 17:11:50,584 DEBUG Query successful
2024-11-15 17:11:50,616 DEBUG Query successful
2024-11-15 17:11:50,617 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:50,643 DEBUG Model set to: gpt-4o
2024-11-15 17:11:50,678 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:50,703 DEBUG Model set to: gpt-4o
2024-11-15 17:11:53,988 DEBUG Query successful
2024-11-15 17:11:54,314 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:54,339 DEBUG Model set to: gpt-4o
2024-11-15 17:11:57,798 DEBUG Query successful
2024-11-15 17:11:57,809 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:11:57,835 DEBUG Model set to: gpt-4o
2024-11-15 17:12:03,752 DEBUG Query successful
2024-11-15 17:12:03,765 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:03,790 DEBUG Model set to: gpt-4o
2024-11-15 17:12:05,563 DEBUG Query successful
2024-11-15 17:12:05,646 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:05,673 DEBUG Model set to: gpt-4o
2024-11-15 17:12:06,348 DEBUG Query successful
2024-11-15 17:12:06,675 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:06,699 DEBUG Model set to: gpt-4o
2024-11-15 17:12:11,774 DEBUG Query successful
2024-11-15 17:12:11,857 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:11,884 DEBUG Model set to: gpt-4o
2024-11-15 17:12:17,137 DEBUG Query successful
2024-11-15 17:12:17,137 DEBUG Query successful
2024-11-15 17:12:17,138 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:17,139 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:17,163 DEBUG Model set to: gpt-4o
2024-11-15 17:12:17,163 DEBUG Model set to: gpt-4o
2024-11-15 17:12:21,913 DEBUG Query successful
2024-11-15 17:12:22,299 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:22,325 DEBUG Model set to: gpt-4o
2024-11-15 17:12:27,401 DEBUG Query successful
2024-11-15 17:12:27,403 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:27,427 DEBUG Model set to: gpt-4o
2024-11-15 17:12:27,470 DEBUG Query successful
2024-11-15 17:12:27,536 DEBUG Query successful
2024-11-15 17:12:27,538 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:27,569 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:27,569 DEBUG Model set to: gpt-4o
2024-11-15 17:12:27,596 DEBUG Model set to: gpt-4o
2024-11-15 17:12:30,103 DEBUG Query successful
2024-11-15 17:12:30,420 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:30,445 DEBUG Model set to: gpt-4o
2024-11-15 17:12:36,323 DEBUG Query successful
2024-11-15 17:12:36,406 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:36,432 DEBUG Model set to: gpt-4o
2024-11-15 17:12:37,612 DEBUG Query successful
2024-11-15 17:12:37,614 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:37,639 DEBUG Model set to: gpt-4o
2024-11-15 17:12:41,990 DEBUG Query successful
2024-11-15 17:12:41,991 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:42,016 DEBUG Model set to: gpt-4o
2024-11-15 17:12:43,338 DEBUG Query successful
2024-11-15 17:12:43,340 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:43,364 DEBUG Model set to: gpt-4o
2024-11-15 17:12:47,411 DEBUG Query successful
2024-11-15 17:12:47,503 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:47,531 DEBUG Model set to: gpt-4o
2024-11-15 17:12:50,966 DEBUG Query successful
2024-11-15 17:12:51,297 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:51,323 DEBUG Model set to: gpt-4o
2024-11-15 17:12:52,651 DEBUG Query successful
2024-11-15 17:12:52,653 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:52,679 DEBUG Model set to: gpt-4o
2024-11-15 17:12:54,165 DEBUG Query successful
2024-11-15 17:12:54,166 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:54,191 DEBUG Model set to: gpt-4o
2024-11-15 17:12:57,281 DEBUG Query successful
2024-11-15 17:12:57,283 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:12:57,307 DEBUG Model set to: gpt-4o
2024-11-15 17:13:00,042 DEBUG Query successful
2024-11-15 17:13:00,250 DEBUG Query successful
2024-11-15 17:13:00,369 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:00,395 DEBUG Model set to: gpt-4o
2024-11-15 17:13:00,637 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:00,665 DEBUG Model set to: gpt-4o
2024-11-15 17:13:02,848 DEBUG Query successful
2024-11-15 17:13:02,932 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:02,957 DEBUG Model set to: gpt-4o
2024-11-15 17:13:06,042 DEBUG Query successful
2024-11-15 17:13:06,088 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:06,117 DEBUG Model set to: gpt-4o
2024-11-15 17:13:09,732 DEBUG Query successful
2024-11-15 17:13:09,733 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:09,768 DEBUG Model set to: gpt-4o
2024-11-15 17:13:12,279 DEBUG Query successful
2024-11-15 17:13:15,505 DEBUG Query successful
2024-11-15 17:13:15,582 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:15,609 DEBUG Model set to: gpt-4o
2024-11-15 17:13:15,884 DEBUG Query successful
2024-11-15 17:13:15,966 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:16,000 DEBUG Model set to: gpt-4o
2024-11-15 17:13:18,885 DEBUG Query successful
2024-11-15 17:13:18,887 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:18,911 DEBUG Model set to: gpt-4o
2024-11-15 17:13:21,068 DEBUG Query successful
2024-11-15 17:13:21,308 DEBUG Query successful
2024-11-15 17:13:21,309 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:21,319 DEBUG Query successful
2024-11-15 17:13:21,320 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:21,338 DEBUG Model set to: gpt-4o
2024-11-15 17:13:21,346 DEBUG Model set to: gpt-4o
2024-11-15 17:13:21,394 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:21,421 DEBUG Model set to: gpt-4o
2024-11-15 17:13:35,816 DEBUG Query successful
2024-11-15 17:13:35,817 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:35,841 DEBUG Model set to: gpt-4o
2024-11-15 17:13:37,282 DEBUG Query successful
2024-11-15 17:13:37,283 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:37,308 DEBUG Model set to: gpt-4o
2024-11-15 17:13:40,210 DEBUG Query successful
2024-11-15 17:13:40,569 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:40,593 DEBUG Model set to: gpt-4o
2024-11-15 17:13:40,624 DEBUG Query successful
2024-11-15 17:13:40,718 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:40,743 DEBUG Model set to: gpt-4o
2024-11-15 17:13:40,928 DEBUG Query successful
2024-11-15 17:13:41,306 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:41,331 DEBUG Model set to: gpt-4o
2024-11-15 17:13:48,248 DEBUG Query successful
2024-11-15 17:13:48,249 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:48,272 DEBUG Model set to: gpt-4o
2024-11-15 17:13:50,548 DEBUG Query successful
2024-11-15 17:13:50,637 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:50,664 DEBUG Model set to: gpt-4o
2024-11-15 17:13:56,395 DEBUG Query successful
2024-11-15 17:13:56,473 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:56,498 DEBUG Model set to: gpt-4o
2024-11-15 17:13:59,813 DEBUG Query successful
2024-11-15 17:13:59,814 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:13:59,839 DEBUG Model set to: gpt-4o
2024-11-15 17:14:00,817 DEBUG Query successful
2024-11-15 17:14:00,818 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:14:00,843 DEBUG Model set to: gpt-4o
2024-11-15 17:14:03,958 DEBUG Query successful
2024-11-15 17:14:03,960 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:14:03,986 DEBUG Model set to: gpt-4o
2024-11-15 17:14:06,796 DEBUG Query successful
2024-11-15 17:14:07,165 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:14:07,191 DEBUG Model set to: gpt-4o
2024-11-15 17:14:08,285 DEBUG Query successful
2024-11-15 17:14:08,287 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:14:08,310 DEBUG Model set to: gpt-4o
2024-11-15 17:14:10,688 DEBUG Query successful
2024-11-15 17:14:10,688 DEBUG Query successful
2024-11-15 17:14:10,689 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:14:10,715 DEBUG Model set to: gpt-4o
2024-11-15 17:14:14,305 DEBUG Query successful
2024-11-15 17:14:17,235 DEBUG Query successful
2024-11-15 17:14:17,312 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:14:17,336 DEBUG Model set to: gpt-4o
2024-11-15 17:14:22,774 DEBUG Query successful
2024-11-15 17:14:22,775 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:14:22,798 DEBUG Model set to: gpt-4o
2024-11-15 17:14:33,395 DEBUG Query successful
2024-11-15 17:14:33,396 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 17:14:33,420 DEBUG Model set to: gpt-4o
2024-11-15 17:14:35,687 DEBUG Query successful
2024-11-15 18:27:50,230 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:27:50,872 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:27:51,536 DEBUG Model set to: gpt-4o
2024-11-15 18:27:51,928 DEBUG Model set to: gpt-4o
2024-11-15 18:27:52,331 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:27:53,131 DEBUG Model set to: gpt-4o
2024-11-15 18:28:02,309 DEBUG Query successful
2024-11-15 18:28:02,310 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:02,335 DEBUG Model set to: gpt-4o
2024-11-15 18:28:04,760 DEBUG Query successful
2024-11-15 18:28:04,761 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:04,786 DEBUG Model set to: gpt-4o
2024-11-15 18:28:04,950 DEBUG Query successful
2024-11-15 18:28:04,952 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:04,977 DEBUG Model set to: gpt-4o
2024-11-15 18:28:14,280 DEBUG Query successful
2024-11-15 18:28:14,360 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:14,384 DEBUG Model set to: gpt-4o
2024-11-15 18:28:16,645 DEBUG Query successful
2024-11-15 18:28:16,725 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:16,749 DEBUG Model set to: gpt-4o
2024-11-15 18:28:18,467 DEBUG Query successful
2024-11-15 18:28:18,545 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:18,569 DEBUG Model set to: gpt-4o
2024-11-15 18:28:21,456 DEBUG Query successful
2024-11-15 18:28:21,457 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:21,481 DEBUG Model set to: gpt-4o
2024-11-15 18:28:22,071 DEBUG Query successful
2024-11-15 18:28:22,072 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:22,097 DEBUG Model set to: gpt-4o
2024-11-15 18:28:24,211 DEBUG Query successful
2024-11-15 18:28:24,212 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:24,235 DEBUG Model set to: gpt-4o
2024-11-15 18:28:31,175 DEBUG Query successful
2024-11-15 18:28:31,188 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:31,212 DEBUG Model set to: gpt-4o
2024-11-15 18:28:33,500 DEBUG Query successful
2024-11-15 18:28:33,514 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:33,539 DEBUG Model set to: gpt-4o
2024-11-15 18:28:34,641 DEBUG Query successful
2024-11-15 18:28:35,017 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:35,048 DEBUG Model set to: gpt-4o
2024-11-15 18:28:38,725 DEBUG Query successful
2024-11-15 18:28:38,726 DEBUG Query successful
2024-11-15 18:28:38,729 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:38,757 DEBUG Model set to: gpt-4o
2024-11-15 18:28:39,096 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:39,120 DEBUG Model set to: gpt-4o
2024-11-15 18:28:43,031 DEBUG Query successful
2024-11-15 18:28:43,420 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:43,445 DEBUG Model set to: gpt-4o
2024-11-15 18:28:45,636 DEBUG Query successful
2024-11-15 18:28:45,727 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:45,752 DEBUG Model set to: gpt-4o
2024-11-15 18:28:48,774 DEBUG Query successful
2024-11-15 18:28:48,851 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:48,875 DEBUG Model set to: gpt-4o
2024-11-15 18:28:51,164 DEBUG Query successful
2024-11-15 18:28:51,165 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:51,189 DEBUG Model set to: gpt-4o
2024-11-15 18:28:52,512 DEBUG Query successful
2024-11-15 18:28:52,513 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:52,539 DEBUG Model set to: gpt-4o
2024-11-15 18:28:55,591 DEBUG Query successful
2024-11-15 18:28:55,673 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:28:55,698 DEBUG Model set to: gpt-4o
2024-11-15 18:29:01,846 DEBUG Query successful
2024-11-15 18:29:01,847 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:29:01,870 DEBUG Model set to: gpt-4o
2024-11-15 18:29:03,205 DEBUG Query successful
2024-11-15 18:29:03,206 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:29:03,230 DEBUG Model set to: gpt-4o
2024-11-15 18:29:11,749 DEBUG Query successful
2024-11-15 18:29:11,750 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:29:11,781 DEBUG Model set to: gpt-4o
2024-11-15 18:29:16,741 DEBUG Query successful
2024-11-15 18:29:17,080 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:29:17,107 DEBUG Model set to: gpt-4o
2024-11-15 18:29:17,344 DEBUG Query successful
2024-11-15 18:29:17,741 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:29:17,766 DEBUG Model set to: gpt-4o
2024-11-15 18:29:24,538 DEBUG Query successful
2024-11-15 18:29:24,539 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:29:24,564 DEBUG Model set to: gpt-4o
2024-11-15 18:29:28,804 DEBUG Query successful
2024-11-15 18:29:29,158 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:29:29,184 DEBUG Model set to: gpt-4o
2024-11-15 18:29:32,564 DEBUG Query successful
2024-11-15 18:29:32,656 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:29:32,680 DEBUG Model set to: gpt-4o
2024-11-15 18:29:40,842 DEBUG Query successful
2024-11-15 18:29:40,846 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:29:40,872 DEBUG Model set to: gpt-4o
2024-11-15 18:29:42,135 DEBUG Query successful
2024-11-15 18:29:42,228 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:29:42,253 DEBUG Model set to: gpt-4o
2024-11-15 18:29:48,819 DEBUG Query successful
2024-11-15 18:29:48,820 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:29:48,856 DEBUG Model set to: gpt-4o
2024-11-15 18:29:52,317 DEBUG Query successful
2024-11-15 18:29:52,408 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:29:52,433 DEBUG Model set to: gpt-4o
2024-11-15 18:29:59,883 DEBUG Query successful
2024-11-15 18:29:59,884 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:29:59,908 DEBUG Model set to: gpt-4o
2024-11-15 18:30:00,573 DEBUG Query successful
2024-11-15 18:30:00,574 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:30:00,599 DEBUG Model set to: gpt-4o
2024-11-15 18:30:04,628 DEBUG Query successful
2024-11-15 18:30:04,979 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:30:05,004 DEBUG Model set to: gpt-4o
2024-11-15 18:30:08,815 DEBUG Query successful
2024-11-15 18:30:08,827 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:30:08,857 DEBUG Model set to: gpt-4o
2024-11-15 18:30:21,506 DEBUG Query successful
2024-11-15 18:30:21,874 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:30:21,899 DEBUG Model set to: gpt-4o
2024-11-15 18:30:25,093 DEBUG Query successful
2024-11-15 18:30:25,095 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:30:25,120 DEBUG Model set to: gpt-4o
2024-11-15 18:30:39,263 DEBUG Query successful
2024-11-15 18:30:39,594 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:30:39,618 DEBUG Model set to: gpt-4o
2024-11-15 18:30:42,845 DEBUG Query successful
2024-11-15 18:30:42,935 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:30:42,958 DEBUG Model set to: gpt-4o
2024-11-15 18:30:55,802 DEBUG Query successful
2024-11-15 18:30:55,803 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:30:55,831 DEBUG Model set to: gpt-4o
2024-11-15 18:30:58,328 DEBUG Query successful
2024-11-15 18:30:58,417 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:30:58,441 DEBUG Model set to: gpt-4o
2024-11-15 18:31:05,610 DEBUG Query successful
2024-11-15 18:31:05,611 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:05,636 DEBUG Model set to: gpt-4o
2024-11-15 18:31:06,924 DEBUG Query successful
2024-11-15 18:31:07,017 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:07,050 DEBUG Model set to: gpt-4o
2024-11-15 18:31:14,960 DEBUG Query successful
2024-11-15 18:31:14,972 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:14,997 DEBUG Model set to: gpt-4o
2024-11-15 18:31:15,772 DEBUG Query successful
2024-11-15 18:31:15,773 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:15,798 DEBUG Model set to: gpt-4o
2024-11-15 18:31:23,260 DEBUG Query successful
2024-11-15 18:31:23,596 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:23,621 DEBUG Model set to: gpt-4o
2024-11-15 18:31:26,934 DEBUG Query successful
2024-11-15 18:31:26,935 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:26,959 DEBUG Model set to: gpt-4o
2024-11-15 18:31:32,663 DEBUG Query successful
2024-11-15 18:31:33,002 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:33,032 DEBUG Model set to: gpt-4o
2024-11-15 18:31:33,887 DEBUG Query successful
2024-11-15 18:31:33,888 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:33,914 DEBUG Model set to: gpt-4o
2024-11-15 18:31:37,556 DEBUG Query successful
2024-11-15 18:31:37,869 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:37,894 DEBUG Model set to: gpt-4o
2024-11-15 18:31:38,015 DEBUG Query successful
2024-11-15 18:31:38,113 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:38,143 DEBUG Model set to: gpt-4o
2024-11-15 18:31:44,578 DEBUG Query successful
2024-11-15 18:31:44,658 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:44,683 DEBUG Model set to: gpt-4o
2024-11-15 18:31:45,412 DEBUG Query successful
2024-11-15 18:31:45,414 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:45,437 DEBUG Model set to: gpt-4o
2024-11-15 18:31:49,481 DEBUG Query successful
2024-11-15 18:31:49,561 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:49,584 DEBUG Model set to: gpt-4o
2024-11-15 18:31:52,342 DEBUG Query successful
2024-11-15 18:31:52,343 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:52,368 DEBUG Model set to: gpt-4o
2024-11-15 18:31:53,523 DEBUG Query successful
2024-11-15 18:31:53,524 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:53,548 DEBUG Model set to: gpt-4o
2024-11-15 18:31:56,306 DEBUG Query successful
2024-11-15 18:31:56,307 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:31:56,331 DEBUG Model set to: gpt-4o
2024-11-15 18:32:03,636 DEBUG Query successful
2024-11-15 18:32:04,051 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:04,076 DEBUG Model set to: gpt-4o
2024-11-15 18:32:04,449 DEBUG Query successful
2024-11-15 18:32:04,450 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:04,478 DEBUG Model set to: gpt-4o
2024-11-15 18:32:08,528 DEBUG Query successful
2024-11-15 18:32:08,845 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:08,870 DEBUG Model set to: gpt-4o
2024-11-15 18:32:10,450 DEBUG Query successful
2024-11-15 18:32:10,451 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:10,479 DEBUG Model set to: gpt-4o
2024-11-15 18:32:15,327 DEBUG Query successful
2024-11-15 18:32:15,411 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:15,435 DEBUG Model set to: gpt-4o
2024-11-15 18:32:18,429 DEBUG Query successful
2024-11-15 18:32:18,804 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:18,829 DEBUG Model set to: gpt-4o
2024-11-15 18:32:20,621 DEBUG Query successful
2024-11-15 18:32:20,622 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:20,646 DEBUG Model set to: gpt-4o
2024-11-15 18:32:20,963 DEBUG Query successful
2024-11-15 18:32:21,045 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:21,069 DEBUG Model set to: gpt-4o
2024-11-15 18:32:29,007 DEBUG Query successful
2024-11-15 18:32:29,008 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:29,051 DEBUG Model set to: gpt-4o
2024-11-15 18:32:31,404 DEBUG Query successful
2024-11-15 18:32:31,488 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:31,512 DEBUG Model set to: gpt-4o
2024-11-15 18:32:34,432 DEBUG Query successful
2024-11-15 18:32:34,445 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:34,469 DEBUG Model set to: gpt-4o
2024-11-15 18:32:37,471 DEBUG Query successful
2024-11-15 18:32:37,472 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:37,498 DEBUG Model set to: gpt-4o
2024-11-15 18:32:37,876 DEBUG Query successful
2024-11-15 18:32:37,878 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:37,902 DEBUG Model set to: gpt-4o
2024-11-15 18:32:38,144 DEBUG Query successful
2024-11-15 18:32:38,556 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:38,582 DEBUG Model set to: gpt-4o
2024-11-15 18:32:42,333 DEBUG Query successful
2024-11-15 18:32:42,788 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:42,815 DEBUG Model set to: gpt-4o
2024-11-15 18:32:50,782 DEBUG Query successful
2024-11-15 18:32:50,783 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:50,812 DEBUG Model set to: gpt-4o
2024-11-15 18:32:55,789 DEBUG Query successful
2024-11-15 18:32:56,179 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:56,207 DEBUG Model set to: gpt-4o
2024-11-15 18:32:58,740 DEBUG Query successful
2024-11-15 18:32:58,819 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:58,844 DEBUG Model set to: gpt-4o
2024-11-15 18:32:58,913 DEBUG Query successful
2024-11-15 18:32:58,996 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:32:59,032 DEBUG Model set to: gpt-4o
2024-11-15 18:33:04,746 DEBUG Query successful
2024-11-15 18:33:04,759 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:04,786 DEBUG Model set to: gpt-4o
2024-11-15 18:33:04,928 DEBUG Query successful
2024-11-15 18:33:04,930 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:04,954 DEBUG Model set to: gpt-4o
2024-11-15 18:33:12,620 DEBUG Query successful
2024-11-15 18:33:12,699 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:12,724 DEBUG Model set to: gpt-4o
2024-11-15 18:33:13,822 DEBUG Query successful
2024-11-15 18:33:13,824 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:13,848 DEBUG Model set to: gpt-4o
2024-11-15 18:33:17,897 DEBUG Query successful
2024-11-15 18:33:18,268 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:18,293 DEBUG Model set to: gpt-4o
2024-11-15 18:33:20,275 DEBUG Query successful
2024-11-15 18:33:20,287 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:20,312 DEBUG Model set to: gpt-4o
2024-11-15 18:33:20,505 DEBUG Query successful
2024-11-15 18:33:20,506 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:20,531 DEBUG Model set to: gpt-4o
2024-11-15 18:33:27,285 DEBUG Query successful
2024-11-15 18:33:27,645 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:27,670 DEBUG Model set to: gpt-4o
2024-11-15 18:33:29,715 DEBUG Query successful
2024-11-15 18:33:29,820 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:29,855 DEBUG Model set to: gpt-4o
2024-11-15 18:33:32,569 DEBUG Query successful
2024-11-15 18:33:32,571 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:32,595 DEBUG Model set to: gpt-4o
2024-11-15 18:33:37,249 DEBUG Query successful
2024-11-15 18:33:37,251 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:37,275 DEBUG Model set to: gpt-4o
2024-11-15 18:33:37,856 DEBUG Query successful
2024-11-15 18:33:38,115 DEBUG Query successful
2024-11-15 18:33:38,229 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:38,269 DEBUG Model set to: gpt-4o
2024-11-15 18:33:38,320 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:38,352 DEBUG Model set to: gpt-4o
2024-11-15 18:33:44,591 DEBUG Query successful
2024-11-15 18:33:44,592 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:44,616 DEBUG Model set to: gpt-4o
2024-11-15 18:33:49,629 DEBUG Query successful
2024-11-15 18:33:49,630 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:49,657 DEBUG Model set to: gpt-4o
2024-11-15 18:33:51,329 DEBUG Query successful
2024-11-15 18:33:51,409 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:51,434 DEBUG Model set to: gpt-4o
2024-11-15 18:33:53,153 DEBUG Query successful
2024-11-15 18:33:53,490 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:53,515 DEBUG Model set to: gpt-4o
2024-11-15 18:33:58,002 DEBUG Query successful
2024-11-15 18:33:58,002 DEBUG Query successful
2024-11-15 18:33:58,003 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:58,003 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:33:58,030 DEBUG Model set to: gpt-4o
2024-11-15 18:33:58,031 DEBUG Model set to: gpt-4o
2024-11-15 18:34:04,506 DEBUG Query successful
2024-11-15 18:34:04,595 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:04,620 DEBUG Model set to: gpt-4o
2024-11-15 18:34:08,058 DEBUG Query successful
2024-11-15 18:34:08,060 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:08,085 DEBUG Model set to: gpt-4o
2024-11-15 18:34:10,062 DEBUG Query successful
2024-11-15 18:34:10,540 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:10,580 DEBUG Model set to: gpt-4o
2024-11-15 18:34:10,859 DEBUG Query successful
2024-11-15 18:34:10,860 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:10,889 DEBUG Model set to: gpt-4o
2024-11-15 18:34:11,649 DEBUG Query successful
2024-11-15 18:34:12,044 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:12,073 DEBUG Model set to: gpt-4o
2024-11-15 18:34:22,608 DEBUG Query successful
2024-11-15 18:34:22,687 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:22,711 DEBUG Model set to: gpt-4o
2024-11-15 18:34:26,438 DEBUG Query successful
2024-11-15 18:34:26,439 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:26,463 DEBUG Model set to: gpt-4o
2024-11-15 18:34:26,629 DEBUG Query successful
2024-11-15 18:34:26,631 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:26,657 DEBUG Model set to: gpt-4o
2024-11-15 18:34:29,638 DEBUG Query successful
2024-11-15 18:34:29,729 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:29,755 DEBUG Model set to: gpt-4o
2024-11-15 18:34:32,608 DEBUG Query successful
2024-11-15 18:34:32,970 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:33,020 DEBUG Model set to: gpt-4o
2024-11-15 18:34:36,266 DEBUG Query successful
2024-11-15 18:34:36,266 DEBUG Query successful
2024-11-15 18:34:36,267 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:36,267 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:36,291 DEBUG Model set to: gpt-4o
2024-11-15 18:34:36,291 DEBUG Model set to: gpt-4o
2024-11-15 18:34:41,700 DEBUG Query successful
2024-11-15 18:34:42,071 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:42,097 DEBUG Model set to: gpt-4o
2024-11-15 18:34:46,291 DEBUG Query successful
2024-11-15 18:34:46,291 DEBUG Query successful
2024-11-15 18:34:46,305 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:46,332 DEBUG Model set to: gpt-4o
2024-11-15 18:34:46,379 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:46,403 DEBUG Model set to: gpt-4o
2024-11-15 18:34:49,552 DEBUG Query successful
2024-11-15 18:34:49,903 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:49,927 DEBUG Model set to: gpt-4o
2024-11-15 18:34:50,145 DEBUG Query successful
2024-11-15 18:34:50,146 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:50,171 DEBUG Model set to: gpt-4o
2024-11-15 18:34:53,977 DEBUG Query successful
2024-11-15 18:34:54,058 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:34:54,082 DEBUG Model set to: gpt-4o
2024-11-15 18:35:00,668 DEBUG Query successful
2024-11-15 18:35:00,748 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:00,773 DEBUG Model set to: gpt-4o
2024-11-15 18:35:01,010 DEBUG Query successful
2024-11-15 18:35:01,012 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:01,039 DEBUG Model set to: gpt-4o
2024-11-15 18:35:02,209 DEBUG Query successful
2024-11-15 18:35:02,210 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:02,235 DEBUG Model set to: gpt-4o
2024-11-15 18:35:05,931 DEBUG Query successful
2024-11-15 18:35:05,944 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:05,969 DEBUG Model set to: gpt-4o
2024-11-15 18:35:06,243 DEBUG Query successful
2024-11-15 18:35:06,563 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:06,588 DEBUG Model set to: gpt-4o
2024-11-15 18:35:09,814 DEBUG Query successful
2024-11-15 18:35:09,816 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:09,838 DEBUG Model set to: gpt-4o
2024-11-15 18:35:13,845 DEBUG Query successful
2024-11-15 18:35:14,184 DEBUG Query successful
2024-11-15 18:35:14,185 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:14,199 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:14,208 DEBUG Model set to: gpt-4o
2024-11-15 18:35:14,223 DEBUG Model set to: gpt-4o
2024-11-15 18:35:17,238 DEBUG Query successful
2024-11-15 18:35:17,572 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:17,598 DEBUG Model set to: gpt-4o
2024-11-15 18:35:20,258 DEBUG Query successful
2024-11-15 18:35:20,336 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:20,361 DEBUG Model set to: gpt-4o
2024-11-15 18:35:25,727 DEBUG Query successful
2024-11-15 18:35:25,740 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:25,765 DEBUG Model set to: gpt-4o
2024-11-15 18:35:27,905 DEBUG Query successful
2024-11-15 18:35:27,985 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:28,011 DEBUG Model set to: gpt-4o
2024-11-15 18:35:28,574 DEBUG Query successful
2024-11-15 18:35:28,653 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:28,678 DEBUG Model set to: gpt-4o
2024-11-15 18:35:32,768 DEBUG Query successful
2024-11-15 18:35:32,768 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:32,793 DEBUG Model set to: gpt-4o
2024-11-15 18:35:33,260 DEBUG Query successful
2024-11-15 18:35:33,261 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:33,285 DEBUG Model set to: gpt-4o
2024-11-15 18:35:34,624 DEBUG Query successful
2024-11-15 18:35:34,626 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:34,649 DEBUG Model set to: gpt-4o
2024-11-15 18:35:39,291 DEBUG Query successful
2024-11-15 18:35:39,618 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:39,643 DEBUG Model set to: gpt-4o
2024-11-15 18:35:41,425 DEBUG Query successful
2024-11-15 18:35:41,427 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:41,452 DEBUG Model set to: gpt-4o
2024-11-15 18:35:45,033 DEBUG Query successful
2024-11-15 18:35:45,035 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:45,059 DEBUG Model set to: gpt-4o
2024-11-15 18:35:48,584 DEBUG Query successful
2024-11-15 18:35:48,946 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:48,970 DEBUG Model set to: gpt-4o
2024-11-15 18:35:49,457 DEBUG Query successful
2024-11-15 18:35:49,834 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:49,865 DEBUG Model set to: gpt-4o
2024-11-15 18:35:50,132 DEBUG Query successful
2024-11-15 18:35:50,216 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:50,242 DEBUG Model set to: gpt-4o
2024-11-15 18:35:54,980 DEBUG Query successful
2024-11-15 18:35:54,981 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:35:55,007 DEBUG Model set to: gpt-4o
2024-11-15 18:36:03,537 DEBUG Query successful
2024-11-15 18:36:03,635 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:03,660 DEBUG Model set to: gpt-4o
2024-11-15 18:36:04,780 DEBUG Query successful
2024-11-15 18:36:04,873 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:04,897 DEBUG Model set to: gpt-4o
2024-11-15 18:36:07,869 DEBUG Query successful
2024-11-15 18:36:07,881 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:07,913 DEBUG Model set to: gpt-4o
2024-11-15 18:36:09,189 DEBUG Query successful
2024-11-15 18:36:09,190 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:09,215 DEBUG Model set to: gpt-4o
2024-11-15 18:36:09,244 DEBUG Query successful
2024-11-15 18:36:09,245 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:09,269 DEBUG Model set to: gpt-4o
2024-11-15 18:36:17,275 DEBUG Query successful
2024-11-15 18:36:17,634 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:17,681 DEBUG Model set to: gpt-4o
2024-11-15 18:36:18,287 DEBUG Query successful
2024-11-15 18:36:18,301 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:18,327 DEBUG Model set to: gpt-4o
2024-11-15 18:36:21,772 DEBUG Query successful
2024-11-15 18:36:22,183 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:22,208 DEBUG Model set to: gpt-4o
2024-11-15 18:36:23,903 DEBUG Query successful
2024-11-15 18:36:23,905 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:23,929 DEBUG Model set to: gpt-4o
2024-11-15 18:36:31,385 DEBUG Query successful
2024-11-15 18:36:31,476 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:31,500 DEBUG Model set to: gpt-4o
2024-11-15 18:36:31,598 DEBUG Query successful
2024-11-15 18:36:31,943 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:31,967 DEBUG Model set to: gpt-4o
2024-11-15 18:36:35,590 DEBUG Query successful
2024-11-15 18:36:35,591 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:35,616 DEBUG Model set to: gpt-4o
2024-11-15 18:36:38,451 DEBUG Query successful
2024-11-15 18:36:38,544 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:38,569 DEBUG Model set to: gpt-4o
2024-11-15 18:36:45,189 DEBUG Query successful
2024-11-15 18:36:45,190 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:45,215 DEBUG Model set to: gpt-4o
2024-11-15 18:36:47,832 DEBUG Query successful
2024-11-15 18:36:47,834 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:47,834 DEBUG Query successful
2024-11-15 18:36:47,859 DEBUG Model set to: gpt-4o
2024-11-15 18:36:47,917 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:47,942 DEBUG Model set to: gpt-4o
2024-11-15 18:36:52,051 DEBUG Query successful
2024-11-15 18:36:52,467 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:52,501 DEBUG Model set to: gpt-4o
2024-11-15 18:36:53,846 DEBUG Query successful
2024-11-15 18:36:53,847 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:53,871 DEBUG Model set to: gpt-4o
2024-11-15 18:36:57,758 DEBUG Query successful
2024-11-15 18:36:57,760 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:36:57,783 DEBUG Model set to: gpt-4o
2024-11-15 18:37:04,485 DEBUG Query successful
2024-11-15 18:37:04,818 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:04,843 DEBUG Model set to: gpt-4o
2024-11-15 18:37:06,330 DEBUG Query successful
2024-11-15 18:37:06,419 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:06,444 DEBUG Model set to: gpt-4o
2024-11-15 18:37:09,438 DEBUG Query successful
2024-11-15 18:37:09,440 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:09,463 DEBUG Model set to: gpt-4o
2024-11-15 18:37:13,209 DEBUG Query successful
2024-11-15 18:37:13,542 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:13,567 DEBUG Model set to: gpt-4o
2024-11-15 18:37:16,616 DEBUG Query successful
2024-11-15 18:37:16,617 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:16,641 DEBUG Model set to: gpt-4o
2024-11-15 18:37:20,599 DEBUG Query successful
2024-11-15 18:37:20,680 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:20,705 DEBUG Model set to: gpt-4o
2024-11-15 18:37:24,620 DEBUG Query successful
2024-11-15 18:37:24,699 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:24,724 DEBUG Model set to: gpt-4o
2024-11-15 18:37:25,836 DEBUG Query successful
2024-11-15 18:37:25,837 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:25,863 DEBUG Model set to: gpt-4o
2024-11-15 18:37:28,493 DEBUG Query successful
2024-11-15 18:37:28,495 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:28,518 DEBUG Model set to: gpt-4o
2024-11-15 18:37:29,080 DEBUG Query successful
2024-11-15 18:37:29,081 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:29,104 DEBUG Model set to: gpt-4o
2024-11-15 18:37:35,396 DEBUG Query successful
2024-11-15 18:37:35,717 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:35,742 DEBUG Model set to: gpt-4o
2024-11-15 18:37:38,868 DEBUG Query successful
2024-11-15 18:37:38,870 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:38,894 DEBUG Model set to: gpt-4o
2024-11-15 18:37:41,438 DEBUG Query successful
2024-11-15 18:37:41,440 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:41,471 DEBUG Model set to: gpt-4o
2024-11-15 18:37:44,286 DEBUG Query successful
2024-11-15 18:37:44,646 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:44,673 DEBUG Model set to: gpt-4o
2024-11-15 18:37:45,092 DEBUG Query successful
2024-11-15 18:37:45,451 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:45,476 DEBUG Model set to: gpt-4o
2024-11-15 18:37:47,020 DEBUG Query successful
2024-11-15 18:37:47,112 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:47,138 DEBUG Model set to: gpt-4o
2024-11-15 18:37:53,867 DEBUG Query successful
2024-11-15 18:37:53,879 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:53,905 DEBUG Model set to: gpt-4o
2024-11-15 18:37:56,945 DEBUG Query successful
2024-11-15 18:37:57,040 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:37:57,065 DEBUG Model set to: gpt-4o
2024-11-15 18:38:00,572 DEBUG Query successful
2024-11-15 18:38:00,654 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:00,678 DEBUG Model set to: gpt-4o
2024-11-15 18:38:00,979 DEBUG Query successful
2024-11-15 18:38:00,980 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:01,005 DEBUG Model set to: gpt-4o
2024-11-15 18:38:04,670 DEBUG Query successful
2024-11-15 18:38:04,671 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:04,681 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:04,695 DEBUG Model set to: gpt-4o
2024-11-15 18:38:04,707 DEBUG Model set to: gpt-4o
2024-11-15 18:38:13,151 DEBUG Query successful
2024-11-15 18:38:13,502 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:13,528 DEBUG Model set to: gpt-4o
2024-11-15 18:38:14,982 DEBUG Query successful
2024-11-15 18:38:14,983 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:15,010 DEBUG Model set to: gpt-4o
2024-11-15 18:38:19,055 DEBUG Query successful
2024-11-15 18:38:19,398 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:19,432 DEBUG Model set to: gpt-4o
2024-11-15 18:38:21,586 DEBUG Query successful
2024-11-15 18:38:21,587 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:21,611 DEBUG Model set to: gpt-4o
2024-11-15 18:38:28,257 DEBUG Query successful
2024-11-15 18:38:28,609 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:28,633 DEBUG Model set to: gpt-4o
2024-11-15 18:38:29,301 DEBUG Query successful
2024-11-15 18:38:29,389 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:29,415 DEBUG Model set to: gpt-4o
2024-11-15 18:38:35,616 DEBUG Query successful
2024-11-15 18:38:35,698 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:35,722 DEBUG Model set to: gpt-4o
2024-11-15 18:38:36,379 DEBUG Query successful
2024-11-15 18:38:36,380 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:36,404 DEBUG Model set to: gpt-4o
2024-11-15 18:38:41,885 DEBUG Query successful
2024-11-15 18:38:41,886 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:41,910 DEBUG Model set to: gpt-4o
2024-11-15 18:38:43,250 DEBUG Query successful
2024-11-15 18:38:43,343 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:43,375 DEBUG Model set to: gpt-4o
2024-11-15 18:38:47,294 DEBUG Query successful
2024-11-15 18:38:47,296 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:47,320 DEBUG Model set to: gpt-4o
2024-11-15 18:38:49,379 DEBUG Query successful
2024-11-15 18:38:49,706 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:49,735 DEBUG Model set to: gpt-4o
2024-11-15 18:38:50,898 DEBUG Query successful
2024-11-15 18:38:50,899 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:50,924 DEBUG Model set to: gpt-4o
2024-11-15 18:38:53,498 DEBUG Query successful
2024-11-15 18:38:53,500 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:53,523 DEBUG Model set to: gpt-4o
2024-11-15 18:38:58,404 DEBUG Query successful
2024-11-15 18:38:58,772 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:58,796 DEBUG Model set to: gpt-4o
2024-11-15 18:38:59,841 DEBUG Query successful
2024-11-15 18:38:59,932 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:38:59,957 DEBUG Model set to: gpt-4o
2024-11-15 18:39:00,878 DEBUG Query successful
2024-11-15 18:39:00,879 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:00,903 DEBUG Model set to: gpt-4o
2024-11-15 18:39:03,436 DEBUG Query successful
2024-11-15 18:39:03,448 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:03,471 DEBUG Model set to: gpt-4o
2024-11-15 18:39:07,394 DEBUG Query successful
2024-11-15 18:39:07,796 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:07,822 DEBUG Model set to: gpt-4o
2024-11-15 18:39:09,909 DEBUG Query successful
2024-11-15 18:39:10,000 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:10,032 DEBUG Model set to: gpt-4o
2024-11-15 18:39:12,278 DEBUG Query successful
2024-11-15 18:39:12,279 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:12,313 DEBUG Model set to: gpt-4o
2024-11-15 18:39:16,155 DEBUG Query successful
2024-11-15 18:39:16,464 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:16,489 DEBUG Model set to: gpt-4o
2024-11-15 18:39:17,333 DEBUG Query successful
2024-11-15 18:39:17,345 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:17,370 DEBUG Model set to: gpt-4o
2024-11-15 18:39:19,160 DEBUG Query successful
2024-11-15 18:39:19,243 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:19,266 DEBUG Model set to: gpt-4o
2024-11-15 18:39:25,706 DEBUG Query successful
2024-11-15 18:39:25,708 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:25,742 DEBUG Model set to: gpt-4o
2024-11-15 18:39:28,687 DEBUG Query successful
2024-11-15 18:39:28,768 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:28,793 DEBUG Model set to: gpt-4o
2024-11-15 18:39:33,872 DEBUG Query successful
2024-11-15 18:39:33,885 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:33,912 DEBUG Model set to: gpt-4o
2024-11-15 18:39:37,616 DEBUG Query successful
2024-11-15 18:39:37,942 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:37,965 DEBUG Model set to: gpt-4o
2024-11-15 18:39:39,830 DEBUG Query successful
2024-11-15 18:39:39,831 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:39,855 DEBUG Model set to: gpt-4o
2024-11-15 18:39:42,269 DEBUG Query successful
2024-11-15 18:39:42,271 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:42,295 DEBUG Model set to: gpt-4o
2024-11-15 18:39:47,059 DEBUG Query successful
2024-11-15 18:39:47,420 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:47,445 DEBUG Model set to: gpt-4o
2024-11-15 18:39:52,158 DEBUG Query successful
2024-11-15 18:39:52,237 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:52,261 DEBUG Model set to: gpt-4o
2024-11-15 18:39:55,283 DEBUG Query successful
2024-11-15 18:39:55,285 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:55,325 DEBUG Model set to: gpt-4o
2024-11-15 18:39:58,838 DEBUG Query successful
2024-11-15 18:39:59,186 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:39:59,210 DEBUG Model set to: gpt-4o
2024-11-15 18:40:00,854 DEBUG Query successful
2024-11-15 18:40:00,866 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:00,901 DEBUG Model set to: gpt-4o
2024-11-15 18:40:03,618 DEBUG Query successful
2024-11-15 18:40:03,695 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:03,722 DEBUG Model set to: gpt-4o
2024-11-15 18:40:11,236 DEBUG Query successful
2024-11-15 18:40:11,238 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:11,263 DEBUG Model set to: gpt-4o
2024-11-15 18:40:11,671 DEBUG Query successful
2024-11-15 18:40:11,762 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:11,788 DEBUG Model set to: gpt-4o
2024-11-15 18:40:13,581 DEBUG Query successful
2024-11-15 18:40:13,582 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:13,607 DEBUG Model set to: gpt-4o
2024-11-15 18:40:14,650 DEBUG Query successful
2024-11-15 18:40:14,971 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:15,002 DEBUG Model set to: gpt-4o
2024-11-15 18:40:15,460 DEBUG Query successful
2024-11-15 18:40:15,461 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:15,486 DEBUG Model set to: gpt-4o
2024-11-15 18:40:22,511 DEBUG Query successful
2024-11-15 18:40:22,512 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:22,537 DEBUG Model set to: gpt-4o
2024-11-15 18:40:24,937 DEBUG Query successful
2024-11-15 18:40:25,260 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:25,285 DEBUG Model set to: gpt-4o
2024-11-15 18:40:26,637 DEBUG Query successful
2024-11-15 18:40:26,638 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:26,662 DEBUG Model set to: gpt-4o
2024-11-15 18:40:27,996 DEBUG Query successful
2024-11-15 18:40:28,077 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:28,101 DEBUG Model set to: gpt-4o
2024-11-15 18:40:29,344 DEBUG Query successful
2024-11-15 18:40:29,652 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:29,677 DEBUG Model set to: gpt-4o
2024-11-15 18:40:33,674 DEBUG Query successful
2024-11-15 18:40:33,675 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:33,699 DEBUG Model set to: gpt-4o
2024-11-15 18:40:36,436 DEBUG Query successful
2024-11-15 18:40:36,522 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:36,552 DEBUG Model set to: gpt-4o
2024-11-15 18:40:43,636 DEBUG Query successful
2024-11-15 18:40:43,637 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:43,661 DEBUG Model set to: gpt-4o
2024-11-15 18:40:43,782 DEBUG Query successful
2024-11-15 18:40:43,783 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:43,807 DEBUG Model set to: gpt-4o
2024-11-15 18:40:43,874 DEBUG Query successful
2024-11-15 18:40:43,959 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:43,983 DEBUG Model set to: gpt-4o
2024-11-15 18:40:45,395 DEBUG Query successful
2024-11-15 18:40:45,756 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:45,781 DEBUG Model set to: gpt-4o
2024-11-15 18:40:48,088 DEBUG Query successful
2024-11-15 18:40:48,088 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:48,113 DEBUG Model set to: gpt-4o
2024-11-15 18:40:55,791 DEBUG Query successful
2024-11-15 18:40:55,793 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:55,819 DEBUG Model set to: gpt-4o
2024-11-15 18:40:57,473 DEBUG Query successful
2024-11-15 18:40:57,792 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:40:57,825 DEBUG Model set to: gpt-4o
2024-11-15 18:41:03,308 DEBUG Query successful
2024-11-15 18:41:03,400 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:03,427 DEBUG Model set to: gpt-4o
2024-11-15 18:41:04,505 DEBUG Query successful
2024-11-15 18:41:04,507 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:04,530 DEBUG Model set to: gpt-4o
2024-11-15 18:41:07,891 DEBUG Query successful
2024-11-15 18:41:08,238 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:08,262 DEBUG Model set to: gpt-4o
2024-11-15 18:41:10,303 DEBUG Query successful
2024-11-15 18:41:10,315 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:10,349 DEBUG Model set to: gpt-4o
2024-11-15 18:41:11,988 DEBUG Query successful
2024-11-15 18:41:12,067 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:12,092 DEBUG Model set to: gpt-4o
2024-11-15 18:41:18,940 DEBUG Query successful
2024-11-15 18:41:18,952 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:18,977 DEBUG Model set to: gpt-4o
2024-11-15 18:41:20,324 DEBUG Query successful
2024-11-15 18:41:20,403 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:20,428 DEBUG Model set to: gpt-4o
2024-11-15 18:41:24,248 DEBUG Query successful
2024-11-15 18:41:24,261 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:24,286 DEBUG Model set to: gpt-4o
2024-11-15 18:41:25,800 DEBUG Query successful
2024-11-15 18:41:28,782 DEBUG Query successful
2024-11-15 18:41:28,783 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:28,808 DEBUG Model set to: gpt-4o
2024-11-15 18:41:28,952 DEBUG Query successful
2024-11-15 18:41:28,953 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:28,978 DEBUG Model set to: gpt-4o
2024-11-15 18:41:30,788 DEBUG Query successful
2024-11-15 18:41:31,108 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:31,132 DEBUG Model set to: gpt-4o
2024-11-15 18:41:38,580 DEBUG Query successful
2024-11-15 18:41:38,581 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:38,605 DEBUG Model set to: gpt-4o
2024-11-15 18:41:40,261 DEBUG Query successful
2024-11-15 18:41:43,822 DEBUG Query successful
2024-11-15 18:41:43,902 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:43,928 DEBUG Model set to: gpt-4o
2024-11-15 18:41:48,115 DEBUG Query successful
2024-11-15 18:41:48,116 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:48,138 DEBUG Model set to: gpt-4o
2024-11-15 18:41:58,010 DEBUG Query successful
2024-11-15 18:41:58,012 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:41:58,034 DEBUG Model set to: gpt-4o
2024-11-15 18:41:59,811 DEBUG Query successful
2024-11-15 18:42:00,132 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:42:00,155 DEBUG Model set to: gpt-4o
2024-11-15 18:42:12,788 DEBUG Query successful
2024-11-15 18:42:12,865 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:42:12,889 DEBUG Model set to: gpt-4o
2024-11-15 18:42:19,047 DEBUG Query successful
2024-11-15 18:42:19,047 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:42:19,071 DEBUG Model set to: gpt-4o
2024-11-15 18:42:31,685 DEBUG Query successful
2024-11-15 18:42:31,686 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-15 18:42:31,710 DEBUG Model set to: gpt-4o
2024-11-15 18:42:33,599 DEBUG Query successful
