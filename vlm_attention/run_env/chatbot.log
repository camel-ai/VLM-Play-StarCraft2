2024-08-12 17:03:02,613 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-12 17:03:02,655 DEBUG OpenAI client created
2024-08-12 17:03:07,723 DEBUG Querying model with image: gpt-4o
2024-08-12 17:03:58,208 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-12 17:03:58,235 DEBUG OpenAI client created
2024-08-12 17:04:03,398 DEBUG Querying model with image: gpt-4o
2024-08-12 17:04:12,617 DEBUG Query with image successful
2024-08-12 17:04:12,696 DEBUG Querying model with image: gpt-4o
2024-08-12 17:04:22,896 DEBUG Query with image successful
2024-08-12 17:04:23,183 DEBUG Querying model with image: gpt-4o
2024-08-12 17:04:32,650 DEBUG Query with image successful
2024-08-12 17:04:32,742 DEBUG Querying model with image: gpt-4o
2024-08-12 17:04:42,076 DEBUG Query with image successful
2024-08-12 17:04:52,348 DEBUG Querying model with image: gpt-4o
2024-08-12 17:05:03,375 DEBUG Query with image successful
2024-08-12 17:05:03,457 DEBUG Querying model with image: gpt-4o
2024-08-12 17:05:12,798 DEBUG Query with image successful
2024-08-12 17:05:13,068 DEBUG Querying model with image: gpt-4o
2024-08-12 17:05:23,522 DEBUG Query with image successful
2024-08-12 17:05:23,602 DEBUG Querying model with image: gpt-4o
2024-08-12 17:05:39,327 DEBUG Query with image successful
2024-08-12 17:05:39,638 DEBUG Querying model with image: gpt-4o
2024-08-12 17:05:50,695 DEBUG Query with image successful
2024-08-12 17:05:50,774 DEBUG Querying model with image: gpt-4o
2024-08-12 17:06:01,878 DEBUG Query with image successful
2024-08-12 17:06:02,159 DEBUG Querying model with image: gpt-4o
2024-08-12 17:06:17,805 DEBUG Query with image successful
2024-08-12 17:06:17,891 DEBUG Querying model with image: gpt-4o
2024-08-12 17:06:32,486 DEBUG Query with image successful
2024-08-12 17:06:32,844 DEBUG Querying model with image: gpt-4o
2024-08-12 17:06:43,649 DEBUG Query with image successful
2024-08-12 17:06:43,728 DEBUG Querying model with image: gpt-4o
2024-08-12 17:06:57,066 DEBUG Query with image successful
2024-08-12 17:06:57,384 DEBUG Querying model with image: gpt-4o
2024-08-12 17:07:08,041 DEBUG Query with image successful
2024-08-12 17:07:08,120 DEBUG Querying model with image: gpt-4o
2024-08-12 17:07:18,040 DEBUG Query with image successful
2024-08-12 17:07:18,349 DEBUG Querying model with image: gpt-4o
2024-08-12 17:07:29,238 DEBUG Query with image successful
2024-08-12 17:07:29,330 DEBUG Querying model with image: gpt-4o
2024-08-12 17:07:41,600 DEBUG Query with image successful
2024-08-12 17:07:41,880 DEBUG Querying model with image: gpt-4o
2024-08-12 17:07:54,776 DEBUG Query with image successful
2024-08-12 17:07:54,855 DEBUG Querying model with image: gpt-4o
2024-08-12 17:08:05,402 DEBUG Query with image successful
2024-08-12 17:08:05,723 DEBUG Querying model with image: gpt-4o
2024-08-12 17:08:17,358 DEBUG Query with image successful
2024-08-12 17:08:17,440 DEBUG Querying model with image: gpt-4o
2024-08-12 17:08:27,904 DEBUG Query with image successful
2024-08-12 17:08:28,171 DEBUG Querying model with image: gpt-4o
2024-08-12 17:08:44,267 DEBUG Query with image successful
2024-08-12 17:08:44,345 DEBUG Querying model with image: gpt-4o
2024-08-12 17:08:57,622 DEBUG Query with image successful
2024-08-12 17:08:57,900 DEBUG Querying model with image: gpt-4o
2024-08-12 17:09:12,162 DEBUG Query with image successful
2024-08-12 17:09:12,240 DEBUG Querying model with image: gpt-4o
2024-08-12 17:09:23,877 DEBUG Query with image successful
2024-08-12 17:09:24,157 DEBUG Querying model with image: gpt-4o
2024-08-12 17:09:36,582 DEBUG Query with image successful
2024-08-12 17:09:36,663 DEBUG Querying model with image: gpt-4o
2024-08-12 17:09:48,862 DEBUG Query with image successful
2024-08-12 17:09:49,173 DEBUG Querying model with image: gpt-4o
2024-08-12 17:10:00,192 DEBUG Query with image successful
2024-08-12 17:10:00,280 DEBUG Querying model with image: gpt-4o
2024-08-12 17:10:10,292 DEBUG Query with image successful
2024-08-12 17:10:10,575 DEBUG Querying model with image: gpt-4o
2024-08-12 17:10:21,413 DEBUG Query with image successful
2024-08-12 17:10:21,490 DEBUG Querying model with image: gpt-4o
2024-08-12 17:10:31,740 DEBUG Query with image successful
2024-08-12 17:10:32,025 DEBUG Querying model with image: gpt-4o
2024-08-12 17:10:43,527 DEBUG Query with image successful
2024-08-12 17:10:43,609 DEBUG Querying model with image: gpt-4o
2024-08-12 17:10:56,009 DEBUG Query with image successful
2024-08-12 17:10:56,271 DEBUG Querying model with image: gpt-4o
2024-08-12 17:11:11,114 DEBUG Query with image successful
2024-08-12 17:11:11,193 DEBUG Querying model with image: gpt-4o
2024-08-12 17:11:22,372 DEBUG Query with image successful
2024-08-12 17:11:22,656 DEBUG Querying model with image: gpt-4o
2024-08-12 17:11:34,328 DEBUG Query with image successful
2024-08-12 17:11:34,407 DEBUG Querying model with image: gpt-4o
2024-08-12 17:11:42,895 DEBUG Query with image successful
2024-08-12 18:31:28,360 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-12 18:31:28,386 DEBUG OpenAI client created
2024-08-12 18:31:33,534 DEBUG Querying model with image: gpt-4o
2024-08-12 18:31:43,567 DEBUG Query with image successful
2024-08-12 18:39:24,066 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-12 18:39:24,104 DEBUG OpenAI client created
2024-08-12 18:39:29,279 DEBUG Querying model with image: gpt-4o
2024-08-12 18:39:38,469 DEBUG Query with image successful
2024-08-12 18:57:04,048 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-12 18:57:04,074 DEBUG OpenAI client created
2024-08-12 18:57:09,259 DEBUG Querying model with image: gpt-4o
2024-08-12 18:57:19,797 DEBUG Query with image successful
2024-08-12 18:57:19,873 DEBUG Querying model with image: gpt-4o
2024-08-12 18:57:28,590 DEBUG Query with image successful
2024-08-12 18:57:28,986 DEBUG Querying model with image: gpt-4o
2024-08-12 18:57:40,085 DEBUG Query with image successful
2024-08-12 18:57:40,162 DEBUG Querying model with image: gpt-4o
2024-08-12 18:57:50,525 DEBUG Query with image successful
2024-08-12 18:58:00,834 DEBUG Querying model with image: gpt-4o
2024-08-12 18:58:08,752 DEBUG Query with image successful
2024-08-12 18:58:08,829 DEBUG Querying model with image: gpt-4o
2024-08-12 18:58:17,068 DEBUG Query with image successful
2024-08-12 18:58:17,367 DEBUG Querying model with image: gpt-4o
2024-08-12 18:58:24,180 DEBUG Query with image successful
2024-08-12 18:58:24,271 DEBUG Querying model with image: gpt-4o
2024-08-12 18:58:31,680 DEBUG Query with image successful
2024-08-12 18:58:31,972 DEBUG Querying model with image: gpt-4o
2024-08-12 18:58:39,887 DEBUG Query with image successful
2024-08-12 18:58:39,972 DEBUG Querying model with image: gpt-4o
2024-08-12 18:58:48,584 DEBUG Query with image successful
2024-08-12 18:58:48,909 DEBUG Querying model with image: gpt-4o
2024-08-12 18:58:55,793 DEBUG Query with image successful
2024-08-12 18:58:55,873 DEBUG Querying model with image: gpt-4o
2024-08-12 18:59:07,080 DEBUG Query with image successful
2024-08-12 18:59:07,398 DEBUG Querying model with image: gpt-4o
2024-08-12 18:59:17,673 DEBUG Query with image successful
2024-08-12 18:59:17,754 DEBUG Querying model with image: gpt-4o
2024-08-12 18:59:26,822 DEBUG Query with image successful
2024-08-12 18:59:27,123 DEBUG Querying model with image: gpt-4o
2024-08-12 18:59:36,426 DEBUG Query with image successful
2024-08-12 18:59:36,508 DEBUG Querying model with image: gpt-4o
2024-08-12 18:59:46,665 DEBUG Query with image successful
2024-08-12 18:59:46,949 DEBUG Querying model with image: gpt-4o
2024-08-12 18:59:58,083 DEBUG Query with image successful
2024-08-12 18:59:58,175 DEBUG Querying model with image: gpt-4o
2024-08-12 19:00:10,539 DEBUG Query with image successful
2024-08-12 19:00:10,852 DEBUG Querying model with image: gpt-4o
2024-08-12 19:00:21,216 DEBUG Query with image successful
2024-08-12 19:00:21,298 DEBUG Querying model with image: gpt-4o
2024-08-12 19:00:31,726 DEBUG Query with image successful
2024-08-12 19:00:32,016 DEBUG Querying model with image: gpt-4o
2024-08-12 19:00:40,088 DEBUG Query with image successful
2024-08-12 19:00:40,169 DEBUG Querying model with image: gpt-4o
2024-08-12 19:00:49,068 DEBUG Query with image successful
2024-08-12 19:00:49,347 DEBUG Querying model with image: gpt-4o
2024-08-12 19:00:56,132 DEBUG Query with image successful
2024-08-12 19:00:56,214 DEBUG Querying model with image: gpt-4o
2024-08-12 19:01:07,697 DEBUG Query with image successful
2024-08-12 19:01:07,981 DEBUG Querying model with image: gpt-4o
2024-08-12 19:01:16,302 DEBUG Query with image successful
2024-08-12 19:01:16,383 DEBUG Querying model with image: gpt-4o
2024-08-12 19:01:24,442 DEBUG Query with image successful
2024-08-12 19:01:24,718 DEBUG Querying model with image: gpt-4o
2024-08-12 19:01:32,575 DEBUG Query with image successful
2024-08-12 19:01:32,656 DEBUG Querying model with image: gpt-4o
2024-08-12 19:01:42,708 DEBUG Query with image successful
2024-08-12 19:01:42,987 DEBUG Querying model with image: gpt-4o
2024-08-12 19:01:52,003 DEBUG Query with image successful
2024-08-12 19:01:52,083 DEBUG Querying model with image: gpt-4o
2024-08-12 19:02:01,363 DEBUG Query with image successful
2024-08-12 19:02:01,655 DEBUG Querying model with image: gpt-4o
2024-08-12 19:02:09,191 DEBUG Query with image successful
2024-08-12 19:02:09,271 DEBUG Querying model with image: gpt-4o
2024-08-12 19:02:18,717 DEBUG Query with image successful
2024-08-12 19:02:18,998 DEBUG Querying model with image: gpt-4o
2024-08-12 19:02:34,965 DEBUG Query with image successful
2024-08-12 19:02:35,062 DEBUG Querying model with image: gpt-4o
2024-08-12 19:02:50,706 DEBUG Query with image successful
2024-08-12 19:02:51,008 DEBUG Querying model with image: gpt-4o
2024-08-12 19:02:58,603 DEBUG Query with image successful
2024-08-12 19:02:58,683 DEBUG Querying model with image: gpt-4o
2024-08-12 19:03:10,055 DEBUG Query with image successful
2024-08-12 19:03:10,324 DEBUG Querying model with image: gpt-4o
2024-08-12 19:03:19,145 DEBUG Query with image successful
2024-08-12 19:03:19,236 DEBUG Querying model with image: gpt-4o
2024-08-12 19:03:28,126 DEBUG Query with image successful
2024-08-12 19:03:28,413 DEBUG Querying model with image: gpt-4o
2024-08-12 19:03:36,830 DEBUG Query with image successful
2024-08-12 19:03:36,912 DEBUG Querying model with image: gpt-4o
2024-08-12 19:03:46,191 DEBUG Query with image successful
2024-08-12 19:03:46,453 DEBUG Querying model with image: gpt-4o
2024-08-12 19:03:54,712 DEBUG Query with image successful
2024-08-12 19:03:54,792 DEBUG Querying model with image: gpt-4o
2024-08-12 19:04:04,751 DEBUG Query with image successful
2024-08-12 19:04:05,032 DEBUG Querying model with image: gpt-4o
2024-08-12 19:04:13,839 DEBUG Query with image successful
2024-08-12 19:04:13,919 DEBUG Querying model with image: gpt-4o
2024-08-12 19:04:21,981 DEBUG Query with image successful
2024-08-12 19:04:22,255 DEBUG Querying model with image: gpt-4o
2024-08-12 19:04:29,613 DEBUG Query with image successful
2024-08-12 19:04:29,694 DEBUG Querying model with image: gpt-4o
2024-08-12 19:04:39,115 DEBUG Query with image successful
2024-08-12 19:04:39,383 DEBUG Querying model with image: gpt-4o
2024-08-12 19:04:49,149 DEBUG Query with image successful
2024-08-12 19:04:49,234 DEBUG Querying model with image: gpt-4o
2024-08-12 19:04:58,093 DEBUG Query with image successful
2024-08-12 19:04:58,356 DEBUG Querying model with image: gpt-4o
2024-08-12 19:05:06,446 DEBUG Query with image successful
2024-08-12 19:05:06,528 DEBUG Querying model with image: gpt-4o
2024-08-12 19:05:14,108 DEBUG Query with image successful
2024-08-13 08:27:56,959 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 08:27:56,996 DEBUG OpenAI client created
2024-08-13 08:28:02,154 DEBUG Querying model with image: gpt-4o
2024-08-13 08:28:10,809 DEBUG Query with image successful
2024-08-13 08:28:10,892 DEBUG Querying model with image: gpt-4o
2024-08-13 08:28:19,118 DEBUG Query with image successful
2024-08-13 08:28:19,424 DEBUG Querying model with image: gpt-4o
2024-08-13 08:28:26,422 DEBUG Query with image successful
2024-08-13 08:28:26,502 DEBUG Querying model with image: gpt-4o
2024-08-13 08:28:34,703 DEBUG Query with image successful
2024-08-13 08:28:45,016 DEBUG Querying model with image: gpt-4o
2024-08-13 08:28:54,330 DEBUG Query with image successful
2024-08-13 08:28:54,408 DEBUG Querying model with image: gpt-4o
2024-08-13 08:29:03,085 DEBUG Query with image successful
2024-08-13 08:29:03,504 DEBUG Querying model with image: gpt-4o
2024-08-13 08:42:35,314 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 08:42:35,340 DEBUG OpenAI client created
2024-08-13 08:42:40,507 DEBUG Querying model with image: gpt-4o
2024-08-13 08:42:54,255 DEBUG Query with image successful
2024-08-13 08:42:54,333 DEBUG Querying model with image: gpt-4o
2024-08-13 08:43:02,802 DEBUG Query with image successful
2024-08-13 08:43:03,134 DEBUG Querying model with image: gpt-4o
2024-08-13 08:43:12,466 DEBUG Query with image successful
2024-08-13 08:43:12,549 DEBUG Querying model with image: gpt-4o
2024-08-13 08:43:20,717 DEBUG Query with image successful
2024-08-13 08:43:31,015 DEBUG Querying model with image: gpt-4o
2024-08-13 08:43:45,560 DEBUG Query with image successful
2024-08-13 08:43:45,640 DEBUG Querying model with image: gpt-4o
2024-08-13 08:43:55,787 DEBUG Query with image successful
2024-08-13 08:43:56,078 DEBUG Querying model with image: gpt-4o
2024-08-13 08:44:03,366 DEBUG Query with image successful
2024-08-13 08:44:03,458 DEBUG Querying model with image: gpt-4o
2024-08-13 08:44:12,349 DEBUG Query with image successful
2024-08-13 08:44:12,648 DEBUG Querying model with image: gpt-4o
2024-08-13 08:44:21,966 DEBUG Query with image successful
2024-08-13 08:44:22,048 DEBUG Querying model with image: gpt-4o
2024-08-13 08:44:30,590 DEBUG Query with image successful
2024-08-13 08:44:30,896 DEBUG Querying model with image: gpt-4o
2024-08-13 08:44:38,500 DEBUG Query with image successful
2024-08-13 08:44:38,582 DEBUG Querying model with image: gpt-4o
2024-08-13 08:44:45,633 DEBUG Query with image successful
2024-08-13 08:44:45,928 DEBUG Querying model with image: gpt-4o
2024-08-13 08:44:54,295 DEBUG Query with image successful
2024-08-13 08:44:54,376 DEBUG Querying model with image: gpt-4o
2024-08-13 08:45:03,415 DEBUG Query with image successful
2024-08-13 08:45:03,726 DEBUG Querying model with image: gpt-4o
2024-08-13 08:45:12,250 DEBUG Query with image successful
2024-08-13 08:45:12,333 DEBUG Querying model with image: gpt-4o
2024-08-13 08:45:21,927 DEBUG Query with image successful
2024-08-13 08:45:22,230 DEBUG Querying model with image: gpt-4o
2024-08-13 08:45:30,141 DEBUG Query with image successful
2024-08-13 08:45:30,225 DEBUG Querying model with image: gpt-4o
2024-08-13 08:45:41,055 DEBUG Query with image successful
2024-08-13 08:45:41,348 DEBUG Querying model with image: gpt-4o
2024-08-13 08:45:51,008 DEBUG Query with image successful
2024-08-13 08:45:51,089 DEBUG Querying model with image: gpt-4o
2024-08-13 08:46:52,467 DEBUG Query with image successful
2024-08-13 08:46:52,773 DEBUG Querying model with image: gpt-4o
2024-08-13 08:47:00,031 DEBUG Query with image successful
2024-08-13 08:47:00,113 DEBUG Querying model with image: gpt-4o
2024-08-13 08:47:08,667 DEBUG Query with image successful
2024-08-13 08:47:08,978 DEBUG Querying model with image: gpt-4o
2024-08-13 08:47:16,895 DEBUG Query with image successful
2024-08-13 08:47:16,987 DEBUG Querying model with image: gpt-4o
2024-08-13 08:47:25,709 DEBUG Query with image successful
2024-08-13 08:47:25,997 DEBUG Querying model with image: gpt-4o
2024-08-13 08:47:36,532 DEBUG Query with image successful
2024-08-13 08:47:36,621 DEBUG Querying model with image: gpt-4o
2024-08-13 08:47:44,246 DEBUG Query with image successful
2024-08-13 08:47:44,526 DEBUG Querying model with image: gpt-4o
2024-08-13 08:47:52,293 DEBUG Query with image successful
2024-08-13 08:47:52,392 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:00,884 DEBUG Query with image successful
2024-08-13 08:48:01,169 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:10,128 DEBUG Query with image successful
2024-08-13 08:48:10,213 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:17,035 DEBUG Query with image successful
2024-08-13 08:48:17,318 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:24,950 DEBUG Query with image successful
2024-08-13 08:48:25,031 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:32,234 DEBUG Query with image successful
2024-08-13 08:48:32,524 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:40,866 DEBUG Query with image successful
2024-08-13 08:48:40,947 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:48,326 DEBUG Query with image successful
2024-08-13 08:48:48,618 DEBUG Querying model with image: gpt-4o
2024-08-13 08:48:56,262 DEBUG Query with image successful
2024-08-13 08:48:56,350 DEBUG Querying model with image: gpt-4o
2024-08-13 08:49:03,091 DEBUG Query with image successful
2024-08-13 08:49:03,376 DEBUG Querying model with image: gpt-4o
2024-08-13 08:49:15,607 DEBUG Query with image successful
2024-08-13 08:49:15,690 DEBUG Querying model with image: gpt-4o
2024-08-13 08:49:24,048 DEBUG Query with image successful
2024-08-13 08:49:24,324 DEBUG Querying model with image: gpt-4o
2024-08-13 08:49:32,695 DEBUG Query with image successful
2024-08-13 08:49:32,775 DEBUG Querying model with image: gpt-4o
2024-08-13 08:49:49,454 DEBUG Query with image successful
2024-08-13 08:49:49,723 DEBUG Querying model with image: gpt-4o
2024-08-13 08:49:57,688 DEBUG Query with image successful
2024-08-13 08:49:57,768 DEBUG Querying model with image: gpt-4o
2024-08-13 08:50:05,125 DEBUG Query with image successful
2024-08-13 08:50:05,393 DEBUG Querying model with image: gpt-4o
2024-08-13 08:50:13,685 DEBUG Query with image successful
2024-08-13 08:50:13,766 DEBUG Querying model with image: gpt-4o
2024-08-13 08:50:21,569 DEBUG Query with image successful
2024-08-13 08:50:21,836 DEBUG Querying model with image: gpt-4o
2024-08-13 08:50:31,874 DEBUG Query with image successful
2024-08-13 08:50:31,954 DEBUG Querying model with image: gpt-4o
2024-08-13 08:50:40,080 DEBUG Query with image successful
2024-08-13 08:50:40,353 DEBUG Querying model with image: gpt-4o
2024-08-13 08:50:51,914 DEBUG Query with image successful
2024-08-13 08:50:51,998 DEBUG Querying model with image: gpt-4o
2024-08-13 08:51:00,289 DEBUG Query with image successful
2024-08-13 08:51:00,595 DEBUG Querying model with image: gpt-4o
2024-08-13 08:51:14,520 DEBUG Query with image successful
2024-08-13 08:51:14,599 DEBUG Querying model with image: gpt-4o
2024-08-13 08:51:21,457 DEBUG Query with image successful
2024-08-13 09:23:37,253 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 09:23:37,286 DEBUG OpenAI client created
2024-08-13 09:23:42,452 DEBUG Querying model with image: gpt-4o
2024-08-13 09:23:51,442 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 94, in connect_tcp
    sock = socket.create_connection(
  File "D:\ide\conda\envs\vlm_sc2\lib\socket.py", line 845, in create_connection
    raise err
  File "D:\ide\conda\envs\vlm_sc2\lib\socket.py", line 833, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 261, in handle_request
    connect_response = self._connection.handle_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 86, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 63, in handle_request
    stream = self._connect(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 111, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 93, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 972, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 94, in connect_tcp
    sock = socket.create_connection(
  File "D:\ide\conda\envs\vlm_sc2\lib\socket.py", line 845, in create_connection
    raise err
  File "D:\ide\conda\envs\vlm_sc2\lib\socket.py", line 833, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 261, in handle_request
    connect_response = self._connection.handle_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 86, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 63, in handle_request
    stream = self._connect(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 111, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 93, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 972, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 10, in map_exceptions
    yield
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 94, in connect_tcp
    sock = socket.create_connection(
  File "D:\ide\conda\envs\vlm_sc2\lib\socket.py", line 845, in create_connection
    raise err
  File "D:\ide\conda\envs\vlm_sc2\lib\socket.py", line 833, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 60, in map_httpcore_exceptions
    yield
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 218, in handle_request
    resp = self._pool.handle_request(req)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 253, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 237, in handle_request
    response = connection.handle_request(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 261, in handle_request
    connect_response = self._connection.handle_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 86, in handle_request
    raise exc
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 63, in handle_request
    stream = self._connect(request)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection.py", line 111, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\backends\sync.py", line 93, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc)
httpcore.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 972, in _request
    response = self._client.send(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 908, in send
    response = self._send_handling_auth(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 936, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 973, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_client.py", line 1009, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 217, in handle_request
    with map_httpcore_exceptions():
  File "D:\ide\conda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\meisah111\AppData\Roaming\Python\Python310\site-packages\httpx\_transports\default.py", line 77, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 61, in query_with_image
    response = self.client.chat.completions.create(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 936, in request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 996, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1074, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 996, in _request
    return self._retry_request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1074, in _retry_request
    return self._request(
  File "D:\ide\conda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1006, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-08-13 09:23:51,659 DEBUG Querying model with image: gpt-4o
2024-08-13 09:24:22,206 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 09:24:22,255 DEBUG OpenAI client created
2024-08-13 09:24:27,422 DEBUG Querying model with image: gpt-4o
2024-08-13 09:24:36,502 DEBUG Query with image successful
2024-08-13 09:24:36,577 DEBUG Querying model with image: gpt-4o
2024-08-13 09:24:45,561 DEBUG Query with image successful
2024-08-13 09:24:45,872 DEBUG Querying model with image: gpt-4o
2024-08-13 09:24:53,250 DEBUG Query with image successful
2024-08-13 09:24:53,327 DEBUG Querying model with image: gpt-4o
2024-08-13 09:25:01,779 DEBUG Query with image successful
2024-08-13 09:25:12,046 DEBUG Querying model with image: gpt-4o
2024-08-13 09:25:19,968 DEBUG Query with image successful
2024-08-13 09:25:20,045 DEBUG Querying model with image: gpt-4o
2024-08-13 09:25:26,750 DEBUG Query with image successful
2024-08-13 09:25:27,035 DEBUG Querying model with image: gpt-4o
2024-08-13 09:25:33,717 DEBUG Query with image successful
2024-08-13 09:25:33,796 DEBUG Querying model with image: gpt-4o
2024-08-13 09:30:02,650 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 09:30:02,682 DEBUG OpenAI client created
2024-08-13 09:30:07,846 DEBUG Querying model with image: gpt-4o
2024-08-13 09:30:17,508 DEBUG Query with image successful
2024-08-13 09:30:17,585 DEBUG Querying model with image: gpt-4o
2024-08-13 09:30:27,358 DEBUG Query with image successful
2024-08-13 09:30:27,656 DEBUG Querying model with image: gpt-4o
2024-08-13 09:30:36,581 DEBUG Query with image successful
2024-08-13 09:30:36,665 DEBUG Querying model with image: gpt-4o
2024-08-13 09:30:45,913 DEBUG Query with image successful
2024-08-13 09:30:56,185 DEBUG Querying model with image: gpt-4o
2024-08-13 09:31:06,306 DEBUG Query with image successful
2024-08-13 09:31:06,384 DEBUG Querying model with image: gpt-4o
2024-08-13 09:31:13,372 DEBUG Query with image successful
2024-08-13 09:31:13,642 DEBUG Querying model with image: gpt-4o
2024-08-13 09:31:21,093 DEBUG Query with image successful
2024-08-13 09:31:21,174 DEBUG Querying model with image: gpt-4o
2024-08-13 09:31:28,785 DEBUG Query with image successful
2024-08-13 09:31:29,053 DEBUG Querying model with image: gpt-4o
2024-08-13 09:31:38,090 DEBUG Query with image successful
2024-08-13 09:31:38,168 DEBUG Querying model with image: gpt-4o
2024-08-13 09:31:47,894 DEBUG Query with image successful
2024-08-13 09:31:48,236 DEBUG Querying model with image: gpt-4o
2024-08-13 09:31:56,416 DEBUG Query with image successful
2024-08-13 09:31:56,495 DEBUG Querying model with image: gpt-4o
2024-08-13 09:32:04,070 DEBUG Query with image successful
2024-08-13 09:32:04,339 DEBUG Querying model with image: gpt-4o
2024-08-13 09:32:11,564 DEBUG Query with image successful
2024-08-13 09:32:11,644 DEBUG Querying model with image: gpt-4o
2024-08-13 09:32:20,111 DEBUG Query with image successful
2024-08-13 09:32:20,382 DEBUG Querying model with image: gpt-4o
2024-08-13 09:32:28,409 DEBUG Query with image successful
2024-08-13 09:32:28,495 DEBUG Querying model with image: gpt-4o
2024-08-13 09:32:37,221 DEBUG Query with image successful
2024-08-13 09:32:37,503 DEBUG Querying model with image: gpt-4o
2024-08-13 09:32:45,392 DEBUG Query with image successful
2024-08-13 09:32:45,484 DEBUG Querying model with image: gpt-4o
2024-08-13 09:32:53,793 DEBUG Query with image successful
2024-08-13 09:32:54,062 DEBUG Querying model with image: gpt-4o
2024-08-13 09:33:01,455 DEBUG Query with image successful
2024-08-13 09:33:01,533 DEBUG Querying model with image: gpt-4o
2024-08-13 09:33:10,734 DEBUG Query with image successful
2024-08-13 09:33:11,002 DEBUG Querying model with image: gpt-4o
2024-08-13 09:33:18,033 DEBUG Query with image successful
2024-08-13 09:33:18,113 DEBUG Querying model with image: gpt-4o
2024-08-13 09:33:27,422 DEBUG Query with image successful
2024-08-13 09:33:27,687 DEBUG Querying model with image: gpt-4o
2024-08-13 09:33:38,562 DEBUG Query with image successful
2024-08-13 09:33:38,643 DEBUG Querying model with image: gpt-4o
2024-08-13 09:33:45,388 DEBUG Query with image successful
2024-08-13 09:33:45,656 DEBUG Querying model with image: gpt-4o
2024-08-13 09:33:55,773 DEBUG Query with image successful
2024-08-13 09:33:55,853 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:03,034 DEBUG Query with image successful
2024-08-13 09:34:03,298 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:10,463 DEBUG Query with image successful
2024-08-13 09:34:10,541 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:17,898 DEBUG Query with image successful
2024-08-13 09:34:18,165 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:25,744 DEBUG Query with image successful
2024-08-13 09:34:25,822 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:34,537 DEBUG Query with image successful
2024-08-13 09:34:34,875 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:43,312 DEBUG Query with image successful
2024-08-13 09:34:43,391 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:50,635 DEBUG Query with image successful
2024-08-13 09:34:50,909 DEBUG Querying model with image: gpt-4o
2024-08-13 09:34:58,691 DEBUG Query with image successful
2024-08-13 09:34:58,782 DEBUG Querying model with image: gpt-4o
2024-08-13 09:35:05,551 DEBUG Query with image successful
2024-08-13 09:35:05,813 DEBUG Querying model with image: gpt-4o
2024-08-13 09:35:15,808 DEBUG Query with image successful
2024-08-13 09:35:15,887 DEBUG Querying model with image: gpt-4o
2024-08-13 09:35:22,854 DEBUG Query with image successful
2024-08-13 09:35:23,116 DEBUG Querying model with image: gpt-4o
2024-08-13 09:35:31,827 DEBUG Query with image successful
2024-08-13 09:35:31,915 DEBUG Querying model with image: gpt-4o
2024-08-13 09:35:41,535 DEBUG Query with image successful
2024-08-13 09:35:41,819 DEBUG Querying model with image: gpt-4o
2024-08-13 09:35:49,231 DEBUG Query with image successful
2024-08-13 09:35:49,309 DEBUG Querying model with image: gpt-4o
2024-08-13 09:35:57,558 DEBUG Query with image successful
2024-08-13 09:35:57,831 DEBUG Querying model with image: gpt-4o
2024-08-13 09:36:04,891 DEBUG Query with image successful
2024-08-13 09:36:04,968 DEBUG Querying model with image: gpt-4o
2024-08-13 09:36:12,636 DEBUG Query with image successful
2024-08-13 09:36:12,922 DEBUG Querying model with image: gpt-4o
2024-08-13 09:36:24,019 DEBUG Query with image successful
2024-08-13 09:36:24,098 DEBUG Querying model with image: gpt-4o
2024-08-13 09:36:34,883 DEBUG Query with image successful
2024-08-13 09:36:35,159 DEBUG Querying model with image: gpt-4o
2024-08-13 09:36:42,788 DEBUG Query with image successful
2024-08-13 09:36:42,865 DEBUG Querying model with image: gpt-4o
2024-08-13 09:36:49,202 DEBUG Query with image successful
2024-08-13 09:36:49,468 DEBUG Querying model with image: gpt-4o
2024-08-13 09:36:56,475 DEBUG Query with image successful
2024-08-13 09:36:56,553 DEBUG Querying model with image: gpt-4o
2024-08-13 09:37:02,872 DEBUG Query with image successful
2024-08-13 09:37:03,132 DEBUG Querying model with image: gpt-4o
2024-08-13 09:37:12,863 DEBUG Query with image successful
2024-08-13 09:37:12,941 DEBUG Querying model with image: gpt-4o
2024-08-13 09:37:20,841 DEBUG Query with image successful
2024-08-13 11:17:50,987 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 11:17:51,016 DEBUG OpenAI client created
2024-08-13 11:17:56,192 DEBUG Querying model with image: gpt-4o
2024-08-13 11:18:05,435 DEBUG Query with image successful
2024-08-13 11:18:05,512 DEBUG Querying model with image: gpt-4o
2024-08-13 11:18:18,377 DEBUG Query with image successful
2024-08-13 11:18:18,634 DEBUG Querying model with image: gpt-4o
2024-08-13 11:18:27,813 DEBUG Query with image successful
2024-08-13 11:18:27,910 DEBUG Querying model with image: gpt-4o
2024-08-13 11:18:35,682 DEBUG Query with image successful
2024-08-13 11:18:46,000 DEBUG Querying model with image: gpt-4o
2024-08-13 11:18:55,052 DEBUG Query with image successful
2024-08-13 11:18:55,141 DEBUG Querying model with image: gpt-4o
2024-08-13 11:19:03,432 DEBUG Query with image successful
2024-08-13 11:19:03,703 DEBUG Querying model with image: gpt-4o
2024-08-13 11:19:11,162 DEBUG Query with image successful
2024-08-13 11:19:11,240 DEBUG Querying model with image: gpt-4o
2024-08-13 11:19:18,191 DEBUG Query with image successful
2024-08-13 11:19:18,461 DEBUG Querying model with image: gpt-4o
2024-08-13 11:19:27,003 DEBUG Query with image successful
2024-08-13 11:19:27,081 DEBUG Querying model with image: gpt-4o
2024-08-13 11:19:38,453 DEBUG Query with image successful
2024-08-13 11:19:38,720 DEBUG Querying model with image: gpt-4o
2024-08-13 11:19:46,498 DEBUG Query with image successful
2024-08-13 11:19:46,578 DEBUG Querying model with image: gpt-4o
2024-08-13 11:19:56,123 DEBUG Query with image successful
2024-08-13 11:19:56,468 DEBUG Querying model with image: gpt-4o
2024-08-13 11:20:06,564 DEBUG Query with image successful
2024-08-13 11:20:06,642 DEBUG Querying model with image: gpt-4o
2024-08-13 11:20:16,050 DEBUG Query with image successful
2024-08-13 11:20:16,336 DEBUG Querying model with image: gpt-4o
2024-08-13 11:20:25,493 DEBUG Query with image successful
2024-08-13 11:20:25,573 DEBUG Querying model with image: gpt-4o
2024-08-13 11:20:33,206 DEBUG Query with image successful
2024-08-13 11:20:33,471 DEBUG Querying model with image: gpt-4o
2024-08-13 11:20:40,801 DEBUG Query with image successful
2024-08-13 11:20:40,910 DEBUG Querying model with image: gpt-4o
2024-08-13 11:20:55,150 DEBUG Query with image successful
2024-08-13 11:20:55,461 DEBUG Querying model with image: gpt-4o
2024-08-13 11:21:05,581 DEBUG Query with image successful
2024-08-13 11:21:05,665 DEBUG Querying model with image: gpt-4o
2024-08-13 11:23:28,443 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 11:23:28,471 DEBUG OpenAI client created
2024-08-13 11:23:33,640 DEBUG Querying model with image: gpt-4o
2024-08-13 11:23:41,661 DEBUG Query with image successful
2024-08-13 11:23:41,745 DEBUG Querying model with image: gpt-4o
2024-08-13 11:23:50,673 DEBUG Query with image successful
2024-08-13 11:23:50,958 DEBUG Querying model with image: gpt-4o
2024-08-13 11:23:59,743 DEBUG Query with image successful
2024-08-13 11:23:59,841 DEBUG Querying model with image: gpt-4o
2024-08-13 11:24:16,496 DEBUG Query with image successful
2024-08-13 11:24:26,765 DEBUG Querying model with image: gpt-4o
2024-08-13 11:24:35,164 DEBUG Query with image successful
2024-08-13 11:24:35,249 DEBUG Querying model with image: gpt-4o
2024-08-13 11:24:41,930 DEBUG Query with image successful
2024-08-13 11:24:42,202 DEBUG Querying model with image: gpt-4o
2024-08-13 11:24:50,777 DEBUG Query with image successful
2024-08-13 11:24:50,862 DEBUG Querying model with image: gpt-4o
2024-08-13 11:24:58,863 DEBUG Query with image successful
2024-08-13 11:24:59,136 DEBUG Querying model with image: gpt-4o
2024-08-13 11:25:06,810 DEBUG Query with image successful
2024-08-13 11:25:06,901 DEBUG Querying model with image: gpt-4o
2024-08-13 11:25:14,303 DEBUG Query with image successful
2024-08-13 11:25:14,666 DEBUG Querying model with image: gpt-4o
2024-08-13 11:25:24,279 DEBUG Query with image successful
2024-08-13 11:25:24,362 DEBUG Querying model with image: gpt-4o
2024-08-13 11:25:32,327 DEBUG Query with image successful
2024-08-13 11:25:32,606 DEBUG Querying model with image: gpt-4o
2024-08-13 11:25:41,305 DEBUG Query with image successful
2024-08-13 11:25:41,391 DEBUG Querying model with image: gpt-4o
2024-08-13 11:25:50,270 DEBUG Query with image successful
2024-08-13 11:25:50,580 DEBUG Querying model with image: gpt-4o
2024-08-13 11:26:00,294 DEBUG Query with image successful
2024-08-13 11:26:00,382 DEBUG Querying model with image: gpt-4o
2024-08-13 11:26:09,668 DEBUG Query with image successful
2024-08-13 11:26:09,938 DEBUG Querying model with image: gpt-4o
2024-08-13 11:26:17,300 DEBUG Query with image successful
2024-08-13 11:26:17,395 DEBUG Querying model with image: gpt-4o
2024-08-13 11:26:27,577 DEBUG Query with image successful
2024-08-13 11:26:27,850 DEBUG Querying model with image: gpt-4o
2024-08-13 11:26:34,971 DEBUG Query with image successful
2024-08-13 11:26:35,055 DEBUG Querying model with image: gpt-4o
2024-08-13 11:26:44,639 DEBUG Query with image successful
2024-08-13 11:26:44,931 DEBUG Querying model with image: gpt-4o
2024-08-13 11:26:52,369 DEBUG Query with image successful
2024-08-13 11:26:52,454 DEBUG Querying model with image: gpt-4o
2024-08-13 11:27:00,419 DEBUG Query with image successful
2024-08-13 11:27:00,692 DEBUG Querying model with image: gpt-4o
2024-08-13 11:27:09,089 DEBUG Query with image successful
2024-08-13 11:27:09,174 DEBUG Querying model with image: gpt-4o
2024-08-13 11:27:18,032 DEBUG Query with image successful
2024-08-13 11:27:18,308 DEBUG Querying model with image: gpt-4o
2024-08-13 11:27:27,152 DEBUG Query with image successful
2024-08-13 11:27:27,236 DEBUG Querying model with image: gpt-4o
2024-08-13 11:27:34,665 DEBUG Query with image successful
2024-08-13 11:27:34,936 DEBUG Querying model with image: gpt-4o
2024-08-13 11:27:42,457 DEBUG Query with image successful
2024-08-13 11:27:42,551 DEBUG Querying model with image: gpt-4o
2024-08-13 11:27:50,231 DEBUG Query with image successful
2024-08-13 11:27:50,531 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:00,610 DEBUG Query with image successful
2024-08-13 11:28:00,697 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:07,072 DEBUG Query with image successful
2024-08-13 11:28:07,354 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:13,933 DEBUG Query with image successful
2024-08-13 11:28:14,023 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:21,257 DEBUG Query with image successful
2024-08-13 11:28:21,544 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:29,099 DEBUG Query with image successful
2024-08-13 11:28:29,182 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:36,371 DEBUG Query with image successful
2024-08-13 11:28:36,653 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:46,539 DEBUG Query with image successful
2024-08-13 11:28:46,623 DEBUG Querying model with image: gpt-4o
2024-08-13 11:28:54,776 DEBUG Query with image successful
2024-08-13 11:28:55,047 DEBUG Querying model with image: gpt-4o
2024-08-13 11:29:05,142 DEBUG Query with image successful
2024-08-13 11:29:05,238 DEBUG Querying model with image: gpt-4o
2024-08-13 11:29:11,486 DEBUG Query with image successful
2024-08-13 11:29:11,767 DEBUG Querying model with image: gpt-4o
2024-08-13 11:29:20,231 DEBUG Query with image successful
2024-08-13 11:29:20,315 DEBUG Querying model with image: gpt-4o
2024-08-13 11:29:29,040 DEBUG Query with image successful
2024-08-13 11:29:29,327 DEBUG Querying model with image: gpt-4o
2024-08-13 11:29:36,498 DEBUG Query with image successful
2024-08-13 11:29:36,581 DEBUG Querying model with image: gpt-4o
2024-08-13 11:29:44,919 DEBUG Query with image successful
2024-08-13 11:29:45,187 DEBUG Querying model with image: gpt-4o
2024-08-13 11:29:54,712 DEBUG Query with image successful
2024-08-13 11:29:54,796 DEBUG Querying model with image: gpt-4o
2024-08-13 11:30:01,891 DEBUG Query with image successful
2024-08-13 11:30:02,158 DEBUG Querying model with image: gpt-4o
2024-08-13 11:30:09,608 DEBUG Query with image successful
2024-08-13 11:30:09,692 DEBUG Querying model with image: gpt-4o
2024-08-13 11:30:16,679 DEBUG Query with image successful
2024-08-13 11:30:16,944 DEBUG Querying model with image: gpt-4o
2024-08-13 11:30:27,663 DEBUG Query with image successful
2024-08-13 11:30:27,749 DEBUG Querying model with image: gpt-4o
2024-08-13 11:30:34,123 DEBUG Query with image successful
2024-08-13 11:30:34,417 DEBUG Querying model with image: gpt-4o
2024-08-13 11:30:44,066 DEBUG Query with image successful
2024-08-13 11:30:44,148 DEBUG Querying model with image: gpt-4o
2024-08-13 11:30:50,009 DEBUG Query with image successful
2024-08-13 12:52:24,894 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 12:52:24,920 DEBUG OpenAI client created
2024-08-13 12:52:30,078 DEBUG Querying model with image: gpt-4o
2024-08-13 12:52:39,686 DEBUG Query with image successful
2024-08-13 12:52:39,775 DEBUG Querying model with image: gpt-4o
2024-08-13 12:52:48,916 DEBUG Query with image successful
2024-08-13 12:52:49,187 DEBUG Querying model with image: gpt-4o
2024-08-13 12:52:58,630 DEBUG Query with image successful
2024-08-13 12:52:58,726 DEBUG Querying model with image: gpt-4o
2024-08-13 12:53:08,374 DEBUG Query with image successful
2024-08-13 12:53:18,697 DEBUG Querying model with image: gpt-4o
2024-08-13 12:53:26,825 DEBUG Query with image successful
2024-08-13 12:53:26,919 DEBUG Querying model with image: gpt-4o
2024-08-13 12:53:35,473 DEBUG Query with image successful
2024-08-13 12:53:35,737 DEBUG Querying model with image: gpt-4o
2024-08-13 12:53:48,204 DEBUG Query with image successful
2024-08-13 12:53:48,288 DEBUG Querying model with image: gpt-4o
2024-08-13 12:53:56,594 DEBUG Query with image successful
2024-08-13 12:53:56,867 DEBUG Querying model with image: gpt-4o
2024-08-13 12:54:29,245 DEBUG Query with image successful
2024-08-13 12:54:29,330 DEBUG Querying model with image: gpt-4o
2024-08-13 12:54:38,128 DEBUG Query with image successful
2024-08-13 12:54:38,400 DEBUG Querying model with image: gpt-4o
2024-08-13 12:54:47,430 DEBUG Query with image successful
2024-08-13 12:54:47,519 DEBUG Querying model with image: gpt-4o
2024-08-13 12:55:13,166 DEBUG Query with image successful
2024-08-13 12:55:13,437 DEBUG Querying model with image: gpt-4o
2024-08-13 12:55:21,325 DEBUG Query with image successful
2024-08-13 12:55:21,419 DEBUG Querying model with image: gpt-4o
2024-08-13 12:55:32,723 DEBUG Query with image successful
2024-08-13 12:55:32,997 DEBUG Querying model with image: gpt-4o
2024-08-13 12:55:41,696 DEBUG Query with image successful
2024-08-13 12:55:41,780 DEBUG Querying model with image: gpt-4o
2024-08-13 12:55:52,550 DEBUG Query with image successful
2024-08-13 12:55:52,854 DEBUG Querying model with image: gpt-4o
2024-08-13 12:56:01,421 DEBUG Query with image successful
2024-08-13 12:56:01,509 DEBUG Querying model with image: gpt-4o
2024-08-13 12:56:10,389 DEBUG Query with image successful
2024-08-13 12:56:10,658 DEBUG Querying model with image: gpt-4o
2024-08-13 12:56:56,273 DEBUG Query with image successful
2024-08-13 12:56:56,356 DEBUG Querying model with image: gpt-4o
2024-08-13 12:57:05,453 DEBUG Query with image successful
2024-08-13 12:57:05,774 DEBUG Querying model with image: gpt-4o
2024-08-13 12:57:13,984 DEBUG Query with image successful
2024-08-13 12:57:14,079 DEBUG Querying model with image: gpt-4o
2024-08-13 12:57:23,873 DEBUG Query with image successful
2024-08-13 12:57:24,155 DEBUG Querying model with image: gpt-4o
2024-08-13 12:57:33,153 DEBUG Query with image successful
2024-08-13 12:57:33,237 DEBUG Querying model with image: gpt-4o
2024-08-13 12:57:44,671 DEBUG Query with image successful
2024-08-13 12:57:44,950 DEBUG Querying model with image: gpt-4o
2024-08-13 12:57:52,962 DEBUG Query with image successful
2024-08-13 12:57:53,045 DEBUG Querying model with image: gpt-4o
2024-08-13 12:58:00,358 DEBUG Query with image successful
2024-08-13 12:58:00,628 DEBUG Querying model with image: gpt-4o
2024-08-13 12:58:08,714 DEBUG Query with image successful
2024-08-13 12:58:08,798 DEBUG Querying model with image: gpt-4o
2024-08-13 12:58:19,791 DEBUG Query with image successful
2024-08-13 12:58:20,070 DEBUG Querying model with image: gpt-4o
2024-08-13 12:58:27,894 DEBUG Query with image successful
2024-08-13 12:58:27,994 DEBUG Querying model with image: gpt-4o
2024-08-13 12:58:34,844 DEBUG Query with image successful
2024-08-13 12:58:35,177 DEBUG Querying model with image: gpt-4o
2024-08-13 12:58:42,617 DEBUG Query with image successful
2024-08-13 12:58:42,711 DEBUG Querying model with image: gpt-4o
2024-08-13 12:58:51,766 DEBUG Query with image successful
2024-08-13 12:58:52,044 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:00,045 DEBUG Query with image successful
2024-08-13 12:59:00,140 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:07,184 DEBUG Query with image successful
2024-08-13 12:59:07,470 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:16,099 DEBUG Query with image successful
2024-08-13 12:59:16,188 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:24,297 DEBUG Query with image successful
2024-08-13 12:59:24,569 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:33,942 DEBUG Query with image successful
2024-08-13 12:59:34,040 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:39,886 DEBUG Query with image successful
2024-08-13 12:59:40,153 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:49,400 DEBUG Query with image successful
2024-08-13 12:59:49,483 DEBUG Querying model with image: gpt-4o
2024-08-13 12:59:59,259 DEBUG Query with image successful
2024-08-13 12:59:59,575 DEBUG Querying model with image: gpt-4o
2024-08-13 13:00:06,306 DEBUG Query with image successful
2024-08-13 13:00:06,387 DEBUG Querying model with image: gpt-4o
2024-08-13 13:00:44,058 DEBUG Query with image successful
2024-08-13 13:00:44,323 DEBUG Querying model with image: gpt-4o
2024-08-13 13:00:52,920 DEBUG Query with image successful
2024-08-13 13:00:53,006 DEBUG Querying model with image: gpt-4o
2024-08-13 13:01:00,377 DEBUG Query with image successful
2024-08-13 13:01:00,640 DEBUG Querying model with image: gpt-4o
2024-08-13 13:01:06,751 DEBUG Query with image successful
2024-08-13 13:01:06,834 DEBUG Querying model with image: gpt-4o
2024-08-13 13:01:14,200 DEBUG Query with image successful
2024-08-13 13:01:14,511 DEBUG Querying model with image: gpt-4o
2024-08-13 13:01:22,329 DEBUG Query with image successful
2024-08-13 13:01:22,412 DEBUG Querying model with image: gpt-4o
2024-08-13 13:01:29,155 DEBUG Query with image successful
2024-08-13 13:01:29,421 DEBUG Querying model with image: gpt-4o
2024-08-13 13:01:36,822 DEBUG Query with image successful
2024-08-13 13:01:36,909 DEBUG Querying model with image: gpt-4o
2024-08-13 13:01:45,135 DEBUG Query with image successful
2024-08-13 15:13:13,454 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 15:13:13,489 DEBUG OpenAI client created
2024-08-13 15:13:18,660 DEBUG Querying model with image: gpt-4o
2024-08-13 15:13:35,428 DEBUG Query with image successful
2024-08-13 15:13:35,510 DEBUG Querying model with image: gpt-4o
2024-08-13 15:13:49,766 DEBUG Query with image successful
2024-08-13 15:13:50,072 DEBUG Querying model with image: gpt-4o
2024-08-13 15:14:00,392 DEBUG Query with image successful
2024-08-13 15:14:00,487 DEBUG Querying model with image: gpt-4o
2024-08-13 15:14:15,489 DEBUG Query with image successful
2024-08-13 15:14:25,763 DEBUG Querying model with image: gpt-4o
2024-08-13 15:14:36,734 DEBUG Query with image successful
2024-08-13 15:14:36,829 DEBUG Querying model with image: gpt-4o
2024-08-13 15:14:45,127 DEBUG Query with image successful
2024-08-13 15:14:45,413 DEBUG Querying model with image: gpt-4o
2024-08-13 15:14:54,443 DEBUG Query with image successful
2024-08-13 15:14:54,530 DEBUG Querying model with image: gpt-4o
2024-08-13 15:15:05,201 DEBUG Query with image successful
2024-08-13 15:15:05,469 DEBUG Querying model with image: gpt-4o
2024-08-13 15:15:13,707 DEBUG Query with image successful
2024-08-13 15:15:13,791 DEBUG Querying model with image: gpt-4o
2024-08-13 15:15:24,968 DEBUG Query with image successful
2024-08-13 15:15:25,240 DEBUG Querying model with image: gpt-4o
2024-08-13 15:15:35,457 DEBUG Query with image successful
2024-08-13 15:15:35,542 DEBUG Querying model with image: gpt-4o
2024-08-13 15:15:46,804 DEBUG Query with image successful
2024-08-13 15:15:47,151 DEBUG Querying model with image: gpt-4o
2024-08-13 15:16:00,019 DEBUG Query with image successful
2024-08-13 15:16:00,104 DEBUG Querying model with image: gpt-4o
2024-08-13 15:16:13,357 DEBUG Query with image successful
2024-08-13 15:16:13,653 DEBUG Querying model with image: gpt-4o
2024-08-13 15:16:24,714 DEBUG Query with image successful
2024-08-13 15:16:24,799 DEBUG Querying model with image: gpt-4o
2024-08-13 15:16:42,797 DEBUG Query with image successful
2024-08-13 15:16:43,069 DEBUG Querying model with image: gpt-4o
2024-08-13 15:16:51,571 DEBUG Query with image successful
2024-08-13 15:16:51,666 DEBUG Querying model with image: gpt-4o
2024-08-13 15:17:03,962 DEBUG Query with image successful
2024-08-13 15:17:04,253 DEBUG Querying model with image: gpt-4o
2024-08-13 15:17:16,217 DEBUG Query with image successful
2024-08-13 15:17:16,301 DEBUG Querying model with image: gpt-4o
2024-08-13 15:17:28,369 DEBUG Query with image successful
2024-08-13 15:17:28,657 DEBUG Querying model with image: gpt-4o
2024-08-13 15:17:37,931 DEBUG Query with image successful
2024-08-13 15:17:38,018 DEBUG Querying model with image: gpt-4o
2024-08-13 15:17:48,356 DEBUG Query with image successful
2024-08-13 15:17:48,626 DEBUG Querying model with image: gpt-4o
2024-08-13 15:17:59,561 DEBUG Query with image successful
2024-08-13 15:17:59,644 DEBUG Querying model with image: gpt-4o
2024-08-13 15:18:12,161 DEBUG Query with image successful
2024-08-13 15:18:12,443 DEBUG Querying model with image: gpt-4o
2024-08-13 15:18:24,739 DEBUG Query with image successful
2024-08-13 15:18:24,833 DEBUG Querying model with image: gpt-4o
2024-08-13 15:18:33,676 DEBUG Query with image successful
2024-08-13 15:18:33,974 DEBUG Querying model with image: gpt-4o
2024-08-13 15:18:45,353 DEBUG Query with image successful
2024-08-13 15:18:45,436 DEBUG Querying model with image: gpt-4o
2024-08-13 15:18:56,041 DEBUG Query with image successful
2024-08-13 15:18:56,325 DEBUG Querying model with image: gpt-4o
2024-08-13 15:19:10,102 DEBUG Query with image successful
2024-08-13 15:19:10,209 DEBUG Querying model with image: gpt-4o
2024-08-13 15:19:19,462 DEBUG Query with image successful
2024-08-13 15:19:19,730 DEBUG Querying model with image: gpt-4o
2024-08-13 15:19:31,747 DEBUG Query with image successful
2024-08-13 15:19:31,831 DEBUG Querying model with image: gpt-4o
2024-08-13 15:19:41,760 DEBUG Query with image successful
2024-08-13 15:19:42,044 DEBUG Querying model with image: gpt-4o
2024-08-13 15:19:52,942 DEBUG Query with image successful
2024-08-13 15:19:53,039 DEBUG Querying model with image: gpt-4o
2024-08-13 15:20:02,348 DEBUG Query with image successful
2024-08-13 15:20:02,629 DEBUG Querying model with image: gpt-4o
2024-08-13 15:20:12,263 DEBUG Query with image successful
2024-08-13 15:20:12,348 DEBUG Querying model with image: gpt-4o
2024-08-13 15:20:21,557 DEBUG Query with image successful
2024-08-13 15:20:21,844 DEBUG Querying model with image: gpt-4o
2024-08-13 15:20:32,860 DEBUG Query with image successful
2024-08-13 15:20:32,945 DEBUG Querying model with image: gpt-4o
2024-08-13 15:20:43,801 DEBUG Query with image successful
2024-08-13 15:20:44,066 DEBUG Querying model with image: gpt-4o
2024-08-13 15:20:54,043 DEBUG Query with image successful
2024-08-13 15:20:54,127 DEBUG Querying model with image: gpt-4o
2024-08-13 15:21:04,169 DEBUG Query with image successful
2024-08-13 15:21:04,449 DEBUG Querying model with image: gpt-4o
2024-08-13 15:21:15,239 DEBUG Query with image successful
2024-08-13 15:21:15,323 DEBUG Querying model with image: gpt-4o
2024-08-13 15:21:26,965 DEBUG Query with image successful
2024-08-13 15:21:27,235 DEBUG Querying model with image: gpt-4o
2024-08-13 15:21:36,455 DEBUG Query with image successful
2024-08-13 15:21:36,540 DEBUG Querying model with image: gpt-4o
2024-08-13 15:21:44,507 DEBUG Query with image successful
2024-08-13 15:21:44,819 DEBUG Querying model with image: gpt-4o
2024-08-13 15:21:57,938 DEBUG Query with image successful
2024-08-13 15:21:58,026 DEBUG Querying model with image: gpt-4o
2024-08-13 15:22:07,107 DEBUG Query with image successful
2024-08-13 15:22:07,384 DEBUG Querying model with image: gpt-4o
2024-08-13 15:22:21,665 DEBUG Query with image successful
2024-08-13 15:22:21,752 DEBUG Querying model with image: gpt-4o
2024-08-13 15:22:34,766 DEBUG Query with image successful
2024-08-13 15:22:35,161 DEBUG Querying model with image: gpt-4o
2024-08-13 15:22:47,436 DEBUG Query with image successful
2024-08-13 15:22:47,531 DEBUG Querying model with image: gpt-4o
2024-08-13 15:22:56,921 DEBUG Query with image successful
2024-08-13 15:29:38,412 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 15:29:38,443 DEBUG OpenAI client created
2024-08-13 15:29:43,480 DEBUG Querying model with image: gpt-4o
2024-08-13 15:29:47,825 DEBUG Query with image successful
2024-08-13 15:29:47,843 DEBUG Querying model with image: gpt-4o
2024-08-13 15:29:54,384 DEBUG Query with image successful
2024-08-13 15:29:54,708 DEBUG Querying model with image: gpt-4o
2024-08-13 15:30:02,581 DEBUG Query with image successful
2024-08-13 15:30:02,677 DEBUG Querying model with image: gpt-4o
2024-08-13 15:30:14,635 DEBUG Query with image successful
2024-08-13 15:30:24,920 DEBUG Querying model with image: gpt-4o
2024-08-13 15:30:35,396 DEBUG Query with image successful
2024-08-13 15:30:35,489 DEBUG Querying model with image: gpt-4o
2024-08-13 15:30:46,489 DEBUG Query with image successful
2024-08-13 15:30:46,774 DEBUG Querying model with image: gpt-4o
2024-08-13 15:30:57,564 DEBUG Query with image successful
2024-08-13 15:30:57,648 DEBUG Querying model with image: gpt-4o
2024-08-13 15:31:07,611 DEBUG Query with image successful
2024-08-13 15:31:07,879 DEBUG Querying model with image: gpt-4o
2024-08-13 15:31:19,260 DEBUG Query with image successful
2024-08-13 15:31:19,346 DEBUG Querying model with image: gpt-4o
2024-08-13 15:31:29,576 DEBUG Query with image successful
2024-08-13 15:31:29,857 DEBUG Querying model with image: gpt-4o
2024-08-13 15:31:42,494 DEBUG Query with image successful
2024-08-13 15:31:42,579 DEBUG Querying model with image: gpt-4o
2024-08-13 15:31:50,785 DEBUG Query with image successful
2024-08-13 15:31:51,150 DEBUG Querying model with image: gpt-4o
2024-08-13 15:32:01,253 DEBUG Query with image successful
2024-08-13 15:32:01,354 DEBUG Querying model with image: gpt-4o
2024-08-13 15:32:13,264 DEBUG Query with image successful
2024-08-13 15:32:13,581 DEBUG Querying model with image: gpt-4o
2024-08-13 15:32:27,021 DEBUG Query with image successful
2024-08-13 15:32:27,105 DEBUG Querying model with image: gpt-4o
2024-08-13 15:32:39,876 DEBUG Query with image successful
2024-08-13 15:32:40,153 DEBUG Querying model with image: gpt-4o
2024-08-13 15:32:51,184 DEBUG Query with image successful
2024-08-13 15:32:51,268 DEBUG Querying model with image: gpt-4o
2024-08-13 15:33:03,358 DEBUG Query with image successful
2024-08-13 15:33:03,628 DEBUG Querying model with image: gpt-4o
2024-08-13 15:33:20,528 DEBUG Query with image successful
2024-08-13 15:33:20,613 DEBUG Querying model with image: gpt-4o
2024-08-13 15:33:35,578 DEBUG Query with image successful
2024-08-13 15:33:35,884 DEBUG Querying model with image: gpt-4o
2024-08-13 15:33:45,379 DEBUG Query with image successful
2024-08-13 15:33:45,461 DEBUG Querying model with image: gpt-4o
2024-08-13 15:33:55,889 DEBUG Query with image successful
2024-08-13 15:33:56,188 DEBUG Querying model with image: gpt-4o
2024-08-13 15:34:08,033 DEBUG Query with image successful
2024-08-13 15:34:08,117 DEBUG Querying model with image: gpt-4o
2024-08-13 15:34:23,353 DEBUG Query with image successful
2024-08-13 15:34:23,625 DEBUG Querying model with image: gpt-4o
2024-08-13 15:34:36,037 DEBUG Query with image successful
2024-08-13 15:34:36,121 DEBUG Querying model with image: gpt-4o
2024-08-13 15:34:46,758 DEBUG Query with image successful
2024-08-13 15:34:47,028 DEBUG Querying model with image: gpt-4o
2024-08-13 15:35:02,017 DEBUG Query with image successful
2024-08-13 15:35:02,103 DEBUG Querying model with image: gpt-4o
2024-08-13 15:35:17,153 DEBUG Query with image successful
2024-08-13 15:35:17,436 DEBUG Querying model with image: gpt-4o
2024-08-13 15:35:35,682 DEBUG Query with image successful
2024-08-13 15:35:35,766 DEBUG Querying model with image: gpt-4o
2024-08-13 15:35:45,807 DEBUG Query with image successful
2024-08-13 15:35:46,123 DEBUG Querying model with image: gpt-4o
2024-08-13 15:35:58,335 DEBUG Query with image successful
2024-08-13 15:35:58,424 DEBUG Querying model with image: gpt-4o
2024-08-13 15:36:10,601 DEBUG Query with image successful
2024-08-13 15:36:10,881 DEBUG Querying model with image: gpt-4o
2024-08-13 15:36:22,570 DEBUG Query with image successful
2024-08-13 15:36:22,671 DEBUG Querying model with image: gpt-4o
2024-08-13 15:36:37,020 DEBUG Query with image successful
2024-08-13 15:36:37,301 DEBUG Querying model with image: gpt-4o
2024-08-13 15:36:47,257 DEBUG Query with image successful
2024-08-13 15:36:47,341 DEBUG Querying model with image: gpt-4o
2024-08-13 15:36:57,874 DEBUG Query with image successful
2024-08-13 15:36:58,170 DEBUG Querying model with image: gpt-4o
2024-08-13 15:37:09,160 DEBUG Query with image successful
2024-08-13 15:37:09,258 DEBUG Querying model with image: gpt-4o
2024-08-13 15:37:21,453 DEBUG Query with image successful
2024-08-13 15:37:21,735 DEBUG Querying model with image: gpt-4o
2024-08-13 15:37:34,926 DEBUG Query with image successful
2024-08-13 15:37:35,009 DEBUG Querying model with image: gpt-4o
2024-08-13 15:37:44,004 DEBUG Query with image successful
2024-08-13 15:37:44,271 DEBUG Querying model with image: gpt-4o
2024-08-13 15:37:56,498 DEBUG Query with image successful
2024-08-13 15:37:56,582 DEBUG Querying model with image: gpt-4o
2024-08-13 15:38:07,861 DEBUG Query with image successful
2024-08-13 15:38:08,149 DEBUG Querying model with image: gpt-4o
2024-08-13 15:38:21,425 DEBUG Query with image successful
2024-08-13 15:38:21,509 DEBUG Querying model with image: gpt-4o
2024-08-13 15:38:30,173 DEBUG Query with image successful
2024-08-13 15:38:30,446 DEBUG Querying model with image: gpt-4o
2024-08-13 15:38:38,907 DEBUG Query with image successful
2024-08-13 15:38:38,990 DEBUG Querying model with image: gpt-4o
2024-08-13 15:38:46,758 DEBUG Query with image successful
2024-08-13 15:38:47,027 DEBUG Querying model with image: gpt-4o
2024-08-13 15:39:00,431 DEBUG Query with image successful
2024-08-13 15:39:00,514 DEBUG Querying model with image: gpt-4o
2024-08-13 15:39:12,676 DEBUG Query with image successful
2024-08-13 15:39:12,945 DEBUG Querying model with image: gpt-4o
2024-08-13 15:39:24,000 DEBUG Query with image successful
2024-08-13 15:39:24,084 DEBUG Querying model with image: gpt-4o
2024-08-13 15:39:37,120 DEBUG Query with image successful
2024-08-13 16:22:38,223 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 16:22:38,251 DEBUG OpenAI client created
2024-08-13 16:22:43,427 DEBUG Querying model with image: gpt-4o
2024-08-13 16:22:55,896 DEBUG Query with image successful
2024-08-13 16:22:55,986 DEBUG Querying model with image: gpt-4o
2024-08-13 16:23:05,890 DEBUG Query with image successful
2024-08-13 16:23:06,174 DEBUG Querying model with image: gpt-4o
2024-08-13 16:23:18,441 DEBUG Query with image successful
2024-08-13 16:23:18,535 DEBUG Querying model with image: gpt-4o
2024-08-13 16:23:46,953 DEBUG Query with image successful
2024-08-13 16:23:57,236 DEBUG Querying model with image: gpt-4o
2024-08-13 16:24:08,588 DEBUG Query with image successful
2024-08-13 16:24:08,674 DEBUG Querying model with image: gpt-4o
2024-08-13 16:24:19,112 DEBUG Query with image successful
2024-08-13 16:24:19,391 DEBUG Querying model with image: gpt-4o
2024-08-13 16:24:29,181 DEBUG Query with image successful
2024-08-13 16:24:29,281 DEBUG Querying model with image: gpt-4o
2024-08-13 16:24:40,717 DEBUG Query with image successful
2024-08-13 16:24:41,013 DEBUG Querying model with image: gpt-4o
2024-08-13 16:24:50,856 DEBUG Query with image successful
2024-08-13 16:24:50,943 DEBUG Querying model with image: gpt-4o
2024-08-13 16:25:05,214 DEBUG Query with image successful
2024-08-13 16:25:05,505 DEBUG Querying model with image: gpt-4o
2024-08-13 16:25:16,668 DEBUG Query with image successful
2024-08-13 16:25:16,756 DEBUG Querying model with image: gpt-4o
2024-08-13 16:25:25,031 DEBUG Query with image successful
2024-08-13 16:25:25,310 DEBUG Querying model with image: gpt-4o
2024-08-13 16:25:35,040 DEBUG Query with image successful
2024-08-13 16:25:35,128 DEBUG Querying model with image: gpt-4o
2024-08-13 16:25:44,512 DEBUG Query with image successful
2024-08-13 16:25:44,814 DEBUG Querying model with image: gpt-4o
2024-08-13 16:25:53,801 DEBUG Query with image successful
2024-08-13 16:25:53,890 DEBUG Querying model with image: gpt-4o
2024-08-13 16:26:04,704 DEBUG Query with image successful
2024-08-13 16:26:04,985 DEBUG Querying model with image: gpt-4o
2024-08-13 16:26:16,688 DEBUG Query with image successful
2024-08-13 16:26:16,786 DEBUG Querying model with image: gpt-4o
2024-08-13 16:26:29,719 DEBUG Query with image successful
2024-08-13 16:26:30,017 DEBUG Querying model with image: gpt-4o
2024-08-13 16:26:38,574 DEBUG Query with image successful
2024-08-13 16:26:38,667 DEBUG Querying model with image: gpt-4o
2024-08-13 16:26:48,329 DEBUG Query with image successful
2024-08-13 16:26:48,609 DEBUG Querying model with image: gpt-4o
2024-08-13 16:26:59,570 DEBUG Query with image successful
2024-08-13 16:26:59,657 DEBUG Querying model with image: gpt-4o
2024-08-13 16:27:09,642 DEBUG Query with image successful
2024-08-13 16:27:09,924 DEBUG Querying model with image: gpt-4o
2024-08-13 16:27:21,751 DEBUG Query with image successful
2024-08-13 16:27:21,838 DEBUG Querying model with image: gpt-4o
2024-08-13 16:27:35,420 DEBUG Query with image successful
2024-08-13 16:27:35,692 DEBUG Querying model with image: gpt-4o
2024-08-13 16:27:47,482 DEBUG Query with image successful
2024-08-13 16:27:47,575 DEBUG Querying model with image: gpt-4o
2024-08-13 16:27:57,228 DEBUG Query with image successful
2024-08-13 16:27:57,510 DEBUG Querying model with image: gpt-4o
2024-08-13 16:28:07,498 DEBUG Query with image successful
2024-08-13 16:28:07,583 DEBUG Querying model with image: gpt-4o
2024-08-13 16:28:16,581 DEBUG Query with image successful
2024-08-13 16:28:16,988 DEBUG Querying model with image: gpt-4o
2024-08-13 16:28:28,549 DEBUG Query with image successful
2024-08-13 16:28:28,640 DEBUG Querying model with image: gpt-4o
2024-08-13 16:28:38,730 DEBUG Query with image successful
2024-08-13 16:28:39,018 DEBUG Querying model with image: gpt-4o
2024-08-13 16:28:49,567 DEBUG Query with image successful
2024-08-13 16:28:49,656 DEBUG Querying model with image: gpt-4o
2024-08-13 16:28:57,817 DEBUG Query with image successful
2024-08-13 16:28:58,213 DEBUG Querying model with image: gpt-4o
2024-08-13 16:29:08,137 DEBUG Query with image successful
2024-08-13 16:29:08,226 DEBUG Querying model with image: gpt-4o
2024-08-13 16:29:19,048 DEBUG Query with image successful
2024-08-13 16:29:19,347 DEBUG Querying model with image: gpt-4o
2024-08-13 16:29:31,169 DEBUG Query with image successful
2024-08-13 16:29:31,264 DEBUG Querying model with image: gpt-4o
2024-08-13 16:29:42,930 DEBUG Query with image successful
2024-08-13 16:29:43,222 DEBUG Querying model with image: gpt-4o
2024-08-13 16:29:51,511 DEBUG Query with image successful
2024-08-13 16:29:51,611 DEBUG Querying model with image: gpt-4o
2024-08-13 16:29:59,998 DEBUG Query with image successful
2024-08-13 16:30:00,298 DEBUG Querying model with image: gpt-4o
2024-08-13 16:30:10,611 DEBUG Query with image successful
2024-08-13 16:30:10,699 DEBUG Querying model with image: gpt-4o
2024-08-13 16:30:20,377 DEBUG Query with image successful
2024-08-13 16:30:20,656 DEBUG Querying model with image: gpt-4o
2024-08-13 16:30:30,208 DEBUG Query with image successful
2024-08-13 16:30:30,298 DEBUG Querying model with image: gpt-4o
2024-08-13 16:30:38,118 DEBUG Query with image successful
2024-08-13 16:30:38,390 DEBUG Querying model with image: gpt-4o
2024-08-13 16:30:49,191 DEBUG Query with image successful
2024-08-13 16:30:49,285 DEBUG Querying model with image: gpt-4o
2024-08-13 16:30:59,527 DEBUG Query with image successful
2024-08-13 16:30:59,815 DEBUG Querying model with image: gpt-4o
2024-08-13 16:31:09,838 DEBUG Query with image successful
2024-08-13 16:31:09,935 DEBUG Querying model with image: gpt-4o
2024-08-13 16:31:17,816 DEBUG Query with image successful
2024-08-13 16:31:18,147 DEBUG Querying model with image: gpt-4o
2024-08-13 16:31:26,927 DEBUG Query with image successful
2024-08-13 16:31:27,019 DEBUG Querying model with image: gpt-4o
2024-08-13 16:31:36,164 DEBUG Query with image successful
2024-08-13 16:31:36,448 DEBUG Querying model with image: gpt-4o
2024-08-13 16:31:46,464 DEBUG Query with image successful
2024-08-13 16:31:46,564 DEBUG Querying model with image: gpt-4o
2024-08-13 16:31:54,201 DEBUG Query with image successful
2024-08-13 18:50:53,540 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-13 18:50:53,578 DEBUG OpenAI client created
2024-08-13 18:50:58,778 DEBUG Querying model with image: gpt-4o
2024-08-13 18:51:07,055 DEBUG Query with image successful
2024-08-13 18:51:07,158 DEBUG Querying model with image: gpt-4o
2024-08-13 18:51:17,610 DEBUG Query with image successful
2024-08-13 18:51:17,896 DEBUG Querying model with image: gpt-4o
2024-08-13 18:51:30,094 DEBUG Query with image successful
2024-08-13 18:51:30,189 DEBUG Querying model with image: gpt-4o
2024-08-13 18:51:47,706 DEBUG Query with image successful
2024-08-13 18:51:58,001 DEBUG Querying model with image: gpt-4o
2024-08-13 18:52:07,105 DEBUG Query with image successful
2024-08-13 18:52:07,206 DEBUG Querying model with image: gpt-4o
2024-08-13 18:52:15,856 DEBUG Query with image successful
2024-08-13 18:52:16,170 DEBUG Querying model with image: gpt-4o
2024-08-13 18:52:24,559 DEBUG Query with image successful
2024-08-13 18:52:24,646 DEBUG Querying model with image: gpt-4o
2024-08-13 18:52:33,714 DEBUG Query with image successful
2024-08-13 18:52:33,996 DEBUG Querying model with image: gpt-4o
2024-08-13 18:52:43,174 DEBUG Query with image successful
2024-08-13 18:52:43,260 DEBUG Querying model with image: gpt-4o
2024-08-13 18:52:53,816 DEBUG Query with image successful
2024-08-13 18:52:54,157 DEBUG Querying model with image: gpt-4o
2024-08-13 18:53:04,028 DEBUG Query with image successful
2024-08-13 18:53:04,112 DEBUG Querying model with image: gpt-4o
2024-08-13 18:53:11,793 DEBUG Query with image successful
2024-08-13 18:53:12,131 DEBUG Querying model with image: gpt-4o
2024-08-13 18:53:20,380 DEBUG Query with image successful
2024-08-13 18:53:20,467 DEBUG Querying model with image: gpt-4o
2024-08-13 18:53:31,384 DEBUG Query with image successful
2024-08-13 18:53:31,717 DEBUG Querying model with image: gpt-4o
2024-08-13 18:53:41,312 DEBUG Query with image successful
2024-08-13 18:53:41,398 DEBUG Querying model with image: gpt-4o
2024-08-13 18:53:49,759 DEBUG Query with image successful
2024-08-13 18:53:50,041 DEBUG Querying model with image: gpt-4o
2024-08-13 18:53:58,627 DEBUG Query with image successful
2024-08-13 18:53:58,725 DEBUG Querying model with image: gpt-4o
2024-08-13 18:54:11,847 DEBUG Query with image successful
2024-08-13 18:54:12,120 DEBUG Querying model with image: gpt-4o
2024-08-13 18:54:23,185 DEBUG Query with image successful
2024-08-13 18:54:23,269 DEBUG Querying model with image: gpt-4o
2024-08-13 18:54:34,726 DEBUG Query with image successful
2024-08-13 18:54:34,996 DEBUG Querying model with image: gpt-4o
2024-08-13 18:54:47,666 DEBUG Query with image successful
2024-08-13 18:54:47,750 DEBUG Querying model with image: gpt-4o
2024-08-13 18:54:58,249 DEBUG Query with image successful
2024-08-13 18:54:58,533 DEBUG Querying model with image: gpt-4o
2024-08-13 18:55:11,183 DEBUG Query with image successful
2024-08-13 18:55:11,273 DEBUG Querying model with image: gpt-4o
2024-08-13 18:55:22,855 DEBUG Query with image successful
2024-08-13 18:55:23,131 DEBUG Querying model with image: gpt-4o
2024-08-13 18:55:34,185 DEBUG Query with image successful
2024-08-13 18:55:34,270 DEBUG Querying model with image: gpt-4o
2024-08-13 18:55:45,150 DEBUG Query with image successful
2024-08-13 18:55:45,421 DEBUG Querying model with image: gpt-4o
2024-08-13 18:55:55,814 DEBUG Query with image successful
2024-08-13 18:55:55,920 DEBUG Querying model with image: gpt-4o
2024-08-13 18:56:04,003 DEBUG Query with image successful
2024-08-13 18:56:04,275 DEBUG Querying model with image: gpt-4o
2024-08-13 18:56:13,896 DEBUG Query with image successful
2024-08-13 18:56:13,979 DEBUG Querying model with image: gpt-4o
2024-08-13 18:56:22,303 DEBUG Query with image successful
2024-08-13 18:56:22,639 DEBUG Querying model with image: gpt-4o
2024-08-13 18:56:31,574 DEBUG Query with image successful
2024-08-13 18:56:31,658 DEBUG Querying model with image: gpt-4o
2024-08-13 18:56:39,984 DEBUG Query with image successful
2024-08-13 18:56:40,272 DEBUG Querying model with image: gpt-4o
2024-08-13 18:56:50,513 DEBUG Query with image successful
2024-08-13 18:56:50,610 DEBUG Querying model with image: gpt-4o
2024-08-13 18:56:58,458 DEBUG Query with image successful
2024-08-13 18:56:58,740 DEBUG Querying model with image: gpt-4o
2024-08-13 18:57:12,151 DEBUG Query with image successful
2024-08-13 18:57:12,234 DEBUG Querying model with image: gpt-4o
2024-08-13 18:57:23,182 DEBUG Query with image successful
2024-08-13 18:57:23,451 DEBUG Querying model with image: gpt-4o
2024-08-13 18:57:33,856 DEBUG Query with image successful
2024-08-13 18:57:33,961 DEBUG Querying model with image: gpt-4o
2024-08-13 18:57:44,776 DEBUG Query with image successful
2024-08-13 18:57:45,062 DEBUG Querying model with image: gpt-4o
2024-08-13 18:57:57,351 DEBUG Query with image successful
2024-08-13 18:57:57,434 DEBUG Querying model with image: gpt-4o
2024-08-13 18:58:09,341 DEBUG Query with image successful
2024-08-13 18:58:09,614 DEBUG Querying model with image: gpt-4o
2024-08-13 18:58:20,744 DEBUG Query with image successful
2024-08-13 18:58:20,826 DEBUG Querying model with image: gpt-4o
2024-08-13 18:58:32,782 DEBUG Query with image successful
2024-08-13 18:58:33,069 DEBUG Querying model with image: gpt-4o
2024-08-13 18:58:45,039 DEBUG Query with image successful
2024-08-13 18:58:45,130 DEBUG Querying model with image: gpt-4o
2024-08-13 18:58:55,624 DEBUG Query with image successful
2024-08-13 18:58:55,894 DEBUG Querying model with image: gpt-4o
2024-08-13 18:59:04,369 DEBUG Query with image successful
2024-08-13 18:59:04,462 DEBUG Querying model with image: gpt-4o
2024-08-13 18:59:11,819 DEBUG Query with image successful
2024-08-13 18:59:12,093 DEBUG Querying model with image: gpt-4o
2024-08-13 18:59:21,334 DEBUG Query with image successful
2024-08-13 18:59:21,418 DEBUG Querying model with image: gpt-4o
2024-08-13 18:59:30,537 DEBUG Query with image successful
2024-08-13 18:59:30,823 DEBUG Querying model with image: gpt-4o
2024-08-13 18:59:43,526 DEBUG Query with image successful
2024-08-13 18:59:43,618 DEBUG Querying model with image: gpt-4o
2024-08-13 18:59:50,960 DEBUG Query with image successful
2024-08-14 15:16:58,271 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-14 15:16:58,313 DEBUG OpenAI client created
2024-08-14 15:17:03,477 DEBUG Querying model with image: gpt-4o
2024-08-14 15:17:15,126 DEBUG Query with image successful
2024-08-14 15:17:15,210 DEBUG Querying model with image: gpt-4o
2024-08-14 15:17:25,572 DEBUG Query with image successful
2024-08-14 15:17:25,840 DEBUG Querying model with image: gpt-4o
2024-08-14 15:17:34,638 DEBUG Query with image successful
2024-08-14 15:17:34,721 DEBUG Querying model with image: gpt-4o
2024-08-14 15:17:42,579 DEBUG Query with image successful
2024-08-14 15:17:52,901 DEBUG Querying model with image: gpt-4o
2024-08-14 15:18:01,901 DEBUG Query with image successful
2024-08-14 15:18:01,996 DEBUG Querying model with image: gpt-4o
2024-08-14 15:18:14,854 DEBUG Query with image successful
2024-08-14 15:18:15,138 DEBUG Querying model with image: gpt-4o
2024-08-14 15:18:27,634 DEBUG Query with image successful
2024-08-14 15:18:27,718 DEBUG Querying model with image: gpt-4o
2024-08-14 15:18:39,752 DEBUG Query with image successful
2024-08-14 15:18:40,056 DEBUG Querying model with image: gpt-4o
2024-08-14 15:18:51,190 DEBUG Query with image successful
2024-08-14 15:18:51,275 DEBUG Querying model with image: gpt-4o
2024-08-14 15:19:04,832 DEBUG Query with image successful
2024-08-14 15:19:05,159 DEBUG Querying model with image: gpt-4o
2024-08-14 15:19:14,110 DEBUG Query with image successful
2024-08-14 15:19:14,197 DEBUG Querying model with image: gpt-4o
2024-08-14 15:19:24,437 DEBUG Query with image successful
2024-08-14 15:19:24,722 DEBUG Querying model with image: gpt-4o
2024-08-14 15:19:37,208 DEBUG Query with image successful
2024-08-14 15:19:37,301 DEBUG Querying model with image: gpt-4o
2024-08-14 15:19:51,281 DEBUG Query with image successful
2024-08-14 15:19:51,562 DEBUG Querying model with image: gpt-4o
2024-08-14 15:20:00,289 DEBUG Query with image successful
2024-08-14 15:20:00,382 DEBUG Querying model with image: gpt-4o
2024-08-14 15:20:17,068 DEBUG Query with image successful
2024-08-14 15:20:17,350 DEBUG Querying model with image: gpt-4o
2024-08-14 15:20:26,735 DEBUG Query with image successful
2024-08-14 15:20:26,822 DEBUG Querying model with image: gpt-4o
2024-08-14 15:20:38,040 DEBUG Query with image successful
2024-08-14 15:20:38,347 DEBUG Querying model with image: gpt-4o
2024-08-14 15:20:48,566 DEBUG Query with image successful
2024-08-14 15:20:48,657 DEBUG Querying model with image: gpt-4o
2024-08-14 15:21:06,106 DEBUG Query with image successful
2024-08-14 15:21:06,382 DEBUG Querying model with image: gpt-4o
2024-08-14 15:21:15,739 DEBUG Query with image successful
2024-08-14 15:21:15,830 DEBUG Querying model with image: gpt-4o
2024-08-14 15:21:26,836 DEBUG Query with image successful
2024-08-14 15:21:27,116 DEBUG Querying model with image: gpt-4o
2024-08-14 15:21:36,156 DEBUG Query with image successful
2024-08-14 15:21:36,255 DEBUG Querying model with image: gpt-4o
2024-08-14 15:21:44,332 DEBUG Query with image successful
2024-08-14 15:21:44,613 DEBUG Querying model with image: gpt-4o
2024-08-14 15:21:53,836 DEBUG Query with image successful
2024-08-14 15:21:53,920 DEBUG Querying model with image: gpt-4o
2024-08-14 15:22:03,327 DEBUG Query with image successful
2024-08-14 15:22:03,619 DEBUG Querying model with image: gpt-4o
2024-08-14 15:22:13,648 DEBUG Query with image successful
2024-08-14 15:22:13,739 DEBUG Querying model with image: gpt-4o
2024-08-14 15:22:25,022 DEBUG Query with image successful
2024-08-14 15:22:25,308 DEBUG Querying model with image: gpt-4o
2024-08-14 15:22:37,236 DEBUG Query with image successful
2024-08-14 15:22:37,323 DEBUG Querying model with image: gpt-4o
2024-08-14 15:22:46,330 DEBUG Query with image successful
2024-08-14 15:22:46,617 DEBUG Querying model with image: gpt-4o
2024-08-14 15:22:53,987 DEBUG Query with image successful
2024-08-14 15:22:54,073 DEBUG Querying model with image: gpt-4o
2024-08-14 15:23:02,045 DEBUG Query with image successful
2024-08-14 15:23:02,325 DEBUG Querying model with image: gpt-4o
2024-08-14 15:23:12,956 DEBUG Query with image successful
2024-08-14 15:23:13,061 DEBUG Querying model with image: gpt-4o
2024-08-14 15:23:26,094 DEBUG Query with image successful
2024-08-14 15:23:26,407 DEBUG Querying model with image: gpt-4o
2024-08-14 15:23:34,973 DEBUG Query with image successful
2024-08-14 15:23:35,063 DEBUG Querying model with image: gpt-4o
2024-08-14 15:23:45,697 DEBUG Query with image successful
2024-08-14 15:23:46,020 DEBUG Querying model with image: gpt-4o
2024-08-14 15:23:54,571 DEBUG Query with image successful
2024-08-14 15:23:54,669 DEBUG Querying model with image: gpt-4o
2024-08-14 15:24:02,075 DEBUG Query with image successful
2024-08-14 15:24:02,375 DEBUG Querying model with image: gpt-4o
2024-08-14 15:24:23,089 DEBUG Query with image successful
2024-08-14 15:24:23,176 DEBUG Querying model with image: gpt-4o
2024-08-14 15:24:33,867 DEBUG Query with image successful
2024-08-14 15:24:34,149 DEBUG Querying model with image: gpt-4o
2024-08-14 15:24:44,180 DEBUG Query with image successful
2024-08-14 15:24:44,267 DEBUG Querying model with image: gpt-4o
2024-08-14 15:24:52,558 DEBUG Query with image successful
2024-08-14 15:24:52,842 DEBUG Querying model with image: gpt-4o
2024-08-14 15:25:02,843 DEBUG Query with image successful
2024-08-14 15:25:02,932 DEBUG Querying model with image: gpt-4o
2024-08-14 15:25:13,565 DEBUG Query with image successful
2024-08-14 15:25:13,844 DEBUG Querying model with image: gpt-4o
2024-08-14 15:25:25,539 DEBUG Query with image successful
2024-08-14 15:25:25,627 DEBUG Querying model with image: gpt-4o
2024-08-14 15:25:32,948 DEBUG Query with image successful
2024-08-14 15:25:33,226 DEBUG Querying model with image: gpt-4o
2024-08-14 15:25:46,833 DEBUG Query with image successful
2024-08-14 15:25:46,919 DEBUG Querying model with image: gpt-4o
2024-08-14 15:25:55,618 DEBUG Query with image successful
2024-08-14 15:25:55,904 DEBUG Querying model with image: gpt-4o
2024-08-14 15:26:07,530 DEBUG Query with image successful
2024-08-14 15:26:07,629 DEBUG Querying model with image: gpt-4o
2024-08-14 15:26:18,127 DEBUG Query with image successful
2024-08-14 15:36:48,167 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-14 15:36:48,194 DEBUG OpenAI client created
2024-08-14 15:36:53,365 DEBUG Querying model with image: gpt-4o
2024-08-14 15:37:05,815 DEBUG Query with image successful
2024-08-14 15:37:05,898 DEBUG Querying model with image: gpt-4o
2024-08-14 15:37:17,034 DEBUG Query with image successful
2024-08-14 15:37:17,311 DEBUG Querying model with image: gpt-4o
2024-08-14 15:37:25,264 DEBUG Query with image successful
2024-08-14 15:37:25,349 DEBUG Querying model with image: gpt-4o
2024-08-14 15:37:34,702 DEBUG Query with image successful
2024-08-14 15:37:44,978 DEBUG Querying model with image: gpt-4o
2024-08-14 15:37:53,802 DEBUG Query with image successful
2024-08-14 15:37:53,887 DEBUG Querying model with image: gpt-4o
2024-08-14 15:38:03,753 DEBUG Query with image successful
2024-08-14 15:38:04,063 DEBUG Querying model with image: gpt-4o
2024-08-14 15:38:12,802 DEBUG Query with image successful
2024-08-14 15:38:12,889 DEBUG Querying model with image: gpt-4o
2024-08-14 15:38:21,344 DEBUG Query with image successful
2024-08-14 15:38:21,618 DEBUG Querying model with image: gpt-4o
2024-08-14 15:38:31,098 DEBUG Query with image successful
2024-08-14 15:38:31,185 DEBUG Querying model with image: gpt-4o
2024-08-14 15:38:42,882 DEBUG Query with image successful
2024-08-14 15:38:43,162 DEBUG Querying model with image: gpt-4o
2024-08-14 15:38:52,254 DEBUG Query with image successful
2024-08-14 15:38:52,344 DEBUG Querying model with image: gpt-4o
2024-08-14 15:39:02,355 DEBUG Query with image successful
2024-08-14 15:39:02,640 DEBUG Querying model with image: gpt-4o
2024-08-14 15:39:12,322 DEBUG Query with image successful
2024-08-14 15:39:12,408 DEBUG Querying model with image: gpt-4o
2024-08-14 15:39:23,836 DEBUG Query with image successful
2024-08-14 15:39:24,147 DEBUG Querying model with image: gpt-4o
2024-08-14 15:39:35,161 DEBUG Query with image successful
2024-08-14 15:39:35,247 DEBUG Querying model with image: gpt-4o
2024-08-14 15:39:48,528 DEBUG Query with image successful
2024-08-14 15:39:48,847 DEBUG Querying model with image: gpt-4o
2024-08-14 15:39:57,238 DEBUG Query with image successful
2024-08-14 15:39:57,334 DEBUG Querying model with image: gpt-4o
2024-08-14 15:40:09,464 DEBUG Query with image successful
2024-08-14 15:40:09,750 DEBUG Querying model with image: gpt-4o
2024-08-14 15:40:19,053 DEBUG Query with image successful
2024-08-14 15:40:19,141 DEBUG Querying model with image: gpt-4o
2024-08-14 15:40:29,590 DEBUG Query with image successful
2024-08-14 15:40:29,877 DEBUG Querying model with image: gpt-4o
2024-08-14 15:40:39,855 DEBUG Query with image successful
2024-08-14 15:40:39,942 DEBUG Querying model with image: gpt-4o
2024-08-14 15:40:48,627 DEBUG Query with image successful
2024-08-14 15:40:48,906 DEBUG Querying model with image: gpt-4o
2024-08-14 15:41:00,102 DEBUG Query with image successful
2024-08-14 15:41:00,186 DEBUG Querying model with image: gpt-4o
2024-08-14 15:41:10,000 DEBUG Query with image successful
2024-08-14 15:41:10,291 DEBUG Querying model with image: gpt-4o
2024-08-14 15:41:20,569 DEBUG Query with image successful
2024-08-14 15:41:20,666 DEBUG Querying model with image: gpt-4o
2024-08-14 15:41:30,424 DEBUG Query with image successful
2024-08-14 15:41:30,699 DEBUG Querying model with image: gpt-4o
2024-08-14 15:41:39,207 DEBUG Query with image successful
2024-08-14 15:41:39,293 DEBUG Querying model with image: gpt-4o
2024-08-14 15:41:47,062 DEBUG Query with image successful
2024-08-14 15:41:47,353 DEBUG Querying model with image: gpt-4o
2024-08-14 15:41:57,606 DEBUG Query with image successful
2024-08-14 15:41:57,692 DEBUG Querying model with image: gpt-4o
2024-08-14 15:42:06,194 DEBUG Query with image successful
2024-08-14 15:42:06,496 DEBUG Querying model with image: gpt-4o
2024-08-14 15:42:18,121 DEBUG Query with image successful
2024-08-14 15:42:18,214 DEBUG Querying model with image: gpt-4o
2024-08-14 15:42:25,300 DEBUG Query with image successful
2024-08-14 15:42:25,575 DEBUG Querying model with image: gpt-4o
2024-08-14 15:42:34,683 DEBUG Query with image successful
2024-08-14 15:42:34,774 DEBUG Querying model with image: gpt-4o
2024-08-14 15:42:42,987 DEBUG Query with image successful
2024-08-14 15:42:43,270 DEBUG Querying model with image: gpt-4o
2024-08-14 15:42:53,030 DEBUG Query with image successful
2024-08-14 15:42:53,117 DEBUG Querying model with image: gpt-4o
2024-08-14 15:43:08,650 DEBUG Query with image successful
2024-08-14 15:43:08,957 DEBUG Querying model with image: gpt-4o
2024-08-14 15:43:18,651 DEBUG Query with image successful
2024-08-14 15:43:18,748 DEBUG Querying model with image: gpt-4o
2024-08-14 15:43:25,450 DEBUG Query with image successful
2024-08-14 15:43:25,719 DEBUG Querying model with image: gpt-4o
2024-08-14 15:43:34,030 DEBUG Query with image successful
2024-08-14 15:43:34,116 DEBUG Querying model with image: gpt-4o
2024-08-14 15:43:43,378 DEBUG Query with image successful
2024-08-14 15:43:43,680 DEBUG Querying model with image: gpt-4o
2024-08-14 15:43:51,801 DEBUG Query with image successful
2024-08-14 15:43:51,886 DEBUG Querying model with image: gpt-4o
2024-08-14 15:44:01,972 DEBUG Query with image successful
2024-08-14 15:44:02,244 DEBUG Querying model with image: gpt-4o
2024-08-14 15:44:10,482 DEBUG Query with image successful
2024-08-14 15:44:10,572 DEBUG Querying model with image: gpt-4o
2024-08-14 15:44:20,775 DEBUG Query with image successful
2024-08-14 15:44:21,096 DEBUG Querying model with image: gpt-4o
2024-08-14 15:44:35,799 DEBUG Query with image successful
2024-08-14 15:44:35,905 DEBUG Querying model with image: gpt-4o
2024-08-14 15:44:43,716 DEBUG Query with image successful
2024-08-14 15:44:44,001 DEBUG Querying model with image: gpt-4o
2024-08-14 15:44:52,732 DEBUG Query with image successful
2024-08-14 15:44:52,835 DEBUG Querying model with image: gpt-4o
2024-08-14 15:45:03,034 DEBUG Query with image successful
2024-08-14 15:45:03,357 DEBUG Querying model with image: gpt-4o
2024-08-14 15:45:12,303 DEBUG Query with image successful
2024-08-14 15:45:12,393 DEBUG Querying model with image: gpt-4o
2024-08-14 15:45:21,691 DEBUG Query with image successful
2024-08-14 15:51:57,042 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-14 15:51:57,069 DEBUG OpenAI client created
2024-08-14 15:52:02,243 DEBUG Querying model with image: gpt-4o
2024-08-14 15:52:13,377 DEBUG Query with image successful
2024-08-14 15:52:13,464 DEBUG Querying model with image: gpt-4o
2024-08-14 15:52:22,976 DEBUG Query with image successful
2024-08-14 15:52:23,270 DEBUG Querying model with image: gpt-4o
2024-08-14 15:52:32,482 DEBUG Query with image successful
2024-08-14 15:52:32,578 DEBUG Querying model with image: gpt-4o
2024-08-14 15:52:43,367 DEBUG Query with image successful
2024-08-14 15:52:53,687 DEBUG Querying model with image: gpt-4o
2024-08-14 15:53:02,537 DEBUG Query with image successful
2024-08-14 15:53:02,636 DEBUG Querying model with image: gpt-4o
2024-08-14 15:53:21,447 DEBUG Query with image successful
2024-08-14 15:53:21,763 DEBUG Querying model with image: gpt-4o
2024-08-14 15:53:30,246 DEBUG Query with image successful
2024-08-14 15:53:30,335 DEBUG Querying model with image: gpt-4o
2024-08-14 15:53:40,966 DEBUG Query with image successful
2024-08-14 15:53:41,324 DEBUG Querying model with image: gpt-4o
2024-08-14 15:53:52,599 DEBUG Query with image successful
2024-08-14 15:53:52,688 DEBUG Querying model with image: gpt-4o
2024-08-14 15:54:03,756 DEBUG Query with image successful
2024-08-14 15:54:04,141 DEBUG Querying model with image: gpt-4o
2024-08-14 15:54:13,389 DEBUG Query with image successful
2024-08-14 15:54:13,479 DEBUG Querying model with image: gpt-4o
2024-08-14 15:54:24,779 DEBUG Query with image successful
2024-08-14 15:54:25,092 DEBUG Querying model with image: gpt-4o
2024-08-14 15:54:33,353 DEBUG Query with image successful
2024-08-14 15:54:33,446 DEBUG Querying model with image: gpt-4o
2024-08-14 15:54:41,674 DEBUG Query with image successful
2024-08-14 15:54:41,992 DEBUG Querying model with image: gpt-4o
2024-08-14 15:54:49,170 DEBUG Query with image successful
2024-08-14 15:54:49,258 DEBUG Querying model with image: gpt-4o
2024-08-14 15:55:01,351 DEBUG Query with image successful
2024-08-14 15:55:01,680 DEBUG Querying model with image: gpt-4o
2024-08-14 15:55:11,627 DEBUG Query with image successful
2024-08-14 15:55:11,728 DEBUG Querying model with image: gpt-4o
2024-08-14 15:55:21,195 DEBUG Query with image successful
2024-08-14 15:55:21,499 DEBUG Querying model with image: gpt-4o
2024-08-14 15:55:32,103 DEBUG Query with image successful
2024-08-14 15:55:32,197 DEBUG Querying model with image: gpt-4o
2024-08-14 15:55:43,921 DEBUG Query with image successful
2024-08-14 15:55:44,305 DEBUG Querying model with image: gpt-4o
2024-08-14 15:55:56,130 DEBUG Query with image successful
2024-08-14 15:55:56,219 DEBUG Querying model with image: gpt-4o
2024-08-14 15:56:07,598 DEBUG Query with image successful
2024-08-14 15:56:07,935 DEBUG Querying model with image: gpt-4o
2024-08-14 15:56:16,399 DEBUG Query with image successful
2024-08-14 15:56:16,491 DEBUG Querying model with image: gpt-4o
2024-08-14 15:56:26,323 DEBUG Query with image successful
2024-08-14 15:56:26,637 DEBUG Querying model with image: gpt-4o
2024-08-14 15:56:36,475 DEBUG Query with image successful
2024-08-14 15:56:36,574 DEBUG Querying model with image: gpt-4o
2024-08-14 15:56:47,100 DEBUG Query with image successful
2024-08-14 15:56:47,421 DEBUG Querying model with image: gpt-4o
2024-08-14 15:56:55,526 DEBUG Query with image successful
2024-08-14 15:56:55,615 DEBUG Querying model with image: gpt-4o
2024-08-14 15:57:02,898 DEBUG Query with image successful
2024-08-14 15:57:03,193 DEBUG Querying model with image: gpt-4o
2024-08-14 15:57:12,566 DEBUG Query with image successful
2024-08-14 15:57:12,653 DEBUG Querying model with image: gpt-4o
2024-08-14 15:57:20,854 DEBUG Query with image successful
2024-08-14 15:57:21,140 DEBUG Querying model with image: gpt-4o
2024-08-14 15:57:30,524 DEBUG Query with image successful
2024-08-14 15:57:30,612 DEBUG Querying model with image: gpt-4o
2024-08-14 15:57:40,234 DEBUG Query with image successful
2024-08-14 15:57:40,555 DEBUG Querying model with image: gpt-4o
2024-08-14 15:57:50,046 DEBUG Query with image successful
2024-08-14 15:57:50,134 DEBUG Querying model with image: gpt-4o
2024-08-14 15:57:59,327 DEBUG Query with image successful
2024-08-14 15:57:59,613 DEBUG Querying model with image: gpt-4o
2024-08-14 15:58:09,084 DEBUG Query with image successful
2024-08-14 15:58:09,172 DEBUG Querying model with image: gpt-4o
2024-08-14 15:58:17,713 DEBUG Query with image successful
2024-08-14 15:58:18,015 DEBUG Querying model with image: gpt-4o
2024-08-14 15:58:30,455 DEBUG Query with image successful
2024-08-14 15:58:30,554 DEBUG Querying model with image: gpt-4o
2024-08-14 15:58:38,310 DEBUG Query with image successful
2024-08-14 15:58:38,584 DEBUG Querying model with image: gpt-4o
2024-08-14 15:58:50,621 DEBUG Query with image successful
2024-08-14 15:58:50,707 DEBUG Querying model with image: gpt-4o
2024-08-14 15:59:05,645 DEBUG Query with image successful
2024-08-14 15:59:05,920 DEBUG Querying model with image: gpt-4o
2024-08-14 15:59:14,660 DEBUG Query with image successful
2024-08-14 15:59:14,746 DEBUG Querying model with image: gpt-4o
2024-08-14 15:59:21,974 DEBUG Query with image successful
2024-08-14 15:59:22,247 DEBUG Querying model with image: gpt-4o
2024-08-14 15:59:31,305 DEBUG Query with image successful
2024-08-14 15:59:31,390 DEBUG Querying model with image: gpt-4o
2024-08-14 15:59:42,357 DEBUG Query with image successful
2024-08-14 15:59:42,640 DEBUG Querying model with image: gpt-4o
2024-08-14 15:59:51,968 DEBUG Query with image successful
2024-08-14 15:59:52,055 DEBUG Querying model with image: gpt-4o
2024-08-14 16:00:00,500 DEBUG Query with image successful
2024-08-14 16:00:00,777 DEBUG Querying model with image: gpt-4o
2024-08-14 16:00:10,190 DEBUG Query with image successful
2024-08-14 16:00:10,277 DEBUG Querying model with image: gpt-4o
2024-08-14 16:00:18,918 DEBUG Query with image successful
2024-08-14 16:00:19,199 DEBUG Querying model with image: gpt-4o
2024-08-14 16:00:29,864 DEBUG Query with image successful
2024-08-14 16:00:29,951 DEBUG Querying model with image: gpt-4o
2024-08-14 16:00:37,971 DEBUG Query with image successful
2024-08-14 16:05:58,632 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-14 16:05:58,657 DEBUG OpenAI client created
2024-08-14 16:06:03,824 DEBUG Querying model with image: gpt-4o
2024-08-14 16:06:13,783 DEBUG Query with image successful
2024-08-14 16:06:13,866 DEBUG Querying model with image: gpt-4o
2024-08-14 16:06:24,371 DEBUG Query with image successful
2024-08-14 16:06:24,648 DEBUG Querying model with image: gpt-4o
2024-08-14 16:06:35,123 DEBUG Query with image successful
2024-08-14 16:06:35,243 DEBUG Querying model with image: gpt-4o
2024-08-14 16:06:45,928 DEBUG Query with image successful
2024-08-14 16:06:56,320 DEBUG Querying model with image: gpt-4o
2024-08-14 16:07:07,203 DEBUG Query with image successful
2024-08-14 16:07:07,310 DEBUG Querying model with image: gpt-4o
2024-08-14 16:07:18,482 DEBUG Query with image successful
2024-08-14 16:07:18,820 DEBUG Querying model with image: gpt-4o
2024-08-14 16:07:28,836 DEBUG Query with image successful
2024-08-14 16:07:28,922 DEBUG Querying model with image: gpt-4o
2024-08-14 16:07:43,053 DEBUG Query with image successful
2024-08-14 16:07:43,454 DEBUG Querying model with image: gpt-4o
2024-08-14 16:07:54,662 DEBUG Query with image successful
2024-08-14 16:07:54,751 DEBUG Querying model with image: gpt-4o
2024-08-14 16:08:12,273 DEBUG Query with image successful
2024-08-14 16:08:12,670 DEBUG Querying model with image: gpt-4o
2024-08-14 16:08:23,455 DEBUG Query with image successful
2024-08-14 16:08:23,542 DEBUG Querying model with image: gpt-4o
2024-08-14 16:08:34,354 DEBUG Query with image successful
2024-08-14 16:08:34,671 DEBUG Querying model with image: gpt-4o
2024-08-14 16:08:45,977 DEBUG Query with image successful
2024-08-14 16:08:46,065 DEBUG Querying model with image: gpt-4o
2024-08-14 16:08:56,974 DEBUG Query with image successful
2024-08-14 16:08:57,309 DEBUG Querying model with image: gpt-4o
2024-08-14 16:09:07,175 DEBUG Query with image successful
2024-08-14 16:09:07,279 DEBUG Querying model with image: gpt-4o
2024-08-14 16:09:15,412 DEBUG Query with image successful
2024-08-14 16:09:15,751 DEBUG Querying model with image: gpt-4o
2024-08-14 16:09:25,280 DEBUG Query with image successful
2024-08-14 16:09:25,381 DEBUG Querying model with image: gpt-4o
2024-08-14 16:09:37,398 DEBUG Query with image successful
2024-08-14 16:09:37,719 DEBUG Querying model with image: gpt-4o
2024-08-14 16:09:50,044 DEBUG Query with image successful
2024-08-14 16:09:50,147 DEBUG Querying model with image: gpt-4o
2024-08-14 16:10:01,961 DEBUG Query with image successful
2024-08-14 16:10:02,281 DEBUG Querying model with image: gpt-4o
2024-08-14 16:10:12,461 DEBUG Query with image successful
2024-08-14 16:10:12,555 DEBUG Querying model with image: gpt-4o
2024-08-14 16:10:25,284 DEBUG Query with image successful
2024-08-14 16:10:25,589 DEBUG Querying model with image: gpt-4o
2024-08-14 16:10:35,695 DEBUG Query with image successful
2024-08-14 16:10:35,790 DEBUG Querying model with image: gpt-4o
2024-08-14 16:10:46,985 DEBUG Query with image successful
2024-08-14 16:10:47,351 DEBUG Querying model with image: gpt-4o
2024-08-14 16:10:57,652 DEBUG Query with image successful
2024-08-14 16:10:57,750 DEBUG Querying model with image: gpt-4o
2024-08-14 16:11:09,872 DEBUG Query with image successful
2024-08-14 16:11:10,219 DEBUG Querying model with image: gpt-4o
2024-08-14 16:11:19,953 DEBUG Query with image successful
2024-08-14 16:11:20,044 DEBUG Querying model with image: gpt-4o
2024-08-14 16:11:27,730 DEBUG Query with image successful
2024-08-14 16:11:28,076 DEBUG Querying model with image: gpt-4o
2024-08-14 16:11:38,229 DEBUG Query with image successful
2024-08-14 16:11:38,320 DEBUG Querying model with image: gpt-4o
2024-08-14 16:11:58,075 DEBUG Query with image successful
2024-08-14 16:11:58,406 DEBUG Querying model with image: gpt-4o
2024-08-14 16:12:08,535 DEBUG Query with image successful
2024-08-14 16:12:08,630 DEBUG Querying model with image: gpt-4o
2024-08-14 16:12:20,615 DEBUG Query with image successful
2024-08-14 16:12:20,915 DEBUG Querying model with image: gpt-4o
2024-08-14 16:12:35,988 DEBUG Query with image successful
2024-08-14 16:12:36,091 DEBUG Querying model with image: gpt-4o
2024-08-14 16:12:44,876 DEBUG Query with image successful
2024-08-14 16:12:45,170 DEBUG Querying model with image: gpt-4o
2024-08-14 16:12:54,721 DEBUG Query with image successful
2024-08-14 16:12:54,819 DEBUG Querying model with image: gpt-4o
2024-08-14 16:13:04,210 DEBUG Query with image successful
2024-08-14 16:13:04,487 DEBUG Querying model with image: gpt-4o
2024-08-14 16:13:25,874 DEBUG Query with image successful
2024-08-14 16:13:25,969 DEBUG Querying model with image: gpt-4o
2024-08-14 16:13:34,935 DEBUG Query with image successful
2024-08-14 16:13:35,238 DEBUG Querying model with image: gpt-4o
2024-08-14 16:13:44,605 DEBUG Query with image successful
2024-08-14 16:13:44,700 DEBUG Querying model with image: gpt-4o
2024-08-14 16:13:53,644 DEBUG Query with image successful
2024-08-14 16:13:53,914 DEBUG Querying model with image: gpt-4o
2024-08-14 16:14:01,848 DEBUG Query with image successful
2024-08-14 16:14:01,934 DEBUG Querying model with image: gpt-4o
2024-08-14 16:14:14,900 DEBUG Query with image successful
2024-08-14 16:14:15,173 DEBUG Querying model with image: gpt-4o
2024-08-14 16:14:25,280 DEBUG Query with image successful
2024-08-14 16:14:25,366 DEBUG Querying model with image: gpt-4o
2024-08-14 16:14:36,029 DEBUG Query with image successful
2024-08-14 16:14:36,484 DEBUG Querying model with image: gpt-4o
2024-08-14 16:14:44,279 DEBUG Query with image successful
2024-08-14 16:14:44,378 DEBUG Querying model with image: gpt-4o
2024-08-14 16:14:56,453 DEBUG Query with image successful
2024-08-14 16:14:56,763 DEBUG Querying model with image: gpt-4o
2024-08-14 16:15:08,925 DEBUG Query with image successful
2024-08-14 16:15:09,014 DEBUG Querying model with image: gpt-4o
2024-08-14 16:15:20,943 DEBUG Query with image successful
2024-08-14 16:15:21,247 DEBUG Querying model with image: gpt-4o
2024-08-14 16:15:30,532 DEBUG Query with image successful
2024-08-14 16:15:30,623 DEBUG Querying model with image: gpt-4o
2024-08-14 16:15:40,530 DEBUG Query with image successful
2024-08-14 16:18:18,658 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-14 16:18:18,685 DEBUG OpenAI client created
2024-08-14 16:18:23,870 DEBUG Querying model with image: gpt-4o
2024-08-14 16:18:32,703 DEBUG Query with image successful
2024-08-14 16:18:32,796 DEBUG Querying model with image: gpt-4o
2024-08-14 16:18:44,158 DEBUG Query with image successful
2024-08-14 16:18:44,461 DEBUG Querying model with image: gpt-4o
2024-08-14 16:18:53,511 DEBUG Query with image successful
2024-08-14 16:18:53,600 DEBUG Querying model with image: gpt-4o
2024-08-14 16:19:03,980 DEBUG Query with image successful
2024-08-14 16:19:14,313 DEBUG Querying model with image: gpt-4o
2024-08-14 16:19:25,450 DEBUG Query with image successful
2024-08-14 16:19:25,536 DEBUG Querying model with image: gpt-4o
2024-08-14 16:19:36,745 DEBUG Query with image successful
2024-08-14 16:19:37,143 DEBUG Querying model with image: gpt-4o
2024-08-14 16:19:46,849 DEBUG Query with image successful
2024-08-14 16:19:46,943 DEBUG Querying model with image: gpt-4o
2024-08-14 16:20:00,515 DEBUG Query with image successful
2024-08-14 16:20:00,883 DEBUG Querying model with image: gpt-4o
2024-08-14 16:20:12,653 DEBUG Query with image successful
2024-08-14 16:20:12,741 DEBUG Querying model with image: gpt-4o
2024-08-14 16:20:25,012 DEBUG Query with image successful
2024-08-14 16:20:25,335 DEBUG Querying model with image: gpt-4o
2024-08-14 16:20:36,385 DEBUG Query with image successful
2024-08-14 16:20:36,494 DEBUG Querying model with image: gpt-4o
2024-08-14 16:20:47,071 DEBUG Query with image successful
2024-08-14 16:20:47,419 DEBUG Querying model with image: gpt-4o
2024-08-14 16:20:58,790 DEBUG Query with image successful
2024-08-14 16:20:58,879 DEBUG Querying model with image: gpt-4o
2024-08-14 16:21:10,400 DEBUG Query with image successful
2024-08-14 16:21:10,702 DEBUG Querying model with image: gpt-4o
2024-08-14 16:21:24,872 DEBUG Query with image successful
2024-08-14 16:21:24,967 DEBUG Querying model with image: gpt-4o
2024-08-14 16:21:36,116 DEBUG Query with image successful
2024-08-14 16:21:36,430 DEBUG Querying model with image: gpt-4o
2024-08-14 16:21:46,509 DEBUG Query with image successful
2024-08-14 16:21:46,609 DEBUG Querying model with image: gpt-4o
2024-08-14 16:22:00,478 DEBUG Query with image successful
2024-08-14 16:22:00,769 DEBUG Querying model with image: gpt-4o
2024-08-14 16:22:11,651 DEBUG Query with image successful
2024-08-14 16:22:11,739 DEBUG Querying model with image: gpt-4o
2024-08-14 16:22:23,963 DEBUG Query with image successful
2024-08-14 16:22:24,263 DEBUG Querying model with image: gpt-4o
2024-08-14 16:22:34,172 DEBUG Query with image successful
2024-08-14 16:22:34,259 DEBUG Querying model with image: gpt-4o
2024-08-14 16:22:42,519 DEBUG Query with image successful
2024-08-14 16:22:42,819 DEBUG Querying model with image: gpt-4o
2024-08-14 16:22:54,969 DEBUG Query with image successful
2024-08-14 16:22:55,056 DEBUG Querying model with image: gpt-4o
2024-08-14 16:23:03,777 DEBUG Query with image successful
2024-08-14 16:23:04,085 DEBUG Querying model with image: gpt-4o
2024-08-14 16:23:16,664 DEBUG Query with image successful
2024-08-14 16:23:16,768 DEBUG Querying model with image: gpt-4o
2024-08-14 16:23:26,238 DEBUG Query with image successful
2024-08-14 16:23:26,547 DEBUG Querying model with image: gpt-4o
2024-08-14 16:23:36,369 DEBUG Query with image successful
2024-08-14 16:23:36,468 DEBUG Querying model with image: gpt-4o
2024-08-14 16:23:46,857 DEBUG Query with image successful
2024-08-14 16:23:47,227 DEBUG Querying model with image: gpt-4o
2024-08-14 16:23:56,223 DEBUG Query with image successful
2024-08-14 16:23:56,311 DEBUG Querying model with image: gpt-4o
2024-08-14 16:24:06,663 DEBUG Query with image successful
2024-08-14 16:24:06,971 DEBUG Querying model with image: gpt-4o
2024-08-14 16:24:14,470 DEBUG Query with image successful
2024-08-14 16:24:14,567 DEBUG Querying model with image: gpt-4o
2024-08-14 16:24:22,510 DEBUG Query with image successful
2024-08-14 16:24:22,814 DEBUG Querying model with image: gpt-4o
2024-08-14 16:24:32,704 DEBUG Query with image successful
2024-08-14 16:24:32,801 DEBUG Querying model with image: gpt-4o
2024-08-14 16:24:42,496 DEBUG Query with image successful
2024-08-14 16:24:42,792 DEBUG Querying model with image: gpt-4o
2024-08-14 16:24:54,782 DEBUG Query with image successful
2024-08-14 16:24:54,876 DEBUG Querying model with image: gpt-4o
2024-08-14 16:25:02,674 DEBUG Query with image successful
2024-08-14 16:25:02,950 DEBUG Querying model with image: gpt-4o
2024-08-14 16:25:13,201 DEBUG Query with image successful
2024-08-14 16:25:13,293 DEBUG Querying model with image: gpt-4o
2024-08-14 16:25:22,931 DEBUG Query with image successful
2024-08-14 16:25:23,194 DEBUG Querying model with image: gpt-4o
2024-08-14 16:25:31,697 DEBUG Query with image successful
2024-08-14 16:25:31,780 DEBUG Querying model with image: gpt-4o
2024-08-14 16:25:40,828 DEBUG Query with image successful
2024-08-14 16:25:41,095 DEBUG Querying model with image: gpt-4o
2024-08-14 16:25:49,776 DEBUG Query with image successful
2024-08-14 16:25:49,859 DEBUG Querying model with image: gpt-4o
2024-08-14 16:25:59,011 DEBUG Query with image successful
2024-08-14 16:25:59,285 DEBUG Querying model with image: gpt-4o
2024-08-14 16:26:08,498 DEBUG Query with image successful
2024-08-14 16:26:08,581 DEBUG Querying model with image: gpt-4o
2024-08-14 16:26:16,400 DEBUG Query with image successful
2024-08-14 16:26:16,678 DEBUG Querying model with image: gpt-4o
2024-08-14 16:26:26,655 DEBUG Query with image successful
2024-08-14 16:26:26,742 DEBUG Querying model with image: gpt-4o
2024-08-14 16:26:35,246 DEBUG Query with image successful
2024-08-14 16:26:35,515 DEBUG Querying model with image: gpt-4o
2024-08-14 16:26:45,457 DEBUG Query with image successful
2024-08-14 16:26:45,539 DEBUG Querying model with image: gpt-4o
2024-08-14 16:26:54,476 DEBUG Query with image successful
2024-08-14 16:26:54,742 DEBUG Querying model with image: gpt-4o
2024-08-14 16:27:07,315 DEBUG Query with image successful
2024-08-14 16:27:07,398 DEBUG Querying model with image: gpt-4o
2024-08-14 16:27:15,750 DEBUG Query with image successful
2024-08-19 13:00:38,321 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 13:00:38,345 DEBUG OpenAI client created
2024-08-19 13:00:43,540 DEBUG Querying model with image: gpt-4o
2024-08-19 13:00:51,704 DEBUG Query with image successful
2024-08-19 13:00:51,798 DEBUG Querying model with image: gpt-4o
2024-08-19 13:00:59,463 DEBUG Query with image successful
2024-08-19 13:00:59,747 DEBUG Querying model with image: gpt-4o
2024-08-19 13:01:07,007 DEBUG Query with image successful
2024-08-19 13:01:07,099 DEBUG Querying model with image: gpt-4o
2024-08-19 13:01:20,745 DEBUG Query with image successful
2024-08-19 13:01:31,051 DEBUG Querying model with image: gpt-4o
2024-08-19 13:01:42,414 DEBUG Query with image successful
2024-08-19 13:01:42,511 DEBUG Querying model with image: gpt-4o
2024-08-19 13:01:51,072 DEBUG Query with image successful
2024-08-19 13:01:51,861 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:00,374 DEBUG Query with image successful
2024-08-19 13:02:00,473 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:08,766 DEBUG Query with image successful
2024-08-19 13:02:09,099 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:15,610 DEBUG Query with image successful
2024-08-19 13:02:15,717 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:24,935 DEBUG Query with image successful
2024-08-19 13:02:25,298 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:32,778 DEBUG Query with image successful
2024-08-19 13:02:32,878 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:40,545 DEBUG Query with image successful
2024-08-19 13:02:40,919 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:48,373 DEBUG Query with image successful
2024-08-19 13:02:48,465 DEBUG Querying model with image: gpt-4o
2024-08-19 13:02:57,178 DEBUG Query with image successful
2024-08-19 13:02:57,489 DEBUG Querying model with image: gpt-4o
2024-08-19 13:03:05,252 DEBUG Query with image successful
2024-08-19 13:03:05,344 DEBUG Querying model with image: gpt-4o
2024-08-19 13:03:13,960 DEBUG Query with image successful
2024-08-19 13:03:14,273 DEBUG Querying model with image: gpt-4o
2024-08-19 13:03:22,129 DEBUG Query with image successful
2024-08-19 13:03:22,221 DEBUG Querying model with image: gpt-4o
2024-08-19 13:03:29,589 DEBUG Query with image successful
2024-08-19 13:03:29,955 DEBUG Querying model with image: gpt-4o
2024-08-19 13:03:36,311 DEBUG Query with image successful
2024-08-19 13:03:36,410 DEBUG Querying model with image: gpt-4o
2024-08-19 13:03:44,106 DEBUG Query with image successful
2024-08-19 13:03:44,427 DEBUG Querying model with image: gpt-4o
2024-08-19 13:03:52,306 DEBUG Query with image successful
2024-08-19 13:03:52,405 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:01,800 DEBUG Query with image successful
2024-08-19 13:04:02,132 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:09,111 DEBUG Query with image successful
2024-08-19 13:04:09,207 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:16,770 DEBUG Query with image successful
2024-08-19 13:04:17,148 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:23,655 DEBUG Query with image successful
2024-08-19 13:04:23,754 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:32,874 DEBUG Query with image successful
2024-08-19 13:04:33,180 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:41,916 DEBUG Query with image successful
2024-08-19 13:04:42,012 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:49,649 DEBUG Query with image successful
2024-08-19 13:04:49,955 DEBUG Querying model with image: gpt-4o
2024-08-19 13:04:57,029 DEBUG Query with image successful
2024-08-19 13:04:57,126 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:05,701 DEBUG Query with image successful
2024-08-19 13:05:05,997 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:14,033 DEBUG Query with image successful
2024-08-19 13:05:14,128 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:21,882 DEBUG Query with image successful
2024-08-19 13:05:22,176 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:28,709 DEBUG Query with image successful
2024-08-19 13:05:28,803 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:36,603 DEBUG Query with image successful
2024-08-19 13:05:36,878 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:44,443 DEBUG Query with image successful
2024-08-19 13:05:44,533 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:51,593 DEBUG Query with image successful
2024-08-19 13:05:51,881 DEBUG Querying model with image: gpt-4o
2024-08-19 13:05:59,431 DEBUG Query with image successful
2024-08-19 13:05:59,523 DEBUG Querying model with image: gpt-4o
2024-08-19 13:06:06,343 DEBUG Query with image successful
2024-08-19 13:06:06,626 DEBUG Querying model with image: gpt-4o
2024-08-19 13:06:13,005 DEBUG Query with image successful
2024-08-19 13:06:13,098 DEBUG Querying model with image: gpt-4o
2024-08-19 13:06:21,466 DEBUG Query with image successful
2024-08-19 13:06:21,750 DEBUG Querying model with image: gpt-4o
2024-08-19 13:06:28,332 DEBUG Query with image successful
2024-08-19 13:06:28,422 DEBUG Querying model with image: gpt-4o
2024-08-19 13:06:35,175 DEBUG Query with image successful
2024-08-19 13:06:35,441 DEBUG Querying model with image: gpt-4o
2024-08-19 13:06:43,213 DEBUG Query with image successful
2024-08-19 13:06:43,306 DEBUG Querying model with image: gpt-4o
2024-08-19 13:06:51,069 DEBUG Query with image successful
2024-08-19 13:06:51,366 DEBUG Querying model with image: gpt-4o
2024-08-19 13:07:00,866 DEBUG Query with image successful
2024-08-19 13:07:00,968 DEBUG Querying model with image: gpt-4o
2024-08-19 13:07:07,924 DEBUG Query with image successful
2024-08-19 13:07:08,205 DEBUG Querying model with image: gpt-4o
2024-08-19 13:07:15,629 DEBUG Query with image successful
2024-08-19 13:07:15,730 DEBUG Querying model with image: gpt-4o
2024-08-19 13:07:23,067 DEBUG Query with image successful
2024-08-19 13:07:23,349 DEBUG Querying model with image: gpt-4o
2024-08-19 13:07:34,848 DEBUG Query with image successful
2024-08-19 13:07:34,940 DEBUG Querying model with image: gpt-4o
2024-08-19 13:07:42,045 DEBUG Query with image successful
2024-08-19 13:41:26,214 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 13:41:26,241 DEBUG OpenAI client created
2024-08-19 13:41:31,418 DEBUG Querying model with image: gpt-4o
2024-08-19 13:41:40,408 DEBUG Query with image successful
2024-08-19 13:41:40,508 DEBUG Querying model with image: gpt-4o
2024-08-19 13:41:50,453 DEBUG Query with image successful
2024-08-19 13:41:50,792 DEBUG Querying model with image: gpt-4o
2024-08-19 13:41:58,327 DEBUG Query with image successful
2024-08-19 13:41:58,424 DEBUG Querying model with image: gpt-4o
2024-08-19 13:42:05,286 DEBUG Query with image successful
2024-08-19 13:42:15,659 DEBUG Querying model with image: gpt-4o
2024-08-19 13:42:26,287 DEBUG Query with image successful
2024-08-19 13:42:26,393 DEBUG Querying model with image: gpt-4o
2024-08-19 13:42:36,414 DEBUG Query with image successful
2024-08-19 13:42:36,755 DEBUG Querying model with image: gpt-4o
2024-08-19 13:42:45,176 DEBUG Query with image successful
2024-08-19 13:42:45,282 DEBUG Querying model with image: gpt-4o
2024-08-19 13:42:52,885 DEBUG Query with image successful
2024-08-19 13:42:53,302 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:00,502 DEBUG Query with image successful
2024-08-19 13:43:00,606 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:09,265 DEBUG Query with image successful
2024-08-19 13:43:09,611 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:19,271 DEBUG Query with image successful
2024-08-19 13:43:19,388 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:27,417 DEBUG Query with image successful
2024-08-19 13:43:27,810 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:34,575 DEBUG Query with image successful
2024-08-19 13:43:34,683 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:41,173 DEBUG Query with image successful
2024-08-19 13:43:41,497 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:48,235 DEBUG Query with image successful
2024-08-19 13:43:48,334 DEBUG Querying model with image: gpt-4o
2024-08-19 13:43:54,942 DEBUG Query with image successful
2024-08-19 13:43:55,276 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:01,935 DEBUG Query with image successful
2024-08-19 13:44:02,047 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:09,405 DEBUG Query with image successful
2024-08-19 13:44:09,727 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:17,227 DEBUG Query with image successful
2024-08-19 13:44:17,327 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:24,967 DEBUG Query with image successful
2024-08-19 13:44:25,396 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:33,072 DEBUG Query with image successful
2024-08-19 13:44:33,175 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:42,955 DEBUG Query with image successful
2024-08-19 13:44:43,283 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:50,171 DEBUG Query with image successful
2024-08-19 13:44:50,280 DEBUG Querying model with image: gpt-4o
2024-08-19 13:44:57,670 DEBUG Query with image successful
2024-08-19 13:44:57,992 DEBUG Querying model with image: gpt-4o
2024-08-19 13:45:05,310 DEBUG Query with image successful
2024-08-19 13:45:05,416 DEBUG Querying model with image: gpt-4o
2024-08-19 13:45:15,746 DEBUG Query with image successful
2024-08-19 13:45:16,074 DEBUG Querying model with image: gpt-4o
2024-08-19 13:45:22,355 DEBUG Query with image successful
2024-08-19 13:45:22,461 DEBUG Querying model with image: gpt-4o
2024-08-19 13:45:30,757 DEBUG Query with image successful
2024-08-19 13:45:31,141 DEBUG Querying model with image: gpt-4o
2024-08-19 13:45:37,932 DEBUG Query with image successful
2024-08-19 13:45:38,034 DEBUG Querying model with image: gpt-4o
2024-08-19 13:45:46,913 DEBUG Query with image successful
2024-08-19 13:45:47,262 DEBUG Querying model with image: gpt-4o
2024-08-19 13:45:54,072 DEBUG Query with image successful
2024-08-19 13:45:54,169 DEBUG Querying model with image: gpt-4o
2024-08-19 13:46:02,299 DEBUG Query with image successful
2024-08-19 13:46:02,636 DEBUG Querying model with image: gpt-4o
2024-08-19 13:46:12,234 DEBUG Query with image successful
2024-08-19 13:46:12,335 DEBUG Querying model with image: gpt-4o
2024-08-19 13:46:19,967 DEBUG Query with image successful
2024-08-19 13:46:20,296 DEBUG Querying model with image: gpt-4o
2024-08-19 13:46:29,315 DEBUG Query with image successful
2024-08-19 13:46:29,417 DEBUG Querying model with image: gpt-4o
2024-08-19 13:46:36,094 DEBUG Query with image successful
2024-08-19 13:46:36,405 DEBUG Querying model with image: gpt-4o
2024-08-19 13:46:43,935 DEBUG Query with image successful
2024-08-19 13:46:44,041 DEBUG Querying model with image: gpt-4o
2024-08-19 13:46:52,454 DEBUG Query with image successful
2024-08-19 13:46:52,783 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:00,915 DEBUG Query with image successful
2024-08-19 13:47:01,012 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:08,029 DEBUG Query with image successful
2024-08-19 13:47:08,352 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:15,313 DEBUG Query with image successful
2024-08-19 13:47:15,414 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:22,633 DEBUG Query with image successful
2024-08-19 13:47:22,953 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:31,554 DEBUG Query with image successful
2024-08-19 13:47:31,659 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:39,047 DEBUG Query with image successful
2024-08-19 13:47:39,350 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:46,576 DEBUG Query with image successful
2024-08-19 13:47:46,674 DEBUG Querying model with image: gpt-4o
2024-08-19 13:47:53,212 DEBUG Query with image successful
2024-08-19 13:47:53,500 DEBUG Querying model with image: gpt-4o
2024-08-19 13:48:00,359 DEBUG Query with image successful
2024-08-19 13:48:00,454 DEBUG Querying model with image: gpt-4o
2024-08-19 13:48:07,726 DEBUG Query with image successful
2024-08-19 13:48:08,010 DEBUG Querying model with image: gpt-4o
2024-08-19 13:48:16,873 DEBUG Query with image successful
2024-08-19 13:48:16,966 DEBUG Querying model with image: gpt-4o
2024-08-19 13:48:25,654 DEBUG Query with image successful
2024-08-19 13:59:29,281 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 13:59:29,308 DEBUG OpenAI client created
2024-08-19 13:59:34,482 DEBUG Querying model with image: gpt-4o
2024-08-19 13:59:44,475 DEBUG Query with image successful
2024-08-19 14:01:53,225 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 14:01:53,253 DEBUG OpenAI client created
2024-08-19 14:01:58,420 DEBUG Querying model with image: gpt-4o
2024-08-19 14:02:07,631 DEBUG Query with image successful
2024-08-19 14:02:07,731 DEBUG Querying model with image: gpt-4o
2024-08-19 14:02:16,940 DEBUG Query with image successful
2024-08-19 14:06:45,449 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 14:06:45,474 DEBUG OpenAI client created
2024-08-19 14:06:50,519 DEBUG Querying model with image: gpt-4o
2024-08-19 14:06:55,394 DEBUG Query with image successful
2024-08-19 14:06:55,422 DEBUG Querying model with image: gpt-4o
2024-08-19 14:07:00,674 DEBUG Query with image successful
2024-08-19 14:11:40,008 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 14:11:40,032 DEBUG OpenAI client created
2024-08-19 14:11:45,204 DEBUG Querying model with image: gpt-4o
2024-08-19 14:11:55,239 DEBUG Query with image successful
2024-08-19 14:11:55,330 DEBUG Querying model with image: gpt-4o
2024-08-19 14:12:03,384 DEBUG Query with image successful
2024-08-19 14:12:03,692 DEBUG Querying model with image: gpt-4o
2024-08-19 14:12:12,772 DEBUG Query with image successful
2024-08-19 14:12:12,864 DEBUG Querying model with image: gpt-4o
2024-08-19 14:12:22,362 DEBUG Query with image successful
2024-08-19 14:14:23,351 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 14:14:23,379 DEBUG OpenAI client created
2024-08-19 14:14:28,543 DEBUG Querying model with image: gpt-4o
2024-08-19 14:14:36,849 DEBUG Query with image successful
2024-08-19 14:14:36,942 DEBUG Querying model with image: gpt-4o
2024-08-19 14:14:45,403 DEBUG Query with image successful
2024-08-19 14:20:00,171 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 14:20:00,204 DEBUG OpenAI client created
2024-08-19 14:20:05,378 DEBUG Querying model with image: gpt-4o
2024-08-19 14:20:16,960 DEBUG Query with image successful
2024-08-19 14:20:17,061 DEBUG Querying model with image: gpt-4o
2024-08-19 14:20:27,252 DEBUG Query with image successful
2024-08-19 14:20:27,566 DEBUG Querying model with image: gpt-4o
2024-08-19 14:20:36,028 DEBUG Query with image successful
2024-08-19 14:20:36,129 DEBUG Querying model with image: gpt-4o
2024-08-19 14:20:46,604 DEBUG Query with image successful
2024-08-19 14:20:56,911 DEBUG Querying model with image: gpt-4o
2024-08-19 14:21:06,936 DEBUG Query with image successful
2024-08-19 14:21:07,039 DEBUG Querying model with image: gpt-4o
2024-08-19 14:21:16,107 DEBUG Query with image successful
2024-08-19 14:21:16,438 DEBUG Querying model with image: gpt-4o
2024-08-19 14:21:23,872 DEBUG Query with image successful
2024-08-19 14:21:23,971 DEBUG Querying model with image: gpt-4o
2024-08-19 14:21:31,647 DEBUG Query with image successful
2024-08-19 14:21:31,995 DEBUG Querying model with image: gpt-4o
2024-08-19 14:21:39,714 DEBUG Query with image successful
2024-08-19 14:21:39,819 DEBUG Querying model with image: gpt-4o
2024-08-19 14:21:48,572 DEBUG Query with image successful
2024-08-19 14:21:48,963 DEBUG Querying model with image: gpt-4o
2024-08-19 14:21:59,118 DEBUG Query with image successful
2024-08-19 14:21:59,218 DEBUG Querying model with image: gpt-4o
2024-08-19 14:22:06,827 DEBUG Query with image successful
2024-08-19 14:22:07,167 DEBUG Querying model with image: gpt-4o
2024-08-19 14:22:14,486 DEBUG Query with image successful
2024-08-19 14:22:14,589 DEBUG Querying model with image: gpt-4o
2024-08-19 14:22:21,658 DEBUG Query with image successful
2024-08-19 14:22:21,971 DEBUG Querying model with image: gpt-4o
2024-08-19 14:22:29,258 DEBUG Query with image successful
2024-08-19 14:22:29,365 DEBUG Querying model with image: gpt-4o
2024-08-19 14:22:38,380 DEBUG Query with image successful
2024-08-19 14:22:38,799 DEBUG Querying model with image: gpt-4o
2024-08-19 14:22:47,598 DEBUG Query with image successful
2024-08-19 14:22:47,706 DEBUG Querying model with image: gpt-4o
2024-08-19 14:22:54,509 DEBUG Query with image successful
2024-08-19 14:22:54,846 DEBUG Querying model with image: gpt-4o
2024-08-19 14:23:00,780 DEBUG Query with image successful
2024-08-19 14:23:00,881 DEBUG Querying model with image: gpt-4o
2024-08-19 14:23:11,190 DEBUG Query with image successful
2024-08-19 14:23:11,518 DEBUG Querying model with image: gpt-4o
2024-08-19 14:23:20,714 DEBUG Query with image successful
2024-08-19 14:23:20,813 DEBUG Querying model with image: gpt-4o
2024-08-19 14:23:29,031 DEBUG Query with image successful
2024-08-19 14:23:29,386 DEBUG Querying model with image: gpt-4o
2024-08-19 14:23:36,836 DEBUG Query with image successful
2024-08-19 14:23:36,941 DEBUG Querying model with image: gpt-4o
2024-08-19 14:23:44,862 DEBUG Query with image successful
2024-08-19 14:23:45,184 DEBUG Querying model with image: gpt-4o
2024-08-19 14:24:36,809 DEBUG Query with image successful
2024-08-19 14:24:36,909 DEBUG Querying model with image: gpt-4o
2024-08-19 14:24:50,926 DEBUG Query with image successful
2024-08-19 14:24:51,244 DEBUG Querying model with image: gpt-4o
2024-08-19 14:24:58,314 DEBUG Query with image successful
2024-08-19 14:24:58,423 DEBUG Querying model with image: gpt-4o
2024-08-19 14:25:05,840 DEBUG Query with image successful
2024-08-19 14:25:06,160 DEBUG Querying model with image: gpt-4o
2024-08-19 14:25:15,283 DEBUG Query with image successful
2024-08-19 14:25:15,386 DEBUG Querying model with image: gpt-4o
2024-08-19 14:25:23,188 DEBUG Query with image successful
2024-08-19 14:25:23,502 DEBUG Querying model with image: gpt-4o
2024-08-19 14:25:32,707 DEBUG Query with image successful
2024-08-19 14:25:32,809 DEBUG Querying model with image: gpt-4o
2024-08-19 14:25:41,359 DEBUG Query with image successful
2024-08-19 14:25:41,694 DEBUG Querying model with image: gpt-4o
2024-08-19 14:25:50,698 DEBUG Query with image successful
2024-08-19 14:25:50,811 DEBUG Querying model with image: gpt-4o
2024-08-19 14:25:58,451 DEBUG Query with image successful
2024-08-19 14:25:58,766 DEBUG Querying model with image: gpt-4o
2024-08-19 14:26:06,720 DEBUG Query with image successful
2024-08-19 14:26:06,825 DEBUG Querying model with image: gpt-4o
2024-08-19 14:26:13,908 DEBUG Query with image successful
2024-08-19 14:26:14,229 DEBUG Querying model with image: gpt-4o
2024-08-19 14:26:24,333 DEBUG Query with image successful
2024-08-19 14:26:24,433 DEBUG Querying model with image: gpt-4o
2024-08-19 14:26:32,254 DEBUG Query with image successful
2024-08-19 14:26:32,564 DEBUG Querying model with image: gpt-4o
2024-08-19 14:26:41,839 DEBUG Query with image successful
2024-08-19 14:26:41,942 DEBUG Querying model with image: gpt-4o
2024-08-19 14:26:48,783 DEBUG Query with image successful
2024-08-19 14:26:49,085 DEBUG Querying model with image: gpt-4o
2024-08-19 14:26:57,715 DEBUG Query with image successful
2024-08-19 14:26:57,822 DEBUG Querying model with image: gpt-4o
2024-08-19 14:27:05,507 DEBUG Query with image successful
2024-08-19 14:27:05,800 DEBUG Querying model with image: gpt-4o
2024-08-19 14:27:13,266 DEBUG Query with image successful
2024-08-19 14:27:13,361 DEBUG Querying model with image: gpt-4o
2024-08-19 14:27:20,934 DEBUG Query with image successful
2024-08-19 14:27:21,223 DEBUG Querying model with image: gpt-4o
2024-08-19 14:27:27,974 DEBUG Query with image successful
2024-08-19 14:27:28,074 DEBUG Querying model with image: gpt-4o
2024-08-19 14:27:35,856 DEBUG Query with image successful
2024-08-19 14:27:36,158 DEBUG Querying model with image: gpt-4o
2024-08-19 14:27:44,475 DEBUG Query with image successful
2024-08-19 14:27:44,580 DEBUG Querying model with image: gpt-4o
2024-08-19 14:27:51,961 DEBUG Query with image successful
2024-08-19 14:27:52,246 DEBUG Querying model with image: gpt-4o
2024-08-19 14:28:00,545 DEBUG Query with image successful
2024-08-19 14:28:00,647 DEBUG Querying model with image: gpt-4o
2024-08-19 14:28:08,975 DEBUG Query with image successful
2024-08-19 14:56:32,803 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 14:56:32,830 DEBUG OpenAI client created
2024-08-19 14:56:37,996 DEBUG Querying model with image: gpt-4o
2024-08-19 14:56:47,594 DEBUG Query with image successful
2024-08-19 14:56:47,696 DEBUG Querying model with image: gpt-4o
2024-08-19 14:57:00,059 DEBUG Query with image successful
2024-08-19 14:57:00,356 DEBUG Querying model with image: gpt-4o
2024-08-19 14:57:09,242 DEBUG Query with image successful
2024-08-19 14:57:09,344 DEBUG Querying model with image: gpt-4o
2024-08-19 14:57:16,171 DEBUG Query with image successful
2024-08-19 14:57:26,475 DEBUG Querying model with image: gpt-4o
2024-08-19 14:57:35,579 DEBUG Query with image successful
2024-08-19 14:57:35,679 DEBUG Querying model with image: gpt-4o
2024-08-19 14:57:45,432 DEBUG Query with image successful
2024-08-19 14:57:45,819 DEBUG Querying model with image: gpt-4o
2024-08-19 14:57:55,751 DEBUG Query with image successful
2024-08-19 14:57:55,851 DEBUG Querying model with image: gpt-4o
2024-08-19 14:58:07,151 DEBUG Query with image successful
2024-08-19 14:58:07,476 DEBUG Querying model with image: gpt-4o
2024-08-19 14:58:15,444 DEBUG Query with image successful
2024-08-19 14:58:15,548 DEBUG Querying model with image: gpt-4o
2024-08-19 14:58:24,668 DEBUG Query with image successful
2024-08-19 14:58:25,071 DEBUG Querying model with image: gpt-4o
2024-08-19 14:58:34,453 DEBUG Query with image successful
2024-08-19 14:58:34,552 DEBUG Querying model with image: gpt-4o
2024-08-19 14:58:43,041 DEBUG Query with image successful
2024-08-19 14:58:43,400 DEBUG Querying model with image: gpt-4o
2024-08-19 14:58:51,048 DEBUG Query with image successful
2024-08-19 14:58:51,152 DEBUG Querying model with image: gpt-4o
2024-08-19 14:58:59,559 DEBUG Query with image successful
2024-08-19 14:58:59,900 DEBUG Querying model with image: gpt-4o
2024-08-19 14:59:07,837 DEBUG Query with image successful
2024-08-19 14:59:07,942 DEBUG Querying model with image: gpt-4o
2024-08-19 14:59:16,719 DEBUG Query with image successful
2024-08-19 14:59:17,092 DEBUG Querying model with image: gpt-4o
2024-08-19 14:59:25,281 DEBUG Query with image successful
2024-08-19 14:59:25,384 DEBUG Querying model with image: gpt-4o
2024-08-19 14:59:35,240 DEBUG Query with image successful
2024-08-19 14:59:35,568 DEBUG Querying model with image: gpt-4o
2024-08-19 14:59:42,853 DEBUG Query with image successful
2024-08-19 14:59:42,957 DEBUG Querying model with image: gpt-4o
2024-08-19 14:59:49,820 DEBUG Query with image successful
2024-08-19 14:59:50,171 DEBUG Querying model with image: gpt-4o
2024-08-19 14:59:58,177 DEBUG Query with image successful
2024-08-19 14:59:58,277 DEBUG Querying model with image: gpt-4o
2024-08-19 15:00:08,788 DEBUG Query with image successful
2024-08-19 15:00:09,195 DEBUG Querying model with image: gpt-4o
2024-08-19 15:00:16,352 DEBUG Query with image successful
2024-08-19 15:00:16,455 DEBUG Querying model with image: gpt-4o
2024-08-19 15:00:25,147 DEBUG Query with image successful
2024-08-19 15:00:25,475 DEBUG Querying model with image: gpt-4o
2024-08-19 15:00:32,436 DEBUG Query with image successful
2024-08-19 15:00:32,547 DEBUG Querying model with image: gpt-4o
2024-08-19 15:00:41,738 DEBUG Query with image successful
2024-08-19 15:00:42,059 DEBUG Querying model with image: gpt-4o
2024-08-19 15:00:51,961 DEBUG Query with image successful
2024-08-19 15:00:52,065 DEBUG Querying model with image: gpt-4o
2024-08-19 15:01:01,761 DEBUG Query with image successful
2024-08-19 15:01:02,082 DEBUG Querying model with image: gpt-4o
2024-08-19 15:01:12,270 DEBUG Query with image successful
2024-08-19 15:01:12,375 DEBUG Querying model with image: gpt-4o
2024-08-19 15:01:19,402 DEBUG Query with image successful
2024-08-19 15:01:19,716 DEBUG Querying model with image: gpt-4o
2024-08-19 15:01:26,638 DEBUG Query with image successful
2024-08-19 15:01:26,741 DEBUG Querying model with image: gpt-4o
2024-08-19 15:01:33,379 DEBUG Query with image successful
2024-08-19 15:01:33,704 DEBUG Querying model with image: gpt-4o
2024-08-19 15:01:41,610 DEBUG Query with image successful
2024-08-19 15:01:41,729 DEBUG Querying model with image: gpt-4o
2024-08-19 15:01:51,637 DEBUG Query with image successful
2024-08-19 15:01:52,022 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:01,383 DEBUG Query with image successful
2024-08-19 15:02:01,486 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:07,932 DEBUG Query with image successful
2024-08-19 15:02:08,252 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:17,567 DEBUG Query with image successful
2024-08-19 15:02:17,666 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:23,269 DEBUG Query with image successful
2024-08-19 15:02:23,574 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:30,950 DEBUG Query with image successful
2024-08-19 15:02:31,052 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:37,267 DEBUG Query with image successful
2024-08-19 15:02:37,761 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:46,036 DEBUG Query with image successful
2024-08-19 15:02:46,142 DEBUG Querying model with image: gpt-4o
2024-08-19 15:02:56,431 DEBUG Query with image successful
2024-08-19 15:02:56,727 DEBUG Querying model with image: gpt-4o
2024-08-19 15:03:05,772 DEBUG Query with image successful
2024-08-19 15:03:05,870 DEBUG Querying model with image: gpt-4o
2024-08-19 15:03:13,320 DEBUG Query with image successful
2024-08-19 15:03:13,638 DEBUG Querying model with image: gpt-4o
2024-08-19 15:03:19,656 DEBUG Query with image successful
2024-08-19 15:03:19,752 DEBUG Querying model with image: gpt-4o
2024-08-19 15:03:29,764 DEBUG Query with image successful
2024-08-19 15:03:30,078 DEBUG Querying model with image: gpt-4o
2024-08-19 15:03:36,682 DEBUG Query with image successful
2024-08-19 15:03:36,781 DEBUG Querying model with image: gpt-4o
2024-08-19 15:03:44,562 DEBUG Query with image successful
2024-08-19 15:03:44,858 DEBUG Querying model with image: gpt-4o
2024-08-19 15:03:51,601 DEBUG Query with image successful
2024-08-19 15:03:51,697 DEBUG Querying model with image: gpt-4o
2024-08-19 15:04:02,594 DEBUG Query with image successful
2024-08-19 15:19:14,523 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-19 15:19:14,549 DEBUG OpenAI client created
2024-08-19 15:19:19,728 DEBUG Querying model with image: gpt-4o
2024-08-19 15:19:29,613 DEBUG Query with image successful
2024-08-19 15:19:29,706 DEBUG Querying model with image: gpt-4o
2024-08-19 15:19:39,779 DEBUG Query with image successful
2024-08-19 15:19:40,170 DEBUG Querying model with image: gpt-4o
2024-08-19 15:19:52,961 DEBUG Query with image successful
2024-08-19 15:19:53,059 DEBUG Querying model with image: gpt-4o
2024-08-19 15:20:01,337 DEBUG Query with image successful
2024-08-19 15:20:11,660 DEBUG Querying model with image: gpt-4o
2024-08-19 15:20:21,437 DEBUG Query with image successful
2024-08-19 15:20:21,539 DEBUG Querying model with image: gpt-4o
2024-08-19 15:20:32,497 DEBUG Query with image successful
2024-08-19 15:20:32,830 DEBUG Querying model with image: gpt-4o
2024-08-19 15:20:41,271 DEBUG Query with image successful
2024-08-19 15:20:41,371 DEBUG Querying model with image: gpt-4o
2024-08-19 15:20:49,767 DEBUG Query with image successful
2024-08-19 15:20:50,215 DEBUG Querying model with image: gpt-4o
2024-08-19 15:20:57,965 DEBUG Query with image successful
2024-08-19 15:20:58,067 DEBUG Querying model with image: gpt-4o
2024-08-19 15:21:09,098 DEBUG Query with image successful
2024-08-19 15:21:09,417 DEBUG Querying model with image: gpt-4o
2024-08-19 15:21:19,230 DEBUG Query with image successful
2024-08-19 15:21:19,328 DEBUG Querying model with image: gpt-4o
2024-08-19 15:21:27,469 DEBUG Query with image successful
2024-08-19 15:21:27,778 DEBUG Querying model with image: gpt-4o
2024-08-19 15:21:37,094 DEBUG Query with image successful
2024-08-19 15:21:37,192 DEBUG Querying model with image: gpt-4o
2024-08-19 15:21:46,846 DEBUG Query with image successful
2024-08-19 15:21:47,190 DEBUG Querying model with image: gpt-4o
2024-08-19 15:21:54,571 DEBUG Query with image successful
2024-08-19 15:21:54,671 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:01,967 DEBUG Query with image successful
2024-08-19 15:22:02,377 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:10,765 DEBUG Query with image successful
2024-08-19 15:22:10,891 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:19,610 DEBUG Query with image successful
2024-08-19 15:22:19,943 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:27,649 DEBUG Query with image successful
2024-08-19 15:22:27,746 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:35,977 DEBUG Query with image successful
2024-08-19 15:22:36,299 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:43,206 DEBUG Query with image successful
2024-08-19 15:22:43,305 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:51,398 DEBUG Query with image successful
2024-08-19 15:22:51,724 DEBUG Querying model with image: gpt-4o
2024-08-19 15:22:58,682 DEBUG Query with image successful
2024-08-19 15:22:58,792 DEBUG Querying model with image: gpt-4o
2024-08-19 15:23:05,680 DEBUG Query with image successful
2024-08-19 15:23:06,039 DEBUG Querying model with image: gpt-4o
2024-08-19 15:23:14,522 DEBUG Query with image successful
2024-08-19 15:23:14,617 DEBUG Querying model with image: gpt-4o
2024-08-19 15:23:26,819 DEBUG Query with image successful
2024-08-19 15:23:27,145 DEBUG Querying model with image: gpt-4o
2024-08-19 15:23:35,772 DEBUG Query with image successful
2024-08-19 15:23:35,873 DEBUG Querying model with image: gpt-4o
2024-08-19 15:23:43,370 DEBUG Query with image successful
2024-08-19 15:23:43,704 DEBUG Querying model with image: gpt-4o
2024-08-19 15:23:50,226 DEBUG Query with image successful
2024-08-19 15:23:50,331 DEBUG Querying model with image: gpt-4o
2024-08-19 15:23:57,419 DEBUG Query with image successful
2024-08-19 15:23:57,724 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:04,381 DEBUG Query with image successful
2024-08-19 15:24:04,482 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:13,848 DEBUG Query with image successful
2024-08-19 15:24:14,184 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:20,955 DEBUG Query with image successful
2024-08-19 15:24:21,058 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:30,965 DEBUG Query with image successful
2024-08-19 15:24:31,277 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:38,483 DEBUG Query with image successful
2024-08-19 15:24:38,582 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:45,996 DEBUG Query with image successful
2024-08-19 15:24:46,300 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:52,396 DEBUG Query with image successful
2024-08-19 15:24:52,492 DEBUG Querying model with image: gpt-4o
2024-08-19 15:24:59,961 DEBUG Query with image successful
2024-08-19 15:25:00,265 DEBUG Querying model with image: gpt-4o
2024-08-19 15:25:07,765 DEBUG Query with image successful
2024-08-19 15:25:07,865 DEBUG Querying model with image: gpt-4o
2024-08-19 15:25:15,523 DEBUG Query with image successful
2024-08-19 15:25:15,828 DEBUG Querying model with image: gpt-4o
2024-08-19 15:25:22,949 DEBUG Query with image successful
2024-08-19 15:25:23,049 DEBUG Querying model with image: gpt-4o
2024-08-19 15:25:34,819 DEBUG Query with image successful
2024-08-19 15:25:35,128 DEBUG Querying model with image: gpt-4o
2024-08-19 15:25:44,922 DEBUG Query with image successful
2024-08-19 15:25:45,027 DEBUG Querying model with image: gpt-4o
2024-08-19 15:25:51,519 DEBUG Query with image successful
2024-08-19 15:25:51,813 DEBUG Querying model with image: gpt-4o
2024-08-19 15:25:58,896 DEBUG Query with image successful
2024-08-19 15:25:59,010 DEBUG Querying model with image: gpt-4o
2024-08-19 15:26:05,847 DEBUG Query with image successful
2024-08-19 15:26:06,148 DEBUG Querying model with image: gpt-4o
2024-08-19 15:26:13,220 DEBUG Query with image successful
2024-08-19 15:26:13,318 DEBUG Querying model with image: gpt-4o
2024-08-19 15:26:19,504 DEBUG Query with image successful
2024-08-19 15:26:19,814 DEBUG Querying model with image: gpt-4o
2024-08-19 15:26:28,572 DEBUG Query with image successful
2024-08-19 15:26:28,673 DEBUG Querying model with image: gpt-4o
2024-08-19 15:26:38,951 DEBUG Query with image successful
2024-08-21 11:01:10,142 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-21 11:01:10,221 DEBUG OpenAI client created
2024-08-21 11:01:15,464 DEBUG Querying model with image: gpt-4o
2024-08-21 11:01:24,505 DEBUG Query with image successful
2024-08-21 11:01:24,602 DEBUG Querying model with image: gpt-4o
2024-08-21 11:01:33,990 DEBUG Query with image successful
2024-08-21 11:01:34,383 DEBUG Querying model with image: gpt-4o
2024-08-21 11:01:41,875 DEBUG Query with image successful
2024-08-21 11:01:41,972 DEBUG Querying model with image: gpt-4o
2024-08-21 11:01:50,450 DEBUG Query with image successful
2024-08-21 11:02:00,760 DEBUG Querying model with image: gpt-4o
2024-08-21 11:02:10,850 DEBUG Query with image successful
2024-08-21 11:02:10,949 DEBUG Querying model with image: gpt-4o
2024-08-21 11:02:17,536 DEBUG Query with image successful
2024-08-21 11:02:17,902 DEBUG Querying model with image: gpt-4o
2024-08-21 11:02:25,564 DEBUG Query with image successful
2024-08-21 11:02:25,662 DEBUG Querying model with image: gpt-4o
2024-08-21 11:02:34,583 DEBUG Query with image successful
2024-08-21 11:02:34,895 DEBUG Querying model with image: gpt-4o
2024-08-21 11:02:42,160 DEBUG Query with image successful
2024-08-21 11:02:42,262 DEBUG Querying model with image: gpt-4o
2024-08-21 11:02:51,349 DEBUG Query with image successful
2024-08-21 11:02:51,688 DEBUG Querying model with image: gpt-4o
2024-08-21 11:03:02,290 DEBUG Query with image successful
2024-08-21 11:03:02,423 DEBUG Querying model with image: gpt-4o
2024-08-21 11:03:09,099 DEBUG Query with image successful
2024-08-21 11:03:09,563 DEBUG Querying model with image: gpt-4o
2024-08-21 11:03:16,444 DEBUG Query with image successful
2024-08-21 11:03:16,549 DEBUG Querying model with image: gpt-4o
2024-08-21 11:03:26,134 DEBUG Query with image successful
2024-08-21 11:03:26,475 DEBUG Querying model with image: gpt-4o
2024-08-21 11:03:34,595 DEBUG Query with image successful
2024-08-21 11:03:34,696 DEBUG Querying model with image: gpt-4o
2024-08-21 11:03:42,429 DEBUG Query with image successful
2024-08-21 11:03:42,741 DEBUG Querying model with image: gpt-4o
2024-08-21 11:03:50,115 DEBUG Query with image successful
2024-08-21 11:03:50,217 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:00,397 DEBUG Query with image successful
2024-08-21 11:04:00,723 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:08,369 DEBUG Query with image successful
2024-08-21 11:04:08,477 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:15,650 DEBUG Query with image successful
2024-08-21 11:04:15,996 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:24,681 DEBUG Query with image successful
2024-08-21 11:04:24,788 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:31,985 DEBUG Query with image successful
2024-08-21 11:04:32,305 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:38,802 DEBUG Query with image successful
2024-08-21 11:04:38,915 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:46,540 DEBUG Query with image successful
2024-08-21 11:04:46,885 DEBUG Querying model with image: gpt-4o
2024-08-21 11:04:56,677 DEBUG Query with image successful
2024-08-21 11:04:56,785 DEBUG Querying model with image: gpt-4o
2024-08-21 11:05:04,655 DEBUG Query with image successful
2024-08-21 11:05:04,952 DEBUG Querying model with image: gpt-4o
2024-08-21 11:05:13,523 DEBUG Query with image successful
2024-08-21 11:05:13,625 DEBUG Querying model with image: gpt-4o
2024-08-21 11:05:21,246 DEBUG Query with image successful
2024-08-21 11:05:21,555 DEBUG Querying model with image: gpt-4o
2024-08-21 11:05:29,223 DEBUG Query with image successful
2024-08-21 11:05:29,318 DEBUG Querying model with image: gpt-4o
2024-08-21 11:05:39,139 DEBUG Query with image successful
2024-08-21 11:05:39,454 DEBUG Querying model with image: gpt-4o
2024-08-21 11:05:48,119 DEBUG Query with image successful
2024-08-21 11:05:48,222 DEBUG Querying model with image: gpt-4o
2024-08-21 11:05:58,533 DEBUG Query with image successful
2024-08-21 11:05:58,852 DEBUG Querying model with image: gpt-4o
2024-08-21 11:06:07,295 DEBUG Query with image successful
2024-08-21 11:06:07,404 DEBUG Querying model with image: gpt-4o
2024-08-21 11:06:16,506 DEBUG Query with image successful
2024-08-21 11:06:16,822 DEBUG Querying model with image: gpt-4o
2024-08-21 11:06:26,224 DEBUG Query with image successful
2024-08-21 11:06:26,321 DEBUG Querying model with image: gpt-4o
2024-08-21 11:06:33,562 DEBUG Query with image successful
2024-08-21 11:06:33,915 DEBUG Querying model with image: gpt-4o
2024-08-21 11:06:42,824 DEBUG Query with image successful
2024-08-21 11:06:42,934 DEBUG Querying model with image: gpt-4o
2024-08-21 11:06:49,335 DEBUG Query with image successful
2024-08-21 11:06:49,646 DEBUG Querying model with image: gpt-4o
2024-08-21 11:06:58,073 DEBUG Query with image successful
2024-08-21 11:06:58,176 DEBUG Querying model with image: gpt-4o
2024-08-21 11:07:06,309 DEBUG Query with image successful
2024-08-21 11:07:06,623 DEBUG Querying model with image: gpt-4o
2024-08-21 11:07:15,865 DEBUG Query with image successful
2024-08-21 11:07:15,974 DEBUG Querying model with image: gpt-4o
2024-08-21 11:07:23,750 DEBUG Query with image successful
2024-08-21 11:07:24,078 DEBUG Querying model with image: gpt-4o
2024-08-21 11:07:32,365 DEBUG Query with image successful
2024-08-21 11:07:32,480 DEBUG Querying model with image: gpt-4o
2024-08-21 11:07:40,248 DEBUG Query with image successful
2024-08-21 11:07:40,583 DEBUG Querying model with image: gpt-4o
2024-08-21 11:07:48,622 DEBUG Query with image successful
2024-08-21 11:07:48,724 DEBUG Querying model with image: gpt-4o
2024-08-21 11:07:56,039 DEBUG Query with image successful
2024-08-21 11:07:56,337 DEBUG Querying model with image: gpt-4o
2024-08-21 11:08:04,570 DEBUG Query with image successful
2024-08-21 11:08:04,664 DEBUG Querying model with image: gpt-4o
2024-08-21 11:08:13,565 DEBUG Query with image successful
2024-08-21 11:08:13,848 DEBUG Querying model with image: gpt-4o
2024-08-21 11:08:23,883 DEBUG Query with image successful
2024-08-21 11:08:23,983 DEBUG Querying model with image: gpt-4o
2024-08-21 11:08:31,574 DEBUG Query with image successful
2024-08-22 13:43:07,357 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 13:43:07,496 DEBUG OpenAI client created
2024-08-22 13:50:49,557 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 13:50:49,581 DEBUG OpenAI client created
2024-08-22 13:50:54,752 DEBUG Querying model with image: gpt-4o
2024-08-22 13:51:13,437 DEBUG Query with image successful
2024-08-22 13:51:13,527 DEBUG Querying model with image: gpt-4o
2024-08-22 13:51:22,332 DEBUG Query with image successful
2024-08-22 13:51:22,766 DEBUG Querying model with image: gpt-4o
2024-08-22 13:51:30,681 DEBUG Query with image successful
2024-08-22 13:51:30,781 DEBUG Querying model with image: gpt-4o
2024-08-22 13:51:51,056 DEBUG Query with image successful
2024-08-22 13:52:01,366 DEBUG Querying model with image: gpt-4o
2024-08-22 13:52:10,227 DEBUG Query with image successful
2024-08-22 13:52:10,330 DEBUG Querying model with image: gpt-4o
2024-08-22 13:52:22,871 DEBUG Query with image successful
2024-08-22 13:52:23,205 DEBUG Querying model with image: gpt-4o
2024-08-22 13:52:30,740 DEBUG Query with image successful
2024-08-22 13:52:30,830 DEBUG Querying model with image: gpt-4o
2024-08-22 13:52:44,635 DEBUG Query with image successful
2024-08-22 13:52:44,966 DEBUG Querying model with image: gpt-4o
2024-08-22 13:52:52,713 DEBUG Query with image successful
2024-08-22 13:52:52,817 DEBUG Querying model with image: gpt-4o
2024-08-22 13:53:04,953 DEBUG Query with image successful
2024-08-22 13:53:05,285 DEBUG Querying model with image: gpt-4o
2024-08-22 13:53:13,014 DEBUG Query with image successful
2024-08-22 13:53:13,109 DEBUG Querying model with image: gpt-4o
2024-08-22 13:53:25,340 DEBUG Query with image successful
2024-08-22 13:53:25,659 DEBUG Querying model with image: gpt-4o
2024-08-22 13:53:33,138 DEBUG Query with image successful
2024-08-22 13:53:33,240 DEBUG Querying model with image: gpt-4o
2024-08-22 13:53:40,706 DEBUG Query with image successful
2024-08-22 13:53:41,012 DEBUG Querying model with image: gpt-4o
2024-08-22 13:53:49,485 DEBUG Query with image successful
2024-08-22 13:53:49,577 DEBUG Querying model with image: gpt-4o
2024-08-22 13:53:59,229 DEBUG Query with image successful
2024-08-22 13:53:59,558 DEBUG Querying model with image: gpt-4o
2024-08-22 13:54:08,601 DEBUG Query with image successful
2024-08-22 13:54:08,710 DEBUG Querying model with image: gpt-4o
2024-08-22 13:54:16,049 DEBUG Query with image successful
2024-08-22 13:54:16,360 DEBUG Querying model with image: gpt-4o
2024-08-22 13:54:31,219 DEBUG Query with image successful
2024-08-22 13:54:31,322 DEBUG Querying model with image: gpt-4o
2024-08-22 13:54:38,720 DEBUG Query with image successful
2024-08-22 13:54:39,018 DEBUG Querying model with image: gpt-4o
2024-08-22 13:54:50,196 DEBUG Query with image successful
2024-08-22 13:54:50,289 DEBUG Querying model with image: gpt-4o
2024-08-22 13:54:57,803 DEBUG Query with image successful
2024-08-22 13:54:58,136 DEBUG Querying model with image: gpt-4o
2024-08-22 13:55:07,980 DEBUG Query with image successful
2024-08-22 13:55:08,076 DEBUG Querying model with image: gpt-4o
2024-08-22 13:55:15,745 DEBUG Query with image successful
2024-08-22 13:55:16,061 DEBUG Querying model with image: gpt-4o
2024-08-22 13:55:23,338 DEBUG Query with image successful
2024-08-22 13:55:23,444 DEBUG Querying model with image: gpt-4o
2024-08-22 13:55:31,479 DEBUG Query with image successful
2024-08-22 13:55:31,774 DEBUG Querying model with image: gpt-4o
2024-08-22 13:55:39,063 DEBUG Query with image successful
2024-08-22 13:55:39,166 DEBUG Querying model with image: gpt-4o
2024-08-22 13:55:45,706 DEBUG Query with image successful
2024-08-22 13:55:46,003 DEBUG Querying model with image: gpt-4o
2024-08-22 13:55:54,091 DEBUG Query with image successful
2024-08-22 13:55:54,188 DEBUG Querying model with image: gpt-4o
2024-08-22 13:56:00,707 DEBUG Query with image successful
2024-08-22 13:56:01,020 DEBUG Querying model with image: gpt-4o
2024-08-22 13:56:12,372 DEBUG Query with image successful
2024-08-22 13:56:12,461 DEBUG Querying model with image: gpt-4o
2024-08-22 13:56:20,151 DEBUG Query with image successful
2024-08-22 13:56:20,444 DEBUG Querying model with image: gpt-4o
2024-08-22 13:56:29,989 DEBUG Query with image successful
2024-08-22 13:56:30,083 DEBUG Querying model with image: gpt-4o
2024-08-22 13:56:38,104 DEBUG Query with image successful
2024-08-22 13:56:38,392 DEBUG Querying model with image: gpt-4o
2024-08-22 13:56:47,195 DEBUG Query with image successful
2024-08-22 13:56:47,293 DEBUG Querying model with image: gpt-4o
2024-08-22 13:56:53,584 DEBUG Query with image successful
2024-08-22 13:56:53,872 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:04,144 DEBUG Query with image successful
2024-08-22 13:57:04,240 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:10,911 DEBUG Query with image successful
2024-08-22 13:57:11,210 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:19,881 DEBUG Query with image successful
2024-08-22 13:57:19,975 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:26,096 DEBUG Query with image successful
2024-08-22 13:57:26,381 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:34,714 DEBUG Query with image successful
2024-08-22 13:57:34,812 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:41,250 DEBUG Query with image successful
2024-08-22 13:57:41,544 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:49,770 DEBUG Query with image successful
2024-08-22 13:57:49,864 DEBUG Querying model with image: gpt-4o
2024-08-22 13:57:57,241 DEBUG Query with image successful
2024-08-22 13:57:57,554 DEBUG Querying model with image: gpt-4o
2024-08-22 13:58:08,407 DEBUG Query with image successful
2024-08-22 13:58:08,505 DEBUG Querying model with image: gpt-4o
2024-08-22 13:58:15,410 DEBUG Query with image successful
2024-08-22 13:58:15,694 DEBUG Querying model with image: gpt-4o
2024-08-22 13:58:23,352 DEBUG Query with image successful
2024-08-22 13:58:23,439 DEBUG Querying model with image: gpt-4o
2024-08-22 13:58:29,111 DEBUG Query with image successful
2024-08-22 13:58:29,402 DEBUG Querying model with image: gpt-4o
2024-08-22 13:58:36,899 DEBUG Query with image successful
2024-08-22 13:58:36,994 DEBUG Querying model with image: gpt-4o
2024-08-22 13:58:45,714 DEBUG Query with image successful
2024-08-22 14:59:31,403 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 14:59:31,427 DEBUG OpenAI client created
2024-08-22 14:59:36,595 DEBUG Querying model with image: gpt-4o
2024-08-22 14:59:46,457 DEBUG Query with image successful
2024-08-22 14:59:46,540 DEBUG Querying model with image: gpt-4o
2024-08-22 14:59:53,834 DEBUG Query with image successful
2024-08-22 14:59:54,190 DEBUG Querying model with image: gpt-4o
2024-08-22 15:00:02,789 DEBUG Query with image successful
2024-08-22 15:00:02,874 DEBUG Querying model with image: gpt-4o
2024-08-22 15:00:22,854 DEBUG Query with image successful
2024-08-22 15:00:33,126 DEBUG Querying model with image: gpt-4o
2024-08-22 15:00:43,876 DEBUG Query with image successful
2024-08-22 15:00:43,961 DEBUG Querying model with image: gpt-4o
2024-08-22 15:00:58,427 DEBUG Query with image successful
2024-08-22 15:00:58,739 DEBUG Querying model with image: gpt-4o
2024-08-22 15:01:16,677 DEBUG Query with image successful
2024-08-22 15:01:16,768 DEBUG Querying model with image: gpt-4o
2024-08-22 15:01:30,105 DEBUG Query with image successful
2024-08-22 15:01:30,587 DEBUG Querying model with image: gpt-4o
2024-08-22 15:01:49,708 DEBUG Query with image successful
2024-08-22 15:01:49,811 DEBUG Querying model with image: gpt-4o
2024-08-22 15:02:07,903 DEBUG Query with image successful
2024-08-22 15:02:08,236 DEBUG Querying model with image: gpt-4o
2024-08-22 15:02:22,446 DEBUG Query with image successful
2024-08-22 15:02:22,542 DEBUG Querying model with image: gpt-4o
2024-08-22 15:02:40,554 DEBUG Query with image successful
2024-08-22 15:02:40,994 DEBUG Querying model with image: gpt-4o
2024-08-22 15:03:00,094 DEBUG Query with image successful
2024-08-22 15:03:00,212 DEBUG Querying model with image: gpt-4o
2024-08-22 15:03:12,294 DEBUG Query with image successful
2024-08-22 15:03:12,716 DEBUG Querying model with image: gpt-4o
2024-08-22 15:03:28,540 DEBUG Query with image successful
2024-08-22 15:03:28,635 DEBUG Querying model with image: gpt-4o
2024-08-22 15:03:47,904 DEBUG Query with image successful
2024-08-22 15:03:48,222 DEBUG Querying model with image: gpt-4o
2024-08-22 15:04:03,569 DEBUG Query with image successful
2024-08-22 15:04:03,672 DEBUG Querying model with image: gpt-4o
2024-08-22 15:04:18,477 DEBUG Query with image successful
2024-08-22 15:04:18,783 DEBUG Querying model with image: gpt-4o
2024-08-22 15:04:34,009 DEBUG Query with image successful
2024-08-22 15:04:34,109 DEBUG Querying model with image: gpt-4o
2024-08-22 15:04:44,300 DEBUG Query with image successful
2024-08-22 15:04:44,641 DEBUG Querying model with image: gpt-4o
2024-08-22 15:05:12,196 DEBUG Query with image successful
2024-08-22 15:05:12,289 DEBUG Querying model with image: gpt-4o
2024-08-22 15:05:21,241 DEBUG Query with image successful
2024-08-22 15:05:21,548 DEBUG Querying model with image: gpt-4o
2024-08-22 15:05:33,159 DEBUG Query with image successful
2024-08-22 15:05:33,256 DEBUG Querying model with image: gpt-4o
2024-08-22 15:05:42,865 DEBUG Query with image successful
2024-08-22 15:05:43,156 DEBUG Querying model with image: gpt-4o
2024-08-22 15:05:54,560 DEBUG Query with image successful
2024-08-22 15:05:54,662 DEBUG Querying model with image: gpt-4o
2024-08-22 15:06:04,193 DEBUG Query with image successful
2024-08-22 15:06:04,500 DEBUG Querying model with image: gpt-4o
2024-08-22 15:06:18,695 DEBUG Query with image successful
2024-08-22 15:06:18,794 DEBUG Querying model with image: gpt-4o
2024-08-22 15:06:31,782 DEBUG Query with image successful
2024-08-22 15:06:32,102 DEBUG Querying model with image: gpt-4o
2024-08-22 15:06:40,665 DEBUG Query with image successful
2024-08-22 15:06:40,764 DEBUG Querying model with image: gpt-4o
2024-08-22 15:06:47,264 DEBUG Query with image successful
2024-08-22 15:06:47,556 DEBUG Querying model with image: gpt-4o
2024-08-22 15:06:54,216 DEBUG Query with image successful
2024-08-22 15:06:54,307 DEBUG Querying model with image: gpt-4o
2024-08-22 15:07:02,644 DEBUG Query with image successful
2024-08-22 15:07:02,945 DEBUG Querying model with image: gpt-4o
2024-08-22 15:07:13,257 DEBUG Query with image successful
2024-08-22 15:07:13,351 DEBUG Querying model with image: gpt-4o
2024-08-22 15:07:20,861 DEBUG Query with image successful
2024-08-22 15:07:21,206 DEBUG Querying model with image: gpt-4o
2024-08-22 15:07:33,095 DEBUG Query with image successful
2024-08-22 15:07:33,194 DEBUG Querying model with image: gpt-4o
2024-08-22 15:07:46,081 DEBUG Query with image successful
2024-08-22 15:07:46,374 DEBUG Querying model with image: gpt-4o
2024-08-22 15:07:58,276 DEBUG Query with image successful
2024-08-22 15:07:58,382 DEBUG Querying model with image: gpt-4o
2024-08-22 15:08:10,746 DEBUG Query with image successful
2024-08-22 15:08:11,036 DEBUG Querying model with image: gpt-4o
2024-08-22 15:08:27,192 DEBUG Query with image successful
2024-08-22 15:08:27,290 DEBUG Querying model with image: gpt-4o
2024-08-22 15:08:37,197 DEBUG Query with image successful
2024-08-22 15:08:37,508 DEBUG Querying model with image: gpt-4o
2024-08-22 15:08:50,046 DEBUG Query with image successful
2024-08-22 15:08:50,145 DEBUG Querying model with image: gpt-4o
2024-08-22 15:09:03,702 DEBUG Query with image successful
2024-08-22 15:09:04,012 DEBUG Querying model with image: gpt-4o
2024-08-22 15:09:17,781 DEBUG Query with image successful
2024-08-22 15:09:17,878 DEBUG Querying model with image: gpt-4o
2024-08-22 15:09:35,290 DEBUG Query with image successful
2024-08-22 15:09:35,580 DEBUG Querying model with image: gpt-4o
2024-08-22 15:09:47,765 DEBUG Query with image successful
2024-08-22 15:09:47,854 DEBUG Querying model with image: gpt-4o
2024-08-22 15:10:04,300 DEBUG Query with image successful
2024-08-22 15:10:04,598 DEBUG Querying model with image: gpt-4o
2024-08-22 15:10:18,752 DEBUG Query with image successful
2024-08-22 15:10:18,842 DEBUG Querying model with image: gpt-4o
2024-08-22 15:10:35,696 DEBUG Query with image successful
2024-08-22 15:10:36,024 DEBUG Querying model with image: gpt-4o
2024-08-22 15:10:49,376 DEBUG Query with image successful
2024-08-22 15:10:49,467 DEBUG Querying model with image: gpt-4o
2024-08-22 15:11:04,491 DEBUG Query with image successful
2024-08-22 15:41:24,465 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 15:41:24,490 DEBUG OpenAI client created
2024-08-22 15:41:29,649 DEBUG Querying model with image: gpt-4o
2024-08-22 15:41:39,341 DEBUG Query with image successful
2024-08-22 15:41:39,428 DEBUG Querying model with image: gpt-4o
2024-08-22 15:41:45,878 DEBUG Query with image successful
2024-08-22 15:41:46,220 DEBUG Querying model with image: gpt-4o
2024-08-22 15:41:53,055 DEBUG Query with image successful
2024-08-22 15:41:53,147 DEBUG Querying model with image: gpt-4o
2024-08-22 15:42:00,919 DEBUG Query with image successful
2024-08-22 15:42:11,235 DEBUG Querying model with image: gpt-4o
2024-08-22 15:42:19,494 DEBUG Query with image successful
2024-08-22 15:42:19,589 DEBUG Querying model with image: gpt-4o
2024-08-22 15:42:29,030 DEBUG Query with image successful
2024-08-22 15:42:29,343 DEBUG Querying model with image: gpt-4o
2024-08-22 15:42:35,945 DEBUG Query with image successful
2024-08-22 15:42:36,038 DEBUG Querying model with image: gpt-4o
2024-08-22 15:42:42,013 DEBUG Query with image successful
2024-08-22 15:42:42,389 DEBUG Querying model with image: gpt-4o
2024-08-22 15:42:48,832 DEBUG Query with image successful
2024-08-22 15:42:48,931 DEBUG Querying model with image: gpt-4o
2024-08-22 15:42:59,752 DEBUG Query with image successful
2024-08-22 15:43:00,082 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:06,762 DEBUG Query with image successful
2024-08-22 15:43:06,857 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:13,158 DEBUG Query with image successful
2024-08-22 15:43:13,524 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:20,403 DEBUG Query with image successful
2024-08-22 15:43:20,500 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:28,731 DEBUG Query with image successful
2024-08-22 15:43:29,043 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:37,093 DEBUG Query with image successful
2024-08-22 15:43:37,187 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:45,161 DEBUG Query with image successful
2024-08-22 15:43:45,482 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:52,437 DEBUG Query with image successful
2024-08-22 15:43:52,533 DEBUG Querying model with image: gpt-4o
2024-08-22 15:43:59,168 DEBUG Query with image successful
2024-08-22 15:43:59,474 DEBUG Querying model with image: gpt-4o
2024-08-22 15:44:10,642 DEBUG Query with image successful
2024-08-22 15:44:10,738 DEBUG Querying model with image: gpt-4o
2024-08-22 15:44:16,911 DEBUG Query with image successful
2024-08-22 15:44:17,219 DEBUG Querying model with image: gpt-4o
2024-08-22 15:44:24,784 DEBUG Query with image successful
2024-08-22 15:44:24,880 DEBUG Querying model with image: gpt-4o
2024-08-22 15:44:32,152 DEBUG Query with image successful
2024-08-22 15:44:32,464 DEBUG Querying model with image: gpt-4o
2024-08-22 15:44:40,426 DEBUG Query with image successful
2024-08-22 15:44:40,523 DEBUG Querying model with image: gpt-4o
2024-08-22 15:44:47,749 DEBUG Query with image successful
2024-08-22 15:44:48,080 DEBUG Querying model with image: gpt-4o
2024-08-22 15:44:56,235 DEBUG Query with image successful
2024-08-22 15:44:56,327 DEBUG Querying model with image: gpt-4o
2024-08-22 15:45:06,042 DEBUG Query with image successful
2024-08-22 15:45:06,357 DEBUG Querying model with image: gpt-4o
2024-08-22 15:45:13,587 DEBUG Query with image successful
2024-08-22 15:45:13,679 DEBUG Querying model with image: gpt-4o
2024-08-22 15:45:20,414 DEBUG Query with image successful
2024-08-22 15:45:20,717 DEBUG Querying model with image: gpt-4o
2024-08-22 15:45:26,846 DEBUG Query with image successful
2024-08-22 15:45:26,940 DEBUG Querying model with image: gpt-4o
2024-08-22 15:45:33,400 DEBUG Query with image successful
2024-08-22 15:45:33,757 DEBUG Querying model with image: gpt-4o
2024-08-22 15:45:43,745 DEBUG Query with image successful
2024-08-22 15:45:43,843 DEBUG Querying model with image: gpt-4o
2024-08-22 15:45:51,625 DEBUG Query with image successful
2024-08-22 15:45:51,936 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:00,281 DEBUG Query with image successful
2024-08-22 15:46:00,376 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:08,210 DEBUG Query with image successful
2024-08-22 15:46:08,555 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:17,906 DEBUG Query with image successful
2024-08-22 15:46:18,011 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:24,125 DEBUG Query with image successful
2024-08-22 15:46:24,431 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:30,061 DEBUG Query with image successful
2024-08-22 15:46:30,153 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:35,679 DEBUG Query with image successful
2024-08-22 15:46:35,970 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:42,943 DEBUG Query with image successful
2024-08-22 15:46:43,038 DEBUG Querying model with image: gpt-4o
2024-08-22 15:46:51,791 DEBUG Query with image successful
2024-08-22 15:46:52,090 DEBUG Querying model with image: gpt-4o
2024-08-22 15:47:00,211 DEBUG Query with image successful
2024-08-22 15:47:00,305 DEBUG Querying model with image: gpt-4o
2024-08-22 15:47:10,314 DEBUG Query with image successful
2024-08-22 15:47:10,619 DEBUG Querying model with image: gpt-4o
2024-08-22 15:47:19,537 DEBUG Query with image successful
2024-08-22 15:47:19,641 DEBUG Querying model with image: gpt-4o
2024-08-22 15:47:31,791 DEBUG Query with image successful
2024-08-22 15:47:32,127 DEBUG Querying model with image: gpt-4o
2024-08-22 15:47:45,034 DEBUG Query with image successful
2024-08-22 15:47:45,131 DEBUG Querying model with image: gpt-4o
2024-08-22 15:47:58,194 DEBUG Query with image successful
2024-08-22 15:47:58,493 DEBUG Querying model with image: gpt-4o
2024-08-22 15:48:14,791 DEBUG Query with image successful
2024-08-22 15:48:14,888 DEBUG Querying model with image: gpt-4o
2024-08-22 15:48:28,157 DEBUG Query with image successful
2024-08-22 15:48:28,470 DEBUG Querying model with image: gpt-4o
2024-08-22 15:48:38,935 DEBUG Query with image successful
2024-08-22 15:48:39,028 DEBUG Querying model with image: gpt-4o
2024-08-22 15:48:48,645 DEBUG Query with image successful
2024-08-22 16:05:44,756 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 16:05:44,794 DEBUG OpenAI client created
2024-08-22 16:05:49,906 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:00,460 DEBUG Query with image successful
2024-08-22 16:06:00,546 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:07,305 DEBUG Query with image successful
2024-08-22 16:06:07,661 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:14,374 DEBUG Query with image successful
2024-08-22 16:06:14,451 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:21,120 DEBUG Query with image successful
2024-08-22 16:06:31,396 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:40,029 DEBUG Query with image successful
2024-08-22 16:06:40,093 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:45,949 DEBUG Query with image successful
2024-08-22 16:06:46,179 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:51,836 DEBUG Query with image successful
2024-08-22 16:06:51,896 DEBUG Querying model with image: gpt-4o
2024-08-22 16:06:58,112 DEBUG Query with image successful
2024-08-22 16:06:58,370 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:03,464 DEBUG Query with image successful
2024-08-22 16:07:03,537 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:10,455 DEBUG Query with image successful
2024-08-22 16:07:10,697 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:17,040 DEBUG Query with image successful
2024-08-22 16:07:17,104 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:23,981 DEBUG Query with image successful
2024-08-22 16:07:24,208 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:31,556 DEBUG Query with image successful
2024-08-22 16:07:31,625 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:39,761 DEBUG Query with image successful
2024-08-22 16:07:39,997 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:45,556 DEBUG Query with image successful
2024-08-22 16:07:45,619 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:51,593 DEBUG Query with image successful
2024-08-22 16:07:51,836 DEBUG Querying model with image: gpt-4o
2024-08-22 16:07:58,567 DEBUG Query with image successful
2024-08-22 16:07:58,625 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:03,630 DEBUG Query with image successful
2024-08-22 16:08:03,864 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:11,896 DEBUG Query with image successful
2024-08-22 16:08:11,960 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:19,072 DEBUG Query with image successful
2024-08-22 16:08:19,313 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:25,774 DEBUG Query with image successful
2024-08-22 16:08:25,845 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:31,913 DEBUG Query with image successful
2024-08-22 16:08:32,147 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:38,307 DEBUG Query with image successful
2024-08-22 16:08:38,367 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:44,324 DEBUG Query with image successful
2024-08-22 16:08:44,560 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:50,116 DEBUG Query with image successful
2024-08-22 16:08:50,175 DEBUG Querying model with image: gpt-4o
2024-08-22 16:08:56,662 DEBUG Query with image successful
2024-08-22 16:08:56,897 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:03,380 DEBUG Query with image successful
2024-08-22 16:09:03,437 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:08,682 DEBUG Query with image successful
2024-08-22 16:09:08,930 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:15,832 DEBUG Query with image successful
2024-08-22 16:09:15,892 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:20,998 DEBUG Query with image successful
2024-08-22 16:09:21,236 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:28,587 DEBUG Query with image successful
2024-08-22 16:09:28,654 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:35,244 DEBUG Query with image successful
2024-08-22 16:09:35,471 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:41,281 DEBUG Query with image successful
2024-08-22 16:09:41,341 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:48,127 DEBUG Query with image successful
2024-08-22 16:09:48,355 DEBUG Querying model with image: gpt-4o
2024-08-22 16:09:56,289 DEBUG Query with image successful
2024-08-22 16:09:56,350 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:01,063 DEBUG Query with image successful
2024-08-22 16:10:01,291 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:07,791 DEBUG Query with image successful
2024-08-22 16:10:07,853 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:13,290 DEBUG Query with image successful
2024-08-22 16:10:13,521 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:18,713 DEBUG Query with image successful
2024-08-22 16:10:18,776 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:24,014 DEBUG Query with image successful
2024-08-22 16:10:24,239 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:29,735 DEBUG Query with image successful
2024-08-22 16:10:29,797 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:39,342 DEBUG Query with image successful
2024-08-22 16:10:39,571 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:45,183 DEBUG Query with image successful
2024-08-22 16:10:45,246 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:50,322 DEBUG Query with image successful
2024-08-22 16:10:50,544 DEBUG Querying model with image: gpt-4o
2024-08-22 16:10:55,223 DEBUG Query with image successful
2024-08-22 16:10:55,287 DEBUG Querying model with image: gpt-4o
2024-08-22 16:11:00,301 DEBUG Query with image successful
2024-08-22 16:11:00,526 DEBUG Querying model with image: gpt-4o
2024-08-22 16:11:07,127 DEBUG Query with image successful
2024-08-22 16:11:07,186 DEBUG Querying model with image: gpt-4o
2024-08-22 16:11:12,335 DEBUG Query with image successful
2024-08-22 16:11:12,556 DEBUG Querying model with image: gpt-4o
2024-08-22 16:11:17,787 DEBUG Query with image successful
2024-08-22 16:11:17,850 DEBUG Querying model with image: gpt-4o
2024-08-22 16:11:22,839 DEBUG Query with image successful
2024-08-22 16:30:43,888 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 16:30:43,919 DEBUG OpenAI client created
2024-08-22 16:30:49,029 DEBUG Querying model with image: gpt-4o
2024-08-22 16:30:54,929 DEBUG Query with image successful
2024-08-22 16:30:54,998 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:00,275 DEBUG Query with image successful
2024-08-22 16:31:00,522 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:06,740 DEBUG Query with image successful
2024-08-22 16:31:06,801 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:13,275 DEBUG Query with image successful
2024-08-22 16:31:23,668 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:30,096 DEBUG Query with image successful
2024-08-22 16:31:30,160 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:35,766 DEBUG Query with image successful
2024-08-22 16:31:36,094 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:43,415 DEBUG Query with image successful
2024-08-22 16:31:43,477 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:49,125 DEBUG Query with image successful
2024-08-22 16:31:49,375 DEBUG Querying model with image: gpt-4o
2024-08-22 16:31:54,468 DEBUG Query with image successful
2024-08-22 16:31:54,528 DEBUG Querying model with image: gpt-4o
2024-08-22 16:32:01,814 DEBUG Query with image successful
2024-08-22 16:32:02,062 DEBUG Querying model with image: gpt-4o
2024-08-22 16:32:08,697 DEBUG Query with image successful
2024-08-22 16:32:08,760 DEBUG Querying model with image: gpt-4o
2024-08-22 16:32:16,339 DEBUG Query with image successful
2024-08-22 16:32:16,612 DEBUG Querying model with image: gpt-4o
2024-08-22 16:32:25,826 DEBUG Query with image successful
2024-08-22 16:32:25,885 DEBUG Querying model with image: gpt-4o
2024-08-22 16:32:31,420 DEBUG Query with image successful
2024-08-22 16:32:31,673 DEBUG Querying model with image: gpt-4o
2024-08-22 16:32:37,227 DEBUG Query with image successful
2024-08-22 16:32:37,288 DEBUG Querying model with image: gpt-4o
2024-08-22 16:32:43,144 DEBUG Query with image successful
2024-08-22 16:32:43,390 DEBUG Querying model with image: gpt-4o
2024-08-22 16:33:19,222 DEBUG Query with image successful
2024-08-22 16:33:19,289 DEBUG Querying model with image: gpt-4o
2024-08-22 16:33:24,599 DEBUG Query with image successful
2024-08-22 16:33:24,911 DEBUG Querying model with image: gpt-4o
2024-08-22 16:33:35,666 DEBUG Query with image successful
2024-08-22 16:33:35,727 DEBUG Querying model with image: gpt-4o
2024-08-22 16:33:42,226 DEBUG Query with image successful
2024-08-22 16:33:42,462 DEBUG Querying model with image: gpt-4o
2024-08-22 16:33:48,885 DEBUG Query with image successful
2024-08-22 16:33:48,946 DEBUG Querying model with image: gpt-4o
2024-08-22 16:33:56,144 DEBUG Query with image successful
2024-08-22 16:33:56,413 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:03,987 DEBUG Query with image successful
2024-08-22 16:34:04,047 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:09,903 DEBUG Query with image successful
2024-08-22 16:34:10,166 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:15,892 DEBUG Query with image successful
2024-08-22 16:34:15,955 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:21,627 DEBUG Query with image successful
2024-08-22 16:34:21,861 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:28,283 DEBUG Query with image successful
2024-08-22 16:34:28,344 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:34,088 DEBUG Query with image successful
2024-08-22 16:34:34,319 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:39,009 DEBUG Query with image successful
2024-08-22 16:34:39,071 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:45,136 DEBUG Query with image successful
2024-08-22 16:34:45,369 DEBUG Querying model with image: gpt-4o
2024-08-22 16:34:50,824 DEBUG Query with image successful
2024-08-22 16:34:50,897 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:10,693 DEBUG Query with image successful
2024-08-22 16:35:10,930 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:15,960 DEBUG Query with image successful
2024-08-22 16:35:16,028 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:22,266 DEBUG Query with image successful
2024-08-22 16:35:22,504 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:27,307 DEBUG Query with image successful
2024-08-22 16:35:27,368 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:32,804 DEBUG Query with image successful
2024-08-22 16:35:33,042 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:38,063 DEBUG Query with image successful
2024-08-22 16:35:38,122 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:43,784 DEBUG Query with image successful
2024-08-22 16:35:44,012 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:49,486 DEBUG Query with image successful
2024-08-22 16:35:49,542 DEBUG Querying model with image: gpt-4o
2024-08-22 16:35:56,692 DEBUG Query with image successful
2024-08-22 16:35:56,929 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:01,932 DEBUG Query with image successful
2024-08-22 16:36:01,994 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:07,409 DEBUG Query with image successful
2024-08-22 16:36:07,653 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:13,191 DEBUG Query with image successful
2024-08-22 16:36:13,255 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:18,940 DEBUG Query with image successful
2024-08-22 16:36:19,167 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:23,766 DEBUG Query with image successful
2024-08-22 16:36:23,831 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:29,492 DEBUG Query with image successful
2024-08-22 16:36:29,723 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:33,932 DEBUG Query with image successful
2024-08-22 16:36:33,995 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:39,231 DEBUG Query with image successful
2024-08-22 16:36:39,449 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:44,456 DEBUG Query with image successful
2024-08-22 16:36:44,516 DEBUG Querying model with image: gpt-4o
2024-08-22 16:36:49,886 DEBUG Query with image successful
2024-08-22 16:46:36,065 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 16:46:36,102 DEBUG OpenAI client created
2024-08-22 16:46:41,267 DEBUG Querying model with image: gpt-4o
2024-08-22 16:46:57,082 DEBUG Query with image successful
2024-08-22 16:46:57,171 DEBUG Querying model with image: gpt-4o
2024-08-22 16:47:06,028 DEBUG Query with image successful
2024-08-22 16:47:06,341 DEBUG Querying model with image: gpt-4o
2024-08-22 16:47:21,311 DEBUG Query with image successful
2024-08-22 16:47:21,411 DEBUG Querying model with image: gpt-4o
2024-08-22 16:47:35,643 DEBUG Query with image successful
2024-08-22 16:47:45,953 DEBUG Querying model with image: gpt-4o
2024-08-22 16:47:56,960 DEBUG Query with image successful
2024-08-22 16:47:57,066 DEBUG Querying model with image: gpt-4o
2024-08-22 16:48:04,031 DEBUG Query with image successful
2024-08-22 16:48:04,406 DEBUG Querying model with image: gpt-4o
2024-08-22 16:48:12,536 DEBUG Query with image successful
2024-08-22 16:48:12,627 DEBUG Querying model with image: gpt-4o
2024-08-22 16:48:20,786 DEBUG Query with image successful
2024-08-22 16:48:21,197 DEBUG Querying model with image: gpt-4o
2024-08-22 16:48:30,586 DEBUG Query with image successful
2024-08-22 16:48:30,684 DEBUG Querying model with image: gpt-4o
2024-08-22 16:48:37,650 DEBUG Query with image successful
2024-08-22 16:48:37,960 DEBUG Querying model with image: gpt-4o
2024-08-22 16:48:48,649 DEBUG Query with image successful
2024-08-22 16:48:48,753 DEBUG Querying model with image: gpt-4o
2024-08-22 16:48:56,856 DEBUG Query with image successful
2024-08-22 16:48:57,257 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:05,161 DEBUG Query with image successful
2024-08-22 16:49:05,268 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:12,005 DEBUG Query with image successful
2024-08-22 16:49:12,322 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:20,045 DEBUG Query with image successful
2024-08-22 16:49:20,141 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:26,505 DEBUG Query with image successful
2024-08-22 16:49:26,804 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:34,953 DEBUG Query with image successful
2024-08-22 16:49:35,045 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:44,755 DEBUG Query with image successful
2024-08-22 16:49:45,076 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:51,911 DEBUG Query with image successful
2024-08-22 16:49:52,010 DEBUG Querying model with image: gpt-4o
2024-08-22 16:49:59,558 DEBUG Query with image successful
2024-08-22 16:49:59,931 DEBUG Querying model with image: gpt-4o
2024-08-22 16:50:08,645 DEBUG Query with image successful
2024-08-22 16:50:08,753 DEBUG Querying model with image: gpt-4o
2024-08-22 16:50:14,589 DEBUG Query with image successful
2024-08-22 16:50:14,935 DEBUG Querying model with image: gpt-4o
2024-08-22 16:50:25,601 DEBUG Query with image successful
2024-08-22 16:50:25,701 DEBUG Querying model with image: gpt-4o
2024-08-22 16:50:35,047 DEBUG Query with image successful
2024-08-22 16:50:35,383 DEBUG Querying model with image: gpt-4o
2024-08-22 16:50:43,321 DEBUG Query with image successful
2024-08-22 16:50:43,407 DEBUG Querying model with image: gpt-4o
2024-08-22 16:50:51,640 DEBUG Query with image successful
2024-08-22 16:50:51,947 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:01,007 DEBUG Query with image successful
2024-08-22 16:51:01,100 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:08,446 DEBUG Query with image successful
2024-08-22 16:51:08,743 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:14,997 DEBUG Query with image successful
2024-08-22 16:51:15,090 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:21,825 DEBUG Query with image successful
2024-08-22 16:51:22,145 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:28,832 DEBUG Query with image successful
2024-08-22 16:51:28,923 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:36,209 DEBUG Query with image successful
2024-08-22 16:51:36,512 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:44,517 DEBUG Query with image successful
2024-08-22 16:51:44,611 DEBUG Querying model with image: gpt-4o
2024-08-22 16:51:53,039 DEBUG Query with image successful
2024-08-22 16:51:53,358 DEBUG Querying model with image: gpt-4o
2024-08-22 16:52:00,216 DEBUG Query with image successful
2024-08-22 16:52:00,305 DEBUG Querying model with image: gpt-4o
2024-08-22 16:52:06,819 DEBUG Query with image successful
2024-08-22 16:52:07,112 DEBUG Querying model with image: gpt-4o
2024-08-22 16:52:14,651 DEBUG Query with image successful
2024-08-22 16:52:14,739 DEBUG Querying model with image: gpt-4o
2024-08-22 16:52:23,469 DEBUG Query with image successful
2024-08-22 16:52:23,791 DEBUG Querying model with image: gpt-4o
2024-08-22 16:52:31,864 DEBUG Query with image successful
2024-08-22 16:52:31,956 DEBUG Querying model with image: gpt-4o
2024-08-22 16:52:39,839 DEBUG Query with image successful
2024-08-22 16:52:40,122 DEBUG Querying model with image: gpt-4o
2024-08-22 16:52:50,416 DEBUG Query with image successful
2024-08-22 16:52:50,505 DEBUG Querying model with image: gpt-4o
2024-08-22 16:53:02,238 DEBUG Query with image successful
2024-08-22 16:53:02,524 DEBUG Querying model with image: gpt-4o
2024-08-22 16:53:14,489 DEBUG Query with image successful
2024-08-22 16:53:14,581 DEBUG Querying model with image: gpt-4o
2024-08-22 16:53:22,173 DEBUG Query with image successful
2024-08-22 16:53:22,454 DEBUG Querying model with image: gpt-4o
2024-08-22 16:53:28,975 DEBUG Query with image successful
2024-08-22 16:53:29,072 DEBUG Querying model with image: gpt-4o
2024-08-22 16:53:36,259 DEBUG Query with image successful
2024-08-22 16:53:36,556 DEBUG Querying model with image: gpt-4o
2024-08-22 16:53:43,141 DEBUG Query with image successful
2024-08-22 16:53:43,235 DEBUG Querying model with image: gpt-4o
2024-08-22 16:54:23,734 DEBUG Query with image successful
2024-08-22 16:54:24,012 DEBUG Querying model with image: gpt-4o
2024-08-22 16:54:31,212 DEBUG Query with image successful
2024-08-22 16:54:31,300 DEBUG Querying model with image: gpt-4o
2024-08-22 16:54:38,715 DEBUG Query with image successful
2024-08-22 19:14:00,827 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 19:14:00,888 DEBUG OpenAI client created
2024-08-22 20:47:46,812 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 20:47:46,839 DEBUG OpenAI client created
2024-08-22 20:53:53,324 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 20:53:53,355 DEBUG OpenAI client created
2024-08-22 21:14:41,848 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-22 21:14:41,881 DEBUG OpenAI client created
2024-08-22 21:14:47,054 DEBUG Querying model with image: gpt-4o
2024-08-22 21:15:00,538 DEBUG Query with image successful
2024-08-22 21:15:00,629 DEBUG Querying model with image: gpt-4o
2024-08-22 21:15:14,831 DEBUG Query with image successful
2024-08-22 21:15:15,275 DEBUG Querying model with image: gpt-4o
2024-08-22 21:15:29,715 DEBUG Query with image successful
2024-08-22 21:15:29,820 DEBUG Querying model with image: gpt-4o
2024-08-22 21:15:49,379 DEBUG Query with image successful
2024-08-22 21:15:59,673 DEBUG Querying model with image: gpt-4o
2024-08-22 21:16:11,435 DEBUG Query with image successful
2024-08-22 21:16:11,538 DEBUG Querying model with image: gpt-4o
2024-08-22 21:16:22,083 DEBUG Query with image successful
2024-08-22 21:16:22,421 DEBUG Querying model with image: gpt-4o
2024-08-22 21:16:30,751 DEBUG Query with image successful
2024-08-22 21:16:30,844 DEBUG Querying model with image: gpt-4o
2024-08-22 21:16:42,406 DEBUG Query with image successful
2024-08-22 21:16:42,731 DEBUG Querying model with image: gpt-4o
2024-08-22 21:16:52,421 DEBUG Query with image successful
2024-08-22 21:16:52,526 DEBUG Querying model with image: gpt-4o
2024-08-22 21:17:01,793 DEBUG Query with image successful
2024-08-22 21:17:02,130 DEBUG Querying model with image: gpt-4o
2024-08-22 21:17:11,178 DEBUG Query with image successful
2024-08-22 21:17:11,275 DEBUG Querying model with image: gpt-4o
2024-08-22 21:17:20,999 DEBUG Query with image successful
2024-08-22 21:17:21,346 DEBUG Querying model with image: gpt-4o
2024-08-22 21:17:30,514 DEBUG Query with image successful
2024-08-22 21:17:30,609 DEBUG Querying model with image: gpt-4o
2024-08-22 21:17:39,469 DEBUG Query with image successful
2024-08-22 21:17:39,898 DEBUG Querying model with image: gpt-4o
2024-08-22 21:17:47,908 DEBUG Query with image successful
2024-08-22 21:17:48,004 DEBUG Querying model with image: gpt-4o
2024-08-22 21:17:55,762 DEBUG Query with image successful
2024-08-22 21:17:56,088 DEBUG Querying model with image: gpt-4o
2024-08-22 21:18:04,520 DEBUG Query with image successful
2024-08-22 21:18:04,620 DEBUG Querying model with image: gpt-4o
2024-08-22 21:18:12,655 DEBUG Query with image successful
2024-08-22 21:18:13,011 DEBUG Querying model with image: gpt-4o
2024-08-22 21:18:21,224 DEBUG Query with image successful
2024-08-22 21:18:21,322 DEBUG Querying model with image: gpt-4o
2024-08-22 21:18:29,821 DEBUG Query with image successful
2024-08-22 21:18:30,187 DEBUG Querying model with image: gpt-4o
2024-08-22 21:18:40,158 DEBUG Query with image successful
2024-08-22 21:18:40,257 DEBUG Querying model with image: gpt-4o
2024-08-22 21:18:49,371 DEBUG Query with image successful
2024-08-22 21:18:49,728 DEBUG Querying model with image: gpt-4o
2024-08-22 21:18:58,118 DEBUG Query with image successful
2024-08-22 21:18:58,211 DEBUG Querying model with image: gpt-4o
2024-08-22 21:19:05,378 DEBUG Query with image successful
2024-08-22 21:19:05,705 DEBUG Querying model with image: gpt-4o
2024-08-22 21:19:13,820 DEBUG Query with image successful
2024-08-22 21:19:13,915 DEBUG Querying model with image: gpt-4o
2024-08-22 21:19:21,733 DEBUG Query with image successful
2024-08-22 21:19:22,049 DEBUG Querying model with image: gpt-4o
2024-08-22 21:19:30,809 DEBUG Query with image successful
2024-08-22 21:19:30,913 DEBUG Querying model with image: gpt-4o
2024-08-22 21:19:37,972 DEBUG Query with image successful
2024-08-22 21:19:38,287 DEBUG Querying model with image: gpt-4o
2024-08-22 21:19:47,333 DEBUG Query with image successful
2024-08-22 21:19:47,427 DEBUG Querying model with image: gpt-4o
2024-08-22 21:19:56,581 DEBUG Query with image successful
2024-08-22 21:19:56,904 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:05,351 DEBUG Query with image successful
2024-08-22 21:20:05,452 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:11,141 DEBUG Query with image successful
2024-08-22 21:20:11,481 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:19,793 DEBUG Query with image successful
2024-08-22 21:20:19,887 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:26,727 DEBUG Query with image successful
2024-08-22 21:20:27,096 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:34,017 DEBUG Query with image successful
2024-08-22 21:20:34,113 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:41,679 DEBUG Query with image successful
2024-08-22 21:20:41,997 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:49,210 DEBUG Query with image successful
2024-08-22 21:20:49,306 DEBUG Querying model with image: gpt-4o
2024-08-22 21:20:56,281 DEBUG Query with image successful
2024-08-22 21:20:56,610 DEBUG Querying model with image: gpt-4o
2024-08-22 21:21:03,519 DEBUG Query with image successful
2024-08-22 21:21:03,616 DEBUG Querying model with image: gpt-4o
2024-08-22 21:21:09,769 DEBUG Query with image successful
2024-08-22 21:21:10,086 DEBUG Querying model with image: gpt-4o
2024-08-22 21:21:17,793 DEBUG Query with image successful
2024-08-22 21:21:17,897 DEBUG Querying model with image: gpt-4o
2024-08-22 21:21:29,080 DEBUG Query with image successful
2024-08-22 21:21:29,399 DEBUG Querying model with image: gpt-4o
2024-08-22 21:21:37,585 DEBUG Query with image successful
2024-08-22 21:21:37,681 DEBUG Querying model with image: gpt-4o
2024-08-22 21:21:44,782 DEBUG Query with image successful
2024-08-22 21:21:45,090 DEBUG Querying model with image: gpt-4o
2024-08-22 21:21:54,051 DEBUG Query with image successful
2024-08-22 21:21:54,150 DEBUG Querying model with image: gpt-4o
2024-08-22 21:22:03,181 DEBUG Query with image successful
2024-08-22 21:22:03,488 DEBUG Querying model with image: gpt-4o
2024-08-22 21:22:10,911 DEBUG Query with image successful
2024-08-22 21:22:11,003 DEBUG Querying model with image: gpt-4o
2024-08-22 21:22:17,501 DEBUG Query with image successful
2024-08-22 21:22:17,839 DEBUG Querying model with image: gpt-4o
2024-08-22 21:22:30,681 DEBUG Query with image successful
2024-08-22 21:22:30,778 DEBUG Querying model with image: gpt-4o
2024-08-22 21:22:38,385 DEBUG Query with image successful
2024-08-23 13:14:17,094 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-23 13:14:17,126 DEBUG OpenAI client created
2024-08-23 13:14:22,243 DEBUG Querying model with image: gpt-4o
2024-08-23 13:14:29,240 DEBUG Query with image successful
2024-08-23 13:14:29,303 DEBUG Querying model with image: gpt-4o
2024-08-23 13:14:36,092 DEBUG Query with image successful
2024-08-23 13:14:36,409 DEBUG Querying model with image: gpt-4o
2024-08-23 13:14:42,758 DEBUG Query with image successful
2024-08-23 13:14:42,816 DEBUG Querying model with image: gpt-4o
2024-08-23 13:14:49,296 DEBUG Query with image successful
2024-08-23 13:14:59,525 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:04,883 DEBUG Query with image successful
2024-08-23 13:15:04,940 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:11,650 DEBUG Query with image successful
2024-08-23 13:15:11,970 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:19,442 DEBUG Query with image successful
2024-08-23 13:15:19,498 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:25,306 DEBUG Query with image successful
2024-08-23 13:15:25,606 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:32,698 DEBUG Query with image successful
2024-08-23 13:15:32,764 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:38,322 DEBUG Query with image successful
2024-08-23 13:15:38,660 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:45,647 DEBUG Query with image successful
2024-08-23 13:15:45,705 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:51,089 DEBUG Query with image successful
2024-08-23 13:15:51,339 DEBUG Querying model with image: gpt-4o
2024-08-23 13:15:57,425 DEBUG Query with image successful
2024-08-23 13:15:57,489 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:04,238 DEBUG Query with image successful
2024-08-23 13:16:04,486 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:11,144 DEBUG Query with image successful
2024-08-23 13:16:11,201 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:17,536 DEBUG Query with image successful
2024-08-23 13:16:17,808 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:24,833 DEBUG Query with image successful
2024-08-23 13:16:24,891 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:32,612 DEBUG Query with image successful
2024-08-23 13:16:32,912 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:39,129 DEBUG Query with image successful
2024-08-23 13:16:39,188 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:44,355 DEBUG Query with image successful
2024-08-23 13:16:44,611 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:50,478 DEBUG Query with image successful
2024-08-23 13:16:50,533 DEBUG Querying model with image: gpt-4o
2024-08-23 13:16:55,553 DEBUG Query with image successful
2024-08-23 13:16:55,846 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:01,150 DEBUG Query with image successful
2024-08-23 13:17:01,210 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:07,089 DEBUG Query with image successful
2024-08-23 13:17:07,337 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:12,123 DEBUG Query with image successful
2024-08-23 13:17:12,180 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:18,083 DEBUG Query with image successful
2024-08-23 13:17:18,378 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:24,110 DEBUG Query with image successful
2024-08-23 13:17:24,167 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:29,251 DEBUG Query with image successful
2024-08-23 13:17:29,491 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:35,780 DEBUG Query with image successful
2024-08-23 13:17:35,837 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:43,722 DEBUG Query with image successful
2024-08-23 13:17:43,959 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:49,657 DEBUG Query with image successful
2024-08-23 13:17:49,717 DEBUG Querying model with image: gpt-4o
2024-08-23 13:17:57,292 DEBUG Query with image successful
2024-08-23 13:17:57,522 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:03,808 DEBUG Query with image successful
2024-08-23 13:18:03,872 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:08,198 DEBUG Query with image successful
2024-08-23 13:18:08,422 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:13,212 DEBUG Query with image successful
2024-08-23 13:18:13,273 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:17,061 DEBUG Query with image successful
2024-08-23 13:18:17,298 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:23,543 DEBUG Query with image successful
2024-08-23 13:18:23,601 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:28,989 DEBUG Query with image successful
2024-08-23 13:18:29,229 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:34,567 DEBUG Query with image successful
2024-08-23 13:18:34,625 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:42,072 DEBUG Query with image successful
2024-08-23 13:18:42,310 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:47,487 DEBUG Query with image successful
2024-08-23 13:18:47,554 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:54,023 DEBUG Query with image successful
2024-08-23 13:18:54,271 DEBUG Querying model with image: gpt-4o
2024-08-23 13:18:59,968 DEBUG Query with image successful
2024-08-23 13:19:00,039 DEBUG Querying model with image: gpt-4o
2024-08-23 13:19:04,853 DEBUG Query with image successful
2024-08-23 13:19:05,092 DEBUG Querying model with image: gpt-4o
2024-08-23 13:19:11,788 DEBUG Query with image successful
2024-08-23 13:19:11,846 DEBUG Querying model with image: gpt-4o
2024-08-23 13:19:17,059 DEBUG Query with image successful
2024-08-23 13:19:17,304 DEBUG Querying model with image: gpt-4o
2024-08-23 13:19:22,057 DEBUG Query with image successful
2024-08-23 13:19:22,117 DEBUG Querying model with image: gpt-4o
2024-08-23 13:19:28,818 DEBUG Query with image successful
2024-08-23 13:19:29,044 DEBUG Querying model with image: gpt-4o
2024-08-23 13:19:33,658 DEBUG Query with image successful
2024-08-23 13:19:33,716 DEBUG Querying model with image: gpt-4o
2024-08-23 13:19:39,459 DEBUG Query with image successful
2024-08-23 13:34:05,643 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-23 13:34:05,672 DEBUG OpenAI client created
2024-08-23 13:34:10,876 DEBUG Querying model with image: gpt-4o
2024-08-23 13:34:29,299 DEBUG Query with image successful
2024-08-23 13:34:29,384 DEBUG Querying model with image: gpt-4o
2024-08-23 13:34:35,975 DEBUG Query with image successful
2024-08-23 13:34:36,326 DEBUG Querying model with image: gpt-4o
2024-08-23 13:34:43,379 DEBUG Query with image successful
2024-08-23 13:34:43,461 DEBUG Querying model with image: gpt-4o
2024-08-23 13:34:51,630 DEBUG Query with image successful
2024-08-23 13:35:01,917 DEBUG Querying model with image: gpt-4o
2024-08-23 13:35:09,325 DEBUG Query with image successful
2024-08-23 13:35:09,417 DEBUG Querying model with image: gpt-4o
2024-08-23 13:35:15,512 DEBUG Query with image successful
2024-08-23 13:35:15,828 DEBUG Querying model with image: gpt-4o
2024-08-23 13:35:22,234 DEBUG Query with image successful
2024-08-23 13:35:22,319 DEBUG Querying model with image: gpt-4o
2024-08-23 13:35:28,249 DEBUG Query with image successful
2024-08-23 13:35:28,634 DEBUG Querying model with image: gpt-4o
2024-08-23 13:35:35,125 DEBUG Query with image successful
2024-08-23 13:35:35,215 DEBUG Querying model with image: gpt-4o
2024-08-23 13:35:43,819 DEBUG Query with image successful
2024-08-23 13:35:44,124 DEBUG Querying model with image: gpt-4o
2024-08-23 13:35:55,823 DEBUG Query with image successful
2024-08-23 13:35:55,914 DEBUG Querying model with image: gpt-4o
2024-08-23 13:36:04,707 DEBUG Query with image successful
2024-08-23 13:36:05,019 DEBUG Querying model with image: gpt-4o
2024-08-23 13:36:26,782 DEBUG Query with image successful
2024-08-23 13:36:26,878 DEBUG Querying model with image: gpt-4o
2024-08-23 13:36:34,669 DEBUG Query with image successful
2024-08-23 13:36:35,056 DEBUG Querying model with image: gpt-4o
2024-08-23 13:37:27,460 DEBUG Query with image successful
2024-08-23 13:37:27,560 DEBUG Querying model with image: gpt-4o
2024-08-23 13:37:34,876 DEBUG Query with image successful
2024-08-23 13:37:35,208 DEBUG Querying model with image: gpt-4o
2024-08-23 13:37:42,269 DEBUG Query with image successful
2024-08-23 13:37:42,367 DEBUG Querying model with image: gpt-4o
2024-08-23 13:37:48,913 DEBUG Query with image successful
2024-08-23 13:37:49,308 DEBUG Querying model with image: gpt-4o
2024-08-23 13:37:55,609 DEBUG Query with image successful
2024-08-23 13:37:55,711 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:02,089 DEBUG Query with image successful
2024-08-23 13:38:02,415 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:08,340 DEBUG Query with image successful
2024-08-23 13:38:08,440 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:14,905 DEBUG Query with image successful
2024-08-23 13:38:15,209 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:22,277 DEBUG Query with image successful
2024-08-23 13:38:22,372 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:28,223 DEBUG Query with image successful
2024-08-23 13:38:28,518 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:35,162 DEBUG Query with image successful
2024-08-23 13:38:35,260 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:42,316 DEBUG Query with image successful
2024-08-23 13:38:42,634 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:49,198 DEBUG Query with image successful
2024-08-23 13:38:49,298 DEBUG Querying model with image: gpt-4o
2024-08-23 13:38:55,741 DEBUG Query with image successful
2024-08-23 13:38:56,070 DEBUG Querying model with image: gpt-4o
2024-08-23 13:39:43,698 DEBUG Query with image successful
2024-08-23 13:39:43,793 DEBUG Querying model with image: gpt-4o
2024-08-23 13:39:50,089 DEBUG Query with image successful
2024-08-23 13:39:50,500 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:02,545 DEBUG Query with image successful
2024-08-23 13:40:02,640 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:08,118 DEBUG Query with image successful
2024-08-23 13:40:08,417 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:15,619 DEBUG Query with image successful
2024-08-23 13:40:15,718 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:21,255 DEBUG Query with image successful
2024-08-23 13:40:21,552 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:27,291 DEBUG Query with image successful
2024-08-23 13:40:27,382 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:33,128 DEBUG Query with image successful
2024-08-23 13:40:33,471 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:41,201 DEBUG Query with image successful
2024-08-23 13:40:41,292 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:46,386 DEBUG Query with image successful
2024-08-23 13:40:46,680 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:51,738 DEBUG Query with image successful
2024-08-23 13:40:51,834 DEBUG Querying model with image: gpt-4o
2024-08-23 13:40:57,067 DEBUG Query with image successful
2024-08-23 13:40:57,361 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:03,941 DEBUG Query with image successful
2024-08-23 13:41:04,050 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:10,515 DEBUG Query with image successful
2024-08-23 13:41:10,824 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:16,681 DEBUG Query with image successful
2024-08-23 13:41:16,778 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:23,271 DEBUG Query with image successful
2024-08-23 13:41:23,579 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:30,199 DEBUG Query with image successful
2024-08-23 13:41:30,290 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:36,007 DEBUG Query with image successful
2024-08-23 13:41:36,298 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:44,253 DEBUG Query with image successful
2024-08-23 13:41:44,353 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:49,817 DEBUG Query with image successful
2024-08-23 13:41:50,106 DEBUG Querying model with image: gpt-4o
2024-08-23 13:41:55,132 DEBUG Query with image successful
2024-08-23 13:41:55,229 DEBUG Querying model with image: gpt-4o
2024-08-23 13:42:00,162 DEBUG Query with image successful
2024-08-23 13:55:21,089 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-23 13:55:21,136 DEBUG OpenAI client created
2024-08-23 13:55:26,310 DEBUG Querying model with image: gpt-4o
2024-08-23 13:55:33,478 DEBUG Query with image successful
2024-08-23 13:55:33,579 DEBUG Querying model with image: gpt-4o
2024-08-23 13:55:41,588 DEBUG Query with image successful
2024-08-23 13:55:41,955 DEBUG Querying model with image: gpt-4o
2024-08-23 13:55:49,034 DEBUG Query with image successful
2024-08-23 13:55:49,138 DEBUG Querying model with image: gpt-4o
2024-08-23 13:55:56,068 DEBUG Query with image successful
2024-08-23 13:56:06,347 DEBUG Querying model with image: gpt-4o
2024-08-23 13:56:14,242 DEBUG Query with image successful
2024-08-23 13:56:14,341 DEBUG Querying model with image: gpt-4o
2024-08-23 13:56:20,867 DEBUG Query with image successful
2024-08-23 13:56:21,259 DEBUG Querying model with image: gpt-4o
2024-08-23 13:56:27,647 DEBUG Query with image successful
2024-08-23 13:56:27,748 DEBUG Querying model with image: gpt-4o
2024-08-23 13:56:34,349 DEBUG Query with image successful
2024-08-23 13:56:34,818 DEBUG Querying model with image: gpt-4o
2024-08-23 13:56:42,373 DEBUG Query with image successful
2024-08-23 13:56:42,469 DEBUG Querying model with image: gpt-4o
2024-08-23 13:56:50,040 DEBUG Query with image successful
2024-08-23 13:56:50,361 DEBUG Querying model with image: gpt-4o
2024-08-23 13:56:58,243 DEBUG Query with image successful
2024-08-23 13:56:58,347 DEBUG Querying model with image: gpt-4o
2024-08-23 13:57:05,191 DEBUG Query with image successful
2024-08-23 13:57:05,491 DEBUG Querying model with image: gpt-4o
2024-08-23 13:57:16,233 DEBUG Query with image successful
2024-08-23 13:57:16,337 DEBUG Querying model with image: gpt-4o
2024-08-23 13:57:22,483 DEBUG Query with image successful
2024-08-23 13:57:22,909 DEBUG Querying model with image: gpt-4o
2024-08-23 13:57:30,299 DEBUG Query with image successful
2024-08-23 13:57:30,395 DEBUG Querying model with image: gpt-4o
2024-08-23 13:58:19,504 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-23 13:58:19,542 DEBUG OpenAI client created
2024-08-23 13:58:24,727 DEBUG Querying model with image: gpt-4o
2024-08-23 13:58:32,146 DEBUG Query with image successful
2024-08-23 13:58:32,238 DEBUG Querying model with image: gpt-4o
2024-08-23 13:58:38,486 DEBUG Query with image successful
2024-08-23 13:58:38,935 DEBUG Querying model with image: gpt-4o
2024-08-23 13:58:47,752 DEBUG Query with image successful
2024-08-23 13:58:47,841 DEBUG Querying model with image: gpt-4o
2024-08-23 13:58:55,637 DEBUG Query with image successful
2024-08-23 13:59:05,927 DEBUG Querying model with image: gpt-4o
2024-08-23 13:59:15,182 DEBUG Query with image successful
2024-08-23 13:59:15,271 DEBUG Querying model with image: gpt-4o
2024-08-23 13:59:24,860 DEBUG Query with image successful
2024-08-23 13:59:25,250 DEBUG Querying model with image: gpt-4o
2024-08-23 13:59:32,731 DEBUG Query with image successful
2024-08-23 13:59:32,831 DEBUG Querying model with image: gpt-4o
2024-08-23 13:59:41,522 DEBUG Query with image successful
2024-08-23 13:59:41,825 DEBUG Querying model with image: gpt-4o
2024-08-23 13:59:50,631 DEBUG Query with image successful
2024-08-23 13:59:50,729 DEBUG Querying model with image: gpt-4o
2024-08-23 13:59:59,142 DEBUG Query with image successful
2024-08-23 13:59:59,446 DEBUG Querying model with image: gpt-4o
2024-08-23 14:00:07,125 DEBUG Query with image successful
2024-08-23 14:00:07,224 DEBUG Querying model with image: gpt-4o
2024-08-23 14:00:13,919 DEBUG Query with image successful
2024-08-23 14:00:14,281 DEBUG Querying model with image: gpt-4o
2024-08-23 14:00:21,216 DEBUG Query with image successful
2024-08-23 14:00:21,312 DEBUG Querying model with image: gpt-4o
2024-08-23 14:00:28,426 DEBUG Query with image successful
2024-08-23 14:00:28,742 DEBUG Querying model with image: gpt-4o
2024-08-23 14:00:35,805 DEBUG Query with image successful
2024-08-23 14:00:35,902 DEBUG Querying model with image: gpt-4o
2024-08-23 14:00:43,817 DEBUG Query with image successful
2024-08-23 14:00:44,132 DEBUG Querying model with image: gpt-4o
2024-08-23 14:00:52,520 DEBUG Query with image successful
2024-08-23 14:00:52,611 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:00,359 DEBUG Query with image successful
2024-08-23 14:01:00,657 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:07,937 DEBUG Query with image successful
2024-08-23 14:01:08,033 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:15,703 DEBUG Query with image successful
2024-08-23 14:01:16,045 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:23,028 DEBUG Query with image successful
2024-08-23 14:01:23,129 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:29,930 DEBUG Query with image successful
2024-08-23 14:01:30,313 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:36,744 DEBUG Query with image successful
2024-08-23 14:01:36,841 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:44,380 DEBUG Query with image successful
2024-08-23 14:01:44,721 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:50,816 DEBUG Query with image successful
2024-08-23 14:01:50,908 DEBUG Querying model with image: gpt-4o
2024-08-23 14:01:59,773 DEBUG Query with image successful
2024-08-23 14:02:00,087 DEBUG Querying model with image: gpt-4o
2024-08-23 14:02:08,902 DEBUG Query with image successful
2024-08-23 14:02:09,006 DEBUG Querying model with image: gpt-4o
2024-08-23 14:02:17,516 DEBUG Query with image successful
2024-08-23 14:02:17,814 DEBUG Querying model with image: gpt-4o
2024-08-23 14:02:25,341 DEBUG Query with image successful
2024-08-23 14:02:25,439 DEBUG Querying model with image: gpt-4o
2024-08-23 14:02:34,381 DEBUG Query with image successful
2024-08-23 14:02:34,705 DEBUG Querying model with image: gpt-4o
2024-08-23 14:02:42,687 DEBUG Query with image successful
2024-08-23 14:02:42,776 DEBUG Querying model with image: gpt-4o
2024-08-23 14:02:49,566 DEBUG Query with image successful
2024-08-23 14:02:49,866 DEBUG Querying model with image: gpt-4o
2024-08-23 14:02:55,380 DEBUG Query with image successful
2024-08-23 14:02:55,482 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:02,909 DEBUG Query with image successful
2024-08-23 14:03:03,211 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:09,570 DEBUG Query with image successful
2024-08-23 14:03:09,660 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:15,463 DEBUG Query with image successful
2024-08-23 14:03:15,768 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:23,959 DEBUG Query with image successful
2024-08-23 14:03:24,052 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:29,993 DEBUG Query with image successful
2024-08-23 14:03:30,301 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:36,804 DEBUG Query with image successful
2024-08-23 14:03:36,899 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:42,179 DEBUG Query with image successful
2024-08-23 14:03:42,474 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:47,981 DEBUG Query with image successful
2024-08-23 14:03:48,074 DEBUG Querying model with image: gpt-4o
2024-08-23 14:03:54,339 DEBUG Query with image successful
2024-08-23 14:03:54,631 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:00,843 DEBUG Query with image successful
2024-08-23 14:04:00,945 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:07,932 DEBUG Query with image successful
2024-08-23 14:04:08,225 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:14,685 DEBUG Query with image successful
2024-08-23 14:04:14,775 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:20,984 DEBUG Query with image successful
2024-08-23 14:04:21,274 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:26,966 DEBUG Query with image successful
2024-08-23 14:04:27,058 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:32,882 DEBUG Query with image successful
2024-08-23 14:04:33,164 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:39,715 DEBUG Query with image successful
2024-08-23 14:04:39,805 DEBUG Querying model with image: gpt-4o
2024-08-23 14:04:46,553 DEBUG Query with image successful
2024-08-23 14:20:09,840 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-23 14:20:09,882 DEBUG OpenAI client created
2024-08-23 14:20:14,969 DEBUG Querying model with image: gpt-4o
2024-08-23 14:20:20,597 DEBUG Query with image successful
2024-08-23 14:20:20,641 DEBUG Querying model with image: gpt-4o
2024-08-23 14:20:25,595 DEBUG Query with image successful
2024-08-23 14:20:25,794 DEBUG Querying model with image: gpt-4o
2024-08-23 14:20:32,967 DEBUG Query with image successful
2024-08-23 14:20:33,008 DEBUG Querying model with image: gpt-4o
2024-08-23 14:20:38,274 DEBUG Query with image successful
2024-08-23 14:20:48,488 DEBUG Querying model with image: gpt-4o
2024-08-23 14:20:54,150 DEBUG Query with image successful
2024-08-23 14:20:54,202 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:00,829 DEBUG Query with image successful
2024-08-23 14:21:01,105 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:05,802 DEBUG Query with image successful
2024-08-23 14:21:05,850 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:11,395 DEBUG Query with image successful
2024-08-23 14:21:11,651 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:17,340 DEBUG Query with image successful
2024-08-23 14:21:17,393 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:23,341 DEBUG Query with image successful
2024-08-23 14:21:23,676 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:30,842 DEBUG Query with image successful
2024-08-23 14:21:30,887 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:35,311 DEBUG Query with image successful
2024-08-23 14:21:35,521 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:41,773 DEBUG Query with image successful
2024-08-23 14:21:41,816 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:49,082 DEBUG Query with image successful
2024-08-23 14:21:49,421 DEBUG Querying model with image: gpt-4o
2024-08-23 14:21:55,401 DEBUG Query with image successful
2024-08-23 14:21:55,445 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:00,128 DEBUG Query with image successful
2024-08-23 14:22:00,511 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:09,583 DEBUG Query with image successful
2024-08-23 14:22:09,644 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:14,371 DEBUG Query with image successful
2024-08-23 14:22:14,638 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:19,191 DEBUG Query with image successful
2024-08-23 14:22:19,248 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:23,556 DEBUG Query with image successful
2024-08-23 14:22:23,779 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:29,976 DEBUG Query with image successful
2024-08-23 14:22:30,019 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:34,323 DEBUG Query with image successful
2024-08-23 14:22:34,667 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:40,376 DEBUG Query with image successful
2024-08-23 14:22:40,422 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:47,257 DEBUG Query with image successful
2024-08-23 14:22:47,501 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:52,380 DEBUG Query with image successful
2024-08-23 14:22:52,429 DEBUG Querying model with image: gpt-4o
2024-08-23 14:22:56,512 DEBUG Query with image successful
2024-08-23 14:22:56,739 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:02,790 DEBUG Query with image successful
2024-08-23 14:23:02,840 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:06,916 DEBUG Query with image successful
2024-08-23 14:23:07,168 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:12,191 DEBUG Query with image successful
2024-08-23 14:23:12,236 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:16,308 DEBUG Query with image successful
2024-08-23 14:23:16,533 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:21,582 DEBUG Query with image successful
2024-08-23 14:23:21,632 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:26,147 DEBUG Query with image successful
2024-08-23 14:23:26,382 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:31,056 DEBUG Query with image successful
2024-08-23 14:23:31,099 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:35,750 DEBUG Query with image successful
2024-08-23 14:23:35,965 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:40,686 DEBUG Query with image successful
2024-08-23 14:23:40,729 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:44,608 DEBUG Query with image successful
2024-08-23 14:23:44,811 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:49,285 DEBUG Query with image successful
2024-08-23 14:23:49,329 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:54,468 DEBUG Query with image successful
2024-08-23 14:23:54,749 DEBUG Querying model with image: gpt-4o
2024-08-23 14:23:58,907 DEBUG Query with image successful
2024-08-23 14:23:58,961 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:02,884 DEBUG Query with image successful
2024-08-23 14:24:03,099 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:07,790 DEBUG Query with image successful
2024-08-23 14:24:07,838 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:12,696 DEBUG Query with image successful
2024-08-23 14:24:12,908 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:17,587 DEBUG Query with image successful
2024-08-23 14:24:17,628 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:21,628 DEBUG Query with image successful
2024-08-23 14:24:21,829 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:26,093 DEBUG Query with image successful
2024-08-23 14:24:26,145 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:29,426 DEBUG Query with image successful
2024-08-23 14:24:29,636 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:33,538 DEBUG Query with image successful
2024-08-23 14:24:33,581 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:37,341 DEBUG Query with image successful
2024-08-23 14:24:37,540 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:42,071 DEBUG Query with image successful
2024-08-23 14:24:42,125 DEBUG Querying model with image: gpt-4o
2024-08-23 14:24:45,761 DEBUG Query with image successful
2024-08-26 16:05:31,274 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:05:31,454 DEBUG OpenAI client created
2024-08-26 16:05:36,520 DEBUG Querying model with image: gpt-4o
2024-08-26 16:05:42,632 DEBUG Query with image successful
2024-08-26 16:05:42,671 DEBUG Querying model with image: gpt-4o
2024-08-26 16:05:47,665 DEBUG Query with image successful
2024-08-26 16:05:47,846 DEBUG Querying model with image: gpt-4o
2024-08-26 16:05:52,406 DEBUG Query with image successful
2024-08-26 16:05:52,440 DEBUG Querying model with image: gpt-4o
2024-08-26 16:05:57,092 DEBUG Query with image successful
2024-08-26 16:06:08,001 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:13,747 DEBUG Query with image successful
2024-08-26 16:06:13,780 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:18,085 DEBUG Query with image successful
2024-08-26 16:06:18,287 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:22,824 DEBUG Query with image successful
2024-08-26 16:06:22,863 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:27,705 DEBUG Query with image successful
2024-08-26 16:06:27,970 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:32,987 DEBUG Query with image successful
2024-08-26 16:06:33,022 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:38,012 DEBUG Query with image successful
2024-08-26 16:06:38,266 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:42,184 DEBUG Query with image successful
2024-08-26 16:06:42,219 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:47,988 DEBUG Query with image successful
2024-08-26 16:06:48,202 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:53,142 DEBUG Query with image successful
2024-08-26 16:06:53,174 DEBUG Querying model with image: gpt-4o
2024-08-26 16:06:57,857 DEBUG Query with image successful
2024-08-26 16:06:58,069 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:04,586 DEBUG Query with image successful
2024-08-26 16:07:04,624 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:08,884 DEBUG Query with image successful
2024-08-26 16:07:09,090 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:14,303 DEBUG Query with image successful
2024-08-26 16:07:14,344 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:22,950 DEBUG Query with image successful
2024-08-26 16:07:23,198 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:28,681 DEBUG Query with image successful
2024-08-26 16:07:28,716 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:33,237 DEBUG Query with image successful
2024-08-26 16:07:33,430 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:38,565 DEBUG Query with image successful
2024-08-26 16:07:38,599 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:43,071 DEBUG Query with image successful
2024-08-26 16:07:43,270 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:47,959 DEBUG Query with image successful
2024-08-26 16:07:47,992 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:52,672 DEBUG Query with image successful
2024-08-26 16:07:52,865 DEBUG Querying model with image: gpt-4o
2024-08-26 16:07:58,217 DEBUG Query with image successful
2024-08-26 16:07:58,249 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:03,124 DEBUG Query with image successful
2024-08-26 16:08:03,318 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:07,634 DEBUG Query with image successful
2024-08-26 16:08:07,667 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:12,435 DEBUG Query with image successful
2024-08-26 16:08:13,110 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:17,299 DEBUG Query with image successful
2024-08-26 16:08:17,333 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:23,450 DEBUG Query with image successful
2024-08-26 16:08:23,648 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:28,341 DEBUG Query with image successful
2024-08-26 16:08:28,378 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:33,131 DEBUG Query with image successful
2024-08-26 16:08:33,330 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:38,115 DEBUG Query with image successful
2024-08-26 16:08:38,149 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:42,439 DEBUG Query with image successful
2024-08-26 16:08:42,628 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:48,234 DEBUG Query with image successful
2024-08-26 16:08:48,268 DEBUG Querying model with image: gpt-4o
2024-08-26 16:08:54,205 DEBUG Query with image successful
2024-08-26 16:08:54,391 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:01,944 DEBUG Query with image successful
2024-08-26 16:09:01,975 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:06,781 DEBUG Query with image successful
2024-08-26 16:09:06,983 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:10,871 DEBUG Query with image successful
2024-08-26 16:09:10,905 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:15,233 DEBUG Query with image successful
2024-08-26 16:09:15,465 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:20,548 DEBUG Query with image successful
2024-08-26 16:09:20,587 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:25,489 DEBUG Query with image successful
2024-08-26 16:09:25,676 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:30,692 DEBUG Query with image successful
2024-08-26 16:09:30,729 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:34,835 DEBUG Query with image successful
2024-08-26 16:09:35,015 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:39,626 DEBUG Query with image successful
2024-08-26 16:09:39,660 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:43,500 DEBUG Query with image successful
2024-08-26 16:09:43,672 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:47,717 DEBUG Query with image successful
2024-08-26 16:09:47,752 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:50,837 DEBUG Query with image successful
2024-08-26 16:09:51,014 DEBUG Querying model with image: gpt-4o
2024-08-26 16:09:58,620 DEBUG Query with image successful
2024-08-26 16:09:58,654 DEBUG Querying model with image: gpt-4o
2024-08-26 16:10:03,072 DEBUG Query with image successful
2024-08-26 16:23:39,291 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:23:39,314 DEBUG OpenAI client created
2024-08-26 16:23:44,378 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:23:53,976 DEBUG Query with image successful
2024-08-26 16:23:54,011 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:24:02,258 DEBUG Query with image successful
2024-08-26 16:24:02,432 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:24:11,333 DEBUG Query with image successful
2024-08-26 16:24:11,366 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:24:24,086 DEBUG Query with image successful
2024-08-26 16:24:34,265 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:24:43,010 DEBUG Query with image successful
2024-08-26 16:24:43,043 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:24:51,821 DEBUG Query with image successful
2024-08-26 16:24:52,003 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:25:01,292 DEBUG Query with image successful
2024-08-26 16:25:01,327 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:25:12,011 DEBUG Query with image successful
2024-08-26 16:25:12,190 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:25:21,285 DEBUG Query with image successful
2024-08-26 16:25:21,321 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:25:32,623 DEBUG Query with image successful
2024-08-26 16:25:32,803 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:25:41,155 DEBUG Query with image successful
2024-08-26 16:25:41,188 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:25:49,014 DEBUG Query with image successful
2024-08-26 16:25:49,195 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:25:59,797 DEBUG Query with image successful
2024-08-26 16:25:59,833 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:26:08,925 DEBUG Query with image successful
2024-08-26 16:26:09,243 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:26:17,737 DEBUG Query with image successful
2024-08-26 16:26:17,772 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:26:26,313 DEBUG Query with image successful
2024-08-26 16:26:26,499 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:26:35,553 DEBUG Query with image successful
2024-08-26 16:26:35,590 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:26:44,897 DEBUG Query with image successful
2024-08-26 16:26:45,081 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:26:53,049 DEBUG Query with image successful
2024-08-26 16:26:53,080 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:04,640 DEBUG Query with image successful
2024-08-26 16:27:04,868 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:12,884 DEBUG Query with image successful
2024-08-26 16:27:12,915 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:19,340 DEBUG Query with image successful
2024-08-26 16:27:19,536 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:26,588 DEBUG Query with image successful
2024-08-26 16:27:26,627 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:34,074 DEBUG Query with image successful
2024-08-26 16:27:34,264 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:42,484 DEBUG Query with image successful
2024-08-26 16:27:42,521 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:49,475 DEBUG Query with image successful
2024-08-26 16:27:49,671 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:27:59,612 DEBUG Query with image successful
2024-08-26 16:27:59,644 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:28:08,297 DEBUG Query with image successful
2024-08-26 16:28:08,522 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:28:17,937 DEBUG Query with image successful
2024-08-26 16:28:17,971 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:28:25,846 DEBUG Query with image successful
2024-08-26 16:28:26,046 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:28:33,937 DEBUG Query with image successful
2024-08-26 16:28:33,972 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:28:41,705 DEBUG Query with image successful
2024-08-26 16:28:41,892 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:28:51,394 DEBUG Query with image successful
2024-08-26 16:28:51,428 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:28:57,521 DEBUG Query with image successful
2024-08-26 16:28:57,711 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:29:07,067 DEBUG Query with image successful
2024-08-26 16:29:07,103 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:29:20,070 DEBUG Query with image successful
2024-08-26 16:29:20,274 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:29:33,627 DEBUG Query with image successful
2024-08-26 16:29:33,660 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:29:42,864 DEBUG Query with image successful
2024-08-26 16:29:43,197 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:29:52,606 DEBUG Query with image successful
2024-08-26 16:29:52,645 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:29:59,043 DEBUG Query with image successful
2024-08-26 16:29:59,225 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:30:08,670 DEBUG Query with image successful
2024-08-26 16:30:08,704 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:30:18,340 DEBUG Query with image successful
2024-08-26 16:30:18,538 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:30:28,869 DEBUG Query with image successful
2024-08-26 16:30:28,900 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:30:35,623 DEBUG Query with image successful
2024-08-26 16:30:35,813 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:30:45,110 DEBUG Query with image successful
2024-08-26 16:30:45,149 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:30:50,558 DEBUG Query with image successful
2024-08-26 16:30:50,750 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:30:57,778 DEBUG Query with image successful
2024-08-26 16:30:57,812 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:31:03,062 DEBUG Query with image successful
2024-08-26 16:31:03,238 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:31:10,262 DEBUG Query with image successful
2024-08-26 16:31:10,295 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:31:16,053 DEBUG Query with image successful
2024-08-26 16:43:12,670 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:43:12,691 DEBUG OpenAI client created
2024-08-26 16:43:17,757 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:43:28,341 DEBUG Query with image successful
2024-08-26 16:43:28,379 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:43:42,484 DEBUG Query with image successful
2024-08-26 16:43:42,661 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:43:50,602 DEBUG Query with image successful
2024-08-26 16:43:50,635 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:44:05,275 DEBUG Query with image successful
2024-08-26 16:44:15,450 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:44:24,531 DEBUG Query with image successful
2024-08-26 16:44:24,569 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:44:37,493 DEBUG Query with image successful
2024-08-26 16:44:37,682 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:44:46,592 DEBUG Query with image successful
2024-08-26 16:44:46,624 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:45:04,791 DEBUG Query with image successful
2024-08-26 16:45:04,971 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:45:18,815 DEBUG Query with image successful
2024-08-26 16:45:18,853 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:56:10,101 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:56:10,128 DEBUG OpenAI client created
2024-08-26 16:56:15,187 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:56:26,885 DEBUG Query with image successful
2024-08-26 16:56:26,918 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:56:40,927 DEBUG Query with image successful
2024-08-26 16:56:41,100 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:56:52,644 DEBUG Query with image successful
2024-08-26 16:56:52,681 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:57:09,606 DEBUG Query with image successful
2024-08-26 16:57:19,779 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:57:33,185 DEBUG Query with image successful
2024-08-26 16:57:33,225 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:57:51,253 DEBUG Query with image successful
2024-08-26 16:57:51,446 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:58:14,814 DEBUG Query with image successful
2024-08-26 16:58:14,848 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:58:54,453 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:58:54,477 DEBUG OpenAI client created
2024-08-26 16:58:59,540 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:59:15,576 DEBUG Query with image successful
2024-08-26 16:59:15,609 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:59:40,315 DEBUG Query with image successful
2024-08-26 16:59:40,508 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:59:50,286 DEBUG Query with image successful
2024-08-26 16:59:50,324 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:00:25,110 DEBUG Query with image successful
2024-08-26 17:00:35,397 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:00:51,921 DEBUG Query with image successful
2024-08-26 17:00:51,968 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:01:29,966 DEBUG Query with image successful
2024-08-26 17:01:30,138 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:01:37,870 DEBUG Query with image successful
2024-08-26 17:01:37,903 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:03:31,136 DEBUG Query with image successful
2024-08-26 17:03:31,335 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:03:48,564 DEBUG Query with image successful
2024-08-26 17:03:48,602 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:04:18,401 DEBUG Query with image successful
2024-08-26 17:04:18,585 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:04:35,620 DEBUG Query with image successful
2024-08-26 17:04:35,654 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:04:59,340 DEBUG Query with image successful
2024-08-26 17:04:59,517 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:05:08,478 DEBUG Query with image successful
2024-08-26 17:05:08,518 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:05:29,369 DEBUG Query with image successful
2024-08-26 17:05:29,599 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:05:40,834 DEBUG Query with image successful
2024-08-26 17:05:40,871 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:06:07,788 DEBUG Query with image successful
2024-08-26 17:06:08,032 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:06:17,760 DEBUG Query with image successful
2024-08-26 17:06:17,796 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:06:48,218 DEBUG Query with image successful
2024-08-26 17:06:48,432 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:06:59,644 DEBUG Query with image successful
2024-08-26 17:06:59,681 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:07:27,582 DEBUG Query with image successful
2024-08-26 17:07:27,770 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:07:39,028 DEBUG Query with image successful
2024-08-26 17:07:39,063 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:08:02,199 DEBUG Query with image successful
2024-08-26 17:08:02,457 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:08:09,706 DEBUG Query with image successful
2024-08-26 17:08:09,739 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:08:40,514 DEBUG Query with image successful
2024-08-26 17:08:40,729 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:08:49,661 DEBUG Query with image successful
2024-08-26 17:08:49,694 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:09:18,494 DEBUG Query with image successful
2024-08-26 17:09:18,678 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:09:37,040 DEBUG Query with image successful
2024-08-26 17:09:37,075 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:09:56,563 DEBUG Query with image successful
2024-08-26 17:09:56,744 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:10:07,137 DEBUG Query with image successful
2024-08-26 17:10:07,169 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:10:31,209 DEBUG Query with image successful
2024-08-26 17:10:31,388 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:10:50,170 DEBUG Query with image successful
2024-08-26 17:10:50,203 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:11:34,176 DEBUG Query with image successful
2024-08-26 17:11:34,424 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:11:44,906 DEBUG Query with image successful
2024-08-26 17:11:44,939 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:12:06,065 DEBUG Query with image successful
2024-08-26 17:12:06,242 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:12:15,726 DEBUG Query with image successful
2024-08-26 17:12:15,757 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:12:32,773 DEBUG Query with image successful
2024-08-26 17:12:32,947 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:12:42,790 DEBUG Query with image successful
2024-08-26 17:12:42,823 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:13:00,607 DEBUG Query with image successful
2024-08-26 17:13:00,787 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:13:12,436 DEBUG Query with image successful
2024-08-26 17:13:12,472 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:13:31,536 DEBUG Query with image successful
2024-08-26 17:13:31,725 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:13:45,520 DEBUG Query with image successful
2024-08-26 17:13:45,552 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:14:02,414 DEBUG Query with image successful
2024-08-26 17:14:02,600 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:14:12,228 DEBUG Query with image successful
2024-08-26 17:14:12,260 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:14:34,317 DEBUG Query with image successful
2024-08-26 17:14:34,493 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:14:43,035 DEBUG Query with image successful
2024-08-26 17:14:43,069 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:14:59,327 DEBUG Query with image successful
2024-08-26 17:14:59,510 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:15:11,130 DEBUG Query with image successful
2024-08-26 17:15:11,163 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:15:28,158 DEBUG Query with image successful
2024-08-26 17:15:28,331 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:15:37,090 DEBUG Query with image successful
2024-08-26 17:15:37,124 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:15:52,909 DEBUG Query with image successful
2024-08-26 17:51:12,607 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 17:51:12,634 DEBUG OpenAI client created
2024-08-26 17:51:12,634 DEBUG Model set to: gpt-4-turbo
2024-08-26 17:51:17,699 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:52:03,744 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 17:52:03,771 DEBUG OpenAI client created
2024-08-26 17:52:03,771 DEBUG Model set to: gpt-4-turbo
2024-08-26 17:52:08,847 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:52:18,780 DEBUG Query with image successful
2024-08-26 17:52:18,813 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:52:34,001 DEBUG Query with image successful
2024-08-26 17:52:34,276 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:52:44,155 DEBUG Query with image successful
2024-08-26 17:52:44,188 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:52:54,537 DEBUG Query with image successful
2024-08-26 17:53:04,712 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:53:16,892 DEBUG Query with image successful
2024-08-26 17:53:16,927 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:53:29,157 DEBUG Query with image successful
2024-08-26 17:53:29,402 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:53:39,221 DEBUG Query with image successful
2024-08-26 17:53:39,254 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:53:50,081 DEBUG Query with image successful
2024-08-26 17:53:50,350 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:54:02,223 DEBUG Query with image successful
2024-08-26 17:54:02,260 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:54:12,495 DEBUG Query with image successful
2024-08-26 17:54:12,728 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:54:23,076 DEBUG Query with image successful
2024-08-26 17:54:23,108 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:54:33,459 DEBUG Query with image successful
2024-08-26 17:54:33,654 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:54:42,569 DEBUG Query with image successful
2024-08-26 17:54:42,603 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:54:52,334 DEBUG Query with image successful
2024-08-26 17:54:52,528 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:55:04,721 DEBUG Query with image successful
2024-08-26 17:55:04,755 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:55:16,230 DEBUG Query with image successful
2024-08-26 17:55:16,483 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:55:29,935 DEBUG Query with image successful
2024-08-26 17:55:29,972 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:55:36,942 DEBUG Query with image successful
2024-08-26 17:55:37,134 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:55:46,346 DEBUG Query with image successful
2024-08-26 17:55:46,379 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:55:55,744 DEBUG Query with image successful
2024-08-26 17:55:55,970 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:56:04,924 DEBUG Query with image successful
2024-08-26 17:56:04,957 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:56:13,768 DEBUG Query with image successful
2024-08-26 17:56:13,996 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:56:23,095 DEBUG Query with image successful
2024-08-26 17:56:23,128 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:56:31,022 DEBUG Query with image successful
2024-08-26 17:56:31,197 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:56:42,010 DEBUG Query with image successful
2024-08-26 17:56:42,042 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:56:48,941 DEBUG Query with image successful
2024-08-26 17:56:49,125 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:56:59,019 DEBUG Query with image successful
2024-08-26 17:56:59,051 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:57:05,040 DEBUG Query with image successful
2024-08-26 17:57:05,228 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:57:15,178 DEBUG Query with image successful
2024-08-26 17:57:15,212 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:57:22,690 DEBUG Query with image successful
2024-08-26 17:57:22,873 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:57:31,085 DEBUG Query with image successful
2024-08-26 17:57:31,118 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:57:38,454 DEBUG Query with image successful
2024-08-26 17:57:38,638 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:57:48,903 DEBUG Query with image successful
2024-08-26 17:57:48,933 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:57:54,076 DEBUG Query with image successful
2024-08-26 17:57:54,249 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:02,917 DEBUG Query with image successful
2024-08-26 17:58:02,947 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:08,611 DEBUG Query with image successful
2024-08-26 17:58:08,782 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:17,704 DEBUG Query with image successful
2024-08-26 17:58:17,737 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:24,040 DEBUG Query with image successful
2024-08-26 17:58:24,215 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:34,083 DEBUG Query with image successful
2024-08-26 17:58:34,114 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:39,756 DEBUG Query with image successful
2024-08-26 17:58:39,933 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:49,002 DEBUG Query with image successful
2024-08-26 17:58:49,035 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:58:53,810 DEBUG Query with image successful
2024-08-26 17:58:54,000 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:02,857 DEBUG Query with image successful
2024-08-26 17:59:02,893 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:10,969 DEBUG Query with image successful
2024-08-26 17:59:11,141 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:20,709 DEBUG Query with image successful
2024-08-26 17:59:20,744 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:27,426 DEBUG Query with image successful
2024-08-26 17:59:27,597 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:36,698 DEBUG Query with image successful
2024-08-26 17:59:36,731 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:41,160 DEBUG Query with image successful
2024-08-26 17:59:41,340 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:51,300 DEBUG Query with image successful
2024-08-26 17:59:51,332 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 17:59:56,430 DEBUG Query with image successful
2024-08-28 20:43:19,108 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-28 20:43:19,150 DEBUG OpenAI client created
2024-08-28 20:43:19,151 DEBUG Model set to: gpt-4-turbo
2024-08-28 20:43:24,184 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:43:28,789 DEBUG Query with image successful
2024-08-28 20:43:28,801 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:43:35,605 DEBUG Query with image successful
2024-08-28 20:43:35,756 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:43:45,166 DEBUG Query with image successful
2024-08-28 20:43:45,183 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:43:53,430 DEBUG Query with image successful
2024-08-28 20:44:15,404 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-28 20:44:15,437 DEBUG OpenAI client created
2024-08-28 20:44:15,437 DEBUG Model set to: gpt-4-turbo
2024-08-28 20:44:20,466 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:44:30,612 DEBUG Query with image successful
2024-08-28 20:44:30,624 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:44:40,237 DEBUG Query with image successful
2024-08-28 20:44:40,447 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:44:47,680 DEBUG Query with image successful
2024-08-28 20:44:47,698 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:44:57,058 DEBUG Query with image successful
2024-08-28 20:45:07,201 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:47:35,398 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-28 20:47:35,424 DEBUG OpenAI client created
2024-08-28 20:47:35,425 DEBUG Model set to: gpt-4-turbo
2024-08-28 20:47:40,462 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:47:52,176 DEBUG Query with image successful
2024-08-28 20:47:52,188 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:48:05,303 DEBUG Query with image successful
2024-08-28 20:48:05,469 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:48:15,631 DEBUG Query with image successful
2024-08-28 20:48:15,647 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:48:24,295 DEBUG Query with image successful
2024-08-28 20:48:34,440 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:48:46,175 DEBUG Query with image successful
2024-08-28 20:48:46,192 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:49:00,185 DEBUG Query with image successful
2024-08-28 20:49:00,459 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:49:15,028 DEBUG Query with image successful
2024-08-28 20:49:15,045 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:49:29,271 DEBUG Query with image successful
2024-08-28 20:49:29,429 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:49:38,464 DEBUG Query with image successful
2024-08-28 20:49:38,486 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:49:51,934 DEBUG Query with image successful
2024-08-28 20:49:52,092 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:50:05,391 DEBUG Query with image successful
2024-08-28 20:50:05,411 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:50:15,471 DEBUG Query with image successful
2024-08-28 20:50:15,712 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:50:25,540 DEBUG Query with image successful
2024-08-28 20:50:25,556 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:50:37,316 DEBUG Query with image successful
2024-08-28 20:50:37,584 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:50:49,238 DEBUG Query with image successful
2024-08-28 20:50:49,268 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:50:58,338 DEBUG Query with image successful
2024-08-28 20:50:58,493 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:51:11,631 DEBUG Query with image successful
2024-08-28 20:51:11,647 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:51:22,497 DEBUG Query with image successful
2024-08-28 20:51:22,657 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:51:33,044 DEBUG Query with image successful
2024-08-28 20:51:33,064 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:51:53,391 DEBUG Query with image successful
2024-08-28 20:51:53,596 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:52:01,954 DEBUG Query with image successful
2024-08-28 20:52:01,983 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:52:10,766 DEBUG Query with image successful
2024-08-28 20:52:10,983 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:52:20,556 DEBUG Query with image successful
2024-08-28 20:52:20,574 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:52:28,485 DEBUG Query with image successful
2024-08-28 20:52:28,699 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:52:36,472 DEBUG Query with image successful
2024-08-28 20:52:36,497 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:52:45,274 DEBUG Query with image successful
2024-08-28 20:52:45,441 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:52:53,902 DEBUG Query with image successful
2024-08-28 20:52:53,930 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:53:01,645 DEBUG Query with image successful
2024-08-28 20:53:01,830 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:53:10,388 DEBUG Query with image successful
2024-08-28 20:53:10,410 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:53:17,397 DEBUG Query with image successful
2024-08-28 20:53:17,565 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:53:28,751 DEBUG Query with image successful
2024-08-28 20:53:28,772 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:53:35,630 DEBUG Query with image successful
2024-08-28 20:53:35,778 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:53:45,594 DEBUG Query with image successful
2024-08-28 20:53:45,614 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:53:53,741 DEBUG Query with image successful
2024-08-28 20:53:53,886 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:54:03,637 DEBUG Query with image successful
2024-08-28 20:54:03,659 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:54:10,129 DEBUG Query with image successful
2024-08-28 20:54:10,290 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:54:20,663 DEBUG Query with image successful
2024-08-28 20:54:20,681 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:54:27,457 DEBUG Query with image successful
2024-08-28 20:54:27,634 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:54:38,316 DEBUG Query with image successful
2024-08-28 20:54:38,336 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:54:45,523 DEBUG Query with image successful
2024-08-28 20:54:45,682 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:54:54,741 DEBUG Query with image successful
2024-08-28 20:54:54,760 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:55:00,140 DEBUG Query with image successful
2024-08-28 20:55:00,301 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:55:06,225 DEBUG Query with image successful
2024-08-28 20:55:06,242 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:55:15,183 DEBUG Query with image successful
2024-08-28 20:55:15,333 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:55:23,265 DEBUG Query with image successful
2024-08-28 20:55:23,284 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:55:33,444 DEBUG Query with image successful
2024-08-28 20:55:33,590 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:55:45,005 DEBUG Query with image successful
2024-08-28 20:55:45,023 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:55:55,500 DEBUG Query with image successful
2024-08-28 20:55:55,658 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:56:06,205 DEBUG Query with image successful
2024-08-28 20:56:06,222 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:56:16,917 DEBUG Query with image successful
2024-08-28 20:58:04,344 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-28 20:58:04,367 DEBUG OpenAI client created
2024-08-28 20:58:04,367 DEBUG Model set to: gpt-4-turbo
2024-08-28 20:58:09,400 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:58:12,814 DEBUG Query with image successful
2024-08-28 20:58:12,831 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:58:21,441 DEBUG Query with image successful
2024-08-28 20:58:21,583 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:58:31,989 DEBUG Query with image successful
2024-08-28 20:58:32,006 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:58:45,256 DEBUG Query with image successful
2024-08-28 20:58:55,401 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:59:04,881 DEBUG Query with image successful
2024-08-28 20:59:04,901 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:59:13,584 DEBUG Query with image successful
2024-08-28 20:59:13,741 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:59:25,476 DEBUG Query with image successful
2024-08-28 20:59:25,493 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:59:37,047 DEBUG Query with image successful
2024-08-28 20:59:37,298 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:59:43,737 DEBUG Query with image successful
2024-08-28 20:59:43,757 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 20:59:52,096 DEBUG Query with image successful
2024-08-28 20:59:52,255 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:00:01,019 DEBUG Query with image successful
2024-08-28 21:00:01,040 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:00:10,971 DEBUG Query with image successful
2024-08-28 21:00:11,131 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:00:26,173 DEBUG Query with image successful
2024-08-28 21:00:26,191 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:00:40,640 DEBUG Query with image successful
2024-08-28 21:00:40,800 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:00:48,256 DEBUG Query with image successful
2024-08-28 21:00:48,271 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:00:59,931 DEBUG Query with image successful
2024-08-28 21:01:00,122 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:01:09,535 DEBUG Query with image successful
2024-08-28 21:01:09,553 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:01:18,521 DEBUG Query with image successful
2024-08-28 21:01:18,695 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:01:28,761 DEBUG Query with image successful
2024-08-28 21:01:28,779 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:01:42,384 DEBUG Query with image successful
2024-08-28 21:01:42,611 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:01:51,436 DEBUG Query with image successful
2024-08-28 21:01:51,452 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:02:06,210 DEBUG Query with image successful
2024-08-28 21:02:06,371 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:02:15,167 DEBUG Query with image successful
2024-08-28 21:02:15,183 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:02:29,237 DEBUG Query with image successful
2024-08-28 21:02:29,421 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:02:36,993 DEBUG Query with image successful
2024-08-28 21:02:37,009 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:02:49,348 DEBUG Query with image successful
2024-08-28 21:02:49,501 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:03:09,855 DEBUG Query with image successful
2024-08-28 21:03:09,872 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:03:20,031 DEBUG Query with image successful
2024-08-28 21:03:20,191 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:03:29,765 DEBUG Query with image successful
2024-08-28 21:03:29,782 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:03:42,905 DEBUG Query with image successful
2024-08-28 21:03:43,063 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:03:57,574 DEBUG Query with image successful
2024-08-28 21:03:57,591 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:04:07,417 DEBUG Query with image successful
2024-08-28 21:04:07,568 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:04:13,983 DEBUG Query with image successful
2024-08-28 21:04:14,004 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:04:22,335 DEBUG Query with image successful
2024-08-28 21:04:22,501 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:04:29,942 DEBUG Query with image successful
2024-08-28 21:04:29,958 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:04:41,750 DEBUG Query with image successful
2024-08-28 21:04:41,903 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:04:49,230 DEBUG Query with image successful
2024-08-28 21:04:49,251 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:00,679 DEBUG Query with image successful
2024-08-28 21:05:00,838 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:06,573 DEBUG Query with image successful
2024-08-28 21:05:06,589 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:13,257 DEBUG Query with image successful
2024-08-28 21:05:13,424 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:20,950 DEBUG Query with image successful
2024-08-28 21:05:20,966 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:27,979 DEBUG Query with image successful
2024-08-28 21:05:28,132 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:37,152 DEBUG Query with image successful
2024-08-28 21:05:37,168 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:43,957 DEBUG Query with image successful
2024-08-28 21:05:44,104 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:51,221 DEBUG Query with image successful
2024-08-28 21:05:51,240 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:05:59,630 DEBUG Query with image successful
2024-08-28 21:05:59,821 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:06:08,485 DEBUG Query with image successful
2024-08-28 21:06:08,500 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:06:21,203 DEBUG Query with image successful
2024-08-28 21:06:21,348 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:06:33,064 DEBUG Query with image successful
2024-08-28 21:06:33,080 DEBUG Querying model with image: gpt-4-turbo
2024-08-28 21:06:37,941 DEBUG Query with image successful
2024-08-29 14:02:29,242 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-29 14:02:29,264 DEBUG OpenAI client created
2024-08-29 14:02:29,264 DEBUG Model set to: gpt-4-turbo
2024-08-29 14:02:34,299 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:02:44,565 DEBUG Query with image successful
2024-08-29 14:02:44,578 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:02:58,306 DEBUG Query with image successful
2024-08-29 14:02:58,512 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:03:11,529 DEBUG Query with image successful
2024-08-29 14:03:11,547 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:03:22,832 DEBUG Query with image successful
2024-08-29 14:03:32,977 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:03:42,219 DEBUG Query with image successful
2024-08-29 14:03:42,243 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:03:53,364 DEBUG Query with image successful
2024-08-29 14:03:53,605 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:04:02,032 DEBUG Query with image successful
2024-08-29 14:04:02,049 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:04:13,854 DEBUG Query with image successful
2024-08-29 14:04:14,107 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:04:24,851 DEBUG Query with image successful
2024-08-29 14:04:24,876 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:04:38,800 DEBUG Query with image successful
2024-08-29 14:04:38,973 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:04:52,201 DEBUG Query with image successful
2024-08-29 14:04:52,218 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:05:02,524 DEBUG Query with image successful
2024-08-29 14:05:02,689 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:05:10,597 DEBUG Query with image successful
2024-08-29 14:05:10,613 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:05:22,561 DEBUG Query with image successful
2024-08-29 14:05:22,723 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:05:30,997 DEBUG Query with image successful
2024-08-29 14:05:31,017 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:05:42,547 DEBUG Query with image successful
2024-08-29 14:05:42,727 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:05:51,478 DEBUG Query with image successful
2024-08-29 14:05:51,497 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:06:00,635 DEBUG Query with image successful
2024-08-29 14:06:00,838 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:06:08,245 DEBUG Query with image successful
2024-08-29 14:06:08,264 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:06:22,642 DEBUG Query with image successful
2024-08-29 14:06:22,792 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:06:32,631 DEBUG Query with image successful
2024-08-29 14:06:32,646 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:06:44,369 DEBUG Query with image successful
2024-08-29 14:06:44,562 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:06:49,871 DEBUG Query with image successful
2024-08-29 14:06:49,889 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:06:57,149 DEBUG Query with image successful
2024-08-29 14:06:57,303 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:07:04,988 DEBUG Query with image successful
2024-08-29 14:07:05,005 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:07:13,081 DEBUG Query with image successful
2024-08-29 14:07:13,234 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:07:24,741 DEBUG Query with image successful
2024-08-29 14:07:24,760 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:07:34,095 DEBUG Query with image successful
2024-08-29 14:07:34,249 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:07:42,185 DEBUG Query with image successful
2024-08-29 14:07:42,205 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:07:53,248 DEBUG Query with image successful
2024-08-29 14:07:53,416 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:01,342 DEBUG Query with image successful
2024-08-29 14:08:01,360 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:10,155 DEBUG Query with image successful
2024-08-29 14:08:10,309 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:17,336 DEBUG Query with image successful
2024-08-29 14:08:17,353 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:26,002 DEBUG Query with image successful
2024-08-29 14:08:26,178 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:33,879 DEBUG Query with image successful
2024-08-29 14:08:33,899 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:40,606 DEBUG Query with image successful
2024-08-29 14:08:40,761 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:47,046 DEBUG Query with image successful
2024-08-29 14:08:47,063 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:08:56,275 DEBUG Query with image successful
2024-08-29 14:08:56,428 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:04,493 DEBUG Query with image successful
2024-08-29 14:09:04,510 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:11,639 DEBUG Query with image successful
2024-08-29 14:09:11,790 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:19,226 DEBUG Query with image successful
2024-08-29 14:09:19,242 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:25,585 DEBUG Query with image successful
2024-08-29 14:09:25,751 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:35,448 DEBUG Query with image successful
2024-08-29 14:09:35,466 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:40,800 DEBUG Query with image successful
2024-08-29 14:09:40,949 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:49,858 DEBUG Query with image successful
2024-08-29 14:09:49,877 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:09:55,618 DEBUG Query with image successful
2024-08-29 14:09:55,761 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:10:03,957 DEBUG Query with image successful
2024-08-29 14:10:03,976 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:10:14,767 DEBUG Query with image successful
2024-08-29 14:10:14,910 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:10:25,629 DEBUG Query with image successful
2024-08-29 14:10:25,645 DEBUG Querying model with image: gpt-4-turbo
2024-08-29 14:10:30,283 DEBUG Query with image successful
2024-09-18 19:51:46,794 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-18 19:51:46,838 DEBUG OpenAI client created
2024-09-18 19:51:46,838 DEBUG Model set to: gpt-4-turbo
2024-09-18 19:51:51,997 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:52:08,160 DEBUG Query with image successful
2024-09-18 19:52:08,238 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:52:23,934 DEBUG Query with image successful
2024-09-18 19:52:24,305 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:52:39,490 DEBUG Query with image successful
2024-09-18 19:52:39,571 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:52:56,512 DEBUG Query with image successful
2024-09-18 19:53:06,778 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:53:20,464 DEBUG Query with image successful
2024-09-18 19:53:20,547 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:53:35,377 DEBUG Query with image successful
2024-09-18 19:53:35,644 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:53:57,314 DEBUG Query with image successful
2024-09-18 19:53:57,395 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:54:15,168 DEBUG Query with image successful
2024-09-18 19:54:15,456 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:54:30,211 DEBUG Query with image successful
2024-09-18 19:54:30,293 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:54:43,898 DEBUG Query with image successful
2024-09-18 19:54:44,187 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:54:59,708 DEBUG Query with image successful
2024-09-18 19:54:59,789 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:55:14,171 DEBUG Query with image successful
2024-09-18 19:55:14,564 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:55:28,858 DEBUG Query with image successful
2024-09-18 19:55:28,948 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:55:43,182 DEBUG Query with image successful
2024-09-18 19:55:43,492 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:56:02,942 DEBUG Query with image successful
2024-09-18 19:56:03,040 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:56:23,454 DEBUG Query with image successful
2024-09-18 19:56:23,766 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:56:41,959 DEBUG Query with image successful
2024-09-18 19:56:42,047 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:56:59,576 DEBUG Query with image successful
2024-09-18 19:56:59,923 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:57:18,008 DEBUG Query with image successful
2024-09-18 19:57:18,099 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:57:39,898 DEBUG Query with image successful
2024-09-18 19:57:40,215 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:57:56,068 DEBUG Query with image successful
2024-09-18 19:57:56,172 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:58:12,833 DEBUG Query with image successful
2024-09-18 19:58:13,166 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:58:28,251 DEBUG Query with image successful
2024-09-18 19:58:28,344 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:58:44,413 DEBUG Query with image successful
2024-09-18 19:58:44,728 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:58:59,141 DEBUG Query with image successful
2024-09-18 19:58:59,229 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:59:14,384 DEBUG Query with image successful
2024-09-18 19:59:14,685 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:59:30,278 DEBUG Query with image successful
2024-09-18 19:59:30,369 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 19:59:45,250 DEBUG Query with image successful
2024-09-18 19:59:45,557 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:00:00,459 DEBUG Query with image successful
2024-09-18 20:00:00,545 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:00:16,494 DEBUG Query with image successful
2024-09-18 20:00:16,798 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:00:31,492 DEBUG Query with image successful
2024-09-18 20:00:31,579 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:00:48,246 DEBUG Query with image successful
2024-09-18 20:00:48,540 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:01:06,863 DEBUG Query with image successful
2024-09-18 20:01:06,966 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:01:21,519 DEBUG Query with image successful
2024-09-18 20:01:21,817 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:01:38,883 DEBUG Query with image successful
2024-09-18 20:01:38,970 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:01:53,294 DEBUG Query with image successful
2024-09-18 20:01:53,587 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:02:10,558 DEBUG Query with image successful
2024-09-18 20:02:10,644 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:02:23,200 DEBUG Query with image successful
2024-09-18 20:02:23,513 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:02:40,055 DEBUG Query with image successful
2024-09-18 20:02:40,148 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:02:53,589 DEBUG Query with image successful
2024-09-18 20:02:53,869 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:03:07,266 DEBUG Query with image successful
2024-09-18 20:03:07,352 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:03:21,637 DEBUG Query with image successful
2024-09-18 20:03:21,931 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:03:36,590 DEBUG Query with image successful
2024-09-18 20:03:36,677 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:03:54,167 DEBUG Query with image successful
2024-09-18 20:03:54,466 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:04:08,781 DEBUG Query with image successful
2024-09-18 20:04:08,872 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:04:22,724 DEBUG Query with image successful
2024-09-18 20:04:23,048 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:04:37,103 DEBUG Query with image successful
2024-09-18 20:04:37,186 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:04:52,017 DEBUG Query with image successful
2024-09-18 20:04:52,315 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:05:03,302 DEBUG Query with image successful
2024-09-18 20:05:03,397 DEBUG Querying model with image: gpt-4-turbo
2024-09-18 20:05:18,634 DEBUG Query with image successful
2024-09-29 19:22:41,921 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-29 19:22:41,949 DEBUG OpenAI client created
2024-09-29 19:22:41,949 DEBUG Model set to: gpt-4-turbo
2024-09-29 19:49:57,584 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-29 19:49:57,608 DEBUG OpenAI client created
2024-09-29 19:49:57,609 DEBUG Model set to: gpt-4-turbo
2024-09-29 19:50:02,777 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:50:13,779 DEBUG Query successful
2024-09-29 19:50:13,874 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:50:24,093 DEBUG Query successful
2024-09-29 19:50:24,376 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:50:35,039 DEBUG Query successful
2024-09-29 19:50:35,129 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:50:44,376 DEBUG Query successful
2024-09-29 19:50:54,662 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:51:05,043 DEBUG Query successful
2024-09-29 19:51:05,135 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:51:26,274 DEBUG Query successful
2024-09-29 19:51:26,554 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:51:37,499 DEBUG Query successful
2024-09-29 19:51:37,588 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:51:46,281 DEBUG Query successful
2024-09-29 19:51:46,552 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:51:54,996 DEBUG Query successful
2024-09-29 19:51:55,086 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:52:07,327 DEBUG Query successful
2024-09-29 19:52:07,712 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:52:17,782 DEBUG Query successful
2024-09-29 19:52:17,869 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:52:35,895 DEBUG Query successful
2024-09-29 19:52:36,283 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:52:49,784 DEBUG Query successful
2024-09-29 19:52:49,880 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:53:05,701 DEBUG Query successful
2024-09-29 19:53:05,982 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:53:24,803 DEBUG Query successful
2024-09-29 19:53:24,894 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:53:34,480 DEBUG Query successful
2024-09-29 19:53:34,781 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:53:50,503 DEBUG Query successful
2024-09-29 19:53:50,595 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:54:01,596 DEBUG Query successful
2024-09-29 19:54:01,892 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:54:12,699 DEBUG Query successful
2024-09-29 19:54:12,789 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:54:25,553 DEBUG Query successful
2024-09-29 19:54:25,854 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:54:39,551 DEBUG Query successful
2024-09-29 19:54:39,645 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:54:51,798 DEBUG Query successful
2024-09-29 19:54:52,094 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:55:03,654 DEBUG Query successful
2024-09-29 19:55:03,758 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:55:12,181 DEBUG Query successful
2024-09-29 19:55:12,478 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:55:28,570 DEBUG Query successful
2024-09-29 19:55:28,664 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:55:36,874 DEBUG Query successful
2024-09-29 19:55:37,172 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:55:46,855 DEBUG Query successful
2024-09-29 19:55:46,946 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:55:54,240 DEBUG Query successful
2024-09-29 19:55:54,528 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:56:10,943 DEBUG Query successful
2024-09-29 19:56:11,038 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:56:18,194 DEBUG Query successful
2024-09-29 19:56:18,485 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:56:34,610 DEBUG Query successful
2024-09-29 19:56:34,709 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:56:41,048 DEBUG Query successful
2024-09-29 19:56:41,347 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:56:53,257 DEBUG Query successful
2024-09-29 19:56:53,353 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:57:01,438 DEBUG Query successful
2024-09-29 19:57:01,726 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:57:15,211 DEBUG Query successful
2024-09-29 19:57:15,304 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:57:23,595 DEBUG Query successful
2024-09-29 19:57:23,880 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:57:35,426 DEBUG Query successful
2024-09-29 19:57:35,524 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:57:41,022 DEBUG Query successful
2024-09-29 19:57:41,307 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:57:51,142 DEBUG Query successful
2024-09-29 19:57:51,244 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:57:58,239 DEBUG Query successful
2024-09-29 19:57:58,532 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:58:07,414 DEBUG Query successful
2024-09-29 19:58:07,509 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:58:16,565 DEBUG Query successful
2024-09-29 19:58:16,856 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:58:30,282 DEBUG Query successful
2024-09-29 19:58:30,380 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:58:38,664 DEBUG Query successful
2024-09-29 19:58:38,951 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:58:49,572 DEBUG Query successful
2024-09-29 19:58:49,669 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:58:56,339 DEBUG Query successful
2024-09-29 19:58:56,629 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:59:05,960 DEBUG Query successful
2024-09-29 19:59:06,052 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:59:13,689 DEBUG Query successful
2024-09-29 19:59:13,977 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:59:22,557 DEBUG Query successful
2024-09-29 19:59:22,662 DEBUG Querying model: gpt-4-turbo
2024-09-29 19:59:29,587 DEBUG Query successful
2024-10-04 19:30:26,025 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:30:26,175 DEBUG OpenAI client created
2024-10-04 19:30:26,175 DEBUG Model set to: gpt-4-turbo
2024-10-04 19:33:20,914 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:33:20,945 DEBUG OpenAI client created
2024-10-04 19:33:20,945 DEBUG Model set to: gpt-4-turbo
2024-10-04 19:36:09,352 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:36:09,381 DEBUG OpenAI client created
2024-10-04 19:36:09,382 DEBUG Model set to: gpt-4-turbo
2024-10-04 19:36:09,397 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:36:09,421 DEBUG OpenAI client created
2024-10-04 19:36:09,421 DEBUG Model set to: gpt-4o
2024-10-04 19:36:14,974 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:36:26,776 DEBUG Query successful
2024-10-04 19:36:26,778 DEBUG Querying model: gpt-4o
2024-10-04 19:36:27,641 DEBUG Query successful
2024-10-04 19:36:27,663 DEBUG Querying model: gpt-4o
2024-10-04 19:36:33,099 DEBUG Query successful
2024-10-04 19:36:33,181 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:37:33,106 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:37:33,135 DEBUG OpenAI client created
2024-10-04 19:37:33,135 DEBUG Model set to: gpt-4-turbo
2024-10-04 19:37:33,142 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:37:33,168 DEBUG OpenAI client created
2024-10-04 19:37:33,168 DEBUG Model set to: gpt-4o
2024-10-04 19:37:38,323 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:37:46,450 DEBUG Query successful
2024-10-04 19:37:46,452 DEBUG Querying model: gpt-4o
2024-10-04 19:37:47,202 DEBUG Query successful
2024-10-04 19:37:47,202 DEBUG Querying model: gpt-4o
2024-10-04 19:37:50,640 DEBUG Query successful
2024-10-04 19:37:50,725 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:38:01,153 DEBUG Query successful
2024-10-04 19:38:01,471 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:38:15,148 DEBUG Query successful
2024-10-04 19:38:15,151 DEBUG Querying model: gpt-4o
2024-10-04 19:38:15,974 DEBUG Query successful
2024-10-04 19:38:15,990 DEBUG Querying model: gpt-4o
2024-10-04 19:38:20,670 DEBUG Query successful
2024-10-04 19:38:20,760 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:38:35,510 DEBUG Query successful
2024-10-04 19:38:45,782 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:38:59,286 DEBUG Query successful
2024-10-04 19:38:59,296 DEBUG Querying model: gpt-4o
2024-10-04 19:39:00,215 DEBUG Query successful
2024-10-04 19:39:00,216 DEBUG Querying model: gpt-4o
2024-10-04 19:39:05,118 DEBUG Query successful
2024-10-04 19:39:05,203 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:39:18,292 DEBUG Query successful
2024-10-04 19:39:18,592 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:39:31,175 DEBUG Query successful
2024-10-04 19:39:31,181 DEBUG Querying model: gpt-4o
2024-10-04 19:39:32,145 DEBUG Query successful
2024-10-04 19:39:32,146 DEBUG Querying model: gpt-4o
2024-10-04 19:39:36,318 DEBUG Query successful
2024-10-04 19:39:36,412 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:39:47,651 DEBUG Query successful
2024-10-04 19:39:47,945 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:40:01,055 DEBUG Query successful
2024-10-04 19:40:01,058 DEBUG Querying model: gpt-4o
2024-10-04 19:40:01,994 DEBUG Query successful
2024-10-04 19:40:01,994 DEBUG Querying model: gpt-4o
2024-10-04 19:40:06,584 DEBUG Query successful
2024-10-04 19:40:06,669 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:40:15,428 DEBUG Query successful
2024-10-04 19:40:15,729 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:40:27,866 DEBUG Query successful
2024-10-04 19:40:27,868 DEBUG Querying model: gpt-4o
2024-10-04 19:40:28,835 DEBUG Query successful
2024-10-04 19:40:28,835 DEBUG Querying model: gpt-4o
2024-10-04 19:40:39,981 DEBUG Query successful
2024-10-04 19:40:40,068 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:40:50,492 DEBUG Query successful
2024-10-04 19:40:50,788 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:41:09,356 DEBUG Query successful
2024-10-04 19:41:09,359 DEBUG Querying model: gpt-4o
2024-10-04 19:41:10,510 DEBUG Query successful
2024-10-04 19:41:10,510 DEBUG Querying model: gpt-4o
2024-10-04 19:41:15,932 DEBUG Query successful
2024-10-04 19:41:16,028 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:41:31,571 DEBUG Query successful
2024-10-04 19:41:31,857 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:41:46,623 DEBUG Query successful
2024-10-04 19:41:46,629 DEBUG Querying model: gpt-4o
2024-10-04 19:41:47,603 DEBUG Query successful
2024-10-04 19:41:47,603 DEBUG Querying model: gpt-4o
2024-10-04 19:41:53,492 DEBUG Query successful
2024-10-04 19:41:53,571 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:42:05,125 DEBUG Query successful
2024-10-04 19:42:05,409 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:42:17,832 DEBUG Query successful
2024-10-04 19:42:17,834 DEBUG Querying model: gpt-4o
2024-10-04 19:42:19,058 DEBUG Query successful
2024-10-04 19:42:19,058 DEBUG Querying model: gpt-4o
2024-10-04 19:42:24,663 DEBUG Query successful
2024-10-04 19:42:24,748 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:42:35,120 DEBUG Query successful
2024-10-04 19:42:35,425 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:42:48,968 DEBUG Query successful
2024-10-04 19:42:48,970 DEBUG Querying model: gpt-4o
2024-10-04 19:42:50,315 DEBUG Query successful
2024-10-04 19:42:50,315 DEBUG Querying model: gpt-4o
2024-10-04 19:42:57,197 DEBUG Query successful
2024-10-04 19:42:57,280 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:43:09,623 DEBUG Query successful
2024-10-04 19:43:09,913 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:43:22,947 DEBUG Query successful
2024-10-04 19:43:22,949 DEBUG Querying model: gpt-4o
2024-10-04 19:43:24,235 DEBUG Query successful
2024-10-04 19:43:24,235 DEBUG Querying model: gpt-4o
2024-10-04 19:43:29,968 DEBUG Query successful
2024-10-04 19:43:30,058 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:43:39,683 DEBUG Query successful
2024-10-04 19:43:39,964 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:43:52,628 DEBUG Query successful
2024-10-04 19:43:52,631 DEBUG Querying model: gpt-4o
2024-10-04 19:43:53,952 DEBUG Query successful
2024-10-04 19:43:53,952 DEBUG Querying model: gpt-4o
2024-10-04 19:43:59,808 DEBUG Query successful
2024-10-04 19:43:59,893 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:44:18,159 DEBUG Query successful
2024-10-04 19:44:18,438 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:44:30,762 DEBUG Query successful
2024-10-04 19:44:30,764 DEBUG Querying model: gpt-4o
2024-10-04 19:44:32,077 DEBUG Query successful
2024-10-04 19:44:32,077 DEBUG Querying model: gpt-4o
2024-10-04 19:44:38,584 DEBUG Query successful
2024-10-04 19:44:38,675 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:44:50,471 DEBUG Query successful
2024-10-04 19:44:50,761 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:45:07,000 DEBUG Query successful
2024-10-04 19:45:07,002 DEBUG Querying model: gpt-4o
2024-10-04 19:45:08,502 DEBUG Query successful
2024-10-04 19:45:08,502 DEBUG Querying model: gpt-4o
2024-10-04 19:45:13,549 DEBUG Query successful
2024-10-04 19:45:13,638 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:45:22,785 DEBUG Query successful
2024-10-04 19:45:23,104 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:45:36,551 DEBUG Query successful
2024-10-04 19:45:36,554 DEBUG Querying model: gpt-4o
2024-10-04 19:45:38,399 DEBUG Query successful
2024-10-04 19:45:38,399 DEBUG Querying model: gpt-4o
2024-10-04 19:45:45,519 DEBUG Query successful
2024-10-04 19:45:45,622 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:45:55,052 DEBUG Query successful
2024-10-04 19:45:55,339 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:46:07,944 DEBUG Query successful
2024-10-04 19:46:07,949 DEBUG Querying model: gpt-4o
2024-10-04 19:46:09,508 DEBUG Query successful
2024-10-04 19:46:09,509 DEBUG Querying model: gpt-4o
2024-10-04 19:46:14,626 DEBUG Query successful
2024-10-04 19:46:14,713 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:46:22,499 DEBUG Query successful
2024-10-04 19:46:22,771 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:46:36,040 DEBUG Query successful
2024-10-04 19:46:36,043 DEBUG Querying model: gpt-4o
2024-10-04 19:46:37,706 DEBUG Query successful
2024-10-04 19:46:37,706 DEBUG Querying model: gpt-4o
2024-10-04 19:46:43,477 DEBUG Query successful
2024-10-04 19:46:43,564 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:46:52,802 DEBUG Query successful
2024-10-04 19:46:53,083 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:47:07,910 DEBUG Query successful
2024-10-04 19:47:07,913 DEBUG Querying model: gpt-4o
2024-10-04 19:47:09,549 DEBUG Query successful
2024-10-04 19:47:09,549 DEBUG Querying model: gpt-4o
2024-10-04 19:47:16,468 DEBUG Query successful
2024-10-04 19:47:16,555 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:47:25,297 DEBUG Query successful
2024-10-04 19:47:25,578 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:47:37,530 DEBUG Query successful
2024-10-04 19:47:37,532 DEBUG Querying model: gpt-4o
2024-10-04 19:47:39,097 DEBUG Query successful
2024-10-04 19:47:39,098 DEBUG Querying model: gpt-4o
2024-10-04 19:47:44,212 DEBUG Query successful
2024-10-04 19:47:44,307 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:47:54,225 DEBUG Query successful
2024-10-04 19:47:54,505 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:48:09,163 DEBUG Query successful
2024-10-04 19:48:09,170 DEBUG Querying model: gpt-4o
2024-10-04 19:48:11,016 DEBUG Query successful
2024-10-04 19:48:11,016 DEBUG Querying model: gpt-4o
2024-10-04 19:48:16,939 DEBUG Query successful
2024-10-04 19:48:17,028 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:48:31,002 DEBUG Query successful
2024-10-04 19:48:31,276 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:48:45,716 DEBUG Query successful
2024-10-04 19:48:45,718 DEBUG Querying model: gpt-4o
2024-10-04 19:48:47,270 DEBUG Query successful
2024-10-04 19:48:47,271 DEBUG Querying model: gpt-4o
2024-10-04 19:48:52,843 DEBUG Query successful
2024-10-04 19:48:52,929 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:49:03,407 DEBUG Query successful
2024-10-04 19:49:03,696 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:49:19,286 DEBUG Query successful
2024-10-04 19:49:19,289 DEBUG Querying model: gpt-4o
2024-10-04 19:49:21,116 DEBUG Query successful
2024-10-04 19:49:21,116 DEBUG Querying model: gpt-4o
2024-10-04 19:49:27,772 DEBUG Query successful
2024-10-04 19:49:27,853 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:49:38,023 DEBUG Query successful
2024-10-04 19:49:38,318 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:49:52,132 DEBUG Query successful
2024-10-04 19:49:52,135 DEBUG Querying model: gpt-4o
2024-10-04 19:49:53,888 DEBUG Query successful
2024-10-04 19:49:53,896 DEBUG Querying model: gpt-4o
2024-10-04 19:50:02,007 DEBUG Query successful
2024-10-04 19:50:02,098 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:50:11,416 DEBUG Query successful
2024-10-04 19:50:11,696 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:50:25,255 DEBUG Query successful
2024-10-04 19:50:25,262 DEBUG Querying model: gpt-4o
2024-10-04 19:50:27,217 DEBUG Query successful
2024-10-04 19:50:27,217 DEBUG Querying model: gpt-4o
2024-10-04 19:50:36,154 DEBUG Query successful
2024-10-04 19:50:36,246 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:50:44,335 DEBUG Query successful
2024-10-04 19:50:44,622 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:50:57,978 DEBUG Query successful
2024-10-04 19:50:57,981 DEBUG Querying model: gpt-4o
2024-10-04 19:50:59,959 DEBUG Query successful
2024-10-04 19:50:59,959 DEBUG Querying model: gpt-4o
2024-10-04 19:51:10,906 DEBUG Query successful
2024-10-04 19:51:10,995 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:51:34,705 DEBUG Query successful
2024-10-04 19:58:23,181 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:58:23,206 DEBUG OpenAI client created
2024-10-04 19:58:23,206 DEBUG Model set to: gpt-4-turbo
2024-10-04 19:58:23,213 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 19:58:23,235 DEBUG OpenAI client created
2024-10-04 19:58:23,235 DEBUG Model set to: gpt-4o
2024-10-04 19:58:28,332 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:58:34,862 DEBUG Query successful
2024-10-04 19:58:34,863 DEBUG Querying model: gpt-4o
2024-10-04 19:58:35,672 DEBUG Query successful
2024-10-04 19:58:35,672 DEBUG Querying model: gpt-4o
2024-10-04 19:58:42,231 DEBUG Query successful
2024-10-04 19:58:42,241 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:58:56,042 DEBUG Query successful
2024-10-04 19:58:56,192 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:59:01,641 DEBUG Query successful
2024-10-04 19:59:01,642 DEBUG Querying model: gpt-4o
2024-10-04 19:59:02,453 DEBUG Query successful
2024-10-04 19:59:02,460 DEBUG Querying model: gpt-4o
2024-10-04 19:59:08,389 DEBUG Query successful
2024-10-04 19:59:08,410 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:59:17,175 DEBUG Query successful
2024-10-04 19:59:27,436 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:59:36,818 DEBUG Query successful
2024-10-04 19:59:36,824 DEBUG Querying model: gpt-4o
2024-10-04 19:59:37,991 DEBUG Query successful
2024-10-04 19:59:37,991 DEBUG Querying model: gpt-4o
2024-10-04 19:59:43,730 DEBUG Query successful
2024-10-04 19:59:43,807 DEBUG Querying model: gpt-4-turbo
2024-10-04 19:59:55,601 DEBUG Query successful
2024-10-04 19:59:55,881 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:00:12,554 DEBUG Query successful
2024-10-04 20:00:12,556 DEBUG Querying model: gpt-4o
2024-10-04 20:00:13,474 DEBUG Query successful
2024-10-04 20:00:13,475 DEBUG Querying model: gpt-4o
2024-10-04 20:00:19,073 DEBUG Query successful
2024-10-04 20:00:19,157 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:00:33,341 DEBUG Query successful
2024-10-04 20:00:33,617 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:00:48,558 DEBUG Query successful
2024-10-04 20:00:48,563 DEBUG Querying model: gpt-4o
2024-10-04 20:00:49,572 DEBUG Query successful
2024-10-04 20:00:49,572 DEBUG Querying model: gpt-4o
2024-10-04 20:00:58,822 DEBUG Query successful
2024-10-04 20:00:58,915 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:01:11,722 DEBUG Query successful
2024-10-04 20:01:12,010 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:01:26,167 DEBUG Query successful
2024-10-04 20:01:26,169 DEBUG Querying model: gpt-4o
2024-10-04 20:01:27,173 DEBUG Query successful
2024-10-04 20:01:27,174 DEBUG Querying model: gpt-4o
2024-10-04 20:01:36,955 DEBUG Query successful
2024-10-04 20:01:37,051 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:01:52,991 DEBUG Query successful
2024-10-04 20:01:53,269 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:02:07,297 DEBUG Query successful
2024-10-04 20:02:07,305 DEBUG Querying model: gpt-4o
2024-10-04 20:02:08,531 DEBUG Query successful
2024-10-04 20:02:08,531 DEBUG Querying model: gpt-4o
2024-10-04 20:58:09,922 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 20:58:09,952 DEBUG OpenAI client created
2024-10-04 20:58:09,952 DEBUG Model set to: gpt-4-turbo
2024-10-04 20:58:09,952 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-04 20:58:09,975 DEBUG OpenAI client created
2024-10-04 20:58:09,976 DEBUG Model set to: gpt-4
2024-10-04 20:58:15,147 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:58:27,286 DEBUG Query successful
2024-10-04 20:58:27,289 DEBUG Querying model: gpt-4
2024-10-04 20:58:29,589 DEBUG Query successful
2024-10-04 20:58:29,614 DEBUG Querying model: gpt-4
2024-10-04 20:59:02,783 DEBUG Query successful
2024-10-04 20:59:02,862 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:59:13,039 DEBUG Query successful
2024-10-04 20:59:13,043 DEBUG Querying model: gpt-4
2024-10-04 20:59:21,741 DEBUG Query successful
2024-10-04 20:59:22,050 DEBUG Querying model: gpt-4-turbo
2024-10-04 20:59:39,367 DEBUG Query successful
2024-10-04 20:59:39,371 DEBUG Querying model: gpt-4
2024-10-04 20:59:41,820 DEBUG Query successful
2024-10-04 20:59:41,820 DEBUG Querying model: gpt-4
2024-10-04 21:00:10,220 DEBUG Query successful
2024-10-04 21:00:10,304 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:25:04,895 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 14:25:04,928 DEBUG OpenAI client created
2024-10-05 14:25:04,928 DEBUG Model set to: gpt-4-turbo
2024-10-05 14:25:04,928 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 14:25:04,954 DEBUG OpenAI client created
2024-10-05 14:25:04,954 DEBUG Model set to: gpt-4
2024-10-05 14:25:10,011 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:25:16,158 DEBUG Query successful
2024-10-05 14:25:16,159 DEBUG Querying model: gpt-4
2024-10-05 14:25:17,796 DEBUG Query successful
2024-10-05 14:25:17,848 DEBUG Querying model: gpt-4
2024-10-05 14:25:37,099 DEBUG Query successful
2024-10-05 14:25:37,112 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:25:43,949 DEBUG Query successful
2024-10-05 14:25:43,954 DEBUG Querying model: gpt-4
2024-10-05 14:25:50,724 DEBUG Query successful
2024-10-05 14:25:50,866 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:25:56,430 DEBUG Query successful
2024-10-05 14:25:56,431 DEBUG Querying model: gpt-4
2024-10-05 14:25:58,644 DEBUG Query successful
2024-10-05 14:25:58,644 DEBUG Querying model: gpt-4
2024-10-05 14:26:19,681 DEBUG Query successful
2024-10-05 14:26:19,700 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:26:32,253 DEBUG Query successful
2024-10-05 14:26:32,253 DEBUG Querying model: gpt-4
2024-10-05 14:26:33,442 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8617 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8617 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:26:43,890 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:26:52,293 DEBUG Query successful
2024-10-05 14:26:52,298 DEBUG Querying model: gpt-4
2024-10-05 14:26:52,900 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8735 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8735 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:26:52,901 DEBUG Querying model: gpt-4
2024-10-05 14:26:53,438 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:26:53,526 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:27:05,243 DEBUG Query successful
2024-10-05 14:27:05,246 DEBUG Querying model: gpt-4
2024-10-05 14:27:05,795 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8617 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8617 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:27:06,080 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:27:32,415 DEBUG Query successful
2024-10-05 14:27:32,417 DEBUG Querying model: gpt-4
2024-10-05 14:27:33,068 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8763 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8763 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:27:33,070 DEBUG Querying model: gpt-4
2024-10-05 14:27:33,728 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:27:33,823 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:27:47,149 DEBUG Query successful
2024-10-05 14:27:47,150 DEBUG Querying model: gpt-4
2024-10-05 14:27:47,789 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8623 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8623 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:27:48,069 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:27:56,434 DEBUG Query successful
2024-10-05 14:27:56,436 DEBUG Querying model: gpt-4
2024-10-05 14:27:57,034 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8747 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8747 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:27:57,035 DEBUG Querying model: gpt-4
2024-10-05 14:27:57,522 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:27:57,613 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:28:15,546 DEBUG Query successful
2024-10-05 14:28:15,552 DEBUG Querying model: gpt-4
2024-10-05 14:28:16,114 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8615 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8615 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:28:16,390 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:28:25,513 DEBUG Query successful
2024-10-05 14:28:25,518 DEBUG Querying model: gpt-4
2024-10-05 14:28:26,205 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8811 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8811 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:28:26,206 DEBUG Querying model: gpt-4
2024-10-05 14:28:26,657 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:28:26,744 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:28:44,076 DEBUG Query successful
2024-10-05 14:28:44,077 DEBUG Querying model: gpt-4
2024-10-05 14:28:44,715 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8623 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8623 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:28:44,984 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:28:52,755 DEBUG Query successful
2024-10-05 14:28:52,761 DEBUG Querying model: gpt-4
2024-10-05 14:28:53,358 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8736 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8736 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:28:53,359 DEBUG Querying model: gpt-4
2024-10-05 14:28:53,856 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:28:53,942 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:29:22,363 DEBUG Query successful
2024-10-05 14:29:22,365 DEBUG Querying model: gpt-4
2024-10-05 14:29:22,979 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8623 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8623 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:29:23,379 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:29:31,983 DEBUG Query successful
2024-10-05 14:29:31,984 DEBUG Querying model: gpt-4
2024-10-05 14:29:32,535 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8679 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8679 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:29:32,536 DEBUG Querying model: gpt-4
2024-10-05 14:29:33,021 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:29:33,105 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:30:00,237 DEBUG Query successful
2024-10-05 14:30:00,238 DEBUG Querying model: gpt-4
2024-10-05 14:30:00,832 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8602 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8602 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:30:01,117 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:30:11,630 DEBUG Query successful
2024-10-05 14:30:11,632 DEBUG Querying model: gpt-4
2024-10-05 14:30:12,211 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8762 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8762 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:30:12,212 DEBUG Querying model: gpt-4
2024-10-05 14:30:12,716 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:30:12,800 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:30:37,752 DEBUG Query successful
2024-10-05 14:30:37,755 DEBUG Querying model: gpt-4
2024-10-05 14:30:38,334 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8602 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8602 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:30:38,601 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:30:47,510 DEBUG Query successful
2024-10-05 14:30:47,512 DEBUG Querying model: gpt-4
2024-10-05 14:30:48,128 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8753 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8753 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:30:48,130 DEBUG Querying model: gpt-4
2024-10-05 14:30:48,578 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:30:48,668 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:31:14,993 DEBUG Query successful
2024-10-05 14:31:14,994 DEBUG Querying model: gpt-4
2024-10-05 14:31:15,597 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8602 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8602 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:31:15,861 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:31:24,075 DEBUG Query successful
2024-10-05 14:31:24,081 DEBUG Querying model: gpt-4
2024-10-05 14:31:24,657 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8741 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8741 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:31:24,658 DEBUG Querying model: gpt-4
2024-10-05 14:31:25,106 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:31:25,184 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:31:53,485 DEBUG Query successful
2024-10-05 14:31:53,486 DEBUG Querying model: gpt-4
2024-10-05 14:31:54,061 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8569 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8569 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:31:54,326 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:32:02,333 DEBUG Query successful
2024-10-05 14:32:02,335 DEBUG Querying model: gpt-4
2024-10-05 14:32:02,890 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8700 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8700 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:32:02,891 DEBUG Querying model: gpt-4
2024-10-05 14:32:03,337 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:32:03,417 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:32:32,469 DEBUG Query successful
2024-10-05 14:32:32,471 DEBUG Querying model: gpt-4
2024-10-05 14:32:33,030 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8569 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8569 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:32:33,300 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:32:40,590 DEBUG Query successful
2024-10-05 14:32:40,592 DEBUG Querying model: gpt-4
2024-10-05 14:32:41,170 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8659 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8659 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:32:41,171 DEBUG Querying model: gpt-4
2024-10-05 14:32:41,615 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:32:41,694 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:33:12,887 DEBUG Query successful
2024-10-05 14:33:12,888 DEBUG Querying model: gpt-4
2024-10-05 14:33:13,503 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8534 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8534 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:33:13,782 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:33:21,307 DEBUG Query successful
2024-10-05 14:33:21,311 DEBUG Querying model: gpt-4
2024-10-05 14:33:21,901 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8654 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8654 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:33:21,903 DEBUG Querying model: gpt-4
2024-10-05 14:33:22,366 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:33:22,457 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:33:54,378 DEBUG Query successful
2024-10-05 14:33:54,379 DEBUG Querying model: gpt-4
2024-10-05 14:33:54,962 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8526 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8526 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:33:55,233 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:34:03,664 DEBUG Query successful
2024-10-05 14:34:03,671 DEBUG Querying model: gpt-4
2024-10-05 14:34:04,484 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8676 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8676 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:34:04,485 DEBUG Querying model: gpt-4
2024-10-05 14:34:04,895 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:34:04,977 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:34:38,269 DEBUG Query successful
2024-10-05 14:34:38,273 DEBUG Querying model: gpt-4
2024-10-05 14:34:38,901 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8526 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8526 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:34:39,163 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:34:49,374 DEBUG Query successful
2024-10-05 14:34:49,378 DEBUG Querying model: gpt-4
2024-10-05 14:34:49,967 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8637 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8637 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:34:49,968 DEBUG Querying model: gpt-4
2024-10-05 14:34:50,430 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:34:50,510 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:35:21,355 DEBUG Query successful
2024-10-05 14:35:21,356 DEBUG Querying model: gpt-4
2024-10-05 14:35:21,951 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8493 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8493 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:35:22,223 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:35:29,776 DEBUG Query successful
2024-10-05 14:35:29,779 DEBUG Querying model: gpt-4
2024-10-05 14:35:30,384 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8608 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8608 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:35:30,385 DEBUG Querying model: gpt-4
2024-10-05 14:35:30,859 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:35:30,950 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:36:10,593 DEBUG Query successful
2024-10-05 14:36:10,598 DEBUG Querying model: gpt-4
2024-10-05 14:36:11,187 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8478 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8478 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:36:11,466 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:36:18,726 DEBUG Query successful
2024-10-05 14:36:18,728 DEBUG Querying model: gpt-4
2024-10-05 14:36:19,336 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8627 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8627 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:36:19,337 DEBUG Querying model: gpt-4
2024-10-05 14:36:19,838 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:36:19,933 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:36:59,437 DEBUG Query successful
2024-10-05 14:36:59,439 DEBUG Querying model: gpt-4
2024-10-05 14:37:00,372 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8478 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8478 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:37:00,668 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:37:11,291 DEBUG Query successful
2024-10-05 14:37:11,297 DEBUG Querying model: gpt-4
2024-10-05 14:37:11,910 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8686 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8686 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:37:11,911 DEBUG Querying model: gpt-4
2024-10-05 14:37:12,363 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:37:12,452 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:37:50,583 DEBUG Query successful
2024-10-05 14:37:50,587 DEBUG Querying model: gpt-4
2024-10-05 14:37:51,242 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8470 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8470 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:37:51,521 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:37:59,871 DEBUG Query successful
2024-10-05 14:37:59,873 DEBUG Querying model: gpt-4
2024-10-05 14:38:00,428 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8648 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8648 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:38:00,430 DEBUG Querying model: gpt-4
2024-10-05 14:38:00,915 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:38:01,000 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:38:40,278 DEBUG Query successful
2024-10-05 14:38:40,280 DEBUG Querying model: gpt-4
2024-10-05 14:38:40,981 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8470 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8470 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:38:41,282 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:38:47,737 DEBUG Query successful
2024-10-05 14:38:47,739 DEBUG Querying model: gpt-4
2024-10-05 14:38:48,330 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8556 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8556 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:38:48,331 DEBUG Querying model: gpt-4
2024-10-05 14:38:48,801 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:38:48,891 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:39:30,393 DEBUG Query successful
2024-10-05 14:39:30,400 DEBUG Querying model: gpt-4
2024-10-05 14:39:31,006 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8447 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8447 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:39:31,290 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:39:42,062 DEBUG Query successful
2024-10-05 14:39:42,066 DEBUG Querying model: gpt-4
2024-10-05 14:39:42,605 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8611 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8611 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:39:42,606 DEBUG Querying model: gpt-4
2024-10-05 14:39:43,074 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:39:43,164 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:40:36,943 DEBUG Query successful
2024-10-05 14:40:36,946 DEBUG Querying model: gpt-4
2024-10-05 14:40:37,543 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8447 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8447 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:40:37,841 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:40:45,319 DEBUG Query successful
2024-10-05 14:40:45,325 DEBUG Querying model: gpt-4
2024-10-05 14:40:45,900 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8566 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8566 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:40:45,901 DEBUG Querying model: gpt-4
2024-10-05 14:40:46,392 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:40:46,478 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:41:36,911 DEBUG Query successful
2024-10-05 14:41:36,912 DEBUG Querying model: gpt-4
2024-10-05 14:41:37,532 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8447 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8447 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:41:37,834 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:41:51,010 DEBUG Query successful
2024-10-05 14:41:51,012 DEBUG Querying model: gpt-4
2024-10-05 14:41:51,613 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8527 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8527 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:41:51,614 DEBUG Querying model: gpt-4
2024-10-05 14:41:52,089 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:41:52,177 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:42:43,748 DEBUG Query successful
2024-10-05 14:42:43,750 DEBUG Querying model: gpt-4
2024-10-05 14:42:44,352 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8414 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8414 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:42:44,627 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:42:50,938 DEBUG Query successful
2024-10-05 14:42:50,940 DEBUG Querying model: gpt-4
2024-10-05 14:42:51,515 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8536 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8536 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:42:51,517 DEBUG Querying model: gpt-4
2024-10-05 14:42:51,960 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8224 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:42:52,050 DEBUG Querying model: gpt-4-turbo
2024-10-05 14:43:38,430 DEBUG Query successful
2024-10-05 14:43:38,436 DEBUG Querying model: gpt-4
2024-10-05 14:43:39,044 ERROR An error occurred: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8422 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8422 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-10-05 14:55:32,713 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 14:55:32,737 DEBUG OpenAI client created
2024-10-05 14:55:32,737 DEBUG Model set to: gpt-4-turbo
2024-10-05 14:55:32,737 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 14:55:32,762 DEBUG OpenAI client created
2024-10-05 14:55:32,762 DEBUG Model set to: gpt-4-turbo
2024-10-05 15:13:03,033 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 15:13:03,061 DEBUG OpenAI client created
2024-10-05 15:13:03,061 DEBUG Model set to: gpt-4-turbo
2024-10-05 15:13:03,061 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 15:13:03,084 DEBUG OpenAI client created
2024-10-05 15:13:03,085 DEBUG Model set to: gpt-4-turbo
2024-10-05 15:13:08,249 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:13:17,196 DEBUG Query successful
2024-10-05 15:22:07,855 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 15:22:07,899 DEBUG OpenAI client created
2024-10-05 15:22:07,899 DEBUG Model set to: gpt-4-turbo
2024-10-05 15:22:07,899 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-05 15:22:07,924 DEBUG OpenAI client created
2024-10-05 15:22:07,924 DEBUG Model set to: gpt-4-turbo
2024-10-05 15:22:13,093 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:22:24,385 DEBUG Query successful
2024-10-05 15:22:24,387 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:22:25,697 DEBUG Query successful
2024-10-05 15:22:25,714 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:22:44,870 DEBUG Query successful
2024-10-05 15:22:44,959 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:22:54,002 DEBUG Query successful
2024-10-05 15:22:54,003 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:23:02,606 DEBUG Query successful
2024-10-05 15:23:02,870 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:23:15,200 DEBUG Query successful
2024-10-05 15:23:15,206 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:23:17,036 DEBUG Query successful
2024-10-05 15:23:17,051 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:23:34,844 DEBUG Query successful
2024-10-05 15:23:34,932 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:23:52,006 DEBUG Query successful
2024-10-05 15:23:52,008 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:24:01,236 DEBUG Query successful
2024-10-05 15:24:11,509 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:24:21,807 DEBUG Query successful
2024-10-05 15:24:21,810 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:24:24,253 DEBUG Query successful
2024-10-05 15:24:24,254 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:24:47,188 DEBUG Query successful
2024-10-05 15:24:47,276 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:25:10,575 DEBUG Query successful
2024-10-05 15:25:10,577 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:25:23,063 DEBUG Query successful
2024-10-05 15:25:23,338 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:25:36,967 DEBUG Query successful
2024-10-05 15:25:36,968 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:25:42,624 DEBUG Query successful
2024-10-05 15:25:42,624 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:26:06,166 DEBUG Query successful
2024-10-05 15:26:06,261 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:26:35,541 DEBUG Query successful
2024-10-05 15:26:35,546 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:26:45,561 DEBUG Query successful
2024-10-05 15:26:45,846 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:26:54,482 DEBUG Query successful
2024-10-05 15:26:54,486 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:26:58,789 DEBUG Query successful
2024-10-05 15:26:58,790 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:27:19,252 DEBUG Query successful
2024-10-05 15:27:19,342 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:27:37,963 DEBUG Query successful
2024-10-05 15:27:37,965 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:27:49,580 DEBUG Query successful
2024-10-05 15:27:49,862 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:28:05,361 DEBUG Query successful
2024-10-05 15:28:05,366 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:28:08,739 DEBUG Query successful
2024-10-05 15:28:08,741 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:28:33,992 DEBUG Query successful
2024-10-05 15:28:34,092 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:28:59,377 DEBUG Query successful
2024-10-05 15:28:59,382 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:29:12,163 DEBUG Query successful
2024-10-05 15:29:12,449 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:29:20,751 DEBUG Query successful
2024-10-05 15:29:20,752 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:29:26,686 DEBUG Query successful
2024-10-05 15:29:26,691 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:29:47,872 DEBUG Query successful
2024-10-05 15:29:47,957 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:30:10,761 DEBUG Query successful
2024-10-05 15:30:10,763 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:30:23,802 DEBUG Query successful
2024-10-05 15:30:24,092 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:30:35,149 DEBUG Query successful
2024-10-05 15:30:35,151 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:30:39,024 DEBUG Query successful
2024-10-05 15:30:39,024 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:31:08,387 DEBUG Query successful
2024-10-05 15:31:08,493 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:31:41,959 DEBUG Query successful
2024-10-05 15:31:41,961 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:31:57,781 DEBUG Query successful
2024-10-05 15:31:58,079 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:32:08,493 DEBUG Query successful
2024-10-05 15:32:08,496 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:32:11,113 DEBUG Query successful
2024-10-05 15:32:11,115 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:32:45,189 DEBUG Query successful
2024-10-05 15:32:45,282 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:33:19,618 DEBUG Query successful
2024-10-05 15:33:19,619 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:33:35,090 DEBUG Query successful
2024-10-05 15:33:35,368 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:33:42,723 DEBUG Query successful
2024-10-05 15:33:42,728 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:33:46,429 DEBUG Query successful
2024-10-05 15:33:46,429 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:34:18,555 DEBUG Query successful
2024-10-05 15:34:18,649 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:34:51,811 DEBUG Query successful
2024-10-05 15:34:51,818 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:35:07,056 DEBUG Query successful
2024-10-05 15:35:07,351 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:35:15,412 DEBUG Query successful
2024-10-05 15:35:15,413 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:35:18,279 DEBUG Query successful
2024-10-05 15:35:18,284 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:35:57,589 DEBUG Query successful
2024-10-05 15:35:57,702 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:36:35,037 DEBUG Query successful
2024-10-05 15:36:35,039 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:36:53,567 DEBUG Query successful
2024-10-05 15:36:53,849 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:37:01,687 DEBUG Query successful
2024-10-05 15:37:01,689 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:37:05,492 DEBUG Query successful
2024-10-05 15:37:05,493 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:37:41,761 DEBUG Query successful
2024-10-05 15:37:41,862 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:38:15,395 DEBUG Query successful
2024-10-05 15:38:15,396 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:38:33,096 DEBUG Query successful
2024-10-05 15:38:33,393 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:38:41,821 DEBUG Query successful
2024-10-05 15:38:41,823 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:38:44,595 DEBUG Query successful
2024-10-05 15:38:44,599 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:39:11,646 DEBUG Query successful
2024-10-05 15:39:11,739 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:39:56,508 DEBUG Query successful
2024-10-05 15:39:56,510 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:40:09,904 DEBUG Query successful
2024-10-05 15:40:10,180 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:40:19,062 DEBUG Query successful
2024-10-05 15:40:19,067 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:40:22,938 DEBUG Query successful
2024-10-05 15:40:22,938 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:40:58,405 DEBUG Query successful
2024-10-05 15:40:58,495 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:41:31,694 DEBUG Query successful
2024-10-05 15:41:31,700 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:41:50,525 DEBUG Query successful
2024-10-05 15:41:50,805 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:41:58,601 DEBUG Query successful
2024-10-05 15:41:58,605 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:42:02,009 DEBUG Query successful
2024-10-05 15:42:02,010 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:42:36,638 DEBUG Query successful
2024-10-05 15:42:36,733 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:43:14,659 DEBUG Query successful
2024-10-05 15:43:14,661 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:43:29,725 DEBUG Query successful
2024-10-05 15:43:30,000 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:43:39,379 DEBUG Query successful
2024-10-05 15:43:39,381 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:43:42,631 DEBUG Query successful
2024-10-05 15:43:42,632 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:44:19,048 DEBUG Query successful
2024-10-05 15:44:19,145 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:44:56,195 DEBUG Query successful
2024-10-05 15:44:56,196 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:46:18,232 DEBUG Query successful
2024-10-05 15:46:18,516 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:46:27,879 DEBUG Query successful
2024-10-05 15:46:27,881 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:46:37,385 DEBUG Query successful
2024-10-05 15:46:37,386 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:47:15,169 DEBUG Query successful
2024-10-05 15:47:15,261 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:47:56,211 DEBUG Query successful
2024-10-05 15:47:56,212 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:48:14,836 DEBUG Query successful
2024-10-05 15:48:15,118 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:48:24,371 DEBUG Query successful
2024-10-05 15:48:24,377 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:48:29,253 DEBUG Query successful
2024-10-05 15:48:29,261 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:49:05,148 DEBUG Query successful
2024-10-05 15:49:05,244 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:49:49,974 DEBUG Query successful
2024-10-05 15:49:49,981 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:50:04,816 DEBUG Query successful
2024-10-05 15:50:05,095 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:50:14,887 DEBUG Query successful
2024-10-05 15:50:14,889 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:50:19,976 DEBUG Query successful
2024-10-05 15:50:19,982 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:51:12,553 DEBUG Query successful
2024-10-05 15:51:12,649 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:51:58,333 DEBUG Query successful
2024-10-05 15:51:58,334 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:52:12,249 DEBUG Query successful
2024-10-05 15:52:12,543 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:52:20,224 DEBUG Query successful
2024-10-05 15:52:20,229 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:52:30,053 DEBUG Query successful
2024-10-05 15:52:30,054 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:53:11,631 DEBUG Query successful
2024-10-05 15:53:11,746 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:54:06,226 DEBUG Query successful
2024-10-05 15:54:06,227 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:54:22,142 DEBUG Query successful
2024-10-05 15:54:22,426 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:54:29,943 DEBUG Query successful
2024-10-05 15:54:29,946 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:54:34,748 DEBUG Query successful
2024-10-05 15:54:34,749 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:55:18,372 DEBUG Query successful
2024-10-05 15:55:18,453 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:56:08,838 DEBUG Query successful
2024-10-05 15:56:08,840 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:56:23,738 DEBUG Query successful
2024-10-05 15:56:24,007 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:56:30,812 DEBUG Query successful
2024-10-05 15:56:30,816 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:56:36,632 DEBUG Query successful
2024-10-05 15:56:36,632 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:57:22,873 DEBUG Query successful
2024-10-05 15:57:22,973 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:58:08,767 DEBUG Query successful
2024-10-05 15:58:08,772 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:58:26,440 DEBUG Query successful
2024-10-05 15:58:26,715 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:58:35,828 DEBUG Query successful
2024-10-05 15:58:35,829 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:58:40,740 DEBUG Query successful
2024-10-05 15:58:40,745 DEBUG Querying model: gpt-4-turbo
2024-10-05 15:59:21,221 DEBUG Query successful
2024-10-05 15:59:21,314 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:01:16,878 DEBUG Query successful
2024-10-05 16:01:16,880 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:01:35,823 DEBUG Query successful
2024-10-05 16:01:36,101 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:01:45,811 DEBUG Query successful
2024-10-05 16:01:45,814 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:01:58,947 DEBUG Query successful
2024-10-05 16:01:58,948 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:02:39,587 DEBUG Query successful
2024-10-05 16:02:39,671 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:03:34,312 DEBUG Query successful
2024-10-05 16:03:34,314 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:03:51,925 DEBUG Query successful
2024-10-05 16:03:52,220 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:04:00,277 DEBUG Query successful
2024-10-05 16:04:00,279 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:04:05,555 DEBUG Query successful
2024-10-05 16:04:05,556 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:04:53,508 DEBUG Query successful
2024-10-05 16:04:53,601 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:05:48,993 DEBUG Query successful
2024-10-05 16:05:48,994 DEBUG Querying model: gpt-4-turbo
2024-10-05 16:06:11,468 DEBUG Query successful
2024-10-06 14:43:44,182 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 14:43:44,340 DEBUG OpenAI client created
2024-10-06 14:43:44,340 DEBUG Model set to: gpt-4-turbo
2024-10-06 14:43:44,340 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 14:43:44,362 DEBUG OpenAI client created
2024-10-06 14:43:44,362 DEBUG Model set to: gpt-4-turbo
2024-10-06 14:43:49,529 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:44:00,675 DEBUG Query successful
2024-10-06 14:44:00,679 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:44:01,807 DEBUG Query successful
2024-10-06 14:44:01,831 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:44:21,027 DEBUG Query successful
2024-10-06 14:44:21,108 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:44:30,429 DEBUG Query successful
2024-10-06 14:44:30,435 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:44:40,460 DEBUG Query successful
2024-10-06 14:44:40,729 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:44:53,120 DEBUG Query successful
2024-10-06 14:44:53,123 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:44:54,535 DEBUG Query successful
2024-10-06 14:44:54,536 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:45:07,667 DEBUG Query successful
2024-10-06 14:45:07,752 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:45:19,680 DEBUG Query successful
2024-10-06 14:45:19,681 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:45:28,981 DEBUG Query successful
2024-10-06 14:45:39,281 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:45:48,871 DEBUG Query successful
2024-10-06 14:45:48,874 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:45:50,612 DEBUG Query successful
2024-10-06 14:45:50,612 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:46:01,852 DEBUG Query successful
2024-10-06 14:46:01,937 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:46:15,259 DEBUG Query successful
2024-10-06 14:46:15,260 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:46:25,316 DEBUG Query successful
2024-10-06 14:46:25,590 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:46:34,758 DEBUG Query successful
2024-10-06 14:46:34,759 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:46:36,308 DEBUG Query successful
2024-10-06 14:46:36,313 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:46:49,895 DEBUG Query successful
2024-10-06 14:46:49,976 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:47:05,076 DEBUG Query successful
2024-10-06 14:47:05,078 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:47:16,195 DEBUG Query successful
2024-10-06 14:47:16,479 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:47:25,494 DEBUG Query successful
2024-10-06 14:47:25,500 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:47:27,732 DEBUG Query successful
2024-10-06 14:47:27,734 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:47:46,292 DEBUG Query successful
2024-10-06 14:47:46,381 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:48:04,506 DEBUG Query successful
2024-10-06 14:48:04,511 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:48:14,662 DEBUG Query successful
2024-10-06 14:48:14,936 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:48:24,130 DEBUG Query successful
2024-10-06 14:48:24,132 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:48:26,702 DEBUG Query successful
2024-10-06 14:48:26,703 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:48:44,169 DEBUG Query successful
2024-10-06 14:48:44,255 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:49:04,765 DEBUG Query successful
2024-10-06 14:49:04,767 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:49:17,255 DEBUG Query successful
2024-10-06 14:49:17,564 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:49:26,410 DEBUG Query successful
2024-10-06 14:49:26,413 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:49:28,839 DEBUG Query successful
2024-10-06 14:49:28,840 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:49:51,675 DEBUG Query successful
2024-10-06 14:49:51,765 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:50:12,959 DEBUG Query successful
2024-10-06 14:50:12,962 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:50:27,317 DEBUG Query successful
2024-10-06 14:50:27,606 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:50:35,093 DEBUG Query successful
2024-10-06 14:50:35,095 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:50:37,044 DEBUG Query successful
2024-10-06 14:50:37,048 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:50:57,004 DEBUG Query successful
2024-10-06 14:50:57,115 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:51:22,081 DEBUG Query successful
2024-10-06 14:51:22,084 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:51:34,183 DEBUG Query successful
2024-10-06 14:51:34,512 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:51:42,760 DEBUG Query successful
2024-10-06 14:51:42,763 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:51:45,328 DEBUG Query successful
2024-10-06 14:51:45,329 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:52:05,906 DEBUG Query successful
2024-10-06 14:52:06,017 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:52:34,737 DEBUG Query successful
2024-10-06 14:52:34,739 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:52:48,484 DEBUG Query successful
2024-10-06 14:52:48,767 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:52:56,336 DEBUG Query successful
2024-10-06 14:52:56,339 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:52:58,942 DEBUG Query successful
2024-10-06 14:52:58,943 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:53:20,200 DEBUG Query successful
2024-10-06 14:53:20,287 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:53:50,131 DEBUG Query successful
2024-10-06 14:53:50,132 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:54:06,826 DEBUG Query successful
2024-10-06 14:54:07,109 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:54:17,310 DEBUG Query successful
2024-10-06 14:54:17,311 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:54:21,650 DEBUG Query successful
2024-10-06 14:54:21,651 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:54:44,696 DEBUG Query successful
2024-10-06 14:54:44,783 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:55:16,716 DEBUG Query successful
2024-10-06 14:55:16,717 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:55:28,605 DEBUG Query successful
2024-10-06 14:55:28,950 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:55:38,435 DEBUG Query successful
2024-10-06 14:55:38,437 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:55:41,309 DEBUG Query successful
2024-10-06 14:55:41,311 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:56:05,378 DEBUG Query successful
2024-10-06 14:56:05,466 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:56:38,658 DEBUG Query successful
2024-10-06 14:56:38,659 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:56:56,132 DEBUG Query successful
2024-10-06 14:56:56,404 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:57:04,967 DEBUG Query successful
2024-10-06 14:57:04,972 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:57:08,059 DEBUG Query successful
2024-10-06 14:57:08,060 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:57:36,430 DEBUG Query successful
2024-10-06 14:57:36,511 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:58:17,084 DEBUG Query successful
2024-10-06 14:58:17,090 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:58:28,870 DEBUG Query successful
2024-10-06 14:58:29,147 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:58:40,991 DEBUG Query successful
2024-10-06 14:58:40,993 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:58:43,480 DEBUG Query successful
2024-10-06 14:58:43,481 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:59:13,590 DEBUG Query successful
2024-10-06 14:59:13,677 DEBUG Querying model: gpt-4-turbo
2024-10-06 14:59:47,148 DEBUG Query successful
2024-10-06 14:59:47,150 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:00:01,667 DEBUG Query successful
2024-10-06 15:00:01,947 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:00:09,738 DEBUG Query successful
2024-10-06 15:00:09,739 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:00:12,976 DEBUG Query successful
2024-10-06 15:00:12,977 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:00:38,077 DEBUG Query successful
2024-10-06 15:00:38,165 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:01:19,801 DEBUG Query successful
2024-10-06 15:01:19,802 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:01:35,373 DEBUG Query successful
2024-10-06 15:01:35,716 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:01:46,893 DEBUG Query successful
2024-10-06 15:01:46,895 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:01:50,616 DEBUG Query successful
2024-10-06 15:01:50,620 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:02:23,459 DEBUG Query successful
2024-10-06 15:02:23,544 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:03:00,737 DEBUG Query successful
2024-10-06 15:03:00,738 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:03:12,347 DEBUG Query successful
2024-10-06 15:03:12,725 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:03:21,365 DEBUG Query successful
2024-10-06 15:03:21,371 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:03:24,166 DEBUG Query successful
2024-10-06 15:03:24,167 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:03:56,808 DEBUG Query successful
2024-10-06 15:03:56,893 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:04:33,761 DEBUG Query successful
2024-10-06 15:04:33,766 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:04:47,200 DEBUG Query successful
2024-10-06 15:04:47,480 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:04:55,601 DEBUG Query successful
2024-10-06 15:04:55,602 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:04:59,069 DEBUG Query successful
2024-10-06 15:04:59,069 DEBUG Querying model: gpt-4-turbo
2024-10-06 15:05:32,116 DEBUG Query successful
2024-10-06 15:05:32,205 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:00:29,170 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 17:00:29,312 DEBUG OpenAI client created
2024-10-06 17:00:29,312 DEBUG Model set to: gpt-4-turbo
2024-10-06 17:00:29,312 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 17:00:29,333 DEBUG OpenAI client created
2024-10-06 17:00:29,333 DEBUG Model set to: gpt-4-turbo
2024-10-06 17:00:34,509 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:00:46,913 DEBUG Query successful
2024-10-06 17:00:46,914 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:00:48,138 DEBUG Query successful
2024-10-06 17:00:48,176 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:01:05,074 DEBUG Query successful
2024-10-06 17:01:05,159 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:01:17,776 DEBUG Query successful
2024-10-06 17:01:17,778 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:01:28,046 DEBUG Query successful
2024-10-06 17:01:28,311 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:01:36,080 DEBUG Query successful
2024-10-06 17:01:36,081 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:01:38,447 DEBUG Query successful
2024-10-06 17:01:38,451 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:01:53,406 DEBUG Query successful
2024-10-06 17:01:53,486 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:02:06,310 DEBUG Query successful
2024-10-06 17:02:06,311 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:02:14,785 DEBUG Query successful
2024-10-06 17:02:25,049 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:02:34,094 DEBUG Query successful
2024-10-06 17:02:34,096 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:02:35,719 DEBUG Query successful
2024-10-06 17:02:35,721 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:02:50,628 DEBUG Query successful
2024-10-06 17:02:50,718 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:03:06,388 DEBUG Query successful
2024-10-06 17:03:06,389 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:03:16,331 DEBUG Query successful
2024-10-06 17:03:16,619 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:03:25,086 DEBUG Query successful
2024-10-06 17:03:25,088 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:03:27,408 DEBUG Query successful
2024-10-06 17:03:27,409 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:03:41,324 DEBUG Query successful
2024-10-06 17:03:41,418 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:03:58,174 DEBUG Query successful
2024-10-06 17:03:58,176 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:04:09,110 DEBUG Query successful
2024-10-06 17:04:09,375 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:04:18,134 DEBUG Query successful
2024-10-06 17:04:18,136 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:04:20,935 DEBUG Query successful
2024-10-06 17:04:20,936 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:04:37,228 DEBUG Query successful
2024-10-06 17:04:37,314 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:04:59,714 DEBUG Query successful
2024-10-06 17:04:59,715 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:05:10,645 DEBUG Query successful
2024-10-06 17:05:10,930 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:05:31,309 DEBUG Query successful
2024-10-06 17:05:31,312 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:05:33,607 DEBUG Query successful
2024-10-06 17:05:33,611 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:05:51,842 DEBUG Query successful
2024-10-06 17:05:51,928 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:06:11,514 DEBUG Query successful
2024-10-06 17:06:11,516 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:06:22,796 DEBUG Query successful
2024-10-06 17:06:23,062 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:06:32,666 DEBUG Query successful
2024-10-06 17:06:32,671 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:06:34,712 DEBUG Query successful
2024-10-06 17:06:34,713 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:06:53,806 DEBUG Query successful
2024-10-06 17:06:53,893 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:07:15,857 DEBUG Query successful
2024-10-06 17:07:15,862 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:07:27,844 DEBUG Query successful
2024-10-06 17:07:28,118 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:07:35,627 DEBUG Query successful
2024-10-06 17:07:35,628 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:07:39,373 DEBUG Query successful
2024-10-06 17:07:39,373 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:07:59,371 DEBUG Query successful
2024-10-06 17:07:59,454 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:08:26,063 DEBUG Query successful
2024-10-06 17:08:26,064 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:08:38,058 DEBUG Query successful
2024-10-06 17:08:38,330 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:08:48,365 DEBUG Query successful
2024-10-06 17:08:48,368 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:08:50,689 DEBUG Query successful
2024-10-06 17:08:50,690 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:09:14,581 DEBUG Query successful
2024-10-06 17:09:14,668 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:09:44,345 DEBUG Query successful
2024-10-06 17:09:44,347 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:09:59,525 DEBUG Query successful
2024-10-06 17:09:59,796 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:10:07,125 DEBUG Query successful
2024-10-06 17:10:07,131 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:10:10,080 DEBUG Query successful
2024-10-06 17:10:10,084 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:10:28,940 DEBUG Query successful
2024-10-06 17:10:29,044 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:11:01,092 DEBUG Query successful
2024-10-06 17:11:01,093 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:11:16,083 DEBUG Query successful
2024-10-06 17:11:16,347 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:11:26,307 DEBUG Query successful
2024-10-06 17:11:26,313 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:11:29,965 DEBUG Query successful
2024-10-06 17:11:29,969 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:11:52,410 DEBUG Query successful
2024-10-06 17:11:52,494 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:12:25,587 DEBUG Query successful
2024-10-06 17:12:25,592 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:12:39,338 DEBUG Query successful
2024-10-06 17:12:39,607 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:12:46,795 DEBUG Query successful
2024-10-06 17:12:46,797 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:12:49,725 DEBUG Query successful
2024-10-06 17:12:49,726 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:13:14,430 DEBUG Query successful
2024-10-06 17:13:14,515 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:13:46,949 DEBUG Query successful
2024-10-06 17:13:46,950 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:14:01,005 DEBUG Query successful
2024-10-06 17:14:01,294 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:14:09,464 DEBUG Query successful
2024-10-06 17:14:09,466 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:14:12,653 DEBUG Query successful
2024-10-06 17:14:12,654 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:14:39,858 DEBUG Query successful
2024-10-06 17:14:39,939 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:15:13,063 DEBUG Query successful
2024-10-06 17:15:13,064 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:15:25,567 DEBUG Query successful
2024-10-06 17:15:25,910 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:15:34,408 DEBUG Query successful
2024-10-06 17:15:34,409 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:15:36,982 DEBUG Query successful
2024-10-06 17:15:36,987 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:16:01,397 DEBUG Query successful
2024-10-06 17:16:01,491 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:16:33,696 DEBUG Query successful
2024-10-06 17:16:33,698 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:16:50,680 DEBUG Query successful
2024-10-06 17:16:51,024 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:16:59,747 DEBUG Query successful
2024-10-06 17:16:59,749 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:17:03,371 DEBUG Query successful
2024-10-06 17:17:03,372 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:17:31,420 DEBUG Query successful
2024-10-06 17:17:31,513 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:18:17,927 DEBUG Query successful
2024-10-06 17:18:17,932 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:18:36,405 DEBUG Query successful
2024-10-06 17:18:36,679 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:18:48,033 DEBUG Query successful
2024-10-06 17:18:48,036 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:18:51,419 DEBUG Query successful
2024-10-06 17:18:51,420 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:19:20,659 DEBUG Query successful
2024-10-06 17:19:20,747 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:20:02,094 DEBUG Query successful
2024-10-06 17:20:02,097 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:20:21,292 DEBUG Query successful
2024-10-06 17:20:21,577 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:20:33,504 DEBUG Query successful
2024-10-06 17:20:33,506 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:20:37,228 DEBUG Query successful
2024-10-06 17:20:37,229 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:21:04,868 DEBUG Query successful
2024-10-06 17:21:04,958 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:21:43,834 DEBUG Query successful
2024-10-06 17:21:43,837 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:22:00,562 DEBUG Query successful
2024-10-06 17:22:00,841 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:22:09,560 DEBUG Query successful
2024-10-06 17:22:09,562 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:22:12,504 DEBUG Query successful
2024-10-06 17:22:12,508 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:22:43,576 DEBUG Query successful
2024-10-06 17:22:43,663 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:23:28,642 DEBUG Query successful
2024-10-06 17:23:28,643 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:23:44,457 DEBUG Query successful
2024-10-06 17:23:44,732 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:23:51,030 DEBUG Query successful
2024-10-06 17:23:51,031 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:23:53,744 DEBUG Query successful
2024-10-06 17:23:53,745 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:24:27,969 DEBUG Query successful
2024-10-06 17:24:28,058 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:25:17,233 DEBUG Query successful
2024-10-06 17:25:17,238 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:25:33,558 DEBUG Query successful
2024-10-06 17:25:33,833 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:25:41,655 DEBUG Query successful
2024-10-06 17:25:41,657 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:25:44,847 DEBUG Query successful
2024-10-06 17:25:44,848 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:26:19,328 DEBUG Query successful
2024-10-06 17:26:19,413 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:27:02,742 DEBUG Query successful
2024-10-06 17:27:02,744 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:27:19,605 DEBUG Query successful
2024-10-06 17:27:19,952 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:27:26,343 DEBUG Query successful
2024-10-06 17:27:26,345 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:27:31,200 DEBUG Query successful
2024-10-06 17:27:31,200 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:28:02,725 DEBUG Query successful
2024-10-06 17:28:02,814 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:28:50,811 DEBUG Query successful
2024-10-06 17:28:50,813 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:29:07,627 DEBUG Query successful
2024-10-06 17:29:07,907 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:29:15,156 DEBUG Query successful
2024-10-06 17:29:15,159 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:29:18,702 DEBUG Query successful
2024-10-06 17:29:18,707 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:29:44,912 DEBUG Query successful
2024-10-06 17:29:45,000 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:30:31,761 DEBUG Query successful
2024-10-06 17:30:31,762 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:30:53,314 DEBUG Query successful
2024-10-06 17:30:53,599 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:31:01,607 DEBUG Query successful
2024-10-06 17:31:01,613 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:31:05,417 DEBUG Query successful
2024-10-06 17:31:05,418 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:31:42,369 DEBUG Query successful
2024-10-06 17:31:42,456 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:33:34,004 DEBUG Query successful
2024-10-06 17:33:34,006 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:34:02,655 DEBUG Query successful
2024-10-06 17:34:02,959 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:34:13,235 DEBUG Query successful
2024-10-06 17:34:13,237 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:34:16,976 DEBUG Query successful
2024-10-06 17:34:16,976 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:34:45,129 DEBUG Query successful
2024-10-06 17:34:45,210 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:35:37,556 DEBUG Query successful
2024-10-06 17:35:37,557 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:35:59,120 DEBUG Query successful
2024-10-06 17:35:59,390 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:36:06,509 DEBUG Query successful
2024-10-06 17:36:06,511 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:36:10,122 DEBUG Query successful
2024-10-06 17:36:10,122 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:36:34,240 DEBUG Query successful
2024-10-06 17:36:34,326 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:37:30,796 DEBUG Query successful
2024-10-06 17:37:30,798 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:37:52,635 DEBUG Query successful
2024-10-06 17:49:56,546 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 17:49:56,576 DEBUG OpenAI client created
2024-10-06 17:49:56,577 DEBUG Model set to: gpt-4-turbo
2024-10-06 17:49:56,577 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 17:49:56,598 DEBUG OpenAI client created
2024-10-06 17:49:56,599 DEBUG Model set to: gpt-4-turbo
2024-10-06 17:50:01,763 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:50:12,152 DEBUG Query successful
2024-10-06 17:50:12,154 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:50:13,864 DEBUG Query successful
2024-10-06 17:50:13,876 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:50:25,300 DEBUG Query successful
2024-10-06 17:50:25,382 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:50:35,538 DEBUG Query successful
2024-10-06 17:50:35,539 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:50:40,988 DEBUG Query successful
2024-10-06 17:50:41,255 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:50:52,819 DEBUG Query successful
2024-10-06 17:50:52,824 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:50:55,306 DEBUG Query successful
2024-10-06 17:50:55,308 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:51:10,910 DEBUG Query successful
2024-10-06 17:51:10,989 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:51:22,709 DEBUG Query successful
2024-10-06 17:51:22,714 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:51:31,244 DEBUG Query successful
2024-10-06 17:51:41,552 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:51:50,409 DEBUG Query successful
2024-10-06 17:51:50,411 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:51:53,838 DEBUG Query successful
2024-10-06 17:51:53,839 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:52:09,132 DEBUG Query successful
2024-10-06 17:52:09,227 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:52:22,440 DEBUG Query successful
2024-10-06 17:52:22,442 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:52:30,921 DEBUG Query successful
2024-10-06 17:52:31,196 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:52:38,873 DEBUG Query successful
2024-10-06 17:52:38,874 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:52:41,589 DEBUG Query successful
2024-10-06 17:52:41,591 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:52:55,458 DEBUG Query successful
2024-10-06 17:52:55,546 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:53:10,365 DEBUG Query successful
2024-10-06 17:53:10,367 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:53:18,531 DEBUG Query successful
2024-10-06 17:53:18,808 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:53:26,871 DEBUG Query successful
2024-10-06 17:53:26,872 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:53:29,641 DEBUG Query successful
2024-10-06 17:53:29,663 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:53:44,968 DEBUG Query successful
2024-10-06 17:53:45,055 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:54:03,913 DEBUG Query successful
2024-10-06 17:54:03,915 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:54:13,287 DEBUG Query successful
2024-10-06 17:54:13,671 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:54:22,275 DEBUG Query successful
2024-10-06 17:54:22,281 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:54:26,208 DEBUG Query successful
2024-10-06 17:54:26,209 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:54:44,523 DEBUG Query successful
2024-10-06 17:54:44,612 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:55:05,774 DEBUG Query successful
2024-10-06 17:55:05,779 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:55:20,689 DEBUG Query successful
2024-10-06 17:55:21,018 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:55:30,403 DEBUG Query successful
2024-10-06 17:55:30,407 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:55:33,372 DEBUG Query successful
2024-10-06 17:55:33,374 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:55:57,849 DEBUG Query successful
2024-10-06 17:55:57,931 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:56:20,070 DEBUG Query successful
2024-10-06 17:56:20,072 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:56:29,826 DEBUG Query successful
2024-10-06 17:56:30,099 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:56:38,661 DEBUG Query successful
2024-10-06 17:56:38,662 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:56:42,552 DEBUG Query successful
2024-10-06 17:56:42,554 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:57:03,790 DEBUG Query successful
2024-10-06 17:57:03,872 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:57:30,123 DEBUG Query successful
2024-10-06 17:57:30,125 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:57:40,340 DEBUG Query successful
2024-10-06 17:57:40,618 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:57:48,825 DEBUG Query successful
2024-10-06 17:57:48,826 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:57:52,027 DEBUG Query successful
2024-10-06 17:57:52,032 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:58:12,545 DEBUG Query successful
2024-10-06 17:58:12,632 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:58:39,834 DEBUG Query successful
2024-10-06 17:58:39,836 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:58:53,714 DEBUG Query successful
2024-10-06 17:58:53,992 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:59:05,121 DEBUG Query successful
2024-10-06 17:59:05,123 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:59:09,517 DEBUG Query successful
2024-10-06 17:59:09,518 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:59:30,038 DEBUG Query successful
2024-10-06 17:59:30,140 DEBUG Querying model: gpt-4-turbo
2024-10-06 17:59:59,979 DEBUG Query successful
2024-10-06 17:59:59,985 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:00:13,274 DEBUG Query successful
2024-10-06 18:00:13,546 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:00:23,131 DEBUG Query successful
2024-10-06 18:00:23,133 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:00:26,845 DEBUG Query successful
2024-10-06 18:00:26,850 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:00:55,431 DEBUG Query successful
2024-10-06 18:00:55,540 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:01:25,304 DEBUG Query successful
2024-10-06 18:01:25,305 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:01:37,483 DEBUG Query successful
2024-10-06 18:01:37,770 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:01:47,495 DEBUG Query successful
2024-10-06 18:01:47,497 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:01:53,979 DEBUG Query successful
2024-10-06 18:01:53,980 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:02:22,370 DEBUG Query successful
2024-10-06 18:02:22,476 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:02:56,301 DEBUG Query successful
2024-10-06 18:02:56,303 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:03:08,778 DEBUG Query successful
2024-10-06 18:03:09,045 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:03:19,023 DEBUG Query successful
2024-10-06 18:03:19,025 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:03:26,887 DEBUG Query successful
2024-10-06 18:03:26,898 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:03:54,472 DEBUG Query successful
2024-10-06 18:03:54,553 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:04:31,856 DEBUG Query successful
2024-10-06 18:04:31,858 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:04:41,663 DEBUG Query successful
2024-10-06 18:04:41,931 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:04:54,264 DEBUG Query successful
2024-10-06 18:04:54,266 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:05:01,473 DEBUG Query successful
2024-10-06 18:05:01,475 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:05:26,737 DEBUG Query successful
2024-10-06 18:05:26,818 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:06:12,224 DEBUG Query successful
2024-10-06 18:06:12,229 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:06:27,067 DEBUG Query successful
2024-10-06 18:06:27,340 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:06:36,994 DEBUG Query successful
2024-10-06 18:06:36,996 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:06:45,107 DEBUG Query successful
2024-10-06 18:06:45,108 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:07:14,515 DEBUG Query successful
2024-10-06 18:07:14,595 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:07:53,052 DEBUG Query successful
2024-10-06 18:07:53,055 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:08:05,609 DEBUG Query successful
2024-10-06 18:08:05,897 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:08:14,941 DEBUG Query successful
2024-10-06 18:08:14,943 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:08:19,531 DEBUG Query successful
2024-10-06 18:08:19,532 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:08:49,972 DEBUG Query successful
2024-10-06 18:08:50,057 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:09:28,807 DEBUG Query successful
2024-10-06 18:09:28,808 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:09:43,026 DEBUG Query successful
2024-10-06 18:09:43,296 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:09:50,573 DEBUG Query successful
2024-10-06 18:09:50,575 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:09:55,636 DEBUG Query successful
2024-10-06 18:09:55,641 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:10:26,137 DEBUG Query successful
2024-10-06 18:10:26,217 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:11:05,677 DEBUG Query successful
2024-10-06 18:11:05,679 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:11:17,817 DEBUG Query successful
2024-10-06 18:11:18,086 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:11:26,728 DEBUG Query successful
2024-10-06 18:11:26,730 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:11:36,615 DEBUG Query successful
2024-10-06 18:11:36,617 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:12:09,781 DEBUG Query successful
2024-10-06 18:12:09,867 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:12:48,760 DEBUG Query successful
2024-10-06 18:12:48,761 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:12:59,820 DEBUG Query successful
2024-10-06 18:13:00,090 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:13:08,069 DEBUG Query successful
2024-10-06 18:13:08,071 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:13:13,863 DEBUG Query successful
2024-10-06 18:13:13,865 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:13:43,415 DEBUG Query successful
2024-10-06 18:13:43,501 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:14:22,278 DEBUG Query successful
2024-10-06 18:14:22,281 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:14:37,151 DEBUG Query successful
2024-10-06 18:14:37,425 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:14:46,493 DEBUG Query successful
2024-10-06 18:14:46,495 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:14:52,950 DEBUG Query successful
2024-10-06 18:14:52,951 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:15:20,296 DEBUG Query successful
2024-10-06 18:15:20,383 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:15:58,920 DEBUG Query successful
2024-10-06 18:15:58,922 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:16:10,656 DEBUG Query successful
2024-10-06 18:16:10,929 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:16:19,398 DEBUG Query successful
2024-10-06 18:16:19,400 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:16:25,660 DEBUG Query successful
2024-10-06 18:16:25,665 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:16:56,262 DEBUG Query successful
2024-10-06 18:16:56,341 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:17:46,223 DEBUG Query successful
2024-10-06 18:17:46,223 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:17:57,390 DEBUG Query successful
2024-10-06 18:17:57,689 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:18:21,522 DEBUG Query successful
2024-10-06 18:18:21,526 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:18:27,185 DEBUG Query successful
2024-10-06 18:18:27,188 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:18:50,789 DEBUG Query successful
2024-10-06 18:18:50,870 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:20:38,460 DEBUG Query successful
2024-10-06 18:20:38,483 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:20:50,003 DEBUG Query successful
2024-10-06 18:20:50,299 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:20:57,342 DEBUG Query successful
2024-10-06 18:20:57,345 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:21:06,381 DEBUG Query successful
2024-10-06 18:21:06,382 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:21:24,111 DEBUG Query successful
2024-10-06 18:21:24,193 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:22:10,447 DEBUG Query successful
2024-10-06 18:22:10,448 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:22:23,978 DEBUG Query successful
2024-10-06 18:22:24,272 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:22:31,586 DEBUG Query successful
2024-10-06 18:22:31,589 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:22:37,587 DEBUG Query successful
2024-10-06 18:22:37,589 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:23:00,563 DEBUG Query successful
2024-10-06 18:23:00,642 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:23:47,266 DEBUG Query successful
2024-10-06 18:23:47,268 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:24:00,731 DEBUG Query successful
2024-10-06 18:24:01,001 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:24:10,480 DEBUG Query successful
2024-10-06 18:24:10,482 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:24:16,191 DEBUG Query successful
2024-10-06 18:24:16,196 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:24:42,308 DEBUG Query successful
2024-10-06 18:24:42,386 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:25:35,760 DEBUG Query successful
2024-10-06 18:25:35,761 DEBUG Querying model: gpt-4-turbo
2024-10-06 18:25:44,176 DEBUG Query successful
2024-10-06 20:09:32,903 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 20:09:32,932 DEBUG OpenAI client created
2024-10-06 20:09:32,932 DEBUG Model set to: gpt-4-turbo
2024-10-06 20:09:38,092 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:09:47,051 DEBUG Query successful
2024-10-06 20:09:47,141 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:09:55,326 DEBUG Query successful
2024-10-06 20:09:55,593 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:10:06,147 DEBUG Query successful
2024-10-06 20:10:06,231 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:10:16,641 DEBUG Query successful
2024-10-06 20:10:26,911 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:10:37,181 DEBUG Query successful
2024-10-06 20:10:37,271 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:10:47,698 DEBUG Query successful
2024-10-06 20:10:47,977 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:10:56,687 DEBUG Query successful
2024-10-06 20:10:56,776 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:11:16,574 DEBUG Query successful
2024-10-06 20:11:16,856 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:11:30,205 DEBUG Query successful
2024-10-06 20:11:30,296 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:11:38,249 DEBUG Query successful
2024-10-06 20:11:38,551 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:11:49,377 DEBUG Query successful
2024-10-06 20:11:49,474 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:11:57,555 DEBUG Query successful
2024-10-06 20:11:57,834 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:12:07,627 DEBUG Query successful
2024-10-06 20:12:07,717 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:12:14,833 DEBUG Query successful
2024-10-06 20:12:15,111 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:12:28,104 DEBUG Query successful
2024-10-06 20:12:28,195 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:12:39,463 DEBUG Query successful
2024-10-06 20:12:39,743 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:12:52,015 DEBUG Query successful
2024-10-06 20:12:52,103 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:12:59,911 DEBUG Query successful
2024-10-06 20:13:00,199 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:13:10,070 DEBUG Query successful
2024-10-06 20:13:10,161 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:13:19,988 DEBUG Query successful
2024-10-06 20:13:20,271 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:13:30,834 DEBUG Query successful
2024-10-06 20:13:30,931 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:13:39,289 DEBUG Query successful
2024-10-06 20:13:39,566 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:13:52,820 DEBUG Query successful
2024-10-06 20:13:52,908 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:14:00,568 DEBUG Query successful
2024-10-06 20:14:00,855 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:14:12,329 DEBUG Query successful
2024-10-06 20:14:12,427 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:14:20,069 DEBUG Query successful
2024-10-06 20:14:20,357 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:15:10,476 DEBUG Query successful
2024-10-06 20:15:10,559 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:15:19,715 DEBUG Query successful
2024-10-06 20:15:19,997 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:15:30,573 DEBUG Query successful
2024-10-06 20:15:30,656 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:15:38,415 DEBUG Query successful
2024-10-06 20:15:38,689 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:15:52,241 DEBUG Query successful
2024-10-06 20:15:52,333 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:16:05,990 DEBUG Query successful
2024-10-06 20:16:06,261 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:16:16,243 DEBUG Query successful
2024-10-06 20:16:16,329 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:16:22,442 DEBUG Query successful
2024-10-06 20:16:22,726 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:16:31,193 DEBUG Query successful
2024-10-06 20:16:31,281 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:16:37,495 DEBUG Query successful
2024-10-06 20:16:37,767 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:16:48,780 DEBUG Query successful
2024-10-06 20:16:48,866 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:16:56,675 DEBUG Query successful
2024-10-06 20:16:56,959 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:17:06,066 DEBUG Query successful
2024-10-06 20:17:06,158 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:17:13,927 DEBUG Query successful
2024-10-06 20:17:14,197 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:17:27,138 DEBUG Query successful
2024-10-06 20:17:27,227 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:17:36,211 DEBUG Query successful
2024-10-06 20:17:36,493 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:17:46,545 DEBUG Query successful
2024-10-06 20:17:46,632 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:17:54,291 DEBUG Query successful
2024-10-06 20:17:54,585 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:18:04,648 DEBUG Query successful
2024-10-06 20:18:04,739 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:18:11,947 DEBUG Query successful
2024-10-06 20:18:12,234 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:18:21,999 DEBUG Query successful
2024-10-06 20:18:22,095 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:18:29,022 DEBUG Query successful
2024-10-06 20:18:29,293 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:18:40,422 DEBUG Query successful
2024-10-06 20:18:40,511 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:18:45,550 DEBUG Query successful
2024-10-06 20:26:22,140 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 20:26:22,164 DEBUG OpenAI client created
2024-10-06 20:26:22,164 DEBUG Model set to: gpt-4-turbo
2024-10-06 20:26:22,164 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 20:26:22,188 DEBUG OpenAI client created
2024-10-06 20:26:22,189 DEBUG Model set to: gpt-4-turbo
2024-10-06 20:26:27,231 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:26:32,520 DEBUG Query successful
2024-10-06 20:26:32,522 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:26:34,305 DEBUG Query successful
2024-10-06 20:26:34,307 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:26:49,188 DEBUG Query successful
2024-10-06 20:26:49,202 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:26:58,176 DEBUG Query successful
2024-10-06 20:26:58,177 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:27:04,138 DEBUG Query successful
2024-10-06 20:27:04,160 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:27:10,881 DEBUG Query successful
2024-10-06 20:27:10,882 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:27:18,355 DEBUG Query successful
2024-10-06 20:27:18,525 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:27:28,856 DEBUG Query successful
2024-10-06 20:27:28,879 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:27:31,918 DEBUG Query successful
2024-10-06 20:27:31,919 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:27:44,096 DEBUG Query successful
2024-10-06 20:27:44,116 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:27:56,541 DEBUG Query successful
2024-10-06 20:27:56,542 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:28:02,792 DEBUG Query successful
2024-10-06 20:28:13,051 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:28:20,779 DEBUG Query successful
2024-10-06 20:28:20,781 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:28:24,422 DEBUG Query successful
2024-10-06 20:28:24,433 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:28:38,780 DEBUG Query successful
2024-10-06 20:28:38,859 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:28:52,280 DEBUG Query successful
2024-10-06 20:28:52,281 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:28:59,337 DEBUG Query successful
2024-10-06 20:28:59,609 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:29:09,124 DEBUG Query successful
2024-10-06 20:29:09,128 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:29:13,559 DEBUG Query successful
2024-10-06 20:29:13,560 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:29:29,745 DEBUG Query successful
2024-10-06 20:29:29,825 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:29:43,489 DEBUG Query successful
2024-10-06 20:29:43,491 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:29:51,780 DEBUG Query successful
2024-10-06 20:29:52,068 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:30:04,007 DEBUG Query successful
2024-10-06 20:30:04,008 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:30:07,934 DEBUG Query successful
2024-10-06 20:30:07,938 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:30:23,713 DEBUG Query successful
2024-10-06 20:30:23,807 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:30:39,150 DEBUG Query successful
2024-10-06 20:30:39,151 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:30:47,446 DEBUG Query successful
2024-10-06 20:30:47,865 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:30:57,829 DEBUG Query successful
2024-10-06 20:30:57,832 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:31:03,061 DEBUG Query successful
2024-10-06 20:31:03,063 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:31:19,242 DEBUG Query successful
2024-10-06 20:31:19,335 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:31:43,736 DEBUG Query successful
2024-10-06 20:31:43,741 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:31:53,554 DEBUG Query successful
2024-10-06 20:31:53,913 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:32:02,489 DEBUG Query successful
2024-10-06 20:32:02,492 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:32:06,906 DEBUG Query successful
2024-10-06 20:32:06,907 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:32:27,081 DEBUG Query successful
2024-10-06 20:32:27,176 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:32:48,866 DEBUG Query successful
2024-10-06 20:32:48,867 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:32:58,449 DEBUG Query successful
2024-10-06 20:32:58,737 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:33:08,158 DEBUG Query successful
2024-10-06 20:33:08,161 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:33:13,450 DEBUG Query successful
2024-10-06 20:33:13,451 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:33:34,011 DEBUG Query successful
2024-10-06 20:33:34,115 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:33:56,055 DEBUG Query successful
2024-10-06 20:33:56,056 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:33:56,470 ERROR An error occurred: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.PermissionDeniedError: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}
2024-10-06 20:33:56,576 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:33:56,795 ERROR An error occurred: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.PermissionDeniedError: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}
2024-10-06 20:33:56,800 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:33:56,969 ERROR An error occurred: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.PermissionDeniedError: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}
2024-10-06 20:33:57,254 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:34:06,456 DEBUG Query successful
2024-10-06 20:34:06,458 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:34:12,851 DEBUG Query successful
2024-10-06 20:34:12,852 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:34:38,674 DEBUG Query successful
2024-10-06 20:34:38,756 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:35:04,411 DEBUG Query successful
2024-10-06 20:35:04,412 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:35:14,173 DEBUG Query successful
2024-10-06 20:35:14,515 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:35:23,031 DEBUG Query successful
2024-10-06 20:35:23,032 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:35:31,259 DEBUG Query successful
2024-10-06 20:35:31,260 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:35:55,105 DEBUG Query successful
2024-10-06 20:35:55,197 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:36:22,542 DEBUG Query successful
2024-10-06 20:36:22,544 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:36:32,125 DEBUG Query successful
2024-10-06 20:36:32,439 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:36:41,052 DEBUG Query successful
2024-10-06 20:36:41,054 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:36:47,375 DEBUG Query successful
2024-10-06 20:36:47,380 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:37:09,520 DEBUG Query successful
2024-10-06 20:37:09,610 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:37:37,971 DEBUG Query successful
2024-10-06 20:37:37,972 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:37:49,261 DEBUG Query successful
2024-10-06 20:37:49,619 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:38:01,359 DEBUG Query successful
2024-10-06 20:38:01,361 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:38:10,228 DEBUG Query successful
2024-10-06 20:38:10,228 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:38:31,560 DEBUG Query successful
2024-10-06 20:38:31,647 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:39:14,925 DEBUG Query successful
2024-10-06 20:39:14,930 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:39:28,736 DEBUG Query successful
2024-10-06 20:39:29,029 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:39:37,883 DEBUG Query successful
2024-10-06 20:39:37,886 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:39:46,379 DEBUG Query successful
2024-10-06 20:39:46,391 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:40:08,764 DEBUG Query successful
2024-10-06 20:40:08,852 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:40:42,712 DEBUG Query successful
2024-10-06 20:40:42,713 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:40:54,691 DEBUG Query successful
2024-10-06 20:40:54,983 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:41:06,159 DEBUG Query successful
2024-10-06 20:41:06,161 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:41:13,205 DEBUG Query successful
2024-10-06 20:41:13,208 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:41:31,755 DEBUG Query successful
2024-10-06 20:41:31,842 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:42:04,603 DEBUG Query successful
2024-10-06 20:42:04,605 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:42:19,779 DEBUG Query successful
2024-10-06 20:42:20,069 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:42:29,366 DEBUG Query successful
2024-10-06 20:42:29,369 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:42:36,489 DEBUG Query successful
2024-10-06 20:42:36,493 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:42:57,403 DEBUG Query successful
2024-10-06 20:42:57,493 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:43:33,923 DEBUG Query successful
2024-10-06 20:43:33,925 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:43:45,704 DEBUG Query successful
2024-10-06 20:43:45,991 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:43:54,310 DEBUG Query successful
2024-10-06 20:43:54,313 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:44:01,000 DEBUG Query successful
2024-10-06 20:44:01,001 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:44:20,619 DEBUG Query successful
2024-10-06 20:44:20,709 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:45:03,103 DEBUG Query successful
2024-10-06 20:45:03,105 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:45:16,971 DEBUG Query successful
2024-10-06 20:45:17,262 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:45:27,990 DEBUG Query successful
2024-10-06 20:45:27,992 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:45:35,660 DEBUG Query successful
2024-10-06 20:45:35,661 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:45:59,682 DEBUG Query successful
2024-10-06 20:45:59,773 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:46:54,045 DEBUG Query successful
2024-10-06 20:46:54,047 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:47:08,738 DEBUG Query successful
2024-10-06 20:47:09,033 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:47:16,983 DEBUG Query successful
2024-10-06 20:47:16,985 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:47:24,325 DEBUG Query successful
2024-10-06 20:47:24,326 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:47:43,897 DEBUG Query successful
2024-10-06 20:47:43,980 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:48:28,985 DEBUG Query successful
2024-10-06 20:48:28,986 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:48:41,376 DEBUG Query successful
2024-10-06 20:48:41,667 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:48:50,923 DEBUG Query successful
2024-10-06 20:48:50,924 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:48:58,806 DEBUG Query successful
2024-10-06 20:48:58,808 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:49:19,828 DEBUG Query successful
2024-10-06 20:49:19,918 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:50:02,139 DEBUG Query successful
2024-10-06 20:50:02,141 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:50:15,560 DEBUG Query successful
2024-10-06 20:50:15,844 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:51:24,021 DEBUG Query successful
2024-10-06 20:51:24,027 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:51:31,117 DEBUG Query successful
2024-10-06 20:51:31,119 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:51:55,333 DEBUG Query successful
2024-10-06 20:51:55,418 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:52:40,616 DEBUG Query successful
2024-10-06 20:52:40,622 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:52:56,140 DEBUG Query successful
2024-10-06 20:52:56,433 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:53:05,519 DEBUG Query successful
2024-10-06 20:53:05,522 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:53:14,074 DEBUG Query successful
2024-10-06 20:53:14,074 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:53:43,547 DEBUG Query successful
2024-10-06 20:53:43,637 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:54:28,275 DEBUG Query successful
2024-10-06 20:54:28,278 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:54:41,598 DEBUG Query successful
2024-10-06 20:54:41,890 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:54:47,939 DEBUG Query successful
2024-10-06 20:54:47,940 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:54:55,590 DEBUG Query successful
2024-10-06 20:54:55,592 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:55:22,141 DEBUG Query successful
2024-10-06 20:55:22,232 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:56:21,009 DEBUG Query successful
2024-10-06 20:56:21,011 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:56:33,363 DEBUG Query successful
2024-10-06 20:56:33,653 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:56:40,960 DEBUG Query successful
2024-10-06 20:56:40,962 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:56:48,354 DEBUG Query successful
2024-10-06 20:56:48,358 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:57:16,516 DEBUG Query successful
2024-10-06 20:57:16,600 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:58:07,404 DEBUG Query successful
2024-10-06 20:58:07,406 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:58:21,471 DEBUG Query successful
2024-10-06 20:58:21,772 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:58:30,237 DEBUG Query successful
2024-10-06 20:58:30,239 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:58:39,182 DEBUG Query successful
2024-10-06 20:58:39,184 DEBUG Querying model: gpt-4-turbo
2024-10-06 20:59:11,949 DEBUG Query successful
2024-10-06 20:59:12,039 DEBUG Querying model: gpt-4-turbo
2024-10-06 21:00:08,128 DEBUG Query successful
2024-10-06 21:00:08,131 DEBUG Querying model: gpt-4-turbo
2024-10-06 21:00:23,819 DEBUG Query successful
2024-10-06 21:00:24,216 DEBUG Querying model: gpt-4-turbo
2024-10-06 21:00:32,138 DEBUG Query successful
2024-10-06 21:00:32,140 DEBUG Querying model: gpt-4-turbo
2024-10-06 21:00:40,539 DEBUG Query successful
2024-10-06 21:00:40,540 DEBUG Querying model: gpt-4-turbo
2024-10-06 21:01:05,936 DEBUG Query successful
2024-10-06 21:01:06,027 DEBUG Querying model: gpt-4-turbo
2024-10-06 21:03:06,126 DEBUG Query successful
2024-10-06 21:03:06,128 DEBUG Querying model: gpt-4-turbo
2024-10-06 21:03:25,027 DEBUG Query successful
2024-10-06 22:08:03,054 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 22:08:03,080 DEBUG OpenAI client created
2024-10-06 22:08:03,080 DEBUG Model set to: gpt-4-turbo
2024-10-06 22:08:03,080 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-06 22:08:03,102 DEBUG OpenAI client created
2024-10-06 22:08:03,102 DEBUG Model set to: gpt-4-turbo
2024-10-06 22:08:08,142 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:08:16,849 DEBUG Query successful
2024-10-06 22:08:16,851 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:08:18,291 DEBUG Query successful
2024-10-06 22:08:18,293 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:08:28,934 DEBUG Query successful
2024-10-06 22:08:28,948 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:08:34,570 DEBUG Query successful
2024-10-06 22:08:34,572 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:08:41,100 DEBUG Query successful
2024-10-06 22:08:41,104 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:08:46,999 DEBUG Query successful
2024-10-06 22:08:47,004 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:08:54,020 DEBUG Query successful
2024-10-06 22:08:54,169 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:09:01,337 DEBUG Query successful
2024-10-06 22:09:01,338 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:09:03,307 DEBUG Query successful
2024-10-06 22:09:03,308 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:09:13,108 DEBUG Query successful
2024-10-06 22:09:13,128 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:09:21,251 DEBUG Query successful
2024-10-06 22:09:21,252 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:09:29,249 DEBUG Query successful
2024-10-06 22:09:29,250 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:09:37,492 DEBUG Query successful
2024-10-06 22:09:37,493 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:09:45,009 DEBUG Query successful
2024-10-06 22:09:55,283 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:10:05,689 DEBUG Query successful
2024-10-06 22:10:05,691 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:10:07,993 DEBUG Query successful
2024-10-06 22:10:07,994 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:10:18,809 DEBUG Query successful
2024-10-06 22:10:18,884 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:10:32,627 DEBUG Query successful
2024-10-06 22:10:32,632 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:10:41,571 DEBUG Query successful
2024-10-06 22:10:41,574 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:10:48,390 DEBUG Query successful
2024-10-06 22:10:48,390 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:10:57,209 DEBUG Query successful
2024-10-06 22:10:57,482 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:11:05,409 DEBUG Query successful
2024-10-06 22:11:05,410 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:11:08,129 DEBUG Query successful
2024-10-06 22:11:08,134 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:11:19,694 DEBUG Query successful
2024-10-06 22:11:19,770 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:11:32,865 DEBUG Query successful
2024-10-06 22:11:32,871 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:11:42,298 DEBUG Query successful
2024-10-06 22:11:42,302 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:11:52,363 DEBUG Query successful
2024-10-06 22:11:52,364 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:12:00,904 DEBUG Query successful
2024-10-06 22:12:01,180 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:12:21,705 DEBUG Query successful
2024-10-06 22:12:21,708 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:12:24,853 DEBUG Query successful
2024-10-06 22:12:24,854 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:12:41,460 DEBUG Query successful
2024-10-06 22:12:41,549 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:12:56,891 DEBUG Query successful
2024-10-06 22:12:56,892 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:13:10,343 DEBUG Query successful
2024-10-06 22:13:10,344 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:13:24,332 DEBUG Query successful
2024-10-06 22:13:24,336 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:13:33,118 DEBUG Query successful
2024-10-06 22:13:33,397 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:13:42,793 DEBUG Query successful
2024-10-06 22:13:42,795 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:13:45,666 DEBUG Query successful
2024-10-06 22:13:45,667 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:14:02,223 DEBUG Query successful
2024-10-06 22:14:02,307 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:14:19,018 DEBUG Query successful
2024-10-06 22:14:19,019 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:14:29,793 DEBUG Query successful
2024-10-06 22:14:29,794 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:14:39,881 DEBUG Query successful
2024-10-06 22:14:39,883 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:14:49,557 DEBUG Query successful
2024-10-06 22:14:49,846 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:14:58,567 DEBUG Query successful
2024-10-06 22:14:58,574 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:15:02,297 DEBUG Query successful
2024-10-06 22:15:02,297 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:15:19,437 DEBUG Query successful
2024-10-06 22:15:19,525 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:15:37,207 DEBUG Query successful
2024-10-06 22:15:37,212 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:15:49,016 DEBUG Query successful
2024-10-06 22:15:49,017 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:16:01,269 DEBUG Query successful
2024-10-06 22:16:01,292 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:16:13,252 DEBUG Query successful
2024-10-06 22:16:13,692 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:16:25,371 DEBUG Query successful
2024-10-06 22:16:25,373 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:16:27,990 DEBUG Query successful
2024-10-06 22:16:27,995 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:16:48,305 DEBUG Query successful
2024-10-06 22:16:48,407 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:17:09,675 DEBUG Query successful
2024-10-06 22:17:09,676 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:17:24,193 DEBUG Query successful
2024-10-06 22:17:24,215 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:17:36,191 DEBUG Query successful
2024-10-06 22:17:36,191 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:17:46,975 DEBUG Query successful
2024-10-06 22:17:47,252 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:17:56,162 DEBUG Query successful
2024-10-06 22:17:56,163 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:17:59,280 DEBUG Query successful
2024-10-06 22:17:59,283 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:18:25,210 DEBUG Query successful
2024-10-06 22:18:25,295 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:18:50,686 DEBUG Query successful
2024-10-06 22:18:50,688 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:19:02,553 DEBUG Query successful
2024-10-06 22:19:02,554 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:19:14,938 DEBUG Query successful
2024-10-06 22:19:14,960 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:19:28,462 DEBUG Query successful
2024-10-06 22:19:28,733 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:19:41,447 DEBUG Query successful
2024-10-06 22:19:41,448 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:19:45,638 DEBUG Query successful
2024-10-06 22:19:45,639 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:20:08,975 DEBUG Query successful
2024-10-06 22:20:09,061 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:20:35,585 DEBUG Query successful
2024-10-06 22:20:35,588 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:20:50,045 DEBUG Query successful
2024-10-06 22:20:50,046 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:21:07,347 DEBUG Query successful
2024-10-06 22:21:07,347 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:21:21,182 DEBUG Query successful
2024-10-06 22:21:21,476 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:21:30,731 DEBUG Query successful
2024-10-06 22:21:30,754 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:21:38,262 DEBUG Query successful
2024-10-06 22:21:38,263 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:22:03,504 DEBUG Query successful
2024-10-06 22:22:03,590 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:22:37,768 DEBUG Query successful
2024-10-06 22:22:37,773 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:22:52,311 DEBUG Query successful
2024-10-06 22:22:52,313 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:23:07,809 DEBUG Query successful
2024-10-06 22:23:07,810 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:23:23,821 DEBUG Query successful
2024-10-06 22:23:24,097 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:23:35,205 DEBUG Query successful
2024-10-06 22:23:35,208 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:23:42,191 DEBUG Query successful
2024-10-06 22:23:42,193 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:24:09,036 DEBUG Query successful
2024-10-06 22:24:09,118 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:24:40,608 DEBUG Query successful
2024-10-06 22:24:40,610 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:24:54,220 DEBUG Query successful
2024-10-06 22:24:54,225 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:25:10,364 DEBUG Query successful
2024-10-06 22:25:10,365 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:25:24,751 DEBUG Query successful
2024-10-06 22:25:25,087 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:25:33,766 DEBUG Query successful
2024-10-06 22:25:33,767 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:25:38,074 DEBUG Query successful
2024-10-06 22:25:38,075 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:26:06,252 DEBUG Query successful
2024-10-06 22:26:06,339 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:26:37,256 DEBUG Query successful
2024-10-06 22:26:37,257 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:26:50,062 DEBUG Query successful
2024-10-06 22:26:50,062 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:27:02,801 DEBUG Query successful
2024-10-06 22:27:02,824 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:27:15,755 DEBUG Query successful
2024-10-06 22:27:16,029 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:27:26,077 DEBUG Query successful
2024-10-06 22:27:26,080 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:27:30,122 DEBUG Query successful
2024-10-06 22:27:30,123 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:28:01,817 DEBUG Query successful
2024-10-06 22:28:01,899 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:28:33,191 DEBUG Query successful
2024-10-06 22:28:33,193 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:28:48,387 DEBUG Query successful
2024-10-06 22:28:48,388 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:29:01,993 DEBUG Query successful
2024-10-06 22:29:01,994 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:29:17,249 DEBUG Query successful
2024-10-06 22:29:17,525 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:29:27,455 DEBUG Query successful
2024-10-06 22:29:27,460 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:29:30,934 DEBUG Query successful
2024-10-06 22:29:30,935 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:30:00,141 DEBUG Query successful
2024-10-06 22:30:00,222 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:30:32,922 DEBUG Query successful
2024-10-06 22:30:32,928 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:30:49,364 DEBUG Query successful
2024-10-06 22:30:49,365 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:31:12,074 DEBUG Query successful
2024-10-06 22:31:12,075 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:31:29,255 DEBUG Query successful
2024-10-06 22:31:29,541 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:31:38,050 DEBUG Query successful
2024-10-06 22:31:38,051 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:31:44,784 DEBUG Query successful
2024-10-06 22:31:44,786 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:32:21,283 DEBUG Query successful
2024-10-06 22:32:21,365 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:32:57,069 DEBUG Query successful
2024-10-06 22:32:57,072 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:33:13,810 DEBUG Query successful
2024-10-06 22:33:13,814 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:33:30,495 DEBUG Query successful
2024-10-06 22:33:30,497 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:33:45,574 DEBUG Query successful
2024-10-06 22:33:45,885 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:33:58,539 DEBUG Query successful
2024-10-06 22:33:58,541 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:34:08,299 DEBUG Query successful
2024-10-06 22:34:08,301 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:34:45,361 DEBUG Query successful
2024-10-06 22:34:45,447 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:35:33,252 DEBUG Query successful
2024-10-06 22:35:33,254 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:35:47,940 DEBUG Query successful
2024-10-06 22:35:47,941 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:36:02,609 DEBUG Query successful
2024-10-06 22:36:02,614 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:36:17,489 DEBUG Query successful
2024-10-06 22:36:17,832 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:36:26,104 DEBUG Query successful
2024-10-06 22:36:26,105 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:36:31,177 DEBUG Query successful
2024-10-06 22:36:31,178 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:37:03,214 DEBUG Query successful
2024-10-06 22:37:03,297 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:37:41,752 DEBUG Query successful
2024-10-06 22:37:41,754 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:37:56,842 DEBUG Query successful
2024-10-06 22:37:56,843 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:38:30,076 DEBUG Query successful
2024-10-06 22:38:30,078 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:38:44,367 DEBUG Query successful
2024-10-06 22:38:44,671 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:38:55,174 DEBUG Query successful
2024-10-06 22:38:55,180 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:39:00,686 DEBUG Query successful
2024-10-06 22:39:00,687 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:39:42,240 DEBUG Query successful
2024-10-06 22:39:42,324 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:40:20,336 DEBUG Query successful
2024-10-06 22:40:20,341 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:40:34,859 DEBUG Query successful
2024-10-06 22:40:34,860 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:40:49,330 DEBUG Query successful
2024-10-06 22:40:49,332 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:41:03,049 DEBUG Query successful
2024-10-06 22:41:03,320 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:41:10,190 DEBUG Query successful
2024-10-06 22:41:10,191 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:41:15,243 DEBUG Query successful
2024-10-06 22:41:15,248 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:41:57,819 DEBUG Query successful
2024-10-06 22:41:57,901 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:42:42,392 DEBUG Query successful
2024-10-06 22:42:42,393 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:42:59,587 DEBUG Query successful
2024-10-06 22:42:59,610 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:43:15,735 DEBUG Query successful
2024-10-06 22:43:15,737 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:43:31,588 DEBUG Query successful
2024-10-06 22:43:31,865 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:43:40,690 DEBUG Query successful
2024-10-06 22:43:40,692 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:43:46,207 DEBUG Query successful
2024-10-06 22:43:46,208 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:44:31,682 DEBUG Query successful
2024-10-06 22:44:31,767 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:45:21,374 DEBUG Query successful
2024-10-06 22:45:21,376 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:45:37,838 DEBUG Query successful
2024-10-06 22:45:37,839 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:45:54,888 DEBUG Query successful
2024-10-06 22:45:54,893 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:46:11,208 DEBUG Query successful
2024-10-06 22:46:11,486 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:46:22,217 DEBUG Query successful
2024-10-06 22:46:22,218 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:46:27,854 DEBUG Query successful
2024-10-06 22:46:27,855 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:47:01,243 DEBUG Query successful
2024-10-06 22:47:01,343 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:47:45,203 DEBUG Query successful
2024-10-06 22:47:45,205 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:48:11,340 DEBUG Query successful
2024-10-06 22:48:11,341 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:48:29,174 DEBUG Query successful
2024-10-06 22:48:29,176 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:48:45,285 DEBUG Query successful
2024-10-06 22:48:45,564 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:48:58,227 DEBUG Query successful
2024-10-06 22:48:58,232 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:49:04,038 DEBUG Query successful
2024-10-06 22:49:04,039 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:49:41,455 DEBUG Query successful
2024-10-06 22:49:41,536 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:50:30,040 DEBUG Query successful
2024-10-06 22:50:30,046 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:50:48,719 DEBUG Query successful
2024-10-06 22:50:48,720 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:51:06,374 DEBUG Query successful
2024-10-06 22:51:06,375 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:51:30,357 DEBUG Query successful
2024-10-06 22:51:30,630 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:51:39,772 DEBUG Query successful
2024-10-06 22:51:39,775 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:51:55,298 DEBUG Query successful
2024-10-06 22:51:55,299 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:52:40,170 DEBUG Query successful
2024-10-06 22:52:40,251 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:54:10,173 DEBUG Query successful
2024-10-06 22:54:10,177 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:54:27,376 DEBUG Query successful
2024-10-06 22:54:27,380 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:54:45,651 DEBUG Query successful
2024-10-06 22:54:45,653 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:55:11,185 DEBUG Query successful
2024-10-06 22:55:11,466 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:55:20,942 DEBUG Query successful
2024-10-06 22:55:20,943 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:55:26,736 DEBUG Query successful
2024-10-06 22:55:26,738 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:56:03,260 DEBUG Query successful
2024-10-06 22:56:03,347 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:56:59,096 DEBUG Query successful
2024-10-06 22:56:59,097 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:57:14,858 DEBUG Query successful
2024-10-06 22:57:14,859 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:57:25,735 DEBUG Query successful
2024-10-06 22:57:25,757 DEBUG Querying model: gpt-4-turbo
2024-10-06 22:57:36,551 DEBUG Query successful
2024-10-10 12:26:13,659 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-10 12:26:13,831 DEBUG OpenAI client created
2024-10-10 12:26:13,831 DEBUG Model set to: gpt-4-turbo
2024-10-10 12:26:13,832 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-10 12:26:13,853 DEBUG OpenAI client created
2024-10-10 12:26:13,853 DEBUG Model set to: gpt-4-turbo
2024-10-10 12:26:19,019 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:26:32,396 DEBUG Query successful
2024-10-10 12:26:32,412 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:26:43,146 DEBUG Query successful
2024-10-10 12:26:43,180 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:27:03,349 DEBUG Query successful
2024-10-10 12:27:03,434 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:27:15,821 DEBUG Query successful
2024-10-10 12:27:15,823 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:27:25,479 DEBUG Query successful
2024-10-10 12:27:25,479 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:27:32,075 DEBUG Query successful
2024-10-10 12:27:32,076 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:27:39,448 DEBUG Query successful
2024-10-10 12:27:39,802 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:27:49,171 DEBUG Query successful
2024-10-10 12:27:49,177 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:28:03,030 DEBUG Query successful
2024-10-10 12:28:03,035 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:28:22,893 DEBUG Query successful
2024-10-10 12:28:22,993 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:28:42,771 DEBUG Query successful
2024-10-10 12:28:42,777 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:28:53,892 DEBUG Query successful
2024-10-10 12:29:04,180 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:29:13,915 DEBUG Query successful
2024-10-10 12:29:13,918 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:29:25,835 DEBUG Query successful
2024-10-10 12:29:25,842 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:29:41,247 DEBUG Query successful
2024-10-10 12:29:41,333 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:29:56,701 DEBUG Query successful
2024-10-10 12:29:56,703 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:30:09,263 DEBUG Query successful
2024-10-10 12:30:09,562 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:30:21,740 DEBUG Query successful
2024-10-10 12:30:21,742 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:30:33,801 DEBUG Query successful
2024-10-10 12:30:33,816 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:30:53,327 DEBUG Query successful
2024-10-10 12:30:53,422 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:31:14,133 DEBUG Query successful
2024-10-10 12:31:14,135 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:31:23,805 DEBUG Query successful
2024-10-10 12:31:24,100 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:31:33,005 DEBUG Query successful
2024-10-10 12:31:33,006 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:31:44,820 DEBUG Query successful
2024-10-10 12:31:44,829 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:32:11,312 DEBUG Query successful
2024-10-10 12:32:11,403 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:32:33,321 DEBUG Query successful
2024-10-10 12:32:33,323 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:32:44,613 DEBUG Query successful
2024-10-10 12:32:44,913 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:32:54,914 DEBUG Query successful
2024-10-10 12:32:54,918 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:33:08,631 DEBUG Query successful
2024-10-10 12:33:08,635 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:33:26,543 DEBUG Query successful
2024-10-10 12:33:26,632 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:33:47,599 DEBUG Query successful
2024-10-10 12:33:47,604 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:33:58,760 DEBUG Query successful
2024-10-10 12:33:59,044 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:34:14,075 DEBUG Query successful
2024-10-10 12:34:14,077 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:34:26,826 DEBUG Query successful
2024-10-10 12:34:26,830 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:34:44,488 DEBUG Query successful
2024-10-10 12:34:44,585 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:35:11,084 DEBUG Query successful
2024-10-10 12:35:11,085 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:35:23,379 DEBUG Query successful
2024-10-10 12:35:23,681 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:35:34,340 DEBUG Query successful
2024-10-10 12:35:34,342 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:35:48,732 DEBUG Query successful
2024-10-10 12:35:48,735 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:36:07,807 DEBUG Query successful
2024-10-10 12:36:07,896 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:36:37,416 DEBUG Query successful
2024-10-10 12:36:37,418 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:36:48,591 DEBUG Query successful
2024-10-10 12:36:48,899 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:37:01,242 DEBUG Query successful
2024-10-10 12:37:01,244 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:37:18,735 DEBUG Query successful
2024-10-10 12:37:18,744 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:37:39,954 DEBUG Query successful
2024-10-10 12:37:40,044 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:38:11,398 DEBUG Query successful
2024-10-10 12:38:11,400 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:38:22,782 DEBUG Query successful
2024-10-10 12:38:23,089 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:38:31,694 DEBUG Query successful
2024-10-10 12:38:31,700 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:38:46,310 DEBUG Query successful
2024-10-10 12:38:46,313 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:39:10,278 DEBUG Query successful
2024-10-10 12:39:10,376 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:39:41,032 DEBUG Query successful
2024-10-10 12:39:41,034 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:39:56,752 DEBUG Query successful
2024-10-10 12:39:57,051 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:40:06,181 DEBUG Query successful
2024-10-10 12:40:06,183 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:40:22,201 DEBUG Query successful
2024-10-10 12:40:22,213 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:40:41,969 DEBUG Query successful
2024-10-10 12:40:42,060 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:42:20,267 DEBUG Query successful
2024-10-10 12:42:20,268 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:42:32,732 DEBUG Query successful
2024-10-10 12:42:33,018 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:42:43,864 DEBUG Query successful
2024-10-10 12:42:43,867 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:43:00,186 DEBUG Query successful
2024-10-10 12:43:00,197 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:43:23,963 DEBUG Query successful
2024-10-10 12:43:24,070 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:44:08,085 DEBUG Query successful
2024-10-10 12:44:08,087 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:44:19,783 DEBUG Query successful
2024-10-10 12:44:20,069 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:44:29,763 DEBUG Query successful
2024-10-10 12:44:29,765 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:44:51,121 DEBUG Query successful
2024-10-10 12:44:51,128 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:45:13,167 DEBUG Query successful
2024-10-10 12:45:13,254 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:45:46,763 DEBUG Query successful
2024-10-10 12:45:46,765 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:46:00,358 DEBUG Query successful
2024-10-10 12:46:00,642 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:46:09,180 DEBUG Query successful
2024-10-10 12:46:09,185 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:46:29,491 DEBUG Query successful
2024-10-10 12:46:29,493 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:46:59,400 DEBUG Query successful
2024-10-10 12:46:59,513 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:47:40,481 DEBUG Query successful
2024-10-10 12:47:40,486 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:47:57,958 DEBUG Query successful
2024-10-10 12:47:58,243 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:48:14,327 DEBUG Query successful
2024-10-10 12:48:14,329 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:48:33,604 DEBUG Query successful
2024-10-10 12:48:33,611 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:49:02,175 DEBUG Query successful
2024-10-10 12:49:02,263 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:49:45,967 DEBUG Query successful
2024-10-10 12:49:45,969 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:49:57,946 DEBUG Query successful
2024-10-10 12:49:58,234 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:50:23,747 DEBUG Query successful
2024-10-10 12:50:23,749 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:50:40,480 DEBUG Query successful
2024-10-10 12:50:40,484 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:51:09,600 DEBUG Query successful
2024-10-10 12:51:09,693 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:51:59,100 DEBUG Query successful
2024-10-10 12:51:59,101 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:52:10,963 DEBUG Query successful
2024-10-10 12:52:11,256 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:52:21,314 DEBUG Query successful
2024-10-10 12:52:21,319 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:52:42,911 DEBUG Query successful
2024-10-10 12:52:42,920 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:53:16,964 DEBUG Query successful
2024-10-10 12:53:17,056 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:54:02,526 DEBUG Query successful
2024-10-10 12:54:02,528 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:54:17,626 DEBUG Query successful
2024-10-10 12:54:17,917 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:54:28,564 DEBUG Query successful
2024-10-10 12:54:28,567 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:54:43,799 DEBUG Query successful
2024-10-10 12:54:43,809 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:55:16,403 DEBUG Query successful
2024-10-10 12:55:16,495 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:56:09,607 DEBUG Query successful
2024-10-10 12:56:09,613 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:56:23,393 DEBUG Query successful
2024-10-10 12:56:23,680 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:56:31,833 DEBUG Query successful
2024-10-10 12:56:31,834 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:56:49,832 DEBUG Query successful
2024-10-10 12:56:49,838 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:57:25,834 DEBUG Query successful
2024-10-10 12:57:25,927 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:58:17,252 DEBUG Query successful
2024-10-10 12:58:17,257 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:58:36,794 DEBUG Query successful
2024-10-10 12:58:37,080 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:58:49,281 DEBUG Query successful
2024-10-10 12:58:49,283 DEBUG Querying model: gpt-4-turbo
2024-10-10 12:59:14,347 DEBUG Query successful
2024-10-10 12:59:14,351 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:00:16,243 DEBUG Query successful
2024-10-10 13:00:16,334 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:01:14,971 DEBUG Query successful
2024-10-10 13:01:14,973 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:01:26,971 DEBUG Query successful
2024-10-10 13:01:27,255 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:01:37,007 DEBUG Query successful
2024-10-10 13:01:37,009 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:02:01,020 DEBUG Query successful
2024-10-10 13:02:01,026 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:02:32,896 DEBUG Query successful
2024-10-10 13:02:32,987 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:03:32,192 DEBUG Query successful
2024-10-10 13:03:32,193 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:03:43,539 DEBUG Query successful
2024-10-10 13:03:43,825 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:03:55,357 DEBUG Query successful
2024-10-10 13:03:55,359 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:04:21,408 DEBUG Query successful
2024-10-10 13:04:21,414 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:04:53,280 DEBUG Query successful
2024-10-10 13:04:53,370 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:05:49,159 DEBUG Query successful
2024-10-10 13:05:49,166 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:06:00,387 DEBUG Query successful
2024-10-10 13:06:00,791 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:06:12,545 DEBUG Query successful
2024-10-10 13:06:12,546 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:06:31,059 DEBUG Query successful
2024-10-10 13:06:31,064 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:06:56,396 DEBUG Query successful
2024-10-10 13:06:56,490 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:08:51,912 DEBUG Query successful
2024-10-10 13:08:51,916 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:09:11,846 DEBUG Query successful
2024-10-10 13:09:12,138 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:09:19,123 DEBUG Query successful
2024-10-10 13:09:19,125 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:09:45,253 DEBUG Query successful
2024-10-10 13:09:45,255 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:10:14,356 DEBUG Query successful
2024-10-10 13:10:14,444 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:11:10,182 DEBUG Query successful
2024-10-10 13:11:10,184 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:11:38,413 DEBUG Query successful
2024-10-10 13:11:38,699 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:11:47,455 DEBUG Query successful
2024-10-10 13:11:47,457 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:12:12,089 DEBUG Query successful
2024-10-10 13:12:12,106 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:12:46,590 DEBUG Query successful
2024-10-10 13:12:46,678 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:13:43,860 DEBUG Query successful
2024-10-10 13:13:43,866 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:14:02,865 DEBUG Query successful
2024-10-10 13:57:29,611 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-10 13:57:29,646 DEBUG OpenAI client created
2024-10-10 13:57:29,646 DEBUG Model set to: gpt-4-turbo
2024-10-10 13:57:29,647 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-10 13:57:29,668 DEBUG OpenAI client created
2024-10-10 13:57:29,668 DEBUG Model set to: gpt-4-turbo
2024-10-10 13:57:34,908 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:57:46,775 DEBUG Query successful
2024-10-10 13:57:46,776 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:57:53,499 DEBUG Query successful
2024-10-10 13:57:53,501 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:58:01,448 DEBUG Query successful
2024-10-10 13:58:01,449 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:58:08,102 DEBUG Query successful
2024-10-10 13:58:08,505 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:59:05,046 DEBUG Query successful
2024-10-10 13:59:05,047 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:59:12,602 DEBUG Query successful
2024-10-10 13:59:22,956 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:59:38,398 DEBUG Query successful
2024-10-10 13:59:38,401 DEBUG Querying model: gpt-4-turbo
2024-10-10 13:59:45,766 DEBUG Query successful
2024-10-10 13:59:46,112 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:00:07,259 DEBUG Query successful
2024-10-10 14:00:07,261 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:00:15,568 DEBUG Query successful
2024-10-10 14:00:15,906 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:00:38,436 DEBUG Query successful
2024-10-10 14:00:38,437 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:00:46,105 DEBUG Query successful
2024-10-10 14:00:46,452 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:01:14,419 DEBUG Query successful
2024-10-10 14:01:14,421 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:01:21,633 DEBUG Query successful
2024-10-10 14:01:21,985 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:01:46,128 DEBUG Query successful
2024-10-10 14:01:46,130 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:01:53,902 DEBUG Query successful
2024-10-10 14:01:54,254 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:02:20,670 DEBUG Query successful
2024-10-10 14:02:20,676 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:02:28,089 DEBUG Query successful
2024-10-10 14:02:28,431 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:03:01,494 DEBUG Query successful
2024-10-10 14:03:01,496 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:03:08,938 DEBUG Query successful
2024-10-10 14:03:09,300 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:03:59,651 DEBUG Query successful
2024-10-10 14:03:59,652 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:04:05,803 DEBUG Query successful
2024-10-10 14:04:06,254 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:04:39,388 DEBUG Query successful
2024-10-10 14:04:39,390 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:04:46,489 DEBUG Query successful
2024-10-10 14:04:46,833 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:05:28,354 DEBUG Query successful
2024-10-10 14:05:28,359 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:05:35,410 DEBUG Query successful
2024-10-10 14:05:35,759 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:06:09,866 DEBUG Query successful
2024-10-10 14:06:09,868 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:06:15,540 DEBUG Query successful
2024-10-10 14:06:15,882 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:07:02,605 DEBUG Query successful
2024-10-10 14:07:02,606 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:07:09,100 DEBUG Query successful
2024-10-10 14:07:09,468 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:08:49,828 DEBUG Query successful
2024-10-10 14:08:49,831 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:08:55,333 DEBUG Query successful
2024-10-10 14:08:55,685 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:09:41,121 DEBUG Query successful
2024-10-10 14:09:41,126 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:09:49,171 DEBUG Query successful
2024-10-10 14:09:49,545 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:10:38,350 DEBUG Query successful
2024-10-10 14:10:38,351 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:10:47,692 DEBUG Query successful
2024-10-10 14:10:48,039 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:12:32,504 DEBUG Query successful
2024-10-10 14:12:32,506 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:12:39,129 DEBUG Query successful
2024-10-10 14:12:39,487 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:13:26,441 DEBUG Query successful
2024-10-10 14:13:26,443 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:13:32,196 DEBUG Query successful
2024-10-10 14:13:32,541 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:14:30,280 DEBUG Query successful
2024-10-10 14:14:30,285 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:14:35,468 DEBUG Query successful
2024-10-10 14:14:35,816 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:15:21,054 DEBUG Query successful
2024-10-10 14:15:21,055 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:15:25,652 DEBUG Query successful
2024-10-10 14:15:26,012 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:16:17,990 DEBUG Query successful
2024-10-10 14:16:17,991 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:16:24,102 DEBUG Query successful
2024-10-10 14:16:24,455 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:17:16,925 DEBUG Query successful
2024-10-10 14:17:16,927 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:17:21,742 DEBUG Query successful
2024-10-10 14:17:22,086 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:18:20,897 DEBUG Query successful
2024-10-10 14:18:20,899 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:18:25,539 DEBUG Query successful
2024-10-10 14:18:25,882 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:19:22,312 DEBUG Query successful
2024-10-10 14:19:22,313 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:19:26,750 DEBUG Query successful
2024-10-10 14:52:33,001 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-10 14:52:33,027 DEBUG OpenAI client created
2024-10-10 14:52:33,028 DEBUG Model set to: gpt-4-turbo
2024-10-10 14:52:33,028 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-10 14:52:33,049 DEBUG OpenAI client created
2024-10-10 14:52:33,049 DEBUG Model set to: gpt-4-turbo
2024-10-10 14:52:38,333 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:52:50,978 DEBUG Query successful
2024-10-10 14:52:50,980 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:52:59,374 DEBUG Query successful
2024-10-10 14:52:59,375 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:53:07,179 DEBUG Query successful
2024-10-10 14:53:07,184 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:53:15,551 DEBUG Query successful
2024-10-10 14:53:15,970 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:53:32,806 DEBUG Query successful
2024-10-10 14:53:32,808 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:53:40,920 DEBUG Query successful
2024-10-10 14:53:51,271 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:54:09,249 DEBUG Query successful
2024-10-10 14:54:09,250 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:54:17,512 DEBUG Query successful
2024-10-10 14:54:17,912 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:54:35,228 DEBUG Query successful
2024-10-10 14:54:35,229 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:54:43,183 DEBUG Query successful
2024-10-10 14:54:43,555 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:55:04,839 DEBUG Query successful
2024-10-10 14:55:04,842 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:55:10,625 DEBUG Query successful
2024-10-10 14:55:11,079 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:55:35,365 DEBUG Query successful
2024-10-10 14:55:35,368 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:55:43,079 DEBUG Query successful
2024-10-10 14:55:43,435 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:56:21,733 DEBUG Query successful
2024-10-10 14:56:21,735 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:56:27,817 DEBUG Query successful
2024-10-10 14:56:28,175 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:56:55,188 DEBUG Query successful
2024-10-10 14:56:55,190 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:57:03,685 DEBUG Query successful
2024-10-10 14:57:04,229 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:57:38,331 DEBUG Query successful
2024-10-10 14:57:38,337 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:57:45,536 DEBUG Query successful
2024-10-10 14:57:45,887 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:58:20,637 DEBUG Query successful
2024-10-10 14:58:20,640 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:58:25,259 DEBUG Query successful
2024-10-10 14:58:25,615 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:58:56,586 DEBUG Query successful
2024-10-10 14:58:56,588 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:59:03,997 DEBUG Query successful
2024-10-10 14:59:04,364 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:59:37,006 DEBUG Query successful
2024-10-10 14:59:37,006 DEBUG Querying model: gpt-4-turbo
2024-10-10 14:59:43,613 DEBUG Query successful
2024-10-10 14:59:43,971 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:00:34,555 DEBUG Query successful
2024-10-10 15:00:34,557 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:00:43,309 DEBUG Query successful
2024-10-10 15:00:43,686 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:01:23,267 DEBUG Query successful
2024-10-10 15:01:23,273 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:01:31,333 DEBUG Query successful
2024-10-10 15:01:31,684 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:03:11,838 DEBUG Query successful
2024-10-10 15:03:11,841 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:03:21,372 DEBUG Query successful
2024-10-10 15:03:21,735 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:04:03,575 DEBUG Query successful
2024-10-10 15:04:03,580 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:04:11,143 DEBUG Query successful
2024-10-10 15:04:11,489 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:04:56,194 DEBUG Query successful
2024-10-10 15:04:56,199 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:05:03,538 DEBUG Query successful
2024-10-10 15:05:03,897 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:06:54,802 DEBUG Query successful
2024-10-10 15:06:54,804 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:07:01,036 DEBUG Query successful
2024-10-10 15:07:01,388 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:07:58,852 DEBUG Query successful
2024-10-10 15:07:58,853 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:08:26,042 DEBUG Query successful
2024-10-10 15:08:26,392 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:09:22,590 DEBUG Query successful
2024-10-10 15:09:22,591 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:09:30,911 DEBUG Query successful
2024-10-10 15:09:31,276 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:10:27,512 DEBUG Query successful
2024-10-10 15:10:27,517 DEBUG Querying model: gpt-4-turbo
2024-10-10 15:10:35,982 DEBUG Query successful
2024-10-17 20:35:42,129 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-17 20:35:42,258 DEBUG OpenAI client created
2024-10-17 20:35:42,258 DEBUG Model set to: gpt-4-turbo
2024-10-17 20:35:42,259 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-17 20:35:42,279 DEBUG OpenAI client created
2024-10-17 20:35:42,279 DEBUG Model set to: gpt-4-turbo
2024-10-17 20:35:47,435 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:36:03,983 DEBUG Query successful
2024-10-17 20:36:03,984 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:36:11,310 DEBUG Query successful
2024-10-17 20:36:11,311 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:36:19,937 DEBUG Query successful
2024-10-17 20:36:19,938 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:36:29,146 DEBUG Query successful
2024-10-17 20:36:29,360 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:36:57,740 DEBUG Query successful
2024-10-17 20:36:57,741 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:37:04,036 DEBUG Query successful
2024-10-17 20:37:14,234 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:37:56,643 DEBUG Query successful
2024-10-17 20:37:56,644 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:38:02,739 DEBUG Query successful
2024-10-17 20:38:02,996 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:38:34,450 DEBUG Query successful
2024-10-17 20:38:34,456 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:38:42,262 DEBUG Query successful
2024-10-17 20:38:42,637 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:39:22,055 DEBUG Query successful
2024-10-17 20:39:22,056 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:39:46,519 DEBUG Query successful
2024-10-17 20:39:46,840 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:40:34,950 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-17 20:40:35,703 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:40:43,591 DEBUG Query successful
2024-10-17 20:40:43,832 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:42:08,374 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-17 20:42:08,401 DEBUG OpenAI client created
2024-10-17 20:42:08,402 DEBUG Model set to: gpt-4-turbo
2024-10-17 20:42:08,402 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-17 20:42:08,423 DEBUG OpenAI client created
2024-10-17 20:42:08,423 DEBUG Model set to: gpt-4-turbo
2024-10-17 20:42:13,478 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:42:38,624 DEBUG Query successful
2024-10-17 20:42:38,625 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:42:44,356 DEBUG Query successful
2024-10-17 20:42:44,357 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:42:50,114 DEBUG Query successful
2024-10-17 20:42:50,117 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:42:59,462 DEBUG Query successful
2024-10-17 20:42:59,610 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:43:16,743 DEBUG Query successful
2024-10-17 20:43:16,743 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:43:24,366 DEBUG Query successful
2024-10-17 20:43:34,717 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:44:27,324 DEBUG Query successful
2024-10-17 20:44:27,326 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:44:36,094 DEBUG Query successful
2024-10-17 20:44:36,495 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:45:42,591 DEBUG Query successful
2024-10-17 20:45:42,593 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:45:50,887 DEBUG Query successful
2024-10-17 20:45:51,252 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:47:19,215 DEBUG Query successful
2024-10-17 20:47:19,217 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:47:25,589 DEBUG Query successful
2024-10-17 20:47:25,956 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:49:31,959 DEBUG Query successful
2024-10-17 20:49:31,961 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:49:40,382 DEBUG Query successful
2024-10-17 20:49:40,810 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:52:00,988 DEBUG Query successful
2024-10-17 20:52:00,990 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:52:09,510 DEBUG Query successful
2024-10-17 20:52:09,941 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:54:45,407 DEBUG Query successful
2024-10-17 20:54:45,413 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:54:53,991 DEBUG Query successful
2024-10-17 20:54:54,362 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:57:59,422 DEBUG Query successful
2024-10-17 20:57:59,423 DEBUG Querying model: gpt-4-turbo
2024-10-17 20:58:08,864 DEBUG Query successful
2024-10-17 20:58:09,277 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:01:43,714 DEBUG Query successful
2024-10-17 21:01:43,716 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:01:53,927 DEBUG Query successful
2024-10-17 21:01:54,340 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:13:56,197 DEBUG Query successful
2024-10-17 21:13:56,199 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:14:07,089 DEBUG Query successful
2024-10-17 21:14:07,505 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:18:18,752 DEBUG Query successful
2024-10-17 21:18:18,758 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:18:26,822 DEBUG Query successful
2024-10-17 21:18:27,176 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:23:16,538 DEBUG Query successful
2024-10-17 21:23:16,539 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:23:23,520 DEBUG Query successful
2024-10-17 21:23:23,884 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:28:37,202 DEBUG Query successful
2024-10-17 21:28:37,204 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:28:44,372 DEBUG Query successful
2024-10-17 21:28:44,732 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:34:05,983 DEBUG Query successful
2024-10-17 21:34:05,987 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:34:13,298 DEBUG Query successful
2024-10-17 21:34:13,672 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:40:05,446 DEBUG Query successful
2024-10-17 21:40:05,448 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:40:12,822 DEBUG Query successful
2024-10-17 21:40:13,207 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:58:46,849 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-17 21:58:46,876 DEBUG Querying model: gpt-4-turbo
2024-10-17 21:58:54,872 DEBUG Query successful
2024-10-17 21:58:55,236 DEBUG Querying model: gpt-4-turbo
2024-10-17 22:11:26,356 DEBUG Query successful
2024-10-17 22:11:26,359 DEBUG Querying model: gpt-4-turbo
2024-10-17 22:11:35,451 DEBUG Query successful
2024-10-17 22:11:35,844 DEBUG Querying model: gpt-4-turbo
2024-10-17 22:25:45,521 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-17 22:25:45,545 DEBUG Querying model: gpt-4-turbo
2024-10-17 22:25:55,553 DEBUG Query successful
2024-10-17 22:25:55,921 DEBUG Querying model: gpt-4-turbo
2024-10-17 22:47:30,268 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-17 22:47:30,293 DEBUG Querying model: gpt-4-turbo
2024-10-17 22:47:39,657 DEBUG Query successful
2024-10-17 22:47:40,006 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:01:11,060 DEBUG Query successful
2024-10-17 23:01:11,063 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:01:19,954 DEBUG Query successful
2024-10-17 23:01:20,305 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:23:24,016 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-17 23:23:24,036 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:23:33,559 DEBUG Query successful
2024-10-17 23:23:33,914 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:30:58,928 DEBUG Query successful
2024-10-17 23:30:58,930 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:31:08,177 DEBUG Query successful
2024-10-17 23:31:08,534 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:39:00,116 DEBUG Query successful
2024-10-17 23:39:00,122 DEBUG Querying model: gpt-4-turbo
2024-10-17 23:39:10,591 DEBUG Query successful
2024-10-17 23:39:10,940 DEBUG Querying model: gpt-4-turbo
2024-10-18 00:01:39,566 DEBUG Query successful
2024-10-18 00:01:39,570 DEBUG Querying model: gpt-4-turbo
2024-10-18 00:01:48,802 DEBUG Query successful
2024-10-18 20:10:06,209 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-18 20:10:06,374 DEBUG OpenAI client created
2024-10-18 20:10:06,374 DEBUG Model set to: gpt-4-turbo
2024-10-18 20:10:06,375 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-18 20:10:06,395 DEBUG OpenAI client created
2024-10-18 20:10:06,395 DEBUG Model set to: gpt-4-turbo
2024-10-18 20:10:23,663 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:10:36,583 DEBUG Query successful
2024-10-18 20:10:36,585 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:10:38,438 DEBUG Query successful
2024-10-18 20:10:38,439 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:10:56,919 DEBUG Query successful
2024-10-18 20:10:57,012 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:11:12,963 DEBUG Query successful
2024-10-18 20:11:12,965 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:11:21,186 DEBUG Query successful
2024-10-18 20:11:21,190 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:11:31,250 DEBUG Query successful
2024-10-18 20:11:31,251 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:11:40,722 DEBUG Query successful
2024-10-18 20:11:41,248 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:11:54,616 DEBUG Query successful
2024-10-18 20:11:54,618 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:11:56,326 DEBUG Query successful
2024-10-18 20:11:56,327 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:12:10,996 DEBUG Query successful
2024-10-18 20:12:11,092 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:12:28,386 DEBUG Query successful
2024-10-18 20:12:28,387 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:12:40,914 DEBUG Query successful
2024-10-18 20:12:40,915 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:12:51,199 DEBUG Query successful
2024-10-18 20:12:51,200 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:12:59,866 DEBUG Query successful
2024-10-18 20:13:10,366 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:18:54,354 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-18 20:18:54,378 DEBUG OpenAI client created
2024-10-18 20:18:54,378 DEBUG Model set to: gpt-4-turbo
2024-10-18 20:18:54,378 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-18 20:18:54,400 DEBUG OpenAI client created
2024-10-18 20:18:54,400 DEBUG Model set to: gpt-4-turbo
2024-10-18 20:19:09,616 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:19:20,850 DEBUG Query successful
2024-10-18 20:19:20,852 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:19:26,418 DEBUG Query successful
2024-10-18 20:19:26,426 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:19:41,629 DEBUG Query successful
2024-10-18 20:19:41,719 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:20:04,094 DEBUG Query successful
2024-10-18 20:20:04,096 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:20:12,129 DEBUG Query successful
2024-10-18 20:20:12,134 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:20:18,541 DEBUG Query successful
2024-10-18 20:20:18,542 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:20:25,500 DEBUG Query successful
2024-10-18 20:20:26,171 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:20:36,939 DEBUG Query successful
2024-10-18 20:20:36,941 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:20:44,340 DEBUG Query successful
2024-10-18 20:20:44,343 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:21:03,308 DEBUG Query successful
2024-10-18 20:21:03,411 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:21:22,888 DEBUG Query successful
2024-10-18 20:21:22,890 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:21:31,937 DEBUG Query successful
2024-10-18 20:21:31,937 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:21:41,649 DEBUG Query successful
2024-10-18 20:21:41,650 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:21:49,247 DEBUG Query successful
2024-10-18 20:21:59,722 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:22:09,389 DEBUG Query successful
2024-10-18 20:22:09,391 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:22:16,254 DEBUG Query successful
2024-10-18 20:22:16,262 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:22:34,120 DEBUG Query successful
2024-10-18 20:22:34,227 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:22:51,896 DEBUG Query successful
2024-10-18 20:22:51,897 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:35:34,479 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-18 20:35:34,502 DEBUG OpenAI client created
2024-10-18 20:35:34,502 DEBUG Model set to: gpt-4-turbo
2024-10-18 20:35:34,502 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-18 20:35:34,525 DEBUG OpenAI client created
2024-10-18 20:35:34,525 DEBUG Model set to: gpt-4-turbo
2024-10-18 20:35:51,700 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:36:04,213 DEBUG Query successful
2024-10-18 20:36:04,214 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:36:05,886 DEBUG Query successful
2024-10-18 20:36:05,888 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:36:24,453 DEBUG Query successful
2024-10-18 20:36:24,546 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:36:41,064 DEBUG Query successful
2024-10-18 20:36:41,067 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:36:49,788 DEBUG Query successful
2024-10-18 20:36:49,789 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:36:56,564 DEBUG Query successful
2024-10-18 20:36:56,564 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:37:04,292 DEBUG Query successful
2024-10-18 20:37:04,755 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:37:17,760 DEBUG Query successful
2024-10-18 20:37:17,762 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:37:19,742 DEBUG Query successful
2024-10-18 20:37:19,743 DEBUG Querying model: gpt-4-turbo
2024-10-18 20:37:40,215 DEBUG Query successful
2024-10-18 20:37:40,336 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:04:02,352 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:04:02,488 DEBUG OpenAI client created
2024-10-19 14:04:02,488 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:04:02,489 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:04:02,511 DEBUG OpenAI client created
2024-10-19 14:04:02,511 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:04:18,583 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:10:12,971 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:10:12,994 DEBUG OpenAI client created
2024-10-19 14:10:12,994 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:10:12,994 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:10:13,016 DEBUG OpenAI client created
2024-10-19 14:10:13,016 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:10:13,051 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:10:13,074 DEBUG OpenAI client created
2024-10-19 14:10:13,074 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:10:13,074 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:10:13,096 DEBUG OpenAI client created
2024-10-19 14:10:13,096 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:10:29,252 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:10:30,723 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:10:40,168 DEBUG Query successful
2024-10-19 14:10:40,169 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:10:43,567 DEBUG Query successful
2024-10-19 14:10:43,569 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:10:45,610 DEBUG Query successful
2024-10-19 14:10:46,210 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:10:48,344 DEBUG Query successful
2024-10-19 14:10:48,344 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:10:53,593 DEBUG Query successful
2024-10-19 14:10:53,594 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:00,789 DEBUG Query successful
2024-10-19 14:11:01,318 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:02,738 DEBUG Query successful
2024-10-19 14:11:02,743 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:08,278 DEBUG Query successful
2024-10-19 14:11:15,488 DEBUG Query successful
2024-10-19 14:11:15,490 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:18,889 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:22,605 DEBUG Query successful
2024-10-19 14:11:33,176 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:37,369 DEBUG Query successful
2024-10-19 14:11:37,372 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:47,265 DEBUG Query successful
2024-10-19 14:11:47,871 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:49,062 DEBUG Query successful
2024-10-19 14:11:49,065 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:11:55,048 DEBUG Query successful
2024-10-19 14:11:55,575 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:10,406 DEBUG Query successful
2024-10-19 14:12:10,412 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:12,885 DEBUG Query successful
2024-10-19 14:12:12,887 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:16,412 DEBUG Query successful
2024-10-19 14:12:17,004 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:18,426 DEBUG Query successful
2024-10-19 14:12:19,246 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:38,598 DEBUG Query successful
2024-10-19 14:12:38,600 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:39,042 DEBUG Query successful
2024-10-19 14:12:39,043 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:44,274 DEBUG Query successful
2024-10-19 14:12:44,682 DEBUG Query successful
2024-10-19 14:12:44,847 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:12:45,343 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:09,134 DEBUG Query successful
2024-10-19 14:13:09,139 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:14,076 DEBUG Query successful
2024-10-19 14:13:14,415 DEBUG Query successful
2024-10-19 14:13:14,420 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:14,584 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:21,630 DEBUG Query successful
2024-10-19 14:13:22,116 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:38,379 DEBUG Query successful
2024-10-19 14:13:38,381 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:44,522 DEBUG Query successful
2024-10-19 14:13:45,047 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:51,360 DEBUG Query successful
2024-10-19 14:13:51,362 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:13:57,816 DEBUG Query successful
2024-10-19 14:13:58,395 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:14:11,556 DEBUG Query successful
2024-10-19 14:14:11,558 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:14:16,801 DEBUG Query successful
2024-10-19 14:14:17,329 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:14:32,488 DEBUG Query successful
2024-10-19 14:14:32,490 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:14:38,530 DEBUG Query successful
2024-10-19 14:14:39,165 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:14:44,106 DEBUG Query successful
2024-10-19 14:14:44,107 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:14:50,020 DEBUG Query successful
2024-10-19 14:14:50,640 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:15:11,181 DEBUG Query successful
2024-10-19 14:15:11,183 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:15:16,495 DEBUG Query successful
2024-10-19 14:15:17,034 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:15:26,298 DEBUG Query successful
2024-10-19 14:15:26,305 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:15:32,934 DEBUG Query successful
2024-10-19 14:15:33,593 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:15:51,259 DEBUG Query successful
2024-10-19 14:15:51,262 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:15:57,222 DEBUG Query successful
2024-10-19 14:15:57,750 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:16:09,113 DEBUG Query successful
2024-10-19 14:16:09,114 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:16:13,581 DEBUG Query successful
2024-10-19 14:16:14,059 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:16:32,526 DEBUG Query successful
2024-10-19 14:16:32,528 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:16:39,334 DEBUG Query successful
2024-10-19 14:16:39,833 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:16:52,512 DEBUG Query successful
2024-10-19 14:16:52,513 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:16:57,321 DEBUG Query successful
2024-10-19 14:16:57,819 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:17:13,246 DEBUG Query successful
2024-10-19 14:17:13,248 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:17:19,011 DEBUG Query successful
2024-10-19 14:17:19,529 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:17:39,642 DEBUG Query successful
2024-10-19 14:17:39,643 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:17:44,882 DEBUG Query successful
2024-10-19 14:17:45,404 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:18:04,329 DEBUG Query successful
2024-10-19 14:18:04,330 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:18:10,003 DEBUG Query successful
2024-10-19 14:18:10,518 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:18:25,746 DEBUG Query successful
2024-10-19 14:18:25,769 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:18:30,723 DEBUG Query successful
2024-10-19 14:18:31,203 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:18:57,600 DEBUG Query successful
2024-10-19 14:18:57,602 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:19:04,149 DEBUG Query successful
2024-10-19 14:19:04,667 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:19:52,641 DEBUG Query successful
2024-10-19 14:19:52,642 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:19:58,605 DEBUG Query successful
2024-10-19 14:19:59,097 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:20:12,370 DEBUG Query successful
2024-10-19 14:20:12,371 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:20:17,933 DEBUG Query successful
2024-10-19 14:20:18,422 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:20:45,361 DEBUG Query successful
2024-10-19 14:20:45,364 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:20:51,015 DEBUG Query successful
2024-10-19 14:20:51,611 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:21:08,120 DEBUG Query successful
2024-10-19 14:21:08,121 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:21:12,693 DEBUG Query successful
2024-10-19 14:21:13,237 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:22:02,570 DEBUG Query successful
2024-10-19 14:22:02,573 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:22:09,946 DEBUG Query successful
2024-10-19 14:22:10,422 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:22:38,026 DEBUG Query successful
2024-10-19 14:22:38,029 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:22:45,002 DEBUG Query successful
2024-10-19 14:22:45,465 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:23:35,862 DEBUG Query successful
2024-10-19 14:23:35,864 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:23:42,595 DEBUG Query successful
2024-10-19 14:23:43,089 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:23:59,542 DEBUG Query successful
2024-10-19 14:23:59,544 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:24:05,180 DEBUG Query successful
2024-10-19 14:24:05,687 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:24:39,615 DEBUG Query successful
2024-10-19 14:24:39,616 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:24:46,237 DEBUG Query successful
2024-10-19 14:24:46,745 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:24:58,755 DEBUG Query successful
2024-10-19 14:24:58,756 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:25:05,346 DEBUG Query successful
2024-10-19 14:25:05,967 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:26:44,702 DEBUG Query successful
2024-10-19 14:26:44,705 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:26:50,940 DEBUG Query successful
2024-10-19 14:26:51,429 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:27:52,562 DEBUG Query successful
2024-10-19 14:27:52,563 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:27:58,944 DEBUG Query successful
2024-10-19 14:27:59,453 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:28:06,346 DEBUG Query successful
2024-10-19 14:28:06,349 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:28:11,107 DEBUG Query successful
2024-10-19 14:28:11,594 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:30:02,554 DEBUG Query successful
2024-10-19 14:30:02,556 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:30:08,760 DEBUG Query successful
2024-10-19 14:30:09,296 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:31:11,829 DEBUG Query successful
2024-10-19 14:31:11,835 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:31:18,207 DEBUG Query successful
2024-10-19 14:31:18,687 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:32:13,753 DEBUG Query successful
2024-10-19 14:32:13,755 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:32:14,445 DEBUG Query successful
2024-10-19 14:32:14,447 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:32:19,399 DEBUG Query successful
2024-10-19 14:32:19,944 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:32:21,643 DEBUG Query successful
2024-10-19 14:35:29,378 DEBUG Query successful
2024-10-19 14:35:29,379 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:35:35,162 DEBUG Query successful
2024-10-19 14:35:38,411 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:35:38,437 DEBUG OpenAI client created
2024-10-19 14:35:38,437 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:35:38,437 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:35:38,460 DEBUG OpenAI client created
2024-10-19 14:35:38,460 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:35:38,493 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:35:38,516 DEBUG OpenAI client created
2024-10-19 14:35:38,516 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:35:38,516 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 14:35:38,538 DEBUG OpenAI client created
2024-10-19 14:35:38,538 DEBUG Model set to: gpt-4-turbo
2024-10-19 14:35:55,280 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:35:57,244 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:06,315 DEBUG Query successful
2024-10-19 14:36:06,319 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:11,168 DEBUG Query successful
2024-10-19 14:36:11,306 DEBUG Query successful
2024-10-19 14:36:11,309 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:11,724 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:16,069 DEBUG Query successful
2024-10-19 14:36:16,070 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:21,103 DEBUG Query successful
2024-10-19 14:36:21,103 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:24,889 DEBUG Query successful
2024-10-19 14:36:24,890 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:26,252 DEBUG Query successful
2024-10-19 14:36:26,887 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:29,577 DEBUG Query successful
2024-10-19 14:36:40,112 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:40,289 DEBUG Query successful
2024-10-19 14:36:40,290 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:47,136 DEBUG Query successful
2024-10-19 14:36:47,137 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:52,807 DEBUG Query successful
2024-10-19 14:36:52,808 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:58,601 DEBUG Query successful
2024-10-19 14:36:58,603 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:36:58,656 DEBUG Query successful
2024-10-19 14:37:04,039 DEBUG Query successful
2024-10-19 14:37:04,617 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:09,204 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:22,760 DEBUG Query successful
2024-10-19 14:37:22,762 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:26,189 DEBUG Query successful
2024-10-19 14:37:26,192 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:29,410 DEBUG Query successful
2024-10-19 14:37:30,055 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:32,524 DEBUG Query successful
2024-10-19 14:37:32,524 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:38,551 DEBUG Query successful
2024-10-19 14:37:38,555 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:46,187 DEBUG Query successful
2024-10-19 14:37:46,654 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:51,388 DEBUG Query successful
2024-10-19 14:37:51,389 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:37:57,269 DEBUG Query successful
2024-10-19 14:37:57,835 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:06,079 DEBUG Query successful
2024-10-19 14:38:06,081 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:13,066 DEBUG Query successful
2024-10-19 14:38:13,068 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:19,548 DEBUG Query successful
2024-10-19 14:38:19,553 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:19,858 DEBUG Query successful
2024-10-19 14:38:19,860 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:26,142 DEBUG Query successful
2024-10-19 14:38:26,856 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:27,349 DEBUG Query successful
2024-10-19 14:38:28,052 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:51,631 DEBUG Query successful
2024-10-19 14:38:51,632 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:53,921 DEBUG Query successful
2024-10-19 14:38:53,922 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:38:59,955 DEBUG Query successful
2024-10-19 14:39:00,496 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:00,691 DEBUG Query successful
2024-10-19 14:39:00,695 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:06,928 DEBUG Query successful
2024-10-19 14:39:06,929 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:13,890 DEBUG Query successful
2024-10-19 14:39:14,487 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:29,733 DEBUG Query successful
2024-10-19 14:39:29,734 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:36,055 DEBUG Query successful
2024-10-19 14:39:36,558 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:41,462 DEBUG Query successful
2024-10-19 14:39:41,464 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:50,161 DEBUG Query successful
2024-10-19 14:39:50,166 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:39:57,899 DEBUG Query successful
2024-10-19 14:39:57,900 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:05,842 DEBUG Query successful
2024-10-19 14:40:06,388 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:09,757 DEBUG Query successful
2024-10-19 14:40:09,759 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:15,401 DEBUG Query successful
2024-10-19 14:40:15,911 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:30,686 DEBUG Query successful
2024-10-19 14:40:30,687 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:38,184 DEBUG Query successful
2024-10-19 14:40:38,185 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:45,983 DEBUG Query successful
2024-10-19 14:40:45,983 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:53,787 DEBUG Query successful
2024-10-19 14:40:54,449 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:40:55,230 DEBUG Query successful
2024-10-19 14:40:55,236 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:41:00,503 DEBUG Query successful
2024-10-19 14:41:01,052 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:41:20,499 DEBUG Query successful
2024-10-19 14:41:20,500 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:41:29,127 DEBUG Query successful
2024-10-19 14:41:29,131 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:41:37,171 DEBUG Query successful
2024-10-19 14:41:37,172 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:41:43,926 DEBUG Query successful
2024-10-19 14:41:43,927 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:41:45,023 DEBUG Query successful
2024-10-19 14:41:45,521 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:41:48,925 DEBUG Query successful
2024-10-19 14:41:49,407 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:42:26,838 DEBUG Query successful
2024-10-19 14:42:26,839 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:42:30,012 DEBUG Query successful
2024-10-19 14:42:30,013 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:42:32,904 DEBUG Query successful
2024-10-19 14:42:33,515 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:42:37,873 DEBUG Query successful
2024-10-19 14:42:37,874 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:42:46,744 DEBUG Query successful
2024-10-19 14:42:46,745 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:42:55,158 DEBUG Query successful
2024-10-19 14:42:55,798 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:43:13,161 DEBUG Query successful
2024-10-19 14:43:13,162 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:43:18,851 DEBUG Query successful
2024-10-19 14:43:19,375 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:43:33,837 DEBUG Query successful
2024-10-19 14:43:33,842 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:43:43,149 DEBUG Query successful
2024-10-19 14:43:43,150 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:43:50,988 DEBUG Query successful
2024-10-19 14:43:50,993 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:43:59,513 DEBUG Query successful
2024-10-19 14:44:00,019 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:44:04,168 DEBUG Query successful
2024-10-19 14:44:04,170 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:44:08,255 DEBUG Query successful
2024-10-19 14:44:08,784 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:44:36,902 DEBUG Query successful
2024-10-19 14:44:36,908 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:44:44,188 DEBUG Query successful
2024-10-19 14:44:44,188 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:44:51,620 DEBUG Query successful
2024-10-19 14:44:51,621 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:44:57,896 DEBUG Query successful
2024-10-19 14:44:57,897 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:44:59,252 DEBUG Query successful
2024-10-19 14:44:59,743 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:45:02,062 DEBUG Query successful
2024-10-19 14:45:02,628 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:45:39,363 DEBUG Query successful
2024-10-19 14:45:39,364 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:45:48,906 DEBUG Query successful
2024-10-19 14:45:48,906 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:45:55,628 DEBUG Query successful
2024-10-19 14:45:55,629 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:46:03,414 DEBUG Query successful
2024-10-19 14:46:03,937 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:46:43,925 DEBUG Query successful
2024-10-19 14:46:43,926 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:46:53,235 DEBUG Query successful
2024-10-19 14:46:53,238 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:46:53,524 DEBUG Query successful
2024-10-19 14:46:53,528 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:46:57,177 DEBUG Query successful
2024-10-19 14:46:57,724 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:47:00,809 DEBUG Query successful
2024-10-19 14:47:00,809 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:47:07,993 DEBUG Query successful
2024-10-19 14:47:08,498 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:47:46,176 DEBUG Query successful
2024-10-19 14:47:46,177 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:47:49,824 DEBUG Query successful
2024-10-19 14:47:50,348 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:47:52,595 DEBUG Query successful
2024-10-19 14:47:52,601 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:48:01,674 DEBUG Query successful
2024-10-19 14:48:01,675 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:48:10,055 DEBUG Query successful
2024-10-19 14:48:10,055 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:48:17,512 DEBUG Query successful
2024-10-19 14:48:18,047 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:48:58,681 DEBUG Query successful
2024-10-19 14:48:58,682 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:49:06,665 DEBUG Query successful
2024-10-19 14:49:06,667 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:49:14,323 DEBUG Query successful
2024-10-19 14:49:14,324 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:49:21,469 DEBUG Query successful
2024-10-19 14:49:22,063 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:49:41,564 DEBUG Query successful
2024-10-19 14:49:41,567 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:49:45,621 DEBUG Query successful
2024-10-19 14:49:46,115 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:50:36,650 DEBUG Query successful
2024-10-19 14:50:36,652 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:50:40,072 DEBUG Query successful
2024-10-19 14:50:40,627 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:51:04,164 DEBUG Query successful
2024-10-19 14:51:04,166 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:51:10,012 DEBUG Query successful
2024-10-19 14:51:10,013 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:51:16,197 DEBUG Query successful
2024-10-19 14:51:16,198 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:51:21,994 DEBUG Query successful
2024-10-19 14:51:22,543 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:52:09,106 DEBUG Query successful
2024-10-19 14:52:09,107 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:52:15,292 DEBUG Query successful
2024-10-19 14:52:15,296 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:52:21,332 DEBUG Query successful
2024-10-19 14:52:21,333 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:52:27,351 DEBUG Query successful
2024-10-19 14:52:27,853 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:52:32,339 DEBUG Query successful
2024-10-19 14:52:32,344 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:52:37,090 DEBUG Query successful
2024-10-19 14:52:37,565 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:53:08,418 DEBUG Query successful
2024-10-19 14:53:08,419 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:53:14,289 DEBUG Query successful
2024-10-19 14:53:14,311 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:53:20,822 DEBUG Query successful
2024-10-19 14:53:20,822 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:53:26,732 DEBUG Query successful
2024-10-19 14:53:26,733 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:53:28,101 DEBUG Query successful
2024-10-19 14:53:28,601 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:53:31,185 DEBUG Query successful
2024-10-19 14:53:31,642 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:54:22,810 DEBUG Query successful
2024-10-19 14:54:22,812 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:54:25,422 DEBUG Query successful
2024-10-19 14:54:25,425 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:54:30,115 DEBUG Query successful
2024-10-19 14:54:30,116 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:54:30,417 DEBUG Query successful
2024-10-19 14:54:30,882 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:54:37,602 DEBUG Query successful
2024-10-19 14:54:37,602 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:54:45,358 DEBUG Query successful
2024-10-19 14:54:45,906 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:55:32,246 DEBUG Query successful
2024-10-19 14:55:32,248 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:55:39,692 DEBUG Query successful
2024-10-19 14:55:39,693 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:55:47,555 DEBUG Query successful
2024-10-19 14:55:47,556 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:55:55,574 DEBUG Query successful
2024-10-19 14:55:56,103 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:57:41,841 DEBUG Query successful
2024-10-19 14:57:41,843 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:57:47,831 DEBUG Query successful
2024-10-19 14:57:48,329 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:57:52,025 DEBUG Query successful
2024-10-19 14:57:52,028 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:58:00,306 DEBUG Query successful
2024-10-19 14:58:00,311 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:58:08,401 DEBUG Query successful
2024-10-19 14:58:08,402 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:58:20,328 DEBUG Query successful
2024-10-19 14:58:20,795 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:58:51,033 DEBUG Query successful
2024-10-19 14:58:51,036 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:58:57,338 DEBUG Query successful
2024-10-19 14:58:57,813 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:59:18,850 DEBUG Query successful
2024-10-19 14:59:18,852 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:59:32,460 DEBUG Query successful
2024-10-19 14:59:32,461 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:59:40,620 DEBUG Query successful
2024-10-19 14:59:40,621 DEBUG Querying model: gpt-4-turbo
2024-10-19 14:59:53,996 DEBUG Query successful
2024-10-19 14:59:54,489 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:01:52,812 DEBUG Query successful
2024-10-19 15:01:52,814 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:02:05,420 DEBUG Query successful
2024-10-19 15:02:05,421 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:02:10,379 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:02:10,491 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:02:15,164 DEBUG Query successful
2024-10-19 15:02:15,165 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:02:16,018 DEBUG Query successful
2024-10-19 15:02:16,508 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:02:30,934 DEBUG Query successful
2024-10-19 15:02:31,468 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:04:16,442 DEBUG Query successful
2024-10-19 15:04:16,445 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:04:22,103 DEBUG Query successful
2024-10-19 15:04:22,531 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:05:43,195 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:05:43,235 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:05:53,231 DEBUG Query successful
2024-10-19 15:05:53,232 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:06:01,822 DEBUG Query successful
2024-10-19 15:06:01,823 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:06:14,397 DEBUG Query successful
2024-10-19 15:06:14,817 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:06:28,692 DEBUG Query successful
2024-10-19 15:06:28,693 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:06:35,881 DEBUG Query successful
2024-10-19 15:06:36,306 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:07:39,755 DEBUG Query successful
2024-10-19 15:07:39,757 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:07:45,827 DEBUG Query successful
2024-10-19 15:07:46,309 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:08:20,985 DEBUG Query successful
2024-10-19 15:08:20,990 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:08:34,801 DEBUG Query successful
2024-10-19 15:08:34,803 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:08:43,014 DEBUG Query successful
2024-10-19 15:08:43,016 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:08:52,064 DEBUG Query successful
2024-10-19 15:08:52,579 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:11:00,535 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:11:00,577 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:11:05,968 DEBUG Query successful
2024-10-19 15:11:06,397 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:12:03,346 DEBUG Query successful
2024-10-19 15:12:03,353 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:12:15,719 DEBUG Query successful
2024-10-19 15:12:15,719 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:12:29,924 DEBUG Query successful
2024-10-19 15:12:29,924 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:12:40,689 DEBUG Query successful
2024-10-19 15:12:41,148 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:13:43,120 DEBUG Query successful
2024-10-19 15:13:43,121 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:13:56,882 DEBUG Query successful
2024-10-19 15:13:56,886 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:14:06,873 DEBUG Query successful
2024-10-19 15:14:06,874 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:14:16,172 DEBUG Query successful
2024-10-19 15:14:16,617 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:14:19,737 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:14:19,774 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:14:26,059 DEBUG Query successful
2024-10-19 15:14:26,505 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:17:29,015 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:17:29,050 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:17:36,084 DEBUG Query successful
2024-10-19 15:17:36,085 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:17:39,644 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:17:39,683 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:17:42,734 DEBUG Query successful
2024-10-19 15:17:42,735 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:17:45,104 DEBUG Query successful
2024-10-19 15:17:45,361 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:17:50,087 DEBUG Query successful
2024-10-19 15:17:50,304 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:20:58,737 DEBUG Query successful
2024-10-19 15:20:58,740 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:20:58,817 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:20:58,852 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:21:04,203 DEBUG Query successful
2024-10-19 15:21:04,425 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:21:07,334 DEBUG Query successful
2024-10-19 15:21:07,335 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:21:13,762 DEBUG Query successful
2024-10-19 15:21:13,763 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:21:28,638 DEBUG Query successful
2024-10-19 15:21:28,855 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:22:07,316 DEBUG Query successful
2024-10-19 15:22:07,317 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:22:13,569 DEBUG Query successful
2024-10-19 15:22:13,742 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:23:15,571 DEBUG Query successful
2024-10-19 15:23:15,573 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:23:21,395 DEBUG Query successful
2024-10-19 15:23:21,588 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:24:41,142 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:24:41,164 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:24:53,214 DEBUG Query successful
2024-10-19 15:24:53,215 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:25:01,066 DEBUG Query successful
2024-10-19 15:25:01,066 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:25:09,460 DEBUG Query successful
2024-10-19 15:25:09,738 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:26:34,510 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:26:34,537 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:26:38,276 DEBUG Query successful
2024-10-19 15:26:38,492 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:28:21,589 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:28:21,626 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:28:29,638 DEBUG Query successful
2024-10-19 15:28:29,640 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:28:37,713 DEBUG Query successful
2024-10-19 15:28:37,713 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:28:43,702 DEBUG Query successful
2024-10-19 15:28:43,707 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:28:46,786 DEBUG Query successful
2024-10-19 15:28:47,667 DEBUG Query successful
2024-10-19 15:28:47,847 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:28:51,597 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:32:00,820 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:32:00,850 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:32:21,898 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:32:21,922 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:32:21,978 DEBUG Query successful
2024-10-19 15:32:22,179 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:32:40,791 DEBUG Query successful
2024-10-19 15:32:40,798 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:32:54,906 DEBUG Query successful
2024-10-19 15:32:54,907 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:33:15,475 DEBUG Query successful
2024-10-19 15:33:16,025 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:35:36,460 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:35:36,489 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:35:40,423 DEBUG Query successful
2024-10-19 15:35:40,636 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:36:28,658 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:36:28,689 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:36:46,212 DEBUG Query successful
2024-10-19 15:36:46,213 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:37:02,281 DEBUG Query successful
2024-10-19 15:37:02,282 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:37:18,731 DEBUG Query successful
2024-10-19 15:37:19,270 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:38:53,188 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:38:53,215 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:38:56,570 DEBUG Query successful
2024-10-19 15:39:01,032 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:40:31,571 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:40:31,603 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:40:48,998 DEBUG Query successful
2024-10-19 15:40:49,001 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:41:05,797 DEBUG Query successful
2024-10-19 15:41:05,797 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:41:23,029 DEBUG Query successful
2024-10-19 15:41:23,515 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:42:14,409 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:42:14,442 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:42:21,435 DEBUG Query successful
2024-10-19 15:42:21,988 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:44:36,617 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:44:36,647 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:44:54,645 DEBUG Query successful
2024-10-19 15:44:54,647 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:45:16,427 DEBUG Query successful
2024-10-19 15:45:16,429 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:45:35,045 DEBUG Query successful
2024-10-19 15:45:35,498 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:45:35,537 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:45:35,701 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:45:43,261 DEBUG Query successful
2024-10-19 15:45:43,804 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:48:49,439 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:48:49,481 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:48:58,679 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:48:58,707 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:49:06,726 DEBUG Query successful
2024-10-19 15:49:06,726 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:49:08,123 DEBUG Query successful
2024-10-19 15:49:08,723 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:49:27,645 DEBUG Query successful
2024-10-19 15:49:27,646 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:49:46,707 DEBUG Query successful
2024-10-19 15:49:47,312 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:51:22,032 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:51:22,073 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:51:32,007 DEBUG Query successful
2024-10-19 15:51:32,696 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:52:59,784 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:52:59,823 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:53:18,927 DEBUG Query successful
2024-10-19 15:53:18,929 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:53:38,686 DEBUG Query successful
2024-10-19 15:53:38,687 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:54:00,103 DEBUG Query successful
2024-10-19 15:54:00,675 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:54:46,006 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:54:46,044 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:54:54,416 DEBUG Query successful
2024-10-19 15:54:54,982 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:57:15,087 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:57:15,117 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:57:34,049 DEBUG Query successful
2024-10-19 15:57:34,050 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:57:53,357 DEBUG Query successful
2024-10-19 15:57:53,358 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:58:08,158 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 15:58:08,184 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:58:13,094 DEBUG Query successful
2024-10-19 15:58:13,770 DEBUG Querying model: gpt-4-turbo
2024-10-19 15:58:16,908 DEBUG Query successful
2024-10-19 15:58:17,439 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:01:26,532 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:01:26,568 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:01:31,409 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:01:31,450 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:01:39,687 DEBUG Query successful
2024-10-19 16:01:40,289 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:01:54,293 DEBUG Query successful
2024-10-19 16:01:54,294 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:02:15,511 DEBUG Query successful
2024-10-19 16:02:15,516 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:02:34,966 DEBUG Query successful
2024-10-19 16:02:35,472 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:04:54,151 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:04:54,193 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:05:01,201 DEBUG Query successful
2024-10-19 16:05:01,789 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:05:48,409 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:05:48,447 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:06:09,156 DEBUG Query successful
2024-10-19 16:06:09,157 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:06:31,131 DEBUG Query successful
2024-10-19 16:06:31,136 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:06:54,434 DEBUG Query successful
2024-10-19 16:06:54,927 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:08:14,875 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:08:14,914 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:08:24,006 DEBUG Query successful
2024-10-19 16:08:24,652 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:10:07,740 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:10:07,773 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:10:31,187 DEBUG Query successful
2024-10-19 16:10:31,187 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:10:54,395 DEBUG Query successful
2024-10-19 16:10:54,396 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:11:19,164 DEBUG Query successful
2024-10-19 16:11:19,625 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:11:38,320 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:11:38,377 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:11:46,210 DEBUG Query successful
2024-10-19 16:11:46,657 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:14:32,774 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:14:32,805 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:14:53,998 DEBUG Query successful
2024-10-19 16:14:54,000 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:15:00,122 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:15:00,159 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:15:08,369 DEBUG Query successful
2024-10-19 16:15:08,803 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:15:16,737 DEBUG Query successful
2024-10-19 16:15:16,738 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:15:37,272 DEBUG Query successful
2024-10-19 16:15:37,735 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:18:22,138 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:18:22,178 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:18:29,725 DEBUG Query successful
2024-10-19 16:18:30,156 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:18:51,300 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:18:51,342 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:19:11,283 DEBUG Query successful
2024-10-19 16:19:11,288 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:19:31,747 DEBUG Query successful
2024-10-19 16:19:31,748 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:19:55,276 DEBUG Query successful
2024-10-19 16:19:55,743 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:21:43,846 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:21:43,885 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:21:51,803 DEBUG Query successful
2024-10-19 16:21:52,259 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:23:09,215 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:23:09,255 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:23:26,906 DEBUG Query successful
2024-10-19 16:23:26,906 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:23:45,794 DEBUG Query successful
2024-10-19 16:23:45,795 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:24:00,842 DEBUG Query successful
2024-10-19 16:24:01,297 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:25:06,494 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:25:06,525 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:25:16,662 DEBUG Query successful
2024-10-19 16:25:17,099 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:27:15,216 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:27:15,251 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:27:31,347 DEBUG Query successful
2024-10-19 16:27:31,352 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:27:51,031 DEBUG Query successful
2024-10-19 16:27:51,031 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:28:07,433 DEBUG Query successful
2024-10-19 16:28:07,875 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:28:30,193 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:28:30,227 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:28:37,698 DEBUG Query successful
2024-10-19 16:28:38,116 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:31:20,505 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:31:20,539 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:31:38,422 DEBUG Query successful
2024-10-19 16:31:38,426 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:31:50,833 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:31:50,868 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:31:57,304 DEBUG Query successful
2024-10-19 16:31:57,305 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:31:58,983 DEBUG Query successful
2024-10-19 16:31:59,433 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:32:16,864 DEBUG Query successful
2024-10-19 16:32:17,275 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:35:12,426 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:35:12,467 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:35:22,526 DEBUG Query successful
2024-10-19 16:35:22,972 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:35:30,478 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:35:30,512 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:35:45,378 DEBUG Query successful
2024-10-19 16:35:45,379 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:36:00,077 DEBUG Query successful
2024-10-19 16:36:00,079 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:36:13,022 DEBUG Query successful
2024-10-19 16:36:13,455 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:38:35,922 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:38:35,956 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:38:46,182 DEBUG Query successful
2024-10-19 16:38:46,613 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:39:26,763 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:39:26,801 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:39:42,483 DEBUG Query successful
2024-10-19 16:39:42,484 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:39:57,947 DEBUG Query successful
2024-10-19 16:39:57,947 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:40:11,349 DEBUG Query successful
2024-10-19 16:40:11,787 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:41:59,701 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:41:59,747 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:42:10,646 DEBUG Query successful
2024-10-19 16:42:11,133 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:43:25,305 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:43:25,339 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:43:41,406 DEBUG Query successful
2024-10-19 16:43:41,407 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:43:58,940 DEBUG Query successful
2024-10-19 16:43:58,941 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:44:14,527 DEBUG Query successful
2024-10-19 16:44:14,958 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:45:28,609 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:45:28,656 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:45:40,625 DEBUG Query successful
2024-10-19 16:45:41,162 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:47:29,594 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:47:29,643 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:47:46,057 DEBUG Query successful
2024-10-19 16:47:46,058 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:48:01,973 DEBUG Query successful
2024-10-19 16:48:01,973 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:48:17,891 DEBUG Query successful
2024-10-19 16:48:18,323 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:48:55,365 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:48:55,409 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:49:06,194 DEBUG Query successful
2024-10-19 16:49:06,678 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:51:31,595 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:51:31,642 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:51:46,746 DEBUG Query successful
2024-10-19 16:51:46,748 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:52:03,039 DEBUG Query successful
2024-10-19 16:52:03,040 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:52:20,384 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:52:20,430 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:52:21,864 DEBUG Query successful
2024-10-19 16:52:22,304 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:52:36,468 DEBUG Query successful
2024-10-19 16:52:36,975 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:55:35,821 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:55:35,860 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:55:50,519 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:55:50,557 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:55:53,201 DEBUG Query successful
2024-10-19 16:55:53,202 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:56:02,608 DEBUG Query successful
2024-10-19 16:56:03,084 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:56:09,517 DEBUG Query successful
2024-10-19 16:56:09,518 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:56:28,670 DEBUG Query successful
2024-10-19 16:56:29,103 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:59:16,731 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:59:16,776 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:59:28,926 DEBUG Query successful
2024-10-19 16:59:29,420 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:59:42,412 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 16:59:42,449 DEBUG Querying model: gpt-4-turbo
2024-10-19 16:59:59,610 DEBUG Query successful
2024-10-19 16:59:59,611 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:00:18,124 DEBUG Query successful
2024-10-19 17:00:18,124 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:00:34,545 DEBUG Query successful
2024-10-19 17:00:35,025 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:02:45,627 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:02:45,670 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:02:58,091 DEBUG Query successful
2024-10-19 17:02:58,608 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:03:50,252 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:03:50,297 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:04:08,682 DEBUG Query successful
2024-10-19 17:04:08,682 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:04:29,318 DEBUG Query successful
2024-10-19 17:04:29,319 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:04:59,338 DEBUG Query successful
2024-10-19 17:04:59,759 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:06:12,153 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:06:12,189 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:06:18,803 DEBUG Query successful
2024-10-19 17:06:19,236 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:08:13,150 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:08:13,191 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:08:26,232 DEBUG Query successful
2024-10-19 17:08:26,232 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:08:41,515 DEBUG Query successful
2024-10-19 17:08:41,515 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:08:56,863 DEBUG Query successful
2024-10-19 17:08:57,274 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:09:32,650 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:09:32,700 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:09:38,796 DEBUG Query successful
2024-10-19 17:12:09,958 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:12:09,996 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:12:24,755 DEBUG Query successful
2024-10-19 17:12:24,756 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:12:39,509 DEBUG Query successful
2024-10-19 17:12:39,510 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:12:55,577 DEBUG Query successful
2024-10-19 17:13:06,907 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 17:13:06,908 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 17:13:06,945 DEBUG OpenAI client created
2024-10-19 17:13:06,945 DEBUG Model set to: gpt-4-turbo
2024-10-19 17:13:06,945 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 17:13:06,946 DEBUG OpenAI client created
2024-10-19 17:13:06,946 DEBUG Model set to: gpt-4-turbo
2024-10-19 17:13:06,946 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 17:13:06,967 DEBUG OpenAI client created
2024-10-19 17:13:06,967 DEBUG Model set to: gpt-4-turbo
2024-10-19 17:13:06,969 DEBUG OpenAI client created
2024-10-19 17:13:06,969 DEBUG Model set to: gpt-4-turbo
2024-10-19 17:13:26,502 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:26,611 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:36,580 DEBUG Query successful
2024-10-19 17:13:36,581 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:39,249 DEBUG Query successful
2024-10-19 17:13:39,252 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:43,181 DEBUG Query successful
2024-10-19 17:13:43,747 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:45,048 DEBUG Query successful
2024-10-19 17:13:45,049 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:51,316 DEBUG Query successful
2024-10-19 17:13:51,317 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:57,744 DEBUG Query successful
2024-10-19 17:13:57,750 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:13:58,516 DEBUG Query successful
2024-10-19 17:13:59,082 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:04,015 DEBUG Query successful
2024-10-19 17:14:13,363 DEBUG Query successful
2024-10-19 17:14:13,365 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:14,534 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:22,008 DEBUG Query successful
2024-10-19 17:14:22,009 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:28,995 DEBUG Query successful
2024-10-19 17:14:28,997 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:32,097 DEBUG Query successful
2024-10-19 17:14:32,098 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:36,813 DEBUG Query successful
2024-10-19 17:14:38,798 DEBUG Query successful
2024-10-19 17:14:39,312 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:47,287 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:14:57,295 DEBUG Query successful
2024-10-19 17:14:57,296 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:03,632 DEBUG Query successful
2024-10-19 17:15:04,237 DEBUG Query successful
2024-10-19 17:15:04,238 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:04,297 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:11,064 DEBUG Query successful
2024-10-19 17:15:11,065 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:18,591 DEBUG Query successful
2024-10-19 17:15:18,592 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:25,956 DEBUG Query successful
2024-10-19 17:15:26,454 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:32,049 DEBUG Query successful
2024-10-19 17:15:32,051 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:39,188 DEBUG Query successful
2024-10-19 17:15:39,762 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:15:52,909 DEBUG Query successful
2024-10-19 17:15:52,910 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:01,140 DEBUG Query successful
2024-10-19 17:16:01,141 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:04,904 DEBUG Query successful
2024-10-19 17:16:04,907 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:08,909 DEBUG Query successful
2024-10-19 17:16:08,910 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:12,626 DEBUG Query successful
2024-10-19 17:16:13,182 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:17,134 DEBUG Query successful
2024-10-19 17:16:17,663 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:44,907 DEBUG Query successful
2024-10-19 17:16:44,909 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:46,323 DEBUG Query successful
2024-10-19 17:16:46,326 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:52,047 DEBUG Query successful
2024-10-19 17:16:52,513 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:16:55,291 DEBUG Query successful
2024-10-19 17:16:55,293 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:17:02,841 DEBUG Query successful
2024-10-19 17:17:02,841 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:17:12,650 DEBUG Query successful
2024-10-19 17:17:13,283 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:17:22,410 DEBUG Query successful
2024-10-19 17:17:22,411 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:17:29,472 DEBUG Query successful
2024-10-19 17:17:29,986 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:17:44,656 DEBUG Query successful
2024-10-19 17:17:44,657 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:17:52,812 DEBUG Query successful
2024-10-19 17:17:52,817 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:01,777 DEBUG Query successful
2024-10-19 17:18:01,778 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:03,753 DEBUG Query successful
2024-10-19 17:18:03,756 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:10,557 DEBUG Query successful
2024-10-19 17:18:10,708 DEBUG Query successful
2024-10-19 17:18:11,097 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:11,310 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:42,586 DEBUG Query successful
2024-10-19 17:18:42,588 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:51,159 DEBUG Query successful
2024-10-19 17:18:51,160 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:52,167 DEBUG Query successful
2024-10-19 17:18:52,167 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:57,743 DEBUG Query successful
2024-10-19 17:18:58,267 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:18:59,827 DEBUG Query successful
2024-10-19 17:18:59,829 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:19:08,969 DEBUG Query successful
2024-10-19 17:19:09,570 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:19:38,030 DEBUG Query successful
2024-10-19 17:19:38,031 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:19:38,783 DEBUG Query successful
2024-10-19 17:19:38,788 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:19:44,128 DEBUG Query successful
2024-10-19 17:19:44,661 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:19:46,000 DEBUG Query successful
2024-10-19 17:19:46,001 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:19:54,901 DEBUG Query successful
2024-10-19 17:19:54,902 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:20:03,476 DEBUG Query successful
2024-10-19 17:20:03,978 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:20:19,516 DEBUG Query successful
2024-10-19 17:20:19,517 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:20:26,671 DEBUG Query successful
2024-10-19 17:20:27,165 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:20:40,811 DEBUG Query successful
2024-10-19 17:20:40,817 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:20:50,379 DEBUG Query successful
2024-10-19 17:20:50,380 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:20:58,786 DEBUG Query successful
2024-10-19 17:20:58,787 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:21:04,786 DEBUG Query successful
2024-10-19 17:21:04,789 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:21:07,909 DEBUG Query successful
2024-10-19 17:21:08,533 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:21:09,920 DEBUG Query successful
2024-10-19 17:21:10,602 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:21:44,288 DEBUG Query successful
2024-10-19 17:21:44,289 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:21:52,795 DEBUG Query successful
2024-10-19 17:21:52,797 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:22:03,062 DEBUG Query successful
2024-10-19 17:22:03,068 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:22:12,193 DEBUG Query successful
2024-10-19 17:22:12,738 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:03,152 DEBUG Query successful
2024-10-19 17:23:03,155 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:04,143 DEBUG Query successful
2024-10-19 17:23:04,148 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:08,395 DEBUG Query successful
2024-10-19 17:23:08,974 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:11,858 DEBUG Query successful
2024-10-19 17:23:11,860 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:19,990 DEBUG Query successful
2024-10-19 17:23:19,991 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:28,273 DEBUG Query successful
2024-10-19 17:23:28,759 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:50,696 DEBUG Query successful
2024-10-19 17:23:50,698 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:23:55,259 DEBUG Query successful
2024-10-19 17:23:55,764 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:24:05,937 DEBUG Query successful
2024-10-19 17:24:05,942 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:24:15,955 DEBUG Query successful
2024-10-19 17:24:15,955 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:24:24,071 DEBUG Query successful
2024-10-19 17:24:24,072 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:24:33,670 DEBUG Query successful
2024-10-19 17:24:34,279 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:24:43,183 DEBUG Query successful
2024-10-19 17:24:43,186 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:24:48,082 DEBUG Query successful
2024-10-19 17:24:48,691 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:25:23,791 DEBUG Query successful
2024-10-19 17:25:23,794 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:25:32,575 DEBUG Query successful
2024-10-19 17:25:32,579 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:25:40,800 DEBUG Query successful
2024-10-19 17:25:40,800 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:25:42,445 DEBUG Query successful
2024-10-19 17:25:42,446 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:25:47,664 DEBUG Query successful
2024-10-19 17:25:48,164 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:25:48,689 DEBUG Query successful
2024-10-19 17:25:49,228 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:26:39,293 DEBUG Query successful
2024-10-19 17:26:39,294 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:26:46,933 DEBUG Query successful
2024-10-19 17:26:46,933 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:26:49,066 DEBUG Query successful
2024-10-19 17:26:49,068 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:26:53,662 DEBUG Query successful
2024-10-19 17:26:54,197 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:26:56,819 DEBUG Query successful
2024-10-19 17:26:56,822 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:27:05,529 DEBUG Query successful
2024-10-19 17:27:06,118 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:27:58,102 DEBUG Query successful
2024-10-19 17:27:58,107 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:28:06,878 DEBUG Query successful
2024-10-19 17:28:06,878 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:28:17,723 DEBUG Query successful
2024-10-19 17:28:17,725 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:28:26,641 DEBUG Query successful
2024-10-19 17:28:27,140 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:28:52,298 DEBUG Query successful
2024-10-19 17:28:52,300 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:28:56,889 DEBUG Query successful
2024-10-19 17:28:57,389 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:29:14,192 DEBUG Query successful
2024-10-19 17:29:14,194 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:29:23,606 DEBUG Query successful
2024-10-19 17:29:23,606 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:29:33,144 DEBUG Query successful
2024-10-19 17:29:33,144 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:29:42,832 DEBUG Query successful
2024-10-19 17:29:43,368 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:30:31,235 DEBUG Query successful
2024-10-19 17:30:31,237 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:30:39,015 DEBUG Query successful
2024-10-19 17:30:39,016 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:30:45,766 DEBUG Query successful
2024-10-19 17:30:45,768 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:30:53,566 DEBUG Query successful
2024-10-19 17:30:54,176 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:31:46,385 DEBUG Query successful
2024-10-19 17:31:46,387 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:31:57,807 DEBUG Query successful
2024-10-19 17:31:57,808 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:32:02,766 DEBUG Query successful
2024-10-19 17:32:02,773 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:32:06,961 DEBUG Query successful
2024-10-19 17:32:06,961 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:32:08,232 DEBUG Query successful
2024-10-19 17:32:08,760 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:32:15,766 DEBUG Query successful
2024-10-19 17:32:16,385 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:33:06,327 DEBUG Query successful
2024-10-19 17:33:06,330 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:33:10,513 DEBUG Query successful
2024-10-19 17:33:10,981 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:34:12,843 DEBUG Query successful
2024-10-19 17:34:12,846 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:34:21,757 DEBUG Query successful
2024-10-19 17:34:21,762 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:34:29,947 DEBUG Query successful
2024-10-19 17:34:29,947 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:34:38,651 DEBUG Query successful
2024-10-19 17:34:39,120 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:35:10,160 DEBUG Query successful
2024-10-19 17:35:10,162 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:35:15,379 DEBUG Query successful
2024-10-19 17:35:15,905 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:35:30,794 DEBUG Query successful
2024-10-19 17:35:30,795 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:35:38,910 DEBUG Query successful
2024-10-19 17:35:38,916 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:35:46,582 DEBUG Query successful
2024-10-19 17:35:46,583 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:35:56,361 DEBUG Query successful
2024-10-19 17:35:56,844 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:36:17,222 DEBUG Query successful
2024-10-19 17:36:17,222 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:36:21,891 DEBUG Query successful
2024-10-19 17:36:22,435 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:36:53,602 DEBUG Query successful
2024-10-19 17:36:53,604 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:37:05,479 DEBUG Query successful
2024-10-19 17:37:05,485 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:37:15,630 DEBUG Query successful
2024-10-19 17:37:15,630 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:37:23,162 DEBUG Query successful
2024-10-19 17:37:23,163 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:37:24,987 DEBUG Query successful
2024-10-19 17:37:25,532 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:37:26,992 DEBUG Query successful
2024-10-19 17:37:27,680 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:40:37,319 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:40:37,379 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:40:42,039 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:40:42,078 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:40:45,944 DEBUG Query successful
2024-10-19 17:40:46,431 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:40:47,974 DEBUG Query successful
2024-10-19 17:40:47,976 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:40:58,553 DEBUG Query successful
2024-10-19 17:40:58,556 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:41:08,576 DEBUG Query successful
2024-10-19 17:41:09,083 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:43:58,699 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:43:58,737 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:44:03,166 DEBUG Query successful
2024-10-19 17:44:03,717 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:44:20,708 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:44:20,737 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:44:31,713 DEBUG Query successful
2024-10-19 17:44:31,714 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:44:42,602 DEBUG Query successful
2024-10-19 17:44:42,603 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:44:55,738 DEBUG Query successful
2024-10-19 17:44:56,331 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:45:55,073 DEBUG Query successful
2024-10-19 17:45:55,074 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:46:03,844 DEBUG Query successful
2024-10-19 17:46:03,850 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:46:12,181 DEBUG Query successful
2024-10-19 17:46:12,182 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:46:19,475 DEBUG Query successful
2024-10-19 17:46:19,985 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:47:16,331 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:47:16,371 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:47:21,469 DEBUG Query successful
2024-10-19 17:47:21,987 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:49:34,387 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:49:34,415 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:49:46,714 DEBUG Query successful
2024-10-19 17:49:46,715 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:49:56,923 DEBUG Query successful
2024-10-19 17:49:56,923 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:50:04,025 DEBUG Query successful
2024-10-19 17:50:04,500 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:50:34,906 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:50:34,943 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:50:40,849 DEBUG Query successful
2024-10-19 17:50:41,344 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:51:03,101 DEBUG Query successful
2024-10-19 17:51:03,102 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:51:11,656 DEBUG Query successful
2024-10-19 17:51:11,657 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:51:25,368 DEBUG Query successful
2024-10-19 17:51:25,370 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:51:33,154 DEBUG Query successful
2024-10-19 17:51:33,662 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:52:31,858 DEBUG Query successful
2024-10-19 17:52:31,860 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:52:41,082 DEBUG Query successful
2024-10-19 17:52:41,082 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:52:50,444 DEBUG Query successful
2024-10-19 17:52:50,445 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:53:01,897 DEBUG Query successful
2024-10-19 17:53:02,348 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:53:53,552 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:53:53,587 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:53:58,774 DEBUG Query successful
2024-10-19 17:53:59,232 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:56:15,676 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:56:15,705 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:56:24,087 DEBUG Query successful
2024-10-19 17:56:24,088 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:56:31,547 DEBUG Query successful
2024-10-19 17:56:31,548 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:56:43,198 DEBUG Query successful
2024-10-19 17:56:43,667 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:57:11,615 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:57:11,644 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:57:17,541 DEBUG Query successful
2024-10-19 17:57:18,009 DEBUG Querying model: gpt-4-turbo
2024-10-19 17:59:55,853 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 17:59:55,878 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:00:05,830 DEBUG Query successful
2024-10-19 18:00:05,831 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:00:15,972 DEBUG Query successful
2024-10-19 18:00:15,976 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:00:25,901 DEBUG Query successful
2024-10-19 18:00:26,384 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:00:30,094 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:00:30,134 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:00:35,342 DEBUG Query successful
2024-10-19 18:00:35,845 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:03:38,691 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:03:38,724 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:03:49,258 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:03:49,296 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:03:49,762 DEBUG Query successful
2024-10-19 18:03:49,766 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:03:53,597 DEBUG Query successful
2024-10-19 18:03:54,092 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:04:04,126 DEBUG Query successful
2024-10-19 18:04:04,129 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:04:14,797 DEBUG Query successful
2024-10-19 18:04:15,265 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:07:06,078 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:07:06,109 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:07:09,049 DEBUG Query successful
2024-10-19 18:07:09,512 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:07:27,837 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:07:27,880 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:07:36,577 DEBUG Query successful
2024-10-19 18:07:36,578 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:07:46,552 DEBUG Query successful
2024-10-19 18:07:46,554 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:07:55,321 DEBUG Query successful
2024-10-19 18:07:55,779 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:10:21,773 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:10:21,796 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:10:24,543 DEBUG Query successful
2024-10-19 18:10:24,996 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:11:08,219 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:11:08,249 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:11:19,092 DEBUG Query successful
2024-10-19 18:11:19,096 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:11:27,756 DEBUG Query successful
2024-10-19 18:11:27,756 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:11:38,361 DEBUG Query successful
2024-10-19 18:13:36,627 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-19 18:13:36,661 DEBUG Querying model: gpt-4-turbo
2024-10-19 18:13:41,051 DEBUG Query successful
2024-10-19 21:51:41,963 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 21:51:41,987 DEBUG OpenAI client created
2024-10-19 21:51:41,988 DEBUG Model set to: gpt-4-turbo
2024-10-19 21:51:41,988 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-19 21:51:42,009 DEBUG OpenAI client created
2024-10-19 21:51:42,009 DEBUG Model set to: gpt-4-turbo
2024-10-19 21:51:58,616 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:09,498 DEBUG Query successful
2024-10-19 21:52:09,500 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:10,486 DEBUG Query successful
2024-10-19 21:52:10,517 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:19,288 DEBUG Query successful
2024-10-19 21:52:19,385 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:30,017 DEBUG Query successful
2024-10-19 21:52:30,019 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:34,610 DEBUG Query successful
2024-10-19 21:52:34,611 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:41,081 DEBUG Query successful
2024-10-19 21:52:41,082 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:47,037 DEBUG Query successful
2024-10-19 21:52:47,542 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:58,394 DEBUG Query successful
2024-10-19 21:52:58,396 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:52:59,921 DEBUG Query successful
2024-10-19 21:52:59,931 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:53:08,903 DEBUG Query successful
2024-10-19 21:53:09,001 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:53:24,659 DEBUG Query successful
2024-10-19 21:53:24,660 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:53:32,071 DEBUG Query successful
2024-10-19 21:53:32,073 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:53:39,751 DEBUG Query successful
2024-10-19 21:53:39,753 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:53:48,009 DEBUG Query successful
2024-10-19 21:53:58,415 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:54:10,909 DEBUG Query successful
2024-10-19 21:54:10,911 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:54:12,918 DEBUG Query successful
2024-10-19 21:54:12,920 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:54:25,130 DEBUG Query successful
2024-10-19 21:54:25,230 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:54:47,898 DEBUG Query successful
2024-10-19 21:54:47,900 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:54:56,552 DEBUG Query successful
2024-10-19 21:54:56,553 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:55:05,265 DEBUG Query successful
2024-10-19 21:55:05,266 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:55:13,779 DEBUG Query successful
2024-10-19 21:55:14,251 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:55:23,225 DEBUG Query successful
2024-10-19 21:55:23,227 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:55:24,885 DEBUG Query successful
2024-10-19 21:55:24,887 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:55:38,608 DEBUG Query successful
2024-10-19 21:55:38,703 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:56:01,170 DEBUG Query successful
2024-10-19 21:56:01,174 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:56:10,109 DEBUG Query successful
2024-10-19 21:56:10,110 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:56:20,331 DEBUG Query successful
2024-10-19 21:56:20,332 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:56:30,148 DEBUG Query successful
2024-10-19 21:56:30,634 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:56:40,308 DEBUG Query successful
2024-10-19 21:56:40,310 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:56:42,425 DEBUG Query successful
2024-10-19 21:56:42,427 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:56:59,675 DEBUG Query successful
2024-10-19 21:56:59,778 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:57:29,058 DEBUG Query successful
2024-10-19 21:57:29,060 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:57:39,235 DEBUG Query successful
2024-10-19 21:57:39,237 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:57:50,391 DEBUG Query successful
2024-10-19 21:57:50,392 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:58:00,272 DEBUG Query successful
2024-10-19 21:58:00,883 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:58:11,475 DEBUG Query successful
2024-10-19 21:58:11,477 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:58:13,638 DEBUG Query successful
2024-10-19 21:58:13,639 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:58:31,594 DEBUG Query successful
2024-10-19 21:58:31,705 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:59:08,185 DEBUG Query successful
2024-10-19 21:59:08,186 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:59:19,454 DEBUG Query successful
2024-10-19 21:59:19,455 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:59:31,679 DEBUG Query successful
2024-10-19 21:59:31,679 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:59:44,004 DEBUG Query successful
2024-10-19 21:59:44,542 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:59:54,543 DEBUG Query successful
2024-10-19 21:59:54,545 DEBUG Querying model: gpt-4-turbo
2024-10-19 21:59:56,635 DEBUG Query successful
2024-10-19 21:59:56,636 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:00:14,594 DEBUG Query successful
2024-10-19 22:00:14,706 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:00:51,359 DEBUG Query successful
2024-10-19 22:00:51,361 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:01:06,367 DEBUG Query successful
2024-10-19 22:01:06,368 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:01:19,202 DEBUG Query successful
2024-10-19 22:01:19,208 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:01:32,467 DEBUG Query successful
2024-10-19 22:01:33,003 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:01:43,885 DEBUG Query successful
2024-10-19 22:01:43,890 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:01:46,105 DEBUG Query successful
2024-10-19 22:01:46,107 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:02:07,779 DEBUG Query successful
2024-10-19 22:02:07,882 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:02:49,618 DEBUG Query successful
2024-10-19 22:02:49,620 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:03:04,775 DEBUG Query successful
2024-10-19 22:03:04,776 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:03:21,830 DEBUG Query successful
2024-10-19 22:03:21,831 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:03:34,350 DEBUG Query successful
2024-10-19 22:03:34,804 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:03:47,572 DEBUG Query successful
2024-10-19 22:03:47,574 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:03:53,128 DEBUG Query successful
2024-10-19 22:03:53,130 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:04:14,755 DEBUG Query successful
2024-10-19 22:04:14,850 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:04:46,033 DEBUG Query successful
2024-10-19 22:04:46,035 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:05:00,399 DEBUG Query successful
2024-10-19 22:05:00,400 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:05:13,646 DEBUG Query successful
2024-10-19 22:05:13,647 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:05:28,499 DEBUG Query successful
2024-10-19 22:05:28,920 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:05:48,587 DEBUG Query successful
2024-10-19 22:05:48,589 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:05:53,968 DEBUG Query successful
2024-10-19 22:05:53,968 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:06:14,268 DEBUG Query successful
2024-10-19 22:06:14,362 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:07:00,121 DEBUG Query successful
2024-10-19 22:07:00,123 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:07:13,261 DEBUG Query successful
2024-10-19 22:07:13,262 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:07:30,302 DEBUG Query successful
2024-10-19 22:07:30,302 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:07:46,965 DEBUG Query successful
2024-10-19 22:07:47,424 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:07:59,130 DEBUG Query successful
2024-10-19 22:07:59,131 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:08:04,626 DEBUG Query successful
2024-10-19 22:08:04,627 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:08:28,314 DEBUG Query successful
2024-10-19 22:08:28,417 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:09:06,059 DEBUG Query successful
2024-10-19 22:09:06,064 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:09:20,879 DEBUG Query successful
2024-10-19 22:09:20,880 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:09:35,676 DEBUG Query successful
2024-10-19 22:09:35,677 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:09:51,980 DEBUG Query successful
2024-10-19 22:09:52,445 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:10:14,812 DEBUG Query successful
2024-10-19 22:10:14,814 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:10:18,562 DEBUG Query successful
2024-10-19 22:10:18,565 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:10:41,955 DEBUG Query successful
2024-10-19 22:10:42,059 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:11:21,554 DEBUG Query successful
2024-10-19 22:11:21,556 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:11:37,572 DEBUG Query successful
2024-10-19 22:11:37,573 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:11:51,994 DEBUG Query successful
2024-10-19 22:11:51,994 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:12:07,118 DEBUG Query successful
2024-10-19 22:12:07,588 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:12:20,719 DEBUG Query successful
2024-10-19 22:12:20,724 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:12:25,224 DEBUG Query successful
2024-10-19 22:12:25,224 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:12:54,183 DEBUG Query successful
2024-10-19 22:12:54,285 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:14:42,666 DEBUG Query successful
2024-10-19 22:14:42,670 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:14:55,351 DEBUG Query successful
2024-10-19 22:14:55,352 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:15:09,337 DEBUG Query successful
2024-10-19 22:15:09,342 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:15:23,943 DEBUG Query successful
2024-10-19 22:15:24,433 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:15:35,580 DEBUG Query successful
2024-10-19 22:15:35,584 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:15:38,948 DEBUG Query successful
2024-10-19 22:15:38,949 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:16:05,723 DEBUG Query successful
2024-10-19 22:16:05,822 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:16:55,670 DEBUG Query successful
2024-10-19 22:16:55,672 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:17:08,963 DEBUG Query successful
2024-10-19 22:17:08,964 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:17:23,740 DEBUG Query successful
2024-10-19 22:17:23,741 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:17:37,355 DEBUG Query successful
2024-10-19 22:17:37,841 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:17:50,289 DEBUG Query successful
2024-10-19 22:17:50,293 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:17:59,068 DEBUG Query successful
2024-10-19 22:17:59,070 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:18:32,265 DEBUG Query successful
2024-10-19 22:18:32,372 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:20:27,766 DEBUG Query successful
2024-10-19 22:20:27,769 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:20:43,564 DEBUG Query successful
2024-10-19 22:20:43,565 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:20:58,857 DEBUG Query successful
2024-10-19 22:20:58,858 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:21:15,316 DEBUG Query successful
2024-10-19 22:21:15,768 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:21:25,592 DEBUG Query successful
2024-10-19 22:21:25,594 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:21:29,281 DEBUG Query successful
2024-10-19 22:21:29,287 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:22:04,654 DEBUG Query successful
2024-10-19 22:22:04,758 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:22:57,777 DEBUG Query successful
2024-10-19 22:22:57,779 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:23:14,643 DEBUG Query successful
2024-10-19 22:23:14,644 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:23:32,293 DEBUG Query successful
2024-10-19 22:23:32,294 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:23:54,793 DEBUG Query successful
2024-10-19 22:23:55,252 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:24:09,024 DEBUG Query successful
2024-10-19 22:24:09,026 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:24:12,823 DEBUG Query successful
2024-10-19 22:24:12,824 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:24:45,291 DEBUG Query successful
2024-10-19 22:24:45,394 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:25:34,954 DEBUG Query successful
2024-10-19 22:25:34,955 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:25:48,166 DEBUG Query successful
2024-10-19 22:25:48,168 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:26:02,659 DEBUG Query successful
2024-10-19 22:26:02,660 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:26:16,426 DEBUG Query successful
2024-10-19 22:26:17,085 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:26:27,800 DEBUG Query successful
2024-10-19 22:26:27,804 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:26:37,421 DEBUG Query successful
2024-10-19 22:26:37,422 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:27:19,451 DEBUG Query successful
2024-10-19 22:27:19,562 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:28:09,228 DEBUG Query successful
2024-10-19 22:28:09,231 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:28:22,248 DEBUG Query successful
2024-10-19 22:28:22,249 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:28:36,119 DEBUG Query successful
2024-10-19 22:28:36,120 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:28:49,173 DEBUG Query successful
2024-10-19 22:28:49,692 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:29:02,060 DEBUG Query successful
2024-10-19 22:29:02,063 DEBUG Querying model: gpt-4-turbo
2024-10-19 22:29:06,591 DEBUG Query successful
2024-10-19 22:29:06,592 DEBUG Querying model: gpt-4-turbo
2024-10-24 16:18:33,667 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:33,775 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:33,817 DEBUG OpenAI client created
2024-10-24 16:18:33,817 DEBUG Model set to: gpt-4-turbo
2024-10-24 16:18:33,817 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:33,822 DEBUG OpenAI client created
2024-10-24 16:18:33,822 DEBUG Model set to: gpt-4-turbo
2024-10-24 16:18:33,822 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:33,845 DEBUG OpenAI client created
2024-10-24 16:18:33,845 DEBUG Model set to: gpt-4-turbo
2024-10-24 16:18:33,850 DEBUG OpenAI client created
2024-10-24 16:18:33,850 DEBUG Model set to: gpt-4-turbo
2024-10-24 16:18:57,901 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:57,933 DEBUG OpenAI client created
2024-10-24 16:18:57,933 DEBUG Model set to: gpt-4-turbo
2024-10-24 16:18:57,933 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:57,956 DEBUG OpenAI client created
2024-10-24 16:18:57,958 DEBUG Model set to: gpt-4-turbo
2024-10-24 16:18:58,004 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:58,033 DEBUG OpenAI client created
2024-10-24 16:18:58,033 DEBUG Model set to: gpt-4-turbo
2024-10-24 16:18:58,033 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 16:18:58,058 DEBUG OpenAI client created
2024-10-24 16:18:58,058 DEBUG Model set to: gpt-4-turbo
2024-10-24 20:27:12,243 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 20:27:12,385 DEBUG OpenAI client created
2024-10-24 20:27:12,385 DEBUG Model set to: gpt-4-turbo
2024-10-24 20:27:12,386 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 20:27:12,408 DEBUG OpenAI client created
2024-10-24 20:27:12,408 DEBUG Model set to: gpt-4-turbo
2024-10-24 20:28:25,882 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 20:28:25,908 DEBUG OpenAI client created
2024-10-24 20:28:25,908 DEBUG Model set to: gpt-4-turbo
2024-10-24 20:28:25,908 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-24 20:28:25,932 DEBUG OpenAI client created
2024-10-24 20:28:25,933 DEBUG Model set to: gpt-4-turbo
2024-10-24 20:28:42,298 DEBUG Querying model: gpt-4-turbo
2024-10-24 20:28:56,504 DEBUG Query successful
2024-10-24 20:28:56,593 DEBUG Querying model: gpt-4-turbo
2024-10-24 20:29:17,340 DEBUG Query successful
2024-10-24 20:29:17,362 DEBUG Querying model: gpt-4-turbo
2024-10-24 20:29:41,815 DEBUG Query successful
2024-10-24 20:29:41,818 DEBUG Querying model: gpt-4-turbo
2024-10-24 20:29:51,580 DEBUG Query successful
2024-10-24 20:29:52,164 DEBUG Querying model: gpt-4-turbo
2024-10-25 12:13:59,464 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 12:13:59,500 DEBUG OpenAI client created
2024-10-25 12:13:59,500 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 12:13:59,500 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 12:13:59,528 DEBUG OpenAI client created
2024-10-25 12:13:59,528 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 12:14:26,131 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:14:35,254 DEBUG Query successful
2024-10-25 12:14:35,332 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:14:52,593 DEBUG Query successful
2024-10-25 12:14:52,618 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:15:01,590 DEBUG Query successful
2024-10-25 12:15:01,591 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:15:04,323 DEBUG Query successful
2024-10-25 12:15:04,863 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:15:16,765 DEBUG Query successful
2024-10-25 12:15:16,843 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:15:34,683 DEBUG Query successful
2024-10-25 12:15:34,714 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:15:45,913 DEBUG Query successful
2024-10-25 12:15:45,914 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:15:49,363 DEBUG Query successful
2024-10-25 12:15:49,793 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:15:57,741 DEBUG Query successful
2024-10-25 12:15:57,817 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:16:10,683 DEBUG Query successful
2024-10-25 12:16:10,711 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:16:24,524 DEBUG Query successful
2024-10-25 12:16:24,524 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:16:28,684 DEBUG Query successful
2024-10-25 12:16:29,146 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:16:38,586 DEBUG Query successful
2024-10-25 12:16:38,662 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:16:55,622 DEBUG Query successful
2024-10-25 12:16:55,649 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:17:10,045 DEBUG Query successful
2024-10-25 12:17:10,046 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:17:13,294 DEBUG Query successful
2024-10-25 12:17:13,806 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:17:21,173 DEBUG Query successful
2024-10-25 12:17:21,256 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:17:38,520 DEBUG Query successful
2024-10-25 12:17:38,542 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:17:54,943 DEBUG Query successful
2024-10-25 12:17:54,945 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:18:00,252 DEBUG Query successful
2024-10-25 12:18:00,827 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:18:08,729 DEBUG Query successful
2024-10-25 12:18:08,803 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:18:23,421 DEBUG Query successful
2024-10-25 12:18:23,449 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:18:43,013 DEBUG Query successful
2024-10-25 12:18:43,015 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:18:46,803 DEBUG Query successful
2024-10-25 12:18:47,355 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:18:54,978 DEBUG Query successful
2024-10-25 12:18:55,056 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:19:11,873 DEBUG Query successful
2024-10-25 12:19:11,899 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:19:35,318 DEBUG Query successful
2024-10-25 12:19:35,320 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:19:40,328 DEBUG Query successful
2024-10-25 12:19:40,850 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:19:48,220 DEBUG Query successful
2024-10-25 12:19:48,297 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:20:20,305 DEBUG Query successful
2024-10-25 12:20:20,333 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:20:42,655 DEBUG Query successful
2024-10-25 12:20:42,656 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:20:46,945 DEBUG Query successful
2024-10-25 12:20:47,423 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:20:54,904 DEBUG Query successful
2024-10-25 12:20:54,983 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:21:43,787 DEBUG Query successful
2024-10-25 12:21:43,814 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:22:06,830 DEBUG Query successful
2024-10-25 12:22:06,831 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:22:10,690 DEBUG Query successful
2024-10-25 12:22:11,184 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:22:18,930 DEBUG Query successful
2024-10-25 12:22:19,007 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:22:38,220 DEBUG Query successful
2024-10-25 12:22:38,242 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:23:07,485 DEBUG Query successful
2024-10-25 12:23:07,486 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:23:11,470 DEBUG Query successful
2024-10-25 12:23:11,978 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:23:20,029 DEBUG Query successful
2024-10-25 12:23:20,104 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:23:40,891 DEBUG Query successful
2024-10-25 12:23:40,922 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:24:09,340 DEBUG Query successful
2024-10-25 12:24:09,341 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:24:13,513 DEBUG Query successful
2024-10-25 12:24:13,972 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:24:22,356 DEBUG Query successful
2024-10-25 12:24:22,438 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:24:37,420 DEBUG Query successful
2024-10-25 12:24:37,444 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:34:52,113 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:34:52,377 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:35:10,089 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:35:10,095 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:35:28,054 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:35:28,061 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:35:45,934 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:35:46,432 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:36:03,922 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:36:04,009 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:36:21,837 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:36:21,860 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:36:39,893 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:36:39,915 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:36:57,594 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:36:57,600 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:37:15,349 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:37:15,351 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:37:33,193 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:37:33,661 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:37:51,370 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:37:51,455 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:38:09,319 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:38:09,349 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:38:27,354 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:38:27,373 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:38:44,870 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:38:44,874 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:39:02,383 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:39:02,388 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:39:19,836 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:39:20,363 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:39:37,910 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:39:37,990 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:39:55,674 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:39:55,703 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:40:14,157 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:40:14,179 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:40:32,056 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:40:32,058 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:40:49,896 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:40:49,901 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:41:06,141 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: EOF occurred in violation of protocol (_ssl.c:997)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: EOF occurred in violation of protocol (_ssl.c:997)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:41:06,668 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:41:16,261 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:41:16,341 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:41:25,893 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:41:25,922 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:41:43,425 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:41:43,447 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:41:56,557 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:41:56,560 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:42:11,574 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:42:11,580 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:42:25,726 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:42:26,269 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:42:38,997 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:42:39,098 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:42:56,986 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:42:57,013 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:43:11,313 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:43:11,332 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:43:24,851 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:43:24,857 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:43:42,426 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:43:42,431 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:44:00,392 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:44:00,897 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:44:18,694 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:44:18,773 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:44:36,355 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:44:36,383 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:44:54,297 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:44:54,313 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:45:11,726 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:45:11,733 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:45:29,314 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:45:29,322 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:45:45,592 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:45:46,064 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:45:58,627 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:45:58,709 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:46:16,326 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:46:16,355 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:46:30,865 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:46:30,882 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:46:48,507 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:46:48,513 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:47:06,173 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:47:06,181 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:47:24,175 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:47:24,708 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:47:42,438 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:47:42,518 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:48:00,313 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:48:00,339 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:48:18,679 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:48:18,698 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:48:32,603 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:48:32,609 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:48:45,509 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:48:45,515 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:48:58,237 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:48:58,723 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:49:16,727 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:49:16,811 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:49:30,830 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:49:30,861 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:49:45,210 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:49:45,228 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:49:58,140 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:49:58,145 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:50:10,623 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:50:10,626 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:50:25,569 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:50:26,095 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:50:43,873 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:50:43,956 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:50:58,562 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:50:58,592 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:51:16,762 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:51:16,785 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:51:30,895 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:51:30,898 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:51:44,943 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:51:44,950 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:51:58,212 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:51:58,711 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:52:11,583 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:52:11,668 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:52:25,238 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:52:25,269 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:52:38,911 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:52:38,928 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:52:46,586 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:52:46,593 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:52:59,390 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 12:52:59,395 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:53:12,213 ERROR An error occurred: Request timed out.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 317, in handle_request
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 383, in start_tls
    return self._stream.start_tls(ssl_context, server_hostname, timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: _ssl.c:980: The handshake operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 63, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 982, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 992, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-10-25 12:53:12,749 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:53:31,438 DEBUG Query successful
2024-10-25 12:53:31,520 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:53:42,681 DEBUG Query successful
2024-10-25 12:53:42,714 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:54:19,872 DEBUG Query successful
2024-10-25 12:54:19,873 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:54:26,738 DEBUG Query successful
2024-10-25 12:54:27,268 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:54:35,467 DEBUG Query successful
2024-10-25 12:54:35,547 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:54:46,352 DEBUG Query successful
2024-10-25 12:54:46,380 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:55:25,284 DEBUG Query successful
2024-10-25 12:55:25,285 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:55:30,407 DEBUG Query successful
2024-10-25 12:55:30,861 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:55:41,023 DEBUG Query successful
2024-10-25 12:55:41,103 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:55:52,642 DEBUG Query successful
2024-10-25 12:55:52,667 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:56:34,281 DEBUG Query successful
2024-10-25 12:56:34,282 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:56:38,878 DEBUG Query successful
2024-10-25 12:56:39,337 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:56:47,689 DEBUG Query successful
2024-10-25 12:56:47,767 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:56:59,333 DEBUG Query successful
2024-10-25 12:56:59,362 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:57:45,869 DEBUG Query successful
2024-10-25 12:57:45,871 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:57:53,558 DEBUG Query successful
2024-10-25 12:57:54,039 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:58:00,519 DEBUG Query successful
2024-10-25 12:58:00,602 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:58:12,621 DEBUG Query successful
2024-10-25 12:58:12,649 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:58:58,638 DEBUG Query successful
2024-10-25 12:58:58,641 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:59:03,348 DEBUG Query successful
2024-10-25 12:59:03,861 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:59:10,758 DEBUG Query successful
2024-10-25 12:59:10,833 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 12:59:24,329 DEBUG Query successful
2024-10-25 12:59:24,353 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:00:10,920 DEBUG Query successful
2024-10-25 13:00:10,922 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:00:16,230 DEBUG Query successful
2024-10-25 13:00:16,675 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:00:26,090 DEBUG Query successful
2024-10-25 13:00:26,167 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:00:39,491 DEBUG Query successful
2024-10-25 13:00:39,513 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:02:29,796 DEBUG Query successful
2024-10-25 13:02:29,799 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:02:34,647 DEBUG Query successful
2024-10-25 13:02:35,125 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:02:44,005 DEBUG Query successful
2024-10-25 13:02:44,080 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:02:57,687 DEBUG Query successful
2024-10-25 13:02:57,710 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:03:49,010 DEBUG Query successful
2024-10-25 13:03:49,012 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:03:54,640 DEBUG Query successful
2024-10-25 13:03:58,930 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:04:07,421 DEBUG Query successful
2024-10-25 13:04:07,501 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:04:22,101 DEBUG Query successful
2024-10-25 13:04:22,123 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:05:12,947 DEBUG Query successful
2024-10-25 13:05:12,950 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:05:18,445 DEBUG Query successful
2024-10-25 13:05:18,970 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:05:28,587 DEBUG Query successful
2024-10-25 13:05:28,663 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:05:43,518 DEBUG Query successful
2024-10-25 13:05:43,545 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:06:43,775 DEBUG Query successful
2024-10-25 13:06:43,777 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:06:49,364 DEBUG Query successful
2024-10-25 13:06:49,873 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:07:00,679 DEBUG Query successful
2024-10-25 13:07:00,765 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:07:15,577 DEBUG Query successful
2024-10-25 13:07:15,600 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:13:02,546 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 13:13:02,572 DEBUG OpenAI client created
2024-10-25 13:13:02,572 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 13:13:02,572 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 13:13:02,599 DEBUG OpenAI client created
2024-10-25 13:13:02,599 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 13:13:20,875 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 13:13:20,899 DEBUG OpenAI client created
2024-10-25 13:13:20,899 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 13:13:20,899 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 13:13:20,921 DEBUG OpenAI client created
2024-10-25 13:13:20,922 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 13:13:39,694 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:13:48,419 DEBUG Query successful
2024-10-25 13:13:48,511 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:13:57,236 DEBUG Query successful
2024-10-25 13:13:57,261 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:05,256 DEBUG Query successful
2024-10-25 13:14:05,258 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:07,747 DEBUG Query successful
2024-10-25 13:14:08,318 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:15,508 DEBUG Query successful
2024-10-25 13:14:15,587 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:21,849 DEBUG Query successful
2024-10-25 13:14:21,876 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:32,131 DEBUG Query successful
2024-10-25 13:14:32,133 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:35,044 DEBUG Query successful
2024-10-25 13:14:35,489 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:45,370 DEBUG Query successful
2024-10-25 13:14:45,448 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:14:53,551 DEBUG Query successful
2024-10-25 13:14:53,573 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:15:05,541 DEBUG Query successful
2024-10-25 13:15:05,542 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:15:08,298 DEBUG Query successful
2024-10-25 13:15:08,796 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:15:16,703 DEBUG Query successful
2024-10-25 13:15:16,784 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:15:26,500 DEBUG Query successful
2024-10-25 13:15:26,536 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:15:41,320 DEBUG Query successful
2024-10-25 13:15:41,322 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:15:44,259 DEBUG Query successful
2024-10-25 13:15:44,750 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:15:53,341 DEBUG Query successful
2024-10-25 13:15:53,423 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:16:03,087 DEBUG Query successful
2024-10-25 13:16:03,110 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:17:13,625 DEBUG Query successful
2024-10-25 13:17:13,627 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:17:16,613 DEBUG Query successful
2024-10-25 13:17:17,371 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:17:24,529 DEBUG Query successful
2024-10-25 13:17:24,610 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:17:35,699 DEBUG Query successful
2024-10-25 13:17:35,729 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:17:54,337 DEBUG Query successful
2024-10-25 13:17:54,341 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:17:57,852 DEBUG Query successful
2024-10-25 13:17:58,359 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:18:08,147 DEBUG Query successful
2024-10-25 13:18:08,225 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:18:22,682 DEBUG Query successful
2024-10-25 13:18:22,711 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:18:42,568 DEBUG Query successful
2024-10-25 13:18:42,569 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:18:45,902 DEBUG Query successful
2024-10-25 13:18:46,444 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:18:54,452 DEBUG Query successful
2024-10-25 13:18:54,532 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:19:08,923 DEBUG Query successful
2024-10-25 13:19:08,961 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:19:32,780 DEBUG Query successful
2024-10-25 13:19:32,783 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:19:35,974 DEBUG Query successful
2024-10-25 13:19:36,480 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:19:43,830 DEBUG Query successful
2024-10-25 13:19:43,923 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:19:57,721 DEBUG Query successful
2024-10-25 13:19:57,746 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:20:21,472 DEBUG Query successful
2024-10-25 13:20:21,474 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:20:24,809 DEBUG Query successful
2024-10-25 13:20:25,274 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:20:33,756 DEBUG Query successful
2024-10-25 13:20:33,834 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:20:48,812 DEBUG Query successful
2024-10-25 13:20:48,837 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:21:16,696 DEBUG Query successful
2024-10-25 13:21:16,698 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:21:20,473 DEBUG Query successful
2024-10-25 13:21:20,990 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:21:31,483 DEBUG Query successful
2024-10-25 13:21:31,565 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:21:48,435 DEBUG Query successful
2024-10-25 13:21:48,461 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:22:16,887 DEBUG Query successful
2024-10-25 13:22:16,889 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:22:20,762 DEBUG Query successful
2024-10-25 13:22:21,386 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:22:32,155 DEBUG Query successful
2024-10-25 13:22:32,233 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:22:48,400 DEBUG Query successful
2024-10-25 13:22:48,420 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:23:17,324 DEBUG Query successful
2024-10-25 13:23:17,326 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:23:20,661 DEBUG Query successful
2024-10-25 13:23:21,222 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:23:29,605 DEBUG Query successful
2024-10-25 13:23:29,683 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:23:47,816 DEBUG Query successful
2024-10-25 13:23:47,840 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:24:20,707 DEBUG Query successful
2024-10-25 13:24:20,709 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:24:23,445 DEBUG Query successful
2024-10-25 13:24:23,997 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:24:31,960 DEBUG Query successful
2024-10-25 13:24:32,045 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:24:51,632 DEBUG Query successful
2024-10-25 13:24:51,661 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:25:26,921 DEBUG Query successful
2024-10-25 13:25:26,923 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:25:30,175 DEBUG Query successful
2024-10-25 13:25:30,684 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:25:38,690 DEBUG Query successful
2024-10-25 13:25:38,769 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:25:58,791 DEBUG Query successful
2024-10-25 13:25:58,820 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:26:35,888 DEBUG Query successful
2024-10-25 13:26:35,890 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:26:39,157 DEBUG Query successful
2024-10-25 13:26:39,724 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:26:49,932 DEBUG Query successful
2024-10-25 13:26:50,010 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:27:11,911 DEBUG Query successful
2024-10-25 13:27:11,938 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:27:49,698 DEBUG Query successful
2024-10-25 13:27:49,699 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:27:52,385 DEBUG Query successful
2024-10-25 13:27:53,020 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:28:02,723 DEBUG Query successful
2024-10-25 13:28:02,804 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:28:21,599 DEBUG Query successful
2024-10-25 13:28:21,627 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:29:01,010 DEBUG Query successful
2024-10-25 13:29:01,012 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:29:04,341 DEBUG Query successful
2024-10-25 13:29:04,786 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:29:13,349 DEBUG Query successful
2024-10-25 13:29:13,428 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:29:34,481 DEBUG Query successful
2024-10-25 13:29:34,510 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:30:18,869 DEBUG Query successful
2024-10-25 13:30:18,870 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:30:22,264 DEBUG Query successful
2024-10-25 13:30:22,748 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:30:29,984 DEBUG Query successful
2024-10-25 13:30:30,068 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:30:51,583 DEBUG Query successful
2024-10-25 13:30:51,611 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:31:34,981 DEBUG Query successful
2024-10-25 13:31:34,982 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:31:38,316 DEBUG Query successful
2024-10-25 13:31:38,815 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:31:47,733 DEBUG Query successful
2024-10-25 13:31:47,821 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:32:11,332 DEBUG Query successful
2024-10-25 13:32:11,362 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:32:56,423 DEBUG Query successful
2024-10-25 13:32:56,425 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:32:59,803 DEBUG Query successful
2024-10-25 13:33:00,345 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:33:09,656 DEBUG Query successful
2024-10-25 13:33:09,734 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:33:27,097 DEBUG Query successful
2024-10-25 13:33:27,123 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:34:22,397 DEBUG Query successful
2024-10-25 13:34:22,398 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:34:26,174 DEBUG Query successful
2024-10-25 13:34:26,700 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:34:34,783 DEBUG Query successful
2024-10-25 13:34:34,863 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:34:53,366 DEBUG Query successful
2024-10-25 13:34:53,397 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:35:50,190 DEBUG Query successful
2024-10-25 13:35:50,191 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:35:53,768 DEBUG Query successful
2024-10-25 13:35:54,246 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:36:02,097 DEBUG Query successful
2024-10-25 13:36:02,174 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:36:25,110 DEBUG Query successful
2024-10-25 13:36:25,138 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:37:21,439 DEBUG Query successful
2024-10-25 13:37:21,442 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:37:24,815 DEBUG Query successful
2024-10-25 13:37:25,441 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:37:32,619 DEBUG Query successful
2024-10-25 13:37:32,702 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:37:52,409 DEBUG Query successful
2024-10-25 13:37:52,438 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:38:46,212 DEBUG Query successful
2024-10-25 13:38:46,215 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:38:49,811 DEBUG Query successful
2024-10-25 13:38:50,328 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:38:57,095 DEBUG Query successful
2024-10-25 13:38:57,172 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:39:16,971 DEBUG Query successful
2024-10-25 13:39:16,996 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:40:14,807 DEBUG Query successful
2024-10-25 13:40:14,808 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:40:19,735 DEBUG Query successful
2024-10-25 13:40:20,284 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:40:27,927 DEBUG Query successful
2024-10-25 13:40:28,009 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:40:48,501 DEBUG Query successful
2024-10-25 13:40:48,529 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:43:42,182 DEBUG Query successful
2024-10-25 13:43:42,184 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:43:46,071 DEBUG Query successful
2024-10-25 13:43:46,608 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:43:53,281 DEBUG Query successful
2024-10-25 13:43:53,360 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:44:14,906 DEBUG Query successful
2024-10-25 13:44:14,934 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:45:13,070 DEBUG Query successful
2024-10-25 13:45:13,071 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:45:17,041 DEBUG Query successful
2024-10-25 13:45:17,547 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:45:25,723 DEBUG Query successful
2024-10-25 13:45:25,805 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:45:48,575 DEBUG Query successful
2024-10-25 13:45:48,605 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:46:45,897 DEBUG Query successful
2024-10-25 13:46:45,898 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:46:49,694 DEBUG Query successful
2024-10-25 13:46:50,288 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:46:58,676 DEBUG Query successful
2024-10-25 13:46:58,754 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:47:17,033 DEBUG Query successful
2024-10-25 13:47:17,064 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:50:21,324 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 13:50:21,357 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:50:26,239 DEBUG Query successful
2024-10-25 13:50:26,713 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:50:33,841 DEBUG Query successful
2024-10-25 13:50:33,926 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:50:54,304 DEBUG Query successful
2024-10-25 13:50:54,330 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:53:57,756 DEBUG Query successful
2024-10-25 13:53:57,759 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:54:03,009 DEBUG Query successful
2024-10-25 13:54:03,467 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:54:11,662 DEBUG Query successful
2024-10-25 13:54:11,742 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:54:32,276 DEBUG Query successful
2024-10-25 13:54:32,301 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:57:36,668 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 13:57:36,719 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:57:41,340 DEBUG Query successful
2024-10-25 13:57:41,926 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:57:50,653 DEBUG Query successful
2024-10-25 13:57:50,730 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 13:58:11,317 DEBUG Query successful
2024-10-25 13:58:11,344 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 14:01:15,763 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 14:01:15,804 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 14:01:21,218 DEBUG Query successful
2024-10-25 16:51:38,752 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 16:51:38,779 DEBUG OpenAI client created
2024-10-25 16:51:38,780 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 16:51:38,780 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 16:51:38,802 DEBUG OpenAI client created
2024-10-25 16:51:38,802 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 16:51:56,809 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:52:06,598 DEBUG Query successful
2024-10-25 16:52:06,683 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:52:28,018 DEBUG Query successful
2024-10-25 16:52:28,047 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:52:37,481 DEBUG Query successful
2024-10-25 16:52:37,483 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:52:44,793 DEBUG Query successful
2024-10-25 16:54:15,575 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 16:54:15,599 DEBUG OpenAI client created
2024-10-25 16:54:15,599 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 16:54:15,599 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 16:54:15,620 DEBUG OpenAI client created
2024-10-25 16:54:15,620 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 16:54:37,482 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:54:47,247 DEBUG Query successful
2024-10-25 16:54:47,323 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:54:58,247 DEBUG Query successful
2024-10-25 16:54:58,274 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:55:06,911 DEBUG Query successful
2024-10-25 16:55:06,913 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 16:55:10,692 DEBUG Query successful
2024-10-25 17:02:37,974 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 17:02:37,998 DEBUG OpenAI client created
2024-10-25 17:02:37,999 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 17:02:37,999 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-25 17:02:38,023 DEBUG OpenAI client created
2024-10-25 17:02:38,024 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-25 17:02:55,463 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:03:05,335 DEBUG Query successful
2024-10-25 17:03:05,412 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:03:20,142 DEBUG Query successful
2024-10-25 17:03:20,168 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:03:29,251 DEBUG Query successful
2024-10-25 17:03:29,253 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:03:32,023 DEBUG Query successful
2024-10-25 17:03:32,834 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:03:40,880 DEBUG Query successful
2024-10-25 17:03:40,957 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:03:53,820 DEBUG Query successful
2024-10-25 17:03:53,846 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:04:04,109 DEBUG Query successful
2024-10-25 17:04:04,112 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:04:06,882 DEBUG Query successful
2024-10-25 17:04:07,343 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:04:14,544 DEBUG Query successful
2024-10-25 17:04:14,622 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:04:28,350 DEBUG Query successful
2024-10-25 17:04:28,375 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:04:42,220 DEBUG Query successful
2024-10-25 17:04:42,221 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:04:45,863 DEBUG Query successful
2024-10-25 17:04:46,460 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:04:57,240 DEBUG Query successful
2024-10-25 17:04:57,319 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:05:11,475 DEBUG Query successful
2024-10-25 17:05:11,505 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:05:27,090 DEBUG Query successful
2024-10-25 17:05:27,091 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:05:30,710 DEBUG Query successful
2024-10-25 17:05:31,260 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:05:38,339 DEBUG Query successful
2024-10-25 17:05:38,415 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:05:53,177 DEBUG Query successful
2024-10-25 17:05:53,202 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:06:12,498 DEBUG Query successful
2024-10-25 17:06:12,501 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:06:18,281 DEBUG Query successful
2024-10-25 17:06:18,824 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:06:26,153 DEBUG Query successful
2024-10-25 17:06:26,227 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:06:42,561 DEBUG Query successful
2024-10-25 17:06:42,591 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:07:02,612 DEBUG Query successful
2024-10-25 17:07:02,613 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:07:07,668 DEBUG Query successful
2024-10-25 17:07:08,182 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:07:18,767 DEBUG Query successful
2024-10-25 17:07:18,841 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:07:38,601 DEBUG Query successful
2024-10-25 17:07:38,628 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:08:00,592 DEBUG Query successful
2024-10-25 17:08:00,595 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:08:05,240 DEBUG Query successful
2024-10-25 17:08:05,809 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:08:17,635 DEBUG Query successful
2024-10-25 17:08:17,712 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:08:34,770 DEBUG Query successful
2024-10-25 17:08:34,800 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:09:00,078 DEBUG Query successful
2024-10-25 17:09:00,079 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:09:04,157 DEBUG Query successful
2024-10-25 17:09:04,725 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:09:13,294 DEBUG Query successful
2024-10-25 17:09:13,370 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:09:29,698 DEBUG Query successful
2024-10-25 17:09:29,725 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:09:55,440 DEBUG Query successful
2024-10-25 17:09:55,442 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:09:59,779 DEBUG Query successful
2024-10-25 17:10:00,267 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:10:08,636 DEBUG Query successful
2024-10-25 17:10:08,713 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:10:26,928 DEBUG Query successful
2024-10-25 17:10:26,954 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:10:57,715 DEBUG Query successful
2024-10-25 17:10:57,716 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:11:01,462 DEBUG Query successful
2024-10-25 17:11:01,942 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:11:10,018 DEBUG Query successful
2024-10-25 17:11:10,093 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:11:27,443 DEBUG Query successful
2024-10-25 17:11:27,467 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:12:00,251 DEBUG Query successful
2024-10-25 17:12:00,252 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:12:06,114 DEBUG Query successful
2024-10-25 17:12:06,626 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:12:14,844 DEBUG Query successful
2024-10-25 17:12:14,919 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:12:33,518 DEBUG Query successful
2024-10-25 17:12:33,542 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:13:05,975 DEBUG Query successful
2024-10-25 17:13:05,977 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:13:11,202 DEBUG Query successful
2024-10-25 17:13:11,856 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:13:18,700 DEBUG Query successful
2024-10-25 17:13:18,777 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:13:38,594 DEBUG Query successful
2024-10-25 17:13:38,618 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:14:18,503 DEBUG Query successful
2024-10-25 17:14:18,504 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:14:22,893 DEBUG Query successful
2024-10-25 17:14:23,424 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:14:32,290 DEBUG Query successful
2024-10-25 17:14:32,369 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:14:52,656 DEBUG Query successful
2024-10-25 17:14:52,681 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:15:34,609 DEBUG Query successful
2024-10-25 17:15:34,610 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:15:38,975 DEBUG Query successful
2024-10-25 17:15:39,434 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:15:47,827 DEBUG Query successful
2024-10-25 17:15:47,904 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:16:13,743 DEBUG Query successful
2024-10-25 17:16:13,776 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:17:06,241 DEBUG Query successful
2024-10-25 17:17:06,243 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:17:10,591 DEBUG Query successful
2024-10-25 17:17:11,445 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:17:19,664 DEBUG Query successful
2024-10-25 17:17:19,744 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:17:46,033 DEBUG Query successful
2024-10-25 17:17:46,069 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:18:36,151 DEBUG Query successful
2024-10-25 17:18:36,153 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:18:41,812 DEBUG Query successful
2024-10-25 17:18:42,350 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:18:50,266 DEBUG Query successful
2024-10-25 17:18:50,348 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:19:14,554 DEBUG Query successful
2024-10-25 17:19:14,585 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:20:05,596 DEBUG Query successful
2024-10-25 17:20:05,598 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:20:10,798 DEBUG Query successful
2024-10-25 17:20:11,359 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:20:19,901 DEBUG Query successful
2024-10-25 17:20:19,983 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:20:46,779 DEBUG Query successful
2024-10-25 17:20:46,810 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:21:45,585 DEBUG Query successful
2024-10-25 17:21:45,586 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:21:50,609 DEBUG Query successful
2024-10-25 17:21:51,177 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:22:01,431 DEBUG Query successful
2024-10-25 17:22:01,508 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:22:29,417 DEBUG Query successful
2024-10-25 17:22:29,449 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:26:29,437 DEBUG Query successful
2024-10-25 17:26:29,440 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:26:35,097 DEBUG Query successful
2024-10-25 17:26:35,576 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:26:45,418 DEBUG Query successful
2024-10-25 17:26:45,494 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:27:12,356 DEBUG Query successful
2024-10-25 17:27:12,379 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:28:29,986 DEBUG Query successful
2024-10-25 17:28:29,988 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:28:35,264 DEBUG Query successful
2024-10-25 17:28:35,710 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:28:41,709 DEBUG Query successful
2024-10-25 17:28:41,785 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:29:11,211 DEBUG Query successful
2024-10-25 17:29:11,238 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:30:41,150 DEBUG Query successful
2024-10-25 17:30:41,152 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:30:47,997 DEBUG Query successful
2024-10-25 17:30:48,610 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:30:57,788 DEBUG Query successful
2024-10-25 17:30:57,861 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:31:26,887 DEBUG Query successful
2024-10-25 17:31:26,913 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:34:48,834 DEBUG Query successful
2024-10-25 17:34:48,839 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:34:54,955 DEBUG Query successful
2024-10-25 17:34:55,439 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:35:01,591 DEBUG Query successful
2024-10-25 17:35:01,665 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:35:33,367 DEBUG Query successful
2024-10-25 17:35:33,390 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:41:37,166 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 17:41:37,199 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:41:42,995 DEBUG Query successful
2024-10-25 17:41:43,461 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:41:51,714 DEBUG Query successful
2024-10-25 17:41:51,787 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:42:21,020 DEBUG Query successful
2024-10-25 17:42:21,042 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:49:55,076 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 17:49:55,104 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:50:01,246 DEBUG Query successful
2024-10-25 17:50:01,757 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:50:10,029 DEBUG Query successful
2024-10-25 17:50:10,103 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:50:42,484 DEBUG Query successful
2024-10-25 17:50:42,511 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:53:40,813 DEBUG Query successful
2024-10-25 17:53:40,814 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:53:52,311 DEBUG Query successful
2024-10-25 17:53:56,323 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:54:08,425 DEBUG Query successful
2024-10-25 17:54:08,504 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:54:42,589 DEBUG Query successful
2024-10-25 17:54:42,611 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:58:07,398 DEBUG Query successful
2024-10-25 17:58:07,400 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:58:14,472 DEBUG Query successful
2024-10-25 17:58:15,010 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:58:28,433 DEBUG Query successful
2024-10-25 17:58:28,510 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 17:59:05,841 DEBUG Query successful
2024-10-25 17:59:05,866 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:10:40,025 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 18:10:40,043 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:10:51,963 DEBUG Query successful
2024-10-25 18:10:52,441 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:11:08,354 DEBUG Query successful
2024-10-25 18:11:08,436 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:11:48,188 DEBUG Query successful
2024-10-25 18:11:48,217 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:26:50,491 DEBUG Query successful
2024-10-25 18:26:50,497 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:27:02,074 DEBUG Query successful
2024-10-25 18:27:02,685 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:27:20,676 DEBUG Query successful
2024-10-25 18:27:20,753 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:27:57,343 DEBUG Query successful
2024-10-25 18:27:57,372 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:45:31,712 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 18:45:31,744 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:45:46,213 DEBUG Query successful
2024-10-25 18:45:46,856 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:46:06,502 DEBUG Query successful
2024-10-25 18:46:06,584 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 18:46:47,339 DEBUG Query successful
2024-10-25 18:46:47,372 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 19:06:51,176 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 19:06:51,216 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 19:07:03,715 DEBUG Query successful
2024-10-25 19:07:04,246 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 19:07:27,356 DEBUG Query successful
2024-10-25 19:07:27,432 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 19:08:13,299 DEBUG Query successful
2024-10-25 19:08:13,326 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 19:31:17,566 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-25 19:31:17,599 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 19:31:33,858 DEBUG Query successful
2024-10-25 19:31:34,469 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-25 19:32:00,374 DEBUG Query successful
2024-10-25 19:32:00,450 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:52:47,201 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-26 22:52:47,226 DEBUG OpenAI client created
2024-10-26 22:52:47,226 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-26 22:52:47,226 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-26 22:52:47,245 DEBUG OpenAI client created
2024-10-26 22:52:47,245 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-26 22:53:03,451 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:53:14,525 DEBUG Query successful
2024-10-26 22:53:14,592 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:53:29,001 DEBUG Query successful
2024-10-26 22:53:29,020 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:53:36,679 DEBUG Query successful
2024-10-26 22:53:36,681 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:53:39,492 DEBUG Query successful
2024-10-26 22:53:39,946 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:53:48,357 DEBUG Query successful
2024-10-26 22:53:48,421 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:53:59,104 DEBUG Query successful
2024-10-26 22:53:59,122 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:54:18,944 DEBUG Query successful
2024-10-26 22:54:18,945 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:54:22,073 DEBUG Query successful
2024-10-26 22:54:22,448 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:54:36,575 DEBUG Query successful
2024-10-26 22:54:36,637 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:54:48,509 DEBUG Query successful
2024-10-26 22:54:48,529 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:55:03,303 DEBUG Query successful
2024-10-26 22:55:03,304 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:55:06,385 DEBUG Query successful
2024-10-26 22:55:06,882 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:55:18,616 DEBUG Query successful
2024-10-26 22:55:18,679 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:55:30,330 DEBUG Query successful
2024-10-26 22:55:30,350 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:55:45,975 DEBUG Query successful
2024-10-26 22:55:45,975 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:55:49,029 DEBUG Query successful
2024-10-26 22:55:49,462 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:55:55,468 DEBUG Query successful
2024-10-26 22:55:55,529 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:56:08,721 DEBUG Query successful
2024-10-26 22:56:08,740 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:56:26,957 DEBUG Query successful
2024-10-26 22:56:26,958 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:56:30,476 DEBUG Query successful
2024-10-26 22:56:30,878 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:56:38,463 DEBUG Query successful
2024-10-26 22:56:38,526 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:56:52,865 DEBUG Query successful
2024-10-26 22:56:52,895 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:57:12,077 DEBUG Query successful
2024-10-26 22:57:12,078 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:57:15,549 DEBUG Query successful
2024-10-26 22:57:15,937 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:57:26,416 DEBUG Query successful
2024-10-26 22:57:26,478 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:57:41,106 DEBUG Query successful
2024-10-26 22:57:41,126 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:58:01,388 DEBUG Query successful
2024-10-26 22:58:01,388 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:58:05,011 DEBUG Query successful
2024-10-26 22:58:05,440 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:58:12,203 DEBUG Query successful
2024-10-26 22:58:12,265 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:58:27,752 DEBUG Query successful
2024-10-26 22:58:27,771 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:58:57,106 DEBUG Query successful
2024-10-26 22:58:57,106 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:59:01,192 DEBUG Query successful
2024-10-26 22:59:01,601 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:59:09,917 DEBUG Query successful
2024-10-26 22:59:09,979 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:59:24,184 DEBUG Query successful
2024-10-26 22:59:24,203 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:59:51,307 DEBUG Query successful
2024-10-26 22:59:51,307 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 22:59:55,666 DEBUG Query successful
2024-10-26 22:59:56,053 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:00:02,812 DEBUG Query successful
2024-10-26 23:00:02,874 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:00:18,790 DEBUG Query successful
2024-10-26 23:00:18,810 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:00:52,730 DEBUG Query successful
2024-10-26 23:00:52,731 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:00:56,558 DEBUG Query successful
2024-10-26 23:00:56,920 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:01:04,415 DEBUG Query successful
2024-10-26 23:01:04,478 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:01:19,950 DEBUG Query successful
2024-10-26 23:01:19,969 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:01:57,160 DEBUG Query successful
2024-10-26 23:01:57,161 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:02:02,083 DEBUG Query successful
2024-10-26 23:02:02,471 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:02:12,450 DEBUG Query successful
2024-10-26 23:02:12,514 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:02:30,896 DEBUG Query successful
2024-10-26 23:02:30,914 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:03:05,471 DEBUG Query successful
2024-10-26 23:03:05,471 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:03:09,410 DEBUG Query successful
2024-10-26 23:03:09,825 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:03:16,850 DEBUG Query successful
2024-10-26 23:03:16,912 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:03:35,645 DEBUG Query successful
2024-10-26 23:03:35,663 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:04:11,227 DEBUG Query successful
2024-10-26 23:04:11,228 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:04:15,824 DEBUG Query successful
2024-10-26 23:04:16,278 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:04:23,395 DEBUG Query successful
2024-10-26 23:04:23,456 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:04:42,817 DEBUG Query successful
2024-10-26 23:04:42,836 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:05:22,585 DEBUG Query successful
2024-10-26 23:05:22,586 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:05:26,683 DEBUG Query successful
2024-10-26 23:05:27,064 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:05:33,191 DEBUG Query successful
2024-10-26 23:05:33,253 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:05:55,951 DEBUG Query successful
2024-10-26 23:05:55,971 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:06:40,720 DEBUG Query successful
2024-10-26 23:06:40,720 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:06:45,422 DEBUG Query successful
2024-10-26 23:06:45,848 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:06:51,988 DEBUG Query successful
2024-10-26 23:06:52,052 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:07:18,537 DEBUG Query successful
2024-10-26 23:07:18,558 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:08:03,752 DEBUG Query successful
2024-10-26 23:08:03,752 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:08:08,656 DEBUG Query successful
2024-10-26 23:08:09,061 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:08:16,810 DEBUG Query successful
2024-10-26 23:08:16,872 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:08:42,680 DEBUG Query successful
2024-10-26 23:08:42,699 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:09:32,018 DEBUG Query successful
2024-10-26 23:09:32,019 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:09:36,958 DEBUG Query successful
2024-10-26 23:09:37,333 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:09:44,366 DEBUG Query successful
2024-10-26 23:09:44,425 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:10:13,206 DEBUG Query successful
2024-10-26 23:10:13,226 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:11:07,262 DEBUG Query successful
2024-10-26 23:11:07,263 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:11:12,642 DEBUG Query successful
2024-10-26 23:11:13,000 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:11:19,623 DEBUG Query successful
2024-10-26 23:11:19,684 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:11:48,247 DEBUG Query successful
2024-10-26 23:11:48,266 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:12:40,483 DEBUG Query successful
2024-10-26 23:12:40,483 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:12:45,452 DEBUG Query successful
2024-10-26 23:12:45,809 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:12:52,260 DEBUG Query successful
2024-10-26 23:12:52,321 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:13:20,690 DEBUG Query successful
2024-10-26 23:13:20,709 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:15:17,508 DEBUG Query successful
2024-10-26 23:15:17,509 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:15:24,423 DEBUG Query successful
2024-10-26 23:15:24,781 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:15:34,547 DEBUG Query successful
2024-10-26 23:15:34,609 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:16:07,955 DEBUG Query successful
2024-10-26 23:16:07,978 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:17:04,593 DEBUG Query successful
2024-10-26 23:17:04,593 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:17:09,798 DEBUG Query successful
2024-10-26 23:17:10,185 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:17:16,908 DEBUG Query successful
2024-10-26 23:17:16,969 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:17:48,872 DEBUG Query successful
2024-10-26 23:17:48,894 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:19:46,656 DEBUG Query successful
2024-10-26 23:19:46,657 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:19:51,627 DEBUG Query successful
2024-10-26 23:19:52,025 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:20:05,647 DEBUG Query successful
2024-10-26 23:20:05,709 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:20:39,397 DEBUG Query successful
2024-10-26 23:20:39,413 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:22:38,626 DEBUG Query successful
2024-10-26 23:22:38,627 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:22:44,071 DEBUG Query successful
2024-10-26 23:22:44,444 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:22:58,641 DEBUG Query successful
2024-10-26 23:22:58,702 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:23:32,885 DEBUG Query successful
2024-10-26 23:23:32,904 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:24:32,866 DEBUG Query successful
2024-10-26 23:24:32,866 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:24:40,024 DEBUG Query successful
2024-10-26 23:24:40,398 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:24:47,155 DEBUG Query successful
2024-10-26 23:24:47,219 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:25:21,917 DEBUG Query successful
2024-10-26 23:25:21,939 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:28:24,390 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 72, in map_httpcore_exceptions
    yield
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 236, in handle_request
    resp = self._pool.handle_request(req)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 990, in _request
    response = self._client.send(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 926, in send
    response = self._send_handling_auth(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 954, in _send_handling_auth
    response = self._send_handling_redirects(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 991, in _send_handling_redirects
    response = self._send_single_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 1027, in _send_single_request
    response = transport.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 235, in handle_request
    with map_httpcore_exceptions():
  File "F:\ide\anaconda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 89, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\python_file\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 815, in create
    return self._post(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1277, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 954, in request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1024, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-26 23:28:24,417 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:28:31,521 DEBUG Query successful
2024-10-26 23:28:34,833 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:28:44,202 DEBUG Query successful
2024-10-26 23:28:44,271 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:34:13,086 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-26 23:34:13,107 DEBUG OpenAI client created
2024-10-26 23:34:13,107 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-26 23:34:13,107 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-26 23:34:13,128 DEBUG OpenAI client created
2024-10-26 23:34:13,128 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-26 23:34:28,053 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:34:35,059 DEBUG Query successful
2024-10-26 23:34:35,124 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:34:56,357 DEBUG Query successful
2024-10-26 23:34:56,377 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:35:06,604 DEBUG Query successful
2024-10-26 23:35:06,605 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:35:10,422 DEBUG Query successful
2024-10-26 23:35:10,874 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:35:19,666 DEBUG Query successful
2024-10-26 23:35:19,737 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:35:31,411 DEBUG Query successful
2024-10-26 23:35:31,436 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:35:43,199 DEBUG Query successful
2024-10-26 23:35:43,199 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:35:46,005 DEBUG Query successful
2024-10-26 23:35:46,426 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:35:52,861 DEBUG Query successful
2024-10-26 23:35:52,931 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:36:04,587 DEBUG Query successful
2024-10-26 23:36:04,614 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:36:17,112 DEBUG Query successful
2024-10-26 23:36:17,113 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:36:19,983 DEBUG Query successful
2024-10-26 23:36:20,428 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:36:27,149 DEBUG Query successful
2024-10-26 23:36:27,213 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:36:40,535 DEBUG Query successful
2024-10-26 23:36:40,555 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:36:56,508 DEBUG Query successful
2024-10-26 23:36:56,509 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:36:59,875 DEBUG Query successful
2024-10-26 23:37:00,336 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:37:09,154 DEBUG Query successful
2024-10-26 23:37:09,216 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:37:22,308 DEBUG Query successful
2024-10-26 23:37:22,327 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:37:39,437 DEBUG Query successful
2024-10-26 23:37:39,438 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:37:44,768 DEBUG Query successful
2024-10-26 23:37:45,219 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:37:53,463 DEBUG Query successful
2024-10-26 23:37:53,528 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:38:12,231 DEBUG Query successful
2024-10-26 23:38:12,248 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:38:34,369 DEBUG Query successful
2024-10-26 23:38:34,369 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:38:38,905 DEBUG Query successful
2024-10-26 23:38:39,306 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:38:49,360 DEBUG Query successful
2024-10-26 23:38:49,424 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:39:07,487 DEBUG Query successful
2024-10-26 23:39:07,508 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:39:29,968 DEBUG Query successful
2024-10-26 23:39:29,969 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:39:34,126 DEBUG Query successful
2024-10-26 23:39:34,547 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:39:44,558 DEBUG Query successful
2024-10-26 23:39:44,621 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:40:03,999 DEBUG Query successful
2024-10-26 23:40:04,016 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:40:26,389 DEBUG Query successful
2024-10-26 23:40:26,389 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:40:31,992 DEBUG Query successful
2024-10-26 23:40:32,361 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:40:41,414 DEBUG Query successful
2024-10-26 23:40:41,480 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:40:56,382 DEBUG Query successful
2024-10-26 23:40:56,404 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:41:22,550 DEBUG Query successful
2024-10-26 23:41:22,551 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:41:26,072 DEBUG Query successful
2024-10-26 23:41:26,434 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:41:39,348 DEBUG Query successful
2024-10-26 23:41:39,411 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:41:55,286 DEBUG Query successful
2024-10-26 23:41:55,308 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:42:23,064 DEBUG Query successful
2024-10-26 23:42:23,065 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:42:27,398 DEBUG Query successful
2024-10-26 23:42:27,805 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:42:36,299 DEBUG Query successful
2024-10-26 23:42:36,370 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:42:55,634 DEBUG Query successful
2024-10-26 23:42:55,655 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:43:25,938 DEBUG Query successful
2024-10-26 23:43:25,939 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:43:29,708 DEBUG Query successful
2024-10-26 23:43:30,072 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:43:36,980 DEBUG Query successful
2024-10-26 23:43:37,050 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:43:54,929 DEBUG Query successful
2024-10-26 23:43:54,947 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:44:29,755 DEBUG Query successful
2024-10-26 23:44:29,756 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:44:34,309 DEBUG Query successful
2024-10-26 23:44:34,678 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:44:41,112 DEBUG Query successful
2024-10-26 23:44:41,177 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:44:57,409 DEBUG Query successful
2024-10-26 23:44:57,428 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:45:33,288 DEBUG Query successful
2024-10-26 23:45:33,290 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:45:37,356 DEBUG Query successful
2024-10-26 23:45:37,735 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:45:46,260 DEBUG Query successful
2024-10-26 23:45:46,326 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:46:04,064 DEBUG Query successful
2024-10-26 23:46:04,083 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:46:39,069 DEBUG Query successful
2024-10-26 23:46:39,070 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:46:42,855 DEBUG Query successful
2024-10-26 23:46:43,215 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:46:49,739 DEBUG Query successful
2024-10-26 23:46:49,802 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:47:12,781 DEBUG Query successful
2024-10-26 23:47:12,799 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:47:52,993 DEBUG Query successful
2024-10-26 23:47:52,994 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:47:57,295 DEBUG Query successful
2024-10-26 23:47:57,670 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:48:12,696 DEBUG Query successful
2024-10-26 23:48:12,760 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:48:40,762 DEBUG Query successful
2024-10-26 23:48:40,781 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:49:22,444 DEBUG Query successful
2024-10-26 23:49:22,445 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:49:26,302 DEBUG Query successful
2024-10-26 23:49:26,669 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:49:34,967 DEBUG Query successful
2024-10-26 23:49:35,033 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:49:54,664 DEBUG Query successful
2024-10-26 23:49:54,683 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:50:43,109 DEBUG Query successful
2024-10-26 23:50:43,110 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:50:47,406 DEBUG Query successful
2024-10-26 23:50:47,872 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:50:54,324 DEBUG Query successful
2024-10-26 23:50:54,393 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:51:14,128 DEBUG Query successful
2024-10-26 23:51:14,147 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:52:02,294 DEBUG Query successful
2024-10-26 23:52:02,295 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:52:06,617 DEBUG Query successful
2024-10-26 23:52:06,989 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:52:13,777 DEBUG Query successful
2024-10-26 23:52:13,843 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:52:36,991 DEBUG Query successful
2024-10-26 23:52:37,011 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:53:26,083 DEBUG Query successful
2024-10-26 23:53:26,084 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:53:30,270 DEBUG Query successful
2024-10-26 23:53:30,636 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:53:37,087 DEBUG Query successful
2024-10-26 23:53:37,154 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:53:59,635 DEBUG Query successful
2024-10-26 23:53:59,658 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:54:49,755 DEBUG Query successful
2024-10-26 23:54:49,756 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:54:53,913 DEBUG Query successful
2024-10-26 23:54:54,285 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:55:01,345 DEBUG Query successful
2024-10-26 23:55:01,411 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:55:23,333 DEBUG Query successful
2024-10-26 23:55:23,351 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:56:20,123 DEBUG Query successful
2024-10-26 23:56:20,125 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:56:24,842 DEBUG Query successful
2024-10-26 23:56:25,218 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:56:32,978 DEBUG Query successful
2024-10-26 23:56:33,045 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:56:49,871 DEBUG Query successful
2024-10-26 23:56:49,892 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:57:50,072 DEBUG Query successful
2024-10-26 23:57:50,072 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:57:54,003 DEBUG Query successful
2024-10-26 23:57:54,403 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:58:01,838 DEBUG Query successful
2024-10-26 23:58:01,917 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-26 23:58:19,296 DEBUG Query successful
2024-10-26 23:58:19,313 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:01:21,753 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 72, in map_httpcore_exceptions
    yield
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 236, in handle_request
    resp = self._pool.handle_request(req)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 990, in _request
    response = self._client.send(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 926, in send
    response = self._send_handling_auth(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 954, in _send_handling_auth
    response = self._send_handling_redirects(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 991, in _send_handling_redirects
    response = self._send_single_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 1027, in _send_single_request
    response = transport.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 235, in handle_request
    with map_httpcore_exceptions():
  File "F:\ide\anaconda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 89, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\python_file\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 815, in create
    return self._post(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1277, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 954, in request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1024, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-27 00:01:21,771 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:01:26,846 DEBUG Query successful
2024-10-27 00:01:27,250 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:01:34,442 DEBUG Query successful
2024-10-27 00:01:34,508 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:01:51,289 DEBUG Query successful
2024-10-27 00:01:51,309 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:02:48,252 DEBUG Query successful
2024-10-27 00:02:48,252 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:02:52,279 DEBUG Query successful
2024-10-27 00:02:52,643 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:02:59,025 DEBUG Query successful
2024-10-27 00:02:59,091 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:03:16,724 DEBUG Query successful
2024-10-27 00:03:16,748 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:05:16,981 DEBUG Query successful
2024-10-27 00:05:16,982 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:05:32,121 DEBUG Query successful
2024-10-27 00:05:32,494 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:05:43,814 DEBUG Query successful
2024-10-27 00:05:43,881 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:06:02,821 DEBUG Query successful
2024-10-27 00:06:02,847 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:09:05,435 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 72, in map_httpcore_exceptions
    yield
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 236, in handle_request
    resp = self._pool.handle_request(req)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 990, in _request
    response = self._client.send(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 926, in send
    response = self._send_handling_auth(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 954, in _send_handling_auth
    response = self._send_handling_redirects(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 991, in _send_handling_redirects
    response = self._send_single_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 1027, in _send_single_request
    response = transport.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 235, in handle_request
    with map_httpcore_exceptions():
  File "F:\ide\anaconda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 89, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\python_file\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 815, in create
    return self._post(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1277, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 954, in request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1024, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-27 00:09:05,460 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:09:10,156 DEBUG Query successful
2024-10-27 00:09:10,541 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:09:22,308 DEBUG Query successful
2024-10-27 00:09:22,374 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:09:40,858 DEBUG Query successful
2024-10-27 00:09:40,876 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:12:43,348 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 72, in map_httpcore_exceptions
    yield
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 236, in handle_request
    resp = self._pool.handle_request(req)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http_proxy.py", line 344, in handle_request
    return self._connection.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 143, in handle_request
    raise exc
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 113, in handle_request
    ) = self._receive_response_headers(**kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 186, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpcore\_sync\http11.py", line 238, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 990, in _request
    response = self._client.send(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 926, in send
    response = self._send_handling_auth(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 954, in _send_handling_auth
    response = self._send_handling_redirects(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 991, in _send_handling_redirects
    response = self._send_single_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_client.py", line 1027, in _send_single_request
    response = transport.handle_request(request)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 235, in handle_request
    with map_httpcore_exceptions():
  File "F:\ide\anaconda\envs\vlm_sc2\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\httpx\_transports\default.py", line 89, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\python_file\vlm_attention_starcraft2\vlm_attention\utils\call_vlm.py", line 109, in query
    response = self.client.chat.completions.create(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\resources\chat\completions.py", line 815, in create
    return self._post(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1277, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 954, in request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1014, in _request
    return self._retry_request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1092, in _retry_request
    return self._request(
  File "F:\ide\anaconda\envs\vlm_sc2\lib\site-packages\openai\_base_client.py", line 1024, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-10-27 00:12:43,366 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:12:48,290 DEBUG Query successful
2024-10-27 00:12:48,632 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 00:12:56,392 DEBUG Query successful
2024-10-27 00:12:56,454 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:13:38,176 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:38,215 DEBUG OpenAI client created
2024-10-27 13:13:38,215 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:38,215 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:38,233 DEBUG OpenAI client created
2024-10-27 13:13:38,233 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:40,263 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:40,284 DEBUG OpenAI client created
2024-10-27 13:13:40,284 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:40,284 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:40,303 DEBUG OpenAI client created
2024-10-27 13:13:40,303 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:42,571 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:42,593 DEBUG OpenAI client created
2024-10-27 13:13:42,593 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:42,593 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:42,613 DEBUG OpenAI client created
2024-10-27 13:13:42,613 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:44,683 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:44,705 DEBUG OpenAI client created
2024-10-27 13:13:44,705 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:44,705 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:13:44,725 DEBUG OpenAI client created
2024-10-27 13:13:44,725 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:13:55,127 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:13:56,010 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:00,472 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:01,274 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:02,974 DEBUG Query successful
2024-10-27 13:14:03,043 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:03,512 DEBUG Query successful
2024-10-27 13:14:03,581 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:09,183 DEBUG Query successful
2024-10-27 13:14:09,250 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:09,929 DEBUG Query successful
2024-10-27 13:14:09,954 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:11,300 DEBUG Query successful
2024-10-27 13:14:11,320 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:15,477 DEBUG Query successful
2024-10-27 13:14:15,546 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:17,954 DEBUG Query successful
2024-10-27 13:14:17,974 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:18,217 DEBUG Query successful
2024-10-27 13:14:18,219 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:19,299 DEBUG Query successful
2024-10-27 13:14:19,300 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:20,583 DEBUG Query successful
2024-10-27 13:14:21,035 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:21,374 DEBUG Query successful
2024-10-27 13:14:21,438 DEBUG Query successful
2024-10-27 13:14:21,460 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:21,837 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:27,452 DEBUG Query successful
2024-10-27 13:14:27,453 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:28,156 DEBUG Query successful
2024-10-27 13:14:28,228 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:29,805 DEBUG Query successful
2024-10-27 13:14:29,999 DEBUG Query successful
2024-10-27 13:14:30,067 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:30,282 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:31,121 DEBUG Query successful
2024-10-27 13:14:31,122 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:33,270 DEBUG Query successful
2024-10-27 13:14:33,741 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:37,569 DEBUG Query successful
2024-10-27 13:14:37,589 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:37,949 DEBUG Query successful
2024-10-27 13:14:37,990 DEBUG Query successful
2024-10-27 13:14:38,013 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:38,015 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:40,160 DEBUG Query successful
2024-10-27 13:14:40,225 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:46,487 DEBUG Query successful
2024-10-27 13:14:46,510 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:48,123 DEBUG Query successful
2024-10-27 13:14:48,151 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:48,652 DEBUG Query successful
2024-10-27 13:14:48,653 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:51,123 DEBUG Query successful
2024-10-27 13:14:51,544 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:55,743 DEBUG Query successful
2024-10-27 13:14:55,744 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:58,139 DEBUG Query successful
2024-10-27 13:14:58,536 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:58,637 DEBUG Query successful
2024-10-27 13:14:58,638 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:14:59,188 DEBUG Query successful
2024-10-27 13:14:59,256 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:08,817 DEBUG Query successful
2024-10-27 13:15:09,017 DEBUG Query successful
2024-10-27 13:15:09,017 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:09,235 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:11,439 DEBUG Query successful
2024-10-27 13:15:11,835 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:11,936 DEBUG Query successful
2024-10-27 13:15:12,007 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:12,090 DEBUG Query successful
2024-10-27 13:15:12,111 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:14,935 DEBUG Query successful
2024-10-27 13:15:14,999 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:18,724 DEBUG Query successful
2024-10-27 13:15:18,791 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:20,596 DEBUG Query successful
2024-10-27 13:15:20,620 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:23,448 DEBUG Query successful
2024-10-27 13:15:23,466 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:24,055 DEBUG Query successful
2024-10-27 13:15:24,056 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:26,836 DEBUG Query successful
2024-10-27 13:15:27,225 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:28,103 DEBUG Query successful
2024-10-27 13:15:28,125 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:32,921 DEBUG Query successful
2024-10-27 13:15:32,922 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:35,356 DEBUG Query successful
2024-10-27 13:15:35,785 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:35,819 DEBUG Query successful
2024-10-27 13:15:35,890 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:36,860 DEBUG Query successful
2024-10-27 13:15:36,861 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:39,262 DEBUG Query successful
2024-10-27 13:15:39,657 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:41,344 DEBUG Query successful
2024-10-27 13:15:41,345 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:42,657 DEBUG Query successful
2024-10-27 13:15:42,729 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:44,147 DEBUG Query successful
2024-10-27 13:15:44,592 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:46,775 DEBUG Query successful
2024-10-27 13:15:46,797 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:48,811 DEBUG Query successful
2024-10-27 13:15:48,876 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:51,724 DEBUG Query successful
2024-10-27 13:15:51,791 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:52,592 DEBUG Query successful
2024-10-27 13:15:52,613 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:15:58,440 DEBUG Query successful
2024-10-27 13:15:58,462 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:02,868 DEBUG Query successful
2024-10-27 13:16:02,870 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:05,747 DEBUG Query successful
2024-10-27 13:16:06,176 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:07,254 DEBUG Query successful
2024-10-27 13:16:07,254 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:09,914 DEBUG Query successful
2024-10-27 13:16:10,366 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:10,879 DEBUG Query successful
2024-10-27 13:16:10,901 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:13,241 DEBUG Query successful
2024-10-27 13:16:13,311 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:13,664 DEBUG Query successful
2024-10-27 13:16:13,665 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:16,242 DEBUG Query successful
2024-10-27 13:16:16,694 DEBUG Query successful
2024-10-27 13:16:16,725 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:16,762 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:22,994 DEBUG Query successful
2024-10-27 13:16:23,060 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:27,406 DEBUG Query successful
2024-10-27 13:16:27,434 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:27,878 DEBUG Query successful
2024-10-27 13:16:27,880 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:30,075 DEBUG Query successful
2024-10-27 13:16:30,095 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:30,766 DEBUG Query successful
2024-10-27 13:16:31,276 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:34,627 DEBUG Query successful
2024-10-27 13:16:34,651 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:38,025 DEBUG Query successful
2024-10-27 13:16:38,089 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:46,200 DEBUG Query successful
2024-10-27 13:16:46,202 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:47,098 DEBUG Query successful
2024-10-27 13:16:47,099 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:49,263 DEBUG Query successful
2024-10-27 13:16:49,616 DEBUG Query successful
2024-10-27 13:16:49,815 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:50,150 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:50,542 DEBUG Query successful
2024-10-27 13:16:50,561 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:51,064 DEBUG Query successful
2024-10-27 13:16:51,065 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:54,235 DEBUG Query successful
2024-10-27 13:16:54,743 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:58,021 DEBUG Query successful
2024-10-27 13:16:58,086 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:16:58,196 DEBUG Query successful
2024-10-27 13:16:58,265 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:00,750 DEBUG Query successful
2024-10-27 13:17:00,815 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:04,978 DEBUG Query successful
2024-10-27 13:17:04,980 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:08,389 DEBUG Query successful
2024-10-27 13:17:08,937 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:10,255 DEBUG Query successful
2024-10-27 13:17:10,273 DEBUG Query successful
2024-10-27 13:17:10,283 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:10,293 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:13,566 DEBUG Query successful
2024-10-27 13:17:13,589 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:19,492 DEBUG Query successful
2024-10-27 13:17:19,557 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:32,372 DEBUG Query successful
2024-10-27 13:17:32,373 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:32,755 DEBUG Query successful
2024-10-27 13:17:32,757 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:33,312 DEBUG Query successful
2024-10-27 13:17:33,339 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:35,217 DEBUG Query successful
2024-10-27 13:17:35,627 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:35,636 DEBUG Query successful
2024-10-27 13:17:36,052 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:36,641 DEBUG Query successful
2024-10-27 13:17:36,642 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:39,710 DEBUG Query successful
2024-10-27 13:17:40,139 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:42,668 DEBUG Query successful
2024-10-27 13:17:42,737 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:42,872 DEBUG Query successful
2024-10-27 13:17:42,940 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:45,877 DEBUG Query successful
2024-10-27 13:17:45,948 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:49,043 DEBUG Query successful
2024-10-27 13:17:49,045 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:56,264 DEBUG Query successful
2024-10-27 13:17:56,287 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:56,465 DEBUG Query successful
2024-10-27 13:17:56,488 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:17:59,262 DEBUG Query successful
2024-10-27 13:17:59,284 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:00,918 DEBUG Query successful
2024-10-27 13:18:01,325 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:10,478 DEBUG Query successful
2024-10-27 13:18:10,541 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:20,929 DEBUG Query successful
2024-10-27 13:18:20,930 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:24,224 DEBUG Query successful
2024-10-27 13:18:24,245 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:24,362 DEBUG Query successful
2024-10-27 13:18:24,835 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:25,190 DEBUG Query successful
2024-10-27 13:18:25,190 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:26,000 DEBUG Query successful
2024-10-27 13:18:26,002 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:28,707 DEBUG Query successful
2024-10-27 13:18:29,201 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:29,385 DEBUG Query successful
2024-10-27 13:18:29,812 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:31,546 DEBUG Query successful
2024-10-27 13:18:31,614 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:36,324 DEBUG Query successful
2024-10-27 13:18:36,391 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:37,385 DEBUG Query successful
2024-10-27 13:18:37,451 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:45,796 DEBUG Query successful
2024-10-27 13:18:45,797 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:46,083 DEBUG Query successful
2024-10-27 13:18:46,105 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:49,145 DEBUG Query successful
2024-10-27 13:18:49,596 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:50,569 DEBUG Query successful
2024-10-27 13:18:50,592 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:51,912 DEBUG Query successful
2024-10-27 13:18:51,938 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:18:56,071 DEBUG Query successful
2024-10-27 13:18:56,137 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:12,906 DEBUG Query successful
2024-10-27 13:19:12,925 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:13,093 DEBUG Query successful
2024-10-27 13:19:13,093 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:14,922 DEBUG Query successful
2024-10-27 13:19:14,922 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:16,714 DEBUG Query successful
2024-10-27 13:19:17,183 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:18,435 DEBUG Query successful
2024-10-27 13:19:18,818 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:19,767 DEBUG Query successful
2024-10-27 13:19:19,767 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:22,698 DEBUG Query successful
2024-10-27 13:19:23,127 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:25,361 DEBUG Query successful
2024-10-27 13:19:25,422 DEBUG Query successful
2024-10-27 13:19:25,428 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:25,491 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:29,586 DEBUG Query successful
2024-10-27 13:19:29,653 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:40,727 DEBUG Query successful
2024-10-27 13:19:40,745 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:43,801 DEBUG Query successful
2024-10-27 13:19:43,802 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:45,867 DEBUG Query successful
2024-10-27 13:19:45,891 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:47,510 DEBUG Query successful
2024-10-27 13:19:47,869 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:19:47,992 DEBUG Query successful
2024-10-27 13:19:48,017 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:04,325 DEBUG Query successful
2024-10-27 13:20:04,326 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:07,823 DEBUG Query successful
2024-10-27 13:20:08,225 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:09,644 DEBUG Query successful
2024-10-27 13:20:09,707 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:14,120 DEBUG Query successful
2024-10-27 13:20:14,121 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:14,906 DEBUG Query successful
2024-10-27 13:20:14,972 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:17,381 DEBUG Query successful
2024-10-27 13:20:17,813 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:19,198 DEBUG Query successful
2024-10-27 13:20:19,200 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:22,755 DEBUG Query successful
2024-10-27 13:20:23,163 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:23,638 DEBUG Query successful
2024-10-27 13:20:23,705 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:27,592 DEBUG Query successful
2024-10-27 13:20:27,613 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:30,245 DEBUG Query successful
2024-10-27 13:20:30,315 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:40,086 DEBUG Query successful
2024-10-27 13:20:40,106 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:41,053 DEBUG Query successful
2024-10-27 13:20:41,073 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:48,364 DEBUG Query successful
2024-10-27 13:20:48,384 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:51,726 DEBUG Query successful
2024-10-27 13:20:51,727 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:20:56,632 DEBUG Query successful
2024-10-27 13:20:57,027 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:03,386 DEBUG Query successful
2024-10-27 13:21:03,450 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:10,044 DEBUG Query successful
2024-10-27 13:21:10,045 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:13,867 DEBUG Query successful
2024-10-27 13:21:14,265 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:14,676 DEBUG Query successful
2024-10-27 13:21:14,677 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:18,170 DEBUG Query successful
2024-10-27 13:21:18,604 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:20,033 DEBUG Query successful
2024-10-27 13:21:20,034 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:20,886 DEBUG Query successful
2024-10-27 13:21:20,955 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:22,576 DEBUG Query successful
2024-10-27 13:21:22,599 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:23,728 DEBUG Query successful
2024-10-27 13:21:24,149 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:24,521 DEBUG Query successful
2024-10-27 13:21:24,586 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:31,309 DEBUG Query successful
2024-10-27 13:21:31,373 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:37,702 DEBUG Query successful
2024-10-27 13:21:37,723 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:40,829 DEBUG Query successful
2024-10-27 13:21:40,849 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:46,722 DEBUG Query successful
2024-10-27 13:21:46,742 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:53,725 DEBUG Query successful
2024-10-27 13:21:53,727 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:21:58,104 DEBUG Query successful
2024-10-27 13:21:58,573 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:05,787 DEBUG Query successful
2024-10-27 13:22:05,852 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:09,380 DEBUG Query successful
2024-10-27 13:22:09,381 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:12,941 DEBUG Query successful
2024-10-27 13:22:13,360 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:15,400 DEBUG Query successful
2024-10-27 13:22:15,400 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:19,284 DEBUG Query successful
2024-10-27 13:22:19,378 DEBUG Query successful
2024-10-27 13:22:19,379 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:19,701 DEBUG Query successful
2024-10-27 13:22:19,709 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:19,766 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:22,447 DEBUG Query successful
2024-10-27 13:22:22,827 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:23,593 DEBUG Query successful
2024-10-27 13:22:23,613 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:26,295 DEBUG Query successful
2024-10-27 13:22:26,367 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:37,039 DEBUG Query successful
2024-10-27 13:22:37,061 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:42,672 DEBUG Query successful
2024-10-27 13:22:42,743 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:43,241 DEBUG Query successful
2024-10-27 13:22:43,267 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:22:58,607 DEBUG Query successful
2024-10-27 13:22:58,644 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:10,644 DEBUG Query successful
2024-10-27 13:23:10,645 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:14,108 DEBUG Query successful
2024-10-27 13:23:14,571 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:16,796 DEBUG Query successful
2024-10-27 13:23:16,797 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:18,596 DEBUG Query successful
2024-10-27 13:23:18,597 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:20,268 DEBUG Query successful
2024-10-27 13:23:20,347 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:20,467 DEBUG Query successful
2024-10-27 13:23:20,907 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:22,783 DEBUG Query successful
2024-10-27 13:23:23,244 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:27,487 DEBUG Query successful
2024-10-27 13:23:27,555 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:28,853 DEBUG Query successful
2024-10-27 13:23:28,853 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:29,305 DEBUG Query successful
2024-10-27 13:23:29,381 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:31,883 DEBUG Query successful
2024-10-27 13:23:32,336 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:37,694 DEBUG Query successful
2024-10-27 13:23:37,720 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:38,886 DEBUG Query successful
2024-10-27 13:23:38,954 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:47,316 DEBUG Query successful
2024-10-27 13:23:47,339 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:47,499 DEBUG Query successful
2024-10-27 13:23:47,521 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:23:57,348 DEBUG Query successful
2024-10-27 13:23:57,367 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:24:09,049 DEBUG Query successful
2024-10-27 13:24:09,050 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:50:57,113 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:50:57,133 DEBUG OpenAI client created
2024-10-27 13:50:57,133 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:50:57,133 DEBUG Proxy set to: http://127.0.0.1:7890
2024-10-27 13:50:57,153 DEBUG OpenAI client created
2024-10-27 13:50:57,153 DEBUG Model set to: gpt-4o-mini-2024-07-18
2024-10-27 13:51:11,577 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:51:19,124 DEBUG Query successful
2024-10-27 13:51:19,188 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:51:26,547 DEBUG Query successful
2024-10-27 13:51:26,567 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:51:34,612 DEBUG Query successful
2024-10-27 13:51:34,613 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:51:37,303 DEBUG Query successful
2024-10-27 13:51:37,867 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:51:45,209 DEBUG Query successful
2024-10-27 13:51:45,271 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:51:53,273 DEBUG Query successful
2024-10-27 13:51:53,294 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:52:03,313 DEBUG Query successful
2024-10-27 13:52:03,314 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:52:14,083 DEBUG Query successful
2024-10-27 13:52:14,450 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:52:21,019 DEBUG Query successful
2024-10-27 13:52:21,083 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:52:30,725 DEBUG Query successful
2024-10-27 13:52:30,745 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:52:44,134 DEBUG Query successful
2024-10-27 13:52:44,135 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:52:58,084 DEBUG Query successful
2024-10-27 13:52:58,534 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:53:05,450 DEBUG Query successful
2024-10-27 13:53:05,515 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:53:16,843 DEBUG Query successful
2024-10-27 13:53:16,866 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:53:33,442 DEBUG Query successful
2024-10-27 13:53:33,444 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:53:36,060 DEBUG Query successful
2024-10-27 13:53:36,489 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:53:44,752 DEBUG Query successful
2024-10-27 13:53:44,817 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:53:57,949 DEBUG Query successful
2024-10-27 13:53:57,967 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:54:16,844 DEBUG Query successful
2024-10-27 13:54:16,845 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:54:20,162 DEBUG Query successful
2024-10-27 13:54:20,523 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:54:28,847 DEBUG Query successful
2024-10-27 13:54:28,908 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:54:42,635 DEBUG Query successful
2024-10-27 13:54:42,654 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:55:01,018 DEBUG Query successful
2024-10-27 13:55:01,019 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:55:03,903 DEBUG Query successful
2024-10-27 13:55:04,291 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:55:10,645 DEBUG Query successful
2024-10-27 13:55:10,706 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:55:25,708 DEBUG Query successful
2024-10-27 13:55:25,729 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:55:45,470 DEBUG Query successful
2024-10-27 13:55:45,471 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:55:49,435 DEBUG Query successful
2024-10-27 13:55:49,774 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:55:56,362 DEBUG Query successful
2024-10-27 13:55:56,423 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:56:12,310 DEBUG Query successful
2024-10-27 13:56:12,329 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:56:44,280 DEBUG Query successful
2024-10-27 13:56:44,281 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:56:48,956 DEBUG Query successful
2024-10-27 13:56:49,296 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:57:02,800 DEBUG Query successful
2024-10-27 13:57:02,862 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:57:19,546 DEBUG Query successful
2024-10-27 13:57:19,566 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:57:44,447 DEBUG Query successful
2024-10-27 13:57:44,449 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:57:47,606 DEBUG Query successful
2024-10-27 13:57:47,931 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:57:54,766 DEBUG Query successful
2024-10-27 13:57:54,827 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:58:13,591 DEBUG Query successful
2024-10-27 13:58:13,611 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:58:47,843 DEBUG Query successful
2024-10-27 13:58:47,845 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:58:51,974 DEBUG Query successful
2024-10-27 13:58:52,313 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:59:03,496 DEBUG Query successful
2024-10-27 13:59:03,558 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:59:23,029 DEBUG Query successful
2024-10-27 13:59:23,048 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:59:55,827 DEBUG Query successful
2024-10-27 13:59:55,828 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 13:59:59,363 DEBUG Query successful
2024-10-27 13:59:59,716 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:00:06,166 DEBUG Query successful
2024-10-27 14:00:06,228 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:00:25,451 DEBUG Query successful
2024-10-27 14:00:25,471 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:00:59,662 DEBUG Query successful
2024-10-27 14:00:59,662 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:01:03,621 DEBUG Query successful
2024-10-27 14:01:03,971 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:01:10,419 DEBUG Query successful
2024-10-27 14:01:10,479 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:01:30,378 DEBUG Query successful
2024-10-27 14:01:30,398 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:02:08,849 DEBUG Query successful
2024-10-27 14:02:08,851 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:02:12,409 DEBUG Query successful
2024-10-27 14:02:12,753 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:02:29,755 DEBUG Query successful
2024-10-27 14:02:29,816 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:02:51,076 DEBUG Query successful
2024-10-27 14:02:51,094 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:03:26,336 DEBUG Query successful
2024-10-27 14:03:26,337 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:03:29,761 DEBUG Query successful
2024-10-27 14:03:30,111 DEBUG Querying model: gpt-4o-mini-2024-07-18
2024-10-27 14:03:38,610 DEBUG Query successful
2024-10-27 14:03:38,673 DEBUG Querying model: gpt-4o-mini-2024-07-18
