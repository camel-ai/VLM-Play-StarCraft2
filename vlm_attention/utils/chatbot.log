2024-08-11 11:14:33,059 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-11 11:14:33,084 DEBUG OpenAI client created
2024-08-11 11:14:33,084 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-11 11:14:33,107 DEBUG OpenAI client created
2024-08-11 11:14:33,107 DEBUG Querying model: gpt-4o
2024-08-11 11:14:33,922 DEBUG Query successful
2024-08-11 11:14:33,922 ERROR An error occurred: [Errno 2] No such file or directory: 'path_to_your_image.jpg'
Traceback (most recent call last):
  File "C:\python_code\vlm_attention_starcraft2-main\vlm_attention\utils\test_for_gpt4o.py", line 59, in query_with_image
    base64_image = self.encode_image(image_path)
  File "C:\python_code\vlm_attention_starcraft2-main\vlm_attention\utils\test_for_gpt4o.py", line 88, in encode_image
    with open(image_path, "rb") as image_file:
FileNotFoundError: [Errno 2] No such file or directory: 'path_to_your_image.jpg'
2024-08-11 11:14:48,424 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-11 11:14:48,449 DEBUG OpenAI client created
2024-08-11 11:14:48,449 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-11 11:14:48,471 DEBUG OpenAI client created
2024-08-11 11:14:48,471 DEBUG Querying model: gpt-4o
2024-08-11 11:14:50,426 DEBUG Query successful
2024-08-11 11:14:50,435 DEBUG Querying model with image: gpt-4o
2024-08-11 11:14:57,491 DEBUG Query with image successful
2024-08-11 12:02:19,347 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-11 12:02:19,378 DEBUG OpenAI client created
2024-08-11 12:02:19,378 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-11 12:02:19,404 DEBUG OpenAI client created
2024-08-11 12:02:19,404 DEBUG Querying model: gpt-4o
2024-08-11 12:02:24,248 DEBUG Query successful
2024-08-11 12:02:24,258 DEBUG Querying model with image: gpt-4o
2024-08-11 12:02:33,443 DEBUG Query with image successful
2024-08-11 20:53:34,411 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-11 20:53:34,448 DEBUG OpenAI client created
2024-08-11 20:53:34,449 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-11 20:53:34,476 DEBUG OpenAI client created
2024-08-11 20:53:34,476 DEBUG Querying model: gpt-4o
2024-08-11 20:53:35,345 DEBUG Query successful
2024-08-11 20:53:35,355 DEBUG Querying model with image: gpt-4o
2024-08-11 20:53:43,420 DEBUG Query with image successful
2024-08-11 21:27:33,671 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-11 21:27:33,697 DEBUG OpenAI client created
2024-08-11 21:27:33,697 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-11 21:27:33,721 DEBUG OpenAI client created
2024-08-11 21:27:33,721 DEBUG Querying model: gpt-4o
2024-08-11 21:27:34,830 DEBUG Query successful
2024-08-11 21:27:34,840 DEBUG Querying model with image: gpt-4o
2024-08-11 21:27:42,494 DEBUG Query with image successful
2024-08-11 21:52:16,443 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-11 21:52:16,469 DEBUG OpenAI client created
2024-08-11 21:52:16,470 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-11 21:52:16,493 DEBUG OpenAI client created
2024-08-11 21:52:16,493 DEBUG Querying model: gpt-4o
2024-08-11 21:52:17,400 DEBUG Query successful
2024-08-11 21:52:17,409 DEBUG Querying model with image: gpt-4o
2024-08-11 21:52:26,772 DEBUG Query with image successful
2024-08-26 16:13:29,972 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:13:30,008 DEBUG OpenAI client created
2024-08-26 16:13:30,008 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:13:30,040 DEBUG OpenAI client created
2024-08-26 16:13:30,040 DEBUG Querying model: gpt-4o
2024-08-26 16:13:31,298 DEBUG Query successful
2024-08-26 16:13:31,322 DEBUG Querying model with image: gpt-4o
2024-08-26 16:13:41,677 DEBUG Query with image successful
2024-08-26 16:15:12,374 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:15:12,411 DEBUG OpenAI client created
2024-08-26 16:15:12,411 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:15:12,446 DEBUG OpenAI client created
2024-08-26 16:15:12,446 DEBUG Querying model: gpt-4
2024-08-26 16:15:13,643 DEBUG Query successful
2024-08-26 16:15:13,654 DEBUG Querying model with image: gpt-4
2024-08-26 16:15:16,184 ERROR An error occurred: Error code: 400 - {'error': {'message': 'Invalid content type. image_url is only supported by certain models.', 'type': 'invalid_request_error', 'param': 'messages.[0].content.[1].type', 'code': None}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 61, in query_with_image
    response = self.client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'Invalid content type. image_url is only supported by certain models.', 'type': 'invalid_request_error', 'param': 'messages.[0].content.[1].type', 'code': None}}
2024-08-26 16:23:03,633 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:23:03,659 DEBUG OpenAI client created
2024-08-26 16:23:03,659 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 16:23:03,683 DEBUG OpenAI client created
2024-08-26 16:23:03,683 DEBUG Querying model: gpt-4-turbo
2024-08-26 16:23:05,137 DEBUG Query successful
2024-08-26 16:23:05,147 DEBUG Querying model with image: gpt-4-turbo
2024-08-26 16:23:18,998 DEBUG Query with image successful
2024-08-26 17:06:18,162 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 17:06:18,187 DEBUG OpenAI client created
2024-08-26 17:06:18,187 DEBUG Model set to: gpt-4o
2024-08-26 17:06:18,187 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 17:06:18,210 DEBUG OpenAI client created
2024-08-26 17:06:18,210 DEBUG Model set to: gpt-4o
2024-08-26 17:06:18,210 DEBUG Querying model: gpt-4o
2024-08-26 17:06:19,621 DEBUG Query successful
2024-08-26 17:06:19,633 DEBUG Querying model with image: gpt-4o
2024-08-26 17:06:28,598 DEBUG Query with image successful
2024-08-26 20:43:18,750 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 20:43:18,863 DEBUG OpenAI client created
2024-08-26 20:43:18,863 DEBUG Model set to: gpt-4o
2024-08-26 20:43:18,863 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-26 20:43:18,883 DEBUG OpenAI client created
2024-08-26 20:43:18,883 DEBUG Model set to: gpt-4o
2024-08-26 20:43:18,883 DEBUG Querying model: gpt-4o
2024-08-26 20:43:19,790 DEBUG Query successful
2024-08-26 20:43:19,815 DEBUG Querying model with image: gpt-4o
2024-08-26 20:43:32,600 DEBUG Query with image successful
2024-08-27 15:29:47,161 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-27 15:29:47,188 DEBUG OpenAI client created
2024-08-27 15:29:47,188 DEBUG Model set to: gpt-4o
2024-08-27 15:29:47,188 DEBUG Proxy set to: http://127.0.0.1:7890
2024-08-27 15:29:47,213 DEBUG OpenAI client created
2024-08-27 15:29:47,213 DEBUG Model set to: gpt-4o
2024-08-27 15:29:47,213 DEBUG Querying model: gpt-4o
2024-08-27 15:29:48,925 DEBUG Query successful
2024-08-27 15:29:48,946 DEBUG Querying model with image: gpt-4o
2024-08-27 15:30:05,247 DEBUG Query with image successful
2024-09-16 23:05:22,850 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-16 23:05:22,876 DEBUG OpenAI client created
2024-09-16 23:05:22,876 DEBUG Model set to: gpt-4o
2024-09-16 23:05:22,876 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-16 23:05:22,900 DEBUG OpenAI client created
2024-09-16 23:05:22,900 DEBUG Model set to: gpt-4o
2024-09-16 23:05:22,900 DEBUG Querying model: gpt-4o
2024-09-16 23:05:23,945 DEBUG Query successful
2024-09-16 23:05:50,151 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-16 23:05:50,183 DEBUG OpenAI client created
2024-09-16 23:05:50,183 DEBUG Model set to: gpt-4o
2024-09-16 23:05:50,183 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-16 23:05:50,219 DEBUG OpenAI client created
2024-09-16 23:05:50,220 DEBUG Model set to: gpt-4o
2024-09-16 23:05:50,220 DEBUG Querying model: gpt-4o
2024-09-16 23:05:53,568 DEBUG Query successful
2024-09-16 23:05:53,577 DEBUG Querying model with image: gpt-4o
2024-09-16 23:06:03,074 DEBUG Query with image successful
2024-09-29 15:20:57,969 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-29 15:20:58,109 DEBUG OpenAI client created
2024-09-29 15:20:58,109 DEBUG Model set to: gpt-4o
2024-09-29 15:20:58,109 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-29 15:20:58,128 DEBUG OpenAI client created
2024-09-29 15:20:58,129 DEBUG Model set to: gpt-4o
2024-09-29 15:20:58,129 DEBUG Querying model: gpt-4o
2024-09-29 15:20:59,141 DEBUG Query successful
2024-09-29 15:20:59,141 ERROR An error occurred: [Errno 2] No such file or directory: 'C:\\python_code\\vlm_attention_starcraft2-main\\vlm_attention\\utils\\screenshot_0009.png'
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 69, in query_with_image
    base64_image = self.encode_image(image_path)
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 97, in encode_image
    with open(image_path, "rb") as image_file:
FileNotFoundError: [Errno 2] No such file or directory: 'C:\\python_code\\vlm_attention_starcraft2-main\\vlm_attention\\utils\\screenshot_0009.png'
2024-09-29 15:32:44,743 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-29 15:32:44,764 DEBUG OpenAI client created
2024-09-29 15:32:44,764 DEBUG Model set to: gpt-4o
2024-09-29 15:32:44,764 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-29 15:32:44,783 DEBUG OpenAI client created
2024-09-29 15:32:44,783 DEBUG Model set to: gpt-4o
2024-09-29 15:32:44,783 DEBUG Querying model: gpt-4o
2024-09-29 15:32:45,477 DEBUG Query successful
2024-09-29 15:32:45,477 ERROR An error occurred: [Errno 2] No such file or directory: 'C:\\python_code\\vlm_attention_starcraft2-main\\vlm_attention\\utils\\screenshot_0009.png'
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 69, in query_with_image
    base64_image = self.encode_image(image_path)
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 97, in encode_image
    with open(image_path, "rb") as image_file:
FileNotFoundError: [Errno 2] No such file or directory: 'C:\\python_code\\vlm_attention_starcraft2-main\\vlm_attention\\utils\\screenshot_0009.png'
2024-09-29 15:44:22,742 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-29 15:44:22,764 DEBUG OpenAI client created
2024-09-29 15:44:22,764 DEBUG Model set to: gpt-4o
2024-09-29 15:44:22,764 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-29 15:44:22,783 DEBUG OpenAI client created
2024-09-29 15:44:22,783 DEBUG Model set to: gpt-4o
2024-09-29 15:44:22,783 DEBUG Querying model: gpt-4o
2024-09-29 15:44:23,515 DEBUG Query successful
2024-09-29 15:44:23,533 DEBUG Querying model with image: gpt-4o
2024-09-29 15:44:29,524 DEBUG Query with image successful
2024-09-29 15:47:38,763 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-29 15:47:38,785 DEBUG OpenAI client created
2024-09-29 15:47:38,785 DEBUG Model set to: gpt-4o
2024-09-29 15:47:38,785 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-29 15:47:38,806 DEBUG OpenAI client created
2024-09-29 15:47:38,806 DEBUG Model set to: gpt-4o
2024-09-29 15:47:38,806 DEBUG Querying model: gpt-4o
2024-09-29 15:47:39,633 DEBUG Query successful
2024-09-29 15:47:39,646 DEBUG Querying model: gpt-4o
2024-09-29 15:47:48,945 DEBUG Query successful
2024-09-29 15:47:48,946 DEBUG Querying model: gpt-4o
2024-09-29 15:47:49,474 DEBUG Query successful
2024-09-29 15:50:50,864 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-29 15:50:50,886 DEBUG OpenAI client created
2024-09-29 15:50:50,886 DEBUG Model set to: gpt-4o
2024-09-29 15:50:50,886 DEBUG Proxy set to: http://127.0.0.1:7890
2024-09-29 15:50:50,908 DEBUG OpenAI client created
2024-09-29 15:50:50,908 DEBUG Model set to: gpt-4o
2024-09-29 15:50:50,908 DEBUG Querying model: gpt-4o
2024-09-29 15:50:51,641 DEBUG Query successful
2024-09-29 15:50:51,641 DEBUG Querying model: gpt-4o
2024-09-29 15:50:52,920 DEBUG Query successful
2024-09-29 15:50:52,934 DEBUG Querying model: gpt-4o
2024-09-29 15:50:57,658 DEBUG Query successful
2024-09-29 15:50:57,658 DEBUG Querying model: gpt-4o
2024-09-29 15:51:05,042 DEBUG Query successful
2024-11-09 16:45:06,762 DEBUG OpenAI client created
2024-11-09 16:45:06,762 DEBUG Model set to: gpt-4o
2024-11-09 16:45:48,502 DEBUG OpenAI client created
2024-11-09 16:45:48,502 DEBUG Model set to: gpt-4o
2024-11-09 16:48:13,703 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 16:48:13,728 DEBUG OpenAI client created
2024-11-09 16:48:13,728 DEBUG Model set to: gpt-4o
2024-11-09 16:48:13,728 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 16:56:40,319 DEBUG Model set to: gpt-4
2024-11-09 16:56:46,927 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    connect_response = self._connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection.py", line 154, in _connect
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: EOF occurred in violation of protocol (_ssl.c:997)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: EOF occurred in violation of protocol (_ssl.c:997)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 98, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 373, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 664, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\utils\commons.py", line 269, in wrapper
    return func(self, *args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\models\openai_model.py", line 96, in run
    response = self._client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-09 16:56:53,453 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    connect_response = self._connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection.py", line 154, in _connect
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: EOF occurred in violation of protocol (_ssl.c:997)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: EOF occurred in violation of protocol (_ssl.c:997)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 98, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 373, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 664, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\utils\commons.py", line 269, in wrapper
    return func(self, *args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\models\openai_model.py", line 96, in run
    response = self._client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-09 16:57:07,891 DEBUG Model set to: gpt-4o
2024-11-09 16:57:14,246 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    connect_response = self._connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection.py", line 154, in _connect
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: EOF occurred in violation of protocol (_ssl.c:997)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: EOF occurred in violation of protocol (_ssl.c:997)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 98, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 373, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 664, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\utils\commons.py", line 269, in wrapper
    return func(self, *args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\models\openai_model.py", line 96, in run
    response = self._client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-09 16:57:20,196 ERROR An error occurred: Connection error.
Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\http_proxy.py", line 289, in handle_request
    connect_response = self._connection.handle_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_sync\connection.py", line 154, in _connect
    stream = stream.start_tls(**kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_backends\sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: EOF occurred in violation of protocol (_ssl.c:997)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 973, in _request
    response = self._client.send(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "E:\anaconda\envs\vlm_attention\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: EOF occurred in violation of protocol (_ssl.c:997)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 98, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 373, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 664, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\utils\commons.py", line 269, in wrapper
    return func(self, *args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\models\openai_model.py", line 96, in run
    response = self._client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 997, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1007, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-11-09 16:57:20,225 DEBUG Model set to: gpt-4o
2024-11-09 16:57:41,232 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 16:57:41,648 DEBUG Model set to: gpt-4o
2024-11-09 16:57:42,573 ERROR An error occurred: Error code: 404 - {'error': {'message': 'The model `gpt-4o` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 98, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 373, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 664, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\utils\commons.py", line 269, in wrapper
    return func(self, *args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\models\openai_model.py", line 96, in run
    response = self._client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-4o` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
2024-11-09 16:57:42,929 ERROR An error occurred: Error code: 404 - {'error': {'message': 'The model `gpt-4o` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 98, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 373, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 664, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\utils\commons.py", line 269, in wrapper
    return func(self, *args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\models\openai_model.py", line 96, in run
    response = self._client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-4o` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
2024-11-09 16:57:42,930 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 16:57:42,953 DEBUG Model set to: gpt-4o
2024-11-09 16:57:45,553 ERROR An error occurred: Error code: 404 - {'error': {'message': 'The model `gpt-4o` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 132, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 373, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 664, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\utils\commons.py", line 269, in wrapper
    return func(self, *args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\models\openai_model.py", line 96, in run
    response = self._client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-4o` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
2024-11-09 16:57:47,686 ERROR An error occurred: Error code: 404 - {'error': {'message': 'The model `gpt-4o` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 132, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 373, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 664, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\utils\commons.py", line 269, in wrapper
    return func(self, *args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\models\openai_model.py", line 96, in run
    response = self._client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-4o` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
2024-11-09 17:01:17,291 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 17:02:09,996 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 17:02:10,413 DEBUG Model set to: gpt-4o-mini
2024-11-09 17:02:18,274 ERROR An error occurred: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 373, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 664, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\utils\commons.py", line 269, in wrapper
    return func(self, *args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\models\openai_model.py", line 96, in run
    response = self._client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1026, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1026, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1026, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}
2024-11-09 17:02:25,637 ERROR An error occurred: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 124, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 373, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 664, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\utils\commons.py", line 269, in wrapper
    return func(self, *args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\models\openai_model.py", line 96, in run
    response = self._client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1026, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1026, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1026, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}
2024-11-09 17:02:25,638 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 17:02:25,659 DEBUG Model set to: gpt-4o-mini
2024-11-09 17:02:38,013 ERROR An error occurred: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 373, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 664, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\utils\commons.py", line 269, in wrapper
    return func(self, *args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\models\openai_model.py", line 96, in run
    response = self._client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1026, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1026, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1026, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}
2024-11-09 17:02:49,007 ERROR An error occurred: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}
Traceback (most recent call last):
  File "D:\pythoncode\vlm_attention_starcraft2-main\vlm_attention\utils\call_vlm.py", line 173, in query
    response: ChatAgentResponse = self.chat_agent.step(user_message)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 373, in step
    ) = self._step_model_response(openai_messages, num_tokens)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\agents\chat_agent.py", line 664, in _step_model_response
    response = self.model_backend.run(openai_messages)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\utils\commons.py", line 269, in wrapper
    return func(self, *args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\camel\models\openai_model.py", line 96, in run
    response = self._client.chat.completions.create(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_utils\_utils.py", line 274, in wrapper
    return func(*args, **kwargs)
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\resources\chat\completions.py", line 668, in create
    return self._post(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1260, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 937, in request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1026, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1026, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1026, in _request
    return self._retry_request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1075, in _retry_request
    return self._request(
  File "E:\anaconda\envs\vlm_attention\lib\site-packages\openai\_base_client.py", line 1041, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}
2024-11-09 17:08:18,058 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 17:08:18,518 DEBUG Model set to: gpt-4o-mini
2024-11-09 17:08:19,853 DEBUG Query successful
2024-11-09 17:08:21,624 DEBUG Query successful
2024-11-09 17:08:21,624 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 17:08:21,645 DEBUG Model set to: gpt-4o-mini
2024-11-09 17:08:30,238 DEBUG Query successful
2024-11-09 17:08:39,916 DEBUG Query successful
2024-11-09 17:16:22,154 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 17:19:05,089 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 17:19:05,512 DEBUG Model set to: gpt-4o-mini
2024-11-09 17:19:06,214 DEBUG Query successful
2024-11-09 17:19:07,697 DEBUG Query successful
2024-11-09 17:19:07,697 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 17:19:07,721 DEBUG Model set to: gpt-4o-mini
2024-11-09 17:19:20,588 DEBUG Query successful
2024-11-09 17:19:30,471 DEBUG Query successful
2024-11-09 17:22:46,727 DEBUG Proxy set to: http://127.0.0.1:7890
2024-11-09 17:22:47,171 DEBUG Model set to: gpt-4o-mini
2024-11-09 17:22:50,584 DEBUG Query successful
2024-11-09 17:22:52,147 DEBUG Query successful
2024-11-09 17:22:57,020 DEBUG Query successful
2024-11-09 17:22:57,781 DEBUG Query successful
